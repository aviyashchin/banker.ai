{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import scrapy\n",
    "\n",
    "from scrapy.http import Request, HtmlResponse\n",
    "from scrapy import signals\n",
    "from scrapy.xlib.pydispatch import dispatcher\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from goose import Goose\n",
    "from twisted.internet import reactor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import MapCompose\n",
    "from goose import Goose\n",
    "from scrapy.xlib.pydispatch import dispatcher\n",
    "from scrapy.settings import Settings\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "def parse_base_url(url):\n",
    "    url = re.sub(r'((http(s)?://)?(www.)?)', '', url.lower())  # strip head\n",
    "    print url.find('/')\n",
    "    return url[:url.find('/')] if url.find('/') != -1 else url\n",
    "\n",
    "class SoloItem(scrapy.Item):\n",
    "    text = scrapy.Field(default = 'none')\n",
    "    siteurl = scrapy.Field()\n",
    "    pageurl = scrapy.Field()\n",
    "    pagetitle = scrapy.Field(default = 'none')\n",
    "\n",
    "\n",
    "class SoloSpider(CrawlSpider):\n",
    "    name = \"solo\"\n",
    "\n",
    "    rules = (Rule(LinkExtractor(), callback='parse_items', follow=True),)\n",
    "\n",
    "    def __init__(self, **kw):\n",
    "        super(SoloSpider, self).__init__(**kw)\n",
    "        url = kw.get('url') or kw.get('domain')\n",
    "\n",
    "        self.g = Goose()\n",
    "        self.url = url\n",
    "        self.allowed_domains = [url]\n",
    "        self.start_urls = ['http://www.' + url]\n",
    "        \n",
    "    def parse_items(self, response):\n",
    "\n",
    "        gooseobj = self.g.extract(response.url)\n",
    "        fulltext = gooseobj.cleaned_text\n",
    "\n",
    "        il = ItemLoader(item=SoloItem(), response=response)\n",
    "        il.default_output_processor = MapCompose(\n",
    "            lambda v: v.rstrip(),\n",
    "            lambda v: re.sub(r'[\\',|!]', '', v),\n",
    "            lambda v: re.sub(r'\\s+', ' ', v)\n",
    "        )\n",
    "\n",
    "        il.add_value('siteurl', parse_base_url(response.url))\n",
    "        il.add_value('pageurl', response.url)\n",
    "        il.add_value('text', fulltext.encode('ascii', 'ignore'))\n",
    "        il.add_xpath('pagetitle', '//title/text()')\n",
    "\n",
    "        return il.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ScrapeSite():\n",
    "    db = 'crunchbase_startups'\n",
    "    sitedomain = raw_input(\"Enter site domain: \") # get user input\n",
    "    sitedomain = parse_base_url(sitedomain) # clean url\n",
    "    \n",
    "    sql = 'SELECT text FROM {} WHERE siteurl = %s'.format(db)\n",
    "    \n",
    "    cur.execute(sql, sitedomain)\n",
    "    sitetext = cur.fetch()\n",
    "    \n",
    "    if sitetext != '': # what does an empty ping return?\n",
    "        print 'Site already scraped.'\n",
    "        return sitetext\n",
    "    \n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'ITEM_PIPELINES': {'pipelines.UserInputPipeline': 100},\n",
    "        'DEPTH_LIMIT': 2,\n",
    "        'DOWNLOAD_HANDLERS': {'s3': None,}\n",
    "        ,'LOG_LEVEL': 'INFO'\n",
    "    })\n",
    "    \n",
    "    process.crawl(SoloSpider, domain = sitedomain)\n",
    "    process.start()\n",
    "    \n",
    "    # presumably finished here - pull newly loaded sitetext for domain\n",
    "    \n",
    "    cur.execute(sql, sitedomain)\n",
    "    return cur.fetch()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InputText():\n",
    "    descrip = raw_input(\"Please enter a description of your product.\") # get user input\n",
    "    \n",
    "    # TODO: read into sql DB with unique identifier\n",
    "    \n",
    "    return descrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to M^3!\n",
      "\n",
      "This tool generates a \"digital fingerprint\" of your startup based on the available\n",
      "text of your website. Optionally, you may enter a description of your product yourself.\n",
      "\n",
      "Please select from one of the following options:\n",
      "\t 1) Input your website\n",
      "\t 2) Enter a text description of your startup\n",
      "\n",
      "1\n",
      "Enter site domain: 3g-capital.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.utils.log:Scrapy 1.0.3 started (bot: scrapybot)\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
      "INFO:scrapy.utils.log:Optional features available: ssl, http11, boto\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
      "INFO:scrapy.utils.log:Overridden settings: {'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Overridden settings: {'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Overridden settings: {'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "INFO:scrapy.middleware:Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "INFO:scrapy.middleware:Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "INFO:scrapy.middleware:Enabled item pipelines: TextPipeline\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled item pipelines: TextPipeline\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Enabled item pipelines: TextPipeline\n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Spider opened\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2015-11-17 23:06:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "DEBUG:scrapy.telnet:Telnet console listening on 127.0.0.1:6024\n",
      "2015-11-17 23:06:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2015-11-17 23:06:04 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-222a27610325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcomparetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minputtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScrapeSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcomparetype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'1'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mInputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0minputtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-32fb11a523c4>\u001b[0m in \u001b[0;36mScrapeSite\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSoloSpider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msitedomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/crawler.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shutdown'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/base.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/base.pyc\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \"\"\"\n\u001b[1;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mReactorBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/base.pyc\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from settings import MYSQL_GSA_CONFIG\n",
    "\n",
    "config = MYSQL_GSA_CONFIG # yes yes it's dumb, this will change later anyway\n",
    "con = msc.connect(**config)\n",
    "cur = con.cursor()\n",
    "\n",
    "# flask forthcoming\n",
    "\n",
    "print \"\"\"Welcome to M^3!\\n\\nThis tool generates a \\\"digital fingerprint\\\" of your startup based on the available\n",
    "text of your website. Optionally, you may enter a description of your product yourself.\\n\n",
    "Please select from one of the following options:\n",
    "\\t 1) Input your website\n",
    "\\t 2) Enter a text description of your startup\\n\"\"\"\n",
    "\n",
    "comparetype = raw_input()\n",
    "\n",
    "inputtext = ScrapeSite() if comparetype == '1' else InputText()\n",
    "\n",
    "print inputtext\n",
    "print siteparsed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BLAHBLAHBLAHbarkbark']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "site = defaultdict(str)\n",
    "\n",
    "sitename = 'tester'\n",
    "text = 'BLAHBLAHBLAH'\n",
    "\n",
    "site[sitename] += text\n",
    "\n",
    "text2 = 'barkbark'\n",
    "\n",
    "site[sitename] += text2\n",
    "\n",
    "\n",
    "print site.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
