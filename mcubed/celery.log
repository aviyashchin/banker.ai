/home/ubuntu/programming/banker.ai/mcubed/app/vcspider/vcspider/pipelines.py:3: ScrapyDeprecationWarning: Module `scrapy.conf` is deprecated, use `crawler.settings` attribute instead
  from scrapy.conf import settings
[2016-02-05 06:11:06,314: WARNING/MainProcess] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/apps/worker.py:161: CDeprecationWarning: 
Starting from version 3.2 Celery will refuse to accept pickle by default.

The pickle serializer is a security concern as it may give attackers
the ability to execute any command.  It's important to secure
your broker from unauthorized access when using pickle, so we think
that enabling pickle should require a deliberate action and not be
the default choice.

If you depend on pickle then you should set a setting to disable this
warning and to be sure that everything will continue working
when you upgrade to Celery 3.2::

    CELERY_ACCEPT_CONTENT = ['pickle', 'json', 'msgpack', 'yaml']

You must only enable the serializers that you will actually use.


  warnings.warn(CDeprecationWarning(W_PICKLE_DEPRECATED))
[2016-02-05 06:11:06,349: INFO/MainProcess] Connected to redis://localhost:6379//
[2016-02-05 06:11:06,361: INFO/MainProcess] mingle: searching for neighbors
[2016-02-05 06:11:07,366: INFO/MainProcess] mingle: all alone
[2016-02-05 06:11:07,374: WARNING/MainProcess] celery@ip-172-31-2-245 ready.
[2016-02-05 16:21:15,493: INFO/MainProcess] Received task: app.scrape[cff256af-9ad5-4bc9-8dfa-2ec620cff2f5]
[2016-02-05 16:21:15,501: INFO/Worker-4] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-05 16:21:15,501: WARNING/Worker-4] 2016-02-05 16:21:15 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-05 16:21:15,502: INFO/Worker-4] Optional features available: ssl, http11, boto
[2016-02-05 16:21:15,502: WARNING/Worker-4] 2016-02-05 16:21:15 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-05 16:21:15,502: INFO/Worker-4] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-05 16:21:15,502: WARNING/Worker-4] 2016-02-05 16:21:15 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-05 16:21:16,070: INFO/Worker-4] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-05 16:21:16,070: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-05 16:21:16,138: INFO/Worker-4] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-05 16:21:16,139: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-05 16:21:16,146: INFO/Worker-4] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-05 16:21:16,146: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-05 16:21:16,361: INFO/Worker-4] Enabled item pipelines: UserInputPipeline
[2016-02-05 16:21:16,361: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-05 16:21:16,362: INFO/Worker-4] Spider opened
[2016-02-05 16:21:16,362: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] INFO: Spider opened
[2016-02-05 16:21:16,364: INFO/Worker-4] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-05 16:21:16,364: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-05 16:21:16,366: DEBUG/Worker-4] Telnet console listening on 127.0.0.1:6023
[2016-02-05 16:21:16,366: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
[2016-02-05 16:21:16,407: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com> (referer: None)
[2016-02-05 16:21:16,408: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com> (referer: None)
[2016-02-05 16:21:16,518: DEBUG/Worker-4] Filtered offsite request to 'twitter.com': <GET https://twitter.com/upscored>
[2016-02-05 16:21:16,518: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Filtered offsite request to 'twitter.com': <GET https://twitter.com/upscored>
[2016-02-05 16:21:16,518: DEBUG/Worker-4] Filtered offsite request to 'www.linkedin.com': <GET https://www.linkedin.com/company/upscored>
[2016-02-05 16:21:16,518: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Filtered offsite request to 'www.linkedin.com': <GET https://www.linkedin.com/company/upscored>
[2016-02-05 16:21:16,518: DEBUG/Worker-4] Filtered offsite request to 'www.facebook.com': <GET https://www.facebook.com/upscored/>
[2016-02-05 16:21:16,519: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Filtered offsite request to 'www.facebook.com': <GET https://www.facebook.com/upscored/>
[2016-02-05 16:21:16,647: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/about/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,647: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/about/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,674: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,674: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,681: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/faq/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,681: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/faq/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,698: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/privacy> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,698: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/privacy> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,700: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/js_signup/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,701: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/js_signup/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,702: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/terms-of-use> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,702: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/terms-of-use> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,722: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blog/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,722: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blog/> (referer: http://www.upscored.com)
[2016-02-05 16:21:16,858: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/about/>
None
[2016-02-05 16:21:16,858: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/about/>
None
[2016-02-05 16:21:16,868: DEBUG/Worker-4] Filtered duplicate request: <GET http://www.upscored.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
[2016-02-05 16:21:16,868: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Filtered duplicate request: <GET http://www.upscored.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
[2016-02-05 16:21:16,985: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/>
None
[2016-02-05 16:21:16,985: WARNING/Worker-4] 2016-02-05 16:21:16 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/>
None
[2016-02-05 16:21:17,049: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/faq/>
None
[2016-02-05 16:21:17,049: WARNING/Worker-4] 2016-02-05 16:21:17 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/faq/>
None
[2016-02-05 16:21:17,144: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/privacy>
None
[2016-02-05 16:21:17,144: WARNING/Worker-4] 2016-02-05 16:21:17 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/privacy>
None
[2016-02-05 16:21:17,218: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/js_signup/>
None
[2016-02-05 16:21:17,218: WARNING/Worker-4] 2016-02-05 16:21:17 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/js_signup/>
None
[2016-02-05 16:21:17,327: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/terms-of-use>
None
[2016-02-05 16:21:17,327: WARNING/Worker-4] 2016-02-05 16:21:17 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/terms-of-use>
None
[2016-02-05 16:21:19,803: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blog/>
None
[2016-02-05 16:21:19,803: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blog/>
None
[2016-02-05 16:21:19,828: DEBUG/Worker-4] Filtered offsite request to 'www.jamsadr.com': <GET http://www.jamsadr.com/>
[2016-02-05 16:21:19,829: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Filtered offsite request to 'www.jamsadr.com': <GET http://www.jamsadr.com/>
[2016-02-05 16:21:19,850: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/14/> from <GET http://www.upscored.com/blogpage/14>
[2016-02-05 16:21:19,851: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/14/> from <GET http://www.upscored.com/blogpage/14>
[2016-02-05 16:21:19,853: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/11/> from <GET http://www.upscored.com/blogpage/11>
[2016-02-05 16:21:19,853: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/11/> from <GET http://www.upscored.com/blogpage/11>
[2016-02-05 16:21:19,855: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/22/> from <GET http://www.upscored.com/blogpage/22>
[2016-02-05 16:21:19,855: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/22/> from <GET http://www.upscored.com/blogpage/22>
[2016-02-05 16:21:19,857: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/17/> from <GET http://www.upscored.com/blogpage/17>
[2016-02-05 16:21:19,857: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/17/> from <GET http://www.upscored.com/blogpage/17>
[2016-02-05 16:21:19,859: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/25/> from <GET http://www.upscored.com/blogpage/25>
[2016-02-05 16:21:19,859: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/25/> from <GET http://www.upscored.com/blogpage/25>
[2016-02-05 16:21:19,861: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/18/> from <GET http://www.upscored.com/blogpage/18>
[2016-02-05 16:21:19,861: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/18/> from <GET http://www.upscored.com/blogpage/18>
[2016-02-05 16:21:19,863: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/21/> from <GET http://www.upscored.com/blogpage/21>
[2016-02-05 16:21:19,863: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/21/> from <GET http://www.upscored.com/blogpage/21>
[2016-02-05 16:21:19,864: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/24/> from <GET http://www.upscored.com/blogpage/24>
[2016-02-05 16:21:19,865: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/24/> from <GET http://www.upscored.com/blogpage/24>
[2016-02-05 16:21:19,867: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/26/> from <GET http://www.upscored.com/blogpage/26>
[2016-02-05 16:21:19,867: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/26/> from <GET http://www.upscored.com/blogpage/26>
[2016-02-05 16:21:19,869: DEBUG/Worker-4] Redirecting (301) to <GET http://www.upscored.com/blogpage/15/> from <GET http://www.upscored.com/blogpage/15>
[2016-02-05 16:21:19,870: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (301) to <GET http://www.upscored.com/blogpage/15/> from <GET http://www.upscored.com/blogpage/15>
[2016-02-05 16:21:19,886: DEBUG/Worker-4] Redirecting (302) to <GET http://upscored.com/accounts/login/?next=/js_account/> from <GET http://upscored.com/js_account/>
[2016-02-05 16:21:19,886: WARNING/Worker-4] 2016-02-05 16:21:19 [scrapy] DEBUG: Redirecting (302) to <GET http://upscored.com/accounts/login/?next=/js_account/> from <GET http://upscored.com/js_account/>
[2016-02-05 16:21:20,097: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/14/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,097: WARNING/Worker-4] 2016-02-05 16:21:20 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/14/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,101: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/11/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,101: WARNING/Worker-4] 2016-02-05 16:21:20 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/11/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,137: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/18/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,137: WARNING/Worker-4] 2016-02-05 16:21:20 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/18/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,139: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/17/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,140: WARNING/Worker-4] 2016-02-05 16:21:20 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/17/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,179: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/22/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,179: WARNING/Worker-4] 2016-02-05 16:21:20 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/22/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,192: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/25/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,192: WARNING/Worker-4] 2016-02-05 16:21:20 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/25/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:20,782: DEBUG/Worker-4] Importing BmpImagePlugin
[2016-02-05 16:21:20,783: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing BmpImagePlugin
[2016-02-05 16:21:20,783: DEBUG/Worker-4] Importing BufrStubImagePlugin
[2016-02-05 16:21:20,783: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing BufrStubImagePlugin
[2016-02-05 16:21:20,784: DEBUG/Worker-4] Importing CurImagePlugin
[2016-02-05 16:21:20,784: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing CurImagePlugin
[2016-02-05 16:21:20,785: DEBUG/Worker-4] Importing DcxImagePlugin
[2016-02-05 16:21:20,785: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing DcxImagePlugin
[2016-02-05 16:21:20,790: DEBUG/Worker-4] Importing EpsImagePlugin
[2016-02-05 16:21:20,790: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing EpsImagePlugin
[2016-02-05 16:21:20,792: DEBUG/Worker-4] Importing FitsStubImagePlugin
[2016-02-05 16:21:20,792: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing FitsStubImagePlugin
[2016-02-05 16:21:20,793: DEBUG/Worker-4] Importing FliImagePlugin
[2016-02-05 16:21:20,793: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing FliImagePlugin
[2016-02-05 16:21:20,794: DEBUG/Worker-4] Importing FpxImagePlugin
[2016-02-05 16:21:20,794: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing FpxImagePlugin
[2016-02-05 16:21:20,799: DEBUG/Worker-4] Importing GbrImagePlugin
[2016-02-05 16:21:20,799: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing GbrImagePlugin
[2016-02-05 16:21:20,800: DEBUG/Worker-4] Importing GifImagePlugin
[2016-02-05 16:21:20,800: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing GifImagePlugin
[2016-02-05 16:21:20,800: DEBUG/Worker-4] Importing GribStubImagePlugin
[2016-02-05 16:21:20,800: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing GribStubImagePlugin
[2016-02-05 16:21:20,801: DEBUG/Worker-4] Importing Hdf5StubImagePlugin
[2016-02-05 16:21:20,801: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing Hdf5StubImagePlugin
[2016-02-05 16:21:20,802: DEBUG/Worker-4] Importing IcnsImagePlugin
[2016-02-05 16:21:20,802: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing IcnsImagePlugin
[2016-02-05 16:21:20,803: DEBUG/Worker-4] Importing IcoImagePlugin
[2016-02-05 16:21:20,803: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing IcoImagePlugin
[2016-02-05 16:21:20,804: DEBUG/Worker-4] Importing ImImagePlugin
[2016-02-05 16:21:20,804: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing ImImagePlugin
[2016-02-05 16:21:20,805: DEBUG/Worker-4] Importing ImtImagePlugin
[2016-02-05 16:21:20,805: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing ImtImagePlugin
[2016-02-05 16:21:20,806: DEBUG/Worker-4] Importing IptcImagePlugin
[2016-02-05 16:21:20,807: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing IptcImagePlugin
[2016-02-05 16:21:20,807: DEBUG/Worker-4] Importing JpegImagePlugin
[2016-02-05 16:21:20,807: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing JpegImagePlugin
[2016-02-05 16:21:20,808: DEBUG/Worker-4] Importing Jpeg2KImagePlugin
[2016-02-05 16:21:20,808: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing Jpeg2KImagePlugin
[2016-02-05 16:21:20,808: DEBUG/Worker-4] Importing McIdasImagePlugin
[2016-02-05 16:21:20,809: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing McIdasImagePlugin
[2016-02-05 16:21:20,809: DEBUG/Worker-4] Importing MicImagePlugin
[2016-02-05 16:21:20,809: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing MicImagePlugin
[2016-02-05 16:21:20,810: DEBUG/Worker-4] Importing MpegImagePlugin
[2016-02-05 16:21:20,810: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing MpegImagePlugin
[2016-02-05 16:21:20,811: DEBUG/Worker-4] Importing MpoImagePlugin
[2016-02-05 16:21:20,811: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing MpoImagePlugin
[2016-02-05 16:21:20,812: DEBUG/Worker-4] Importing MspImagePlugin
[2016-02-05 16:21:20,812: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing MspImagePlugin
[2016-02-05 16:21:20,813: DEBUG/Worker-4] Importing PalmImagePlugin
[2016-02-05 16:21:20,813: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PalmImagePlugin
[2016-02-05 16:21:20,815: DEBUG/Worker-4] Importing PcdImagePlugin
[2016-02-05 16:21:20,815: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PcdImagePlugin
[2016-02-05 16:21:20,816: DEBUG/Worker-4] Importing PcxImagePlugin
[2016-02-05 16:21:20,816: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PcxImagePlugin
[2016-02-05 16:21:20,816: DEBUG/Worker-4] Importing PdfImagePlugin
[2016-02-05 16:21:20,816: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PdfImagePlugin
[2016-02-05 16:21:20,817: DEBUG/Worker-4] Importing PixarImagePlugin
[2016-02-05 16:21:20,817: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PixarImagePlugin
[2016-02-05 16:21:20,819: DEBUG/Worker-4] Importing PngImagePlugin
[2016-02-05 16:21:20,819: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PngImagePlugin
[2016-02-05 16:21:20,820: DEBUG/Worker-4] Importing PpmImagePlugin
[2016-02-05 16:21:20,820: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PpmImagePlugin
[2016-02-05 16:21:20,820: DEBUG/Worker-4] Importing PsdImagePlugin
[2016-02-05 16:21:20,820: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing PsdImagePlugin
[2016-02-05 16:21:20,821: DEBUG/Worker-4] Importing SgiImagePlugin
[2016-02-05 16:21:20,821: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing SgiImagePlugin
[2016-02-05 16:21:20,821: DEBUG/Worker-4] Importing SpiderImagePlugin
[2016-02-05 16:21:20,822: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing SpiderImagePlugin
[2016-02-05 16:21:20,822: DEBUG/Worker-4] Importing SunImagePlugin
[2016-02-05 16:21:20,823: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing SunImagePlugin
[2016-02-05 16:21:20,823: DEBUG/Worker-4] Importing TgaImagePlugin
[2016-02-05 16:21:20,823: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing TgaImagePlugin
[2016-02-05 16:21:20,824: DEBUG/Worker-4] Importing TiffImagePlugin
[2016-02-05 16:21:20,824: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing TiffImagePlugin
[2016-02-05 16:21:20,824: DEBUG/Worker-4] Importing WebPImagePlugin
[2016-02-05 16:21:20,825: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing WebPImagePlugin
[2016-02-05 16:21:20,825: DEBUG/Worker-4] Image: failed to import WebPImagePlugin: cannot import name _webp
[2016-02-05 16:21:20,825: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Image: failed to import WebPImagePlugin: cannot import name _webp
[2016-02-05 16:21:20,826: DEBUG/Worker-4] Importing WmfImagePlugin
[2016-02-05 16:21:20,826: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing WmfImagePlugin
[2016-02-05 16:21:20,826: DEBUG/Worker-4] Importing XbmImagePlugin
[2016-02-05 16:21:20,827: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing XbmImagePlugin
[2016-02-05 16:21:20,829: DEBUG/Worker-4] Importing XpmImagePlugin
[2016-02-05 16:21:20,829: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing XpmImagePlugin
[2016-02-05 16:21:20,830: DEBUG/Worker-4] Importing XVThumbImagePlugin
[2016-02-05 16:21:20,830: WARNING/Worker-4] 2016-02-05 16:21:20 [PIL.Image] DEBUG: Importing XVThumbImagePlugin
[2016-02-05 16:21:22,598: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/14/>
None
[2016-02-05 16:21:22,598: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/14/>
None
[2016-02-05 16:21:22,600: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/21/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,600: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/21/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,602: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/24/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,602: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/24/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,604: DEBUG/Worker-4] Crawled (200) <GET http://upscored.com/accounts/login/?next=/js_account/> (referer: http://www.upscored.com/privacy)
[2016-02-05 16:21:22,604: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Crawled (200) <GET http://upscored.com/accounts/login/?next=/js_account/> (referer: http://www.upscored.com/privacy)
[2016-02-05 16:21:22,606: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/26/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,606: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/26/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,608: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/accounts/login/> (referer: http://www.upscored.com/js_signup/)
[2016-02-05 16:21:22,608: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/accounts/login/> (referer: http://www.upscored.com/js_signup/)
[2016-02-05 16:21:22,610: DEBUG/Worker-4] Crawled (200) <GET http://www.upscored.com/blogpage/15/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,610: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Crawled (200) <GET http://www.upscored.com/blogpage/15/> (referer: http://www.upscored.com/blog/)
[2016-02-05 16:21:22,688: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:22,689: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:22,689: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:22,689: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:22,690: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/14/ 
[2016-02-05 16:21:22,690: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/14/
[2016-02-05 16:21:22,690: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:22,690: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:22,690: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:22,690: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:22,691: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:22,691: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:22,691: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Finding+Your+Dream+Job+Shouldn%E2%80%99t+Be+A+Second+Job%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F14%2F&via=upscored 
[2016-02-05 16:21:22,691: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Finding+Your+Dream+Job+Shouldn%E2%80%99t+Be+A+Second+Job%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F14%2F&via=upscored
[2016-02-05 16:21:22,691: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F14%2F 
[2016-02-05 16:21:22,691: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F14%2F
[2016-02-05 16:21:22,692: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Finding+Your+Dream+Job+Shouldn%E2%80%99t+Be+A+Second+Job&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F14%2F 
[2016-02-05 16:21:22,692: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Finding+Your+Dream+Job+Shouldn%E2%80%99t+Be+A+Second+Job&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F14%2F
[2016-02-05 16:21:22,692: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:22,692: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:22,692: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:22,692: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:22,692: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:22,693: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:22,693: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:22,693: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:22,693: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:22,693: WARNING/Worker-4] 2016-02-05 16:21:22 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:26,610: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/11/>
None
[2016-02-05 16:21:26,611: WARNING/Worker-4] 2016-02-05 16:21:26 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/11/>
None
[2016-02-05 16:21:27,938: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/18/>
None
[2016-02-05 16:21:27,938: WARNING/Worker-4] 2016-02-05 16:21:27 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/18/>
None
[2016-02-05 16:21:28,527: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/17/>
None
[2016-02-05 16:21:28,528: WARNING/Worker-4] 2016-02-05 16:21:28 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/17/>
None
[2016-02-05 16:21:28,610: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/22/>
None
[2016-02-05 16:21:28,611: WARNING/Worker-4] 2016-02-05 16:21:28 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/22/>
None
[2016-02-05 16:21:30,464: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/25/>
None
[2016-02-05 16:21:30,464: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/25/>
None
[2016-02-05 16:21:30,475: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:30,475: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:30,475: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:30,475: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:30,475: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/11/ 
[2016-02-05 16:21:30,476: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/11/
[2016-02-05 16:21:30,476: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:30,476: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:30,476: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:30,476: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:30,476: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:30,476: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:30,477: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=5+Tips+For+Job-Seeking+Veterans+From+A+Veteran%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F11%2F&via=upscored 
[2016-02-05 16:21:30,477: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=5+Tips+For+Job-Seeking+Veterans+From+A+Veteran%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F11%2F&via=upscored
[2016-02-05 16:21:30,477: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F11%2F 
[2016-02-05 16:21:30,477: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F11%2F
[2016-02-05 16:21:30,477: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=5+Tips+For+Job-Seeking+Veterans+From+A+Veteran&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F11%2F 
[2016-02-05 16:21:30,477: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=5+Tips+For+Job-Seeking+Veterans+From+A+Veteran&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F11%2F
[2016-02-05 16:21:30,478: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.acp-usa.org/ 
[2016-02-05 16:21:30,478: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.acp-usa.org/
[2016-02-05 16:21:30,478: DEBUG/Worker-4] Ignoring link (depth > 2): https://hbr.org/2015/09/what-i-learned-from-a-year-of-job-rejections 
[2016-02-05 16:21:30,478: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://hbr.org/2015/09/what-i-learned-from-a-year-of-job-rejections
[2016-02-05 16:21:30,478: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.nina4airbnb.com/ 
[2016-02-05 16:21:30,478: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.nina4airbnb.com/
[2016-02-05 16:21:30,478: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.nytimes.com/2015/05/10/jobs/veterans-battle-for-jobs-on-the-home-front.html 
[2016-02-05 16:21:30,479: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.nytimes.com/2015/05/10/jobs/veterans-battle-for-jobs-on-the-home-front.html
[2016-02-05 16:21:30,479: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:30,479: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:30,479: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:30,479: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:30,479: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:30,480: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:30,480: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:30,480: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:30,480: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:30,480: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:30,485: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:30,485: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:30,485: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:30,485: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:30,486: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/18/ 
[2016-02-05 16:21:30,486: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/18/
[2016-02-05 16:21:30,486: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:30,486: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:30,486: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:30,486: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:30,487: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:30,487: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:30,487: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=We+Need+More+Women+in+Tech+%28And+Diversity+in+All+Workplaces%29%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F18%2F&via=upscored 
[2016-02-05 16:21:30,487: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=We+Need+More+Women+in+Tech+%28And+Diversity+in+All+Workplaces%29%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F18%2F&via=upscored
[2016-02-05 16:21:30,487: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F18%2F 
[2016-02-05 16:21:30,487: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F18%2F
[2016-02-05 16:21:30,487: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=We+Need+More+Women+in+Tech+%28And+Diversity+in+All+Workplaces%29&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F18%2F 
[2016-02-05 16:21:30,488: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=We+Need+More+Women+in+Tech+%28And+Diversity+in+All+Workplaces%29&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F18%2F
[2016-02-05 16:21:30,488: DEBUG/Worker-4] Ignoring link (depth > 2): http://womenwhotech.com/ 
[2016-02-05 16:21:30,488: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://womenwhotech.com/
[2016-02-05 16:21:30,488: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/karaswisher 
[2016-02-05 16:21:30,488: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/karaswisher
[2016-02-05 16:21:30,488: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.grandcentraltech.com/gct-blog/ 
[2016-02-05 16:21:30,488: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.grandcentraltech.com/gct-blog/
[2016-02-05 16:21:30,489: DEBUG/Worker-4] Ignoring link (depth > 2): http://womenwhotech.com/women-startup-challenge 
[2016-02-05 16:21:30,489: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://womenwhotech.com/women-startup-challenge
[2016-02-05 16:21:30,489: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/RundeVoss 
[2016-02-05 16:21:30,489: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/RundeVoss
[2016-02-05 16:21:30,489: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/AmyVernon 
[2016-02-05 16:21:30,489: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/AmyVernon
[2016-02-05 16:21:30,490: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.inc.com/amy-vernon/finding-and-funding-the-next-generation-of-female-founders.html 
[2016-02-05 16:21:30,490: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.inc.com/amy-vernon/finding-and-funding-the-next-generation-of-female-founders.html
[2016-02-05 16:21:30,490: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:30,490: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:30,490: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:30,490: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:30,490: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:30,491: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:30,491: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:30,491: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:30,491: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:30,491: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:30,494: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:30,495: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:30,495: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:30,495: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:30,495: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/17/ 
[2016-02-05 16:21:30,495: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/17/
[2016-02-05 16:21:30,496: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:30,496: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:30,496: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:30,496: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:30,496: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:30,497: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:30,497: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=When+Your+Job+Title+Doesn%27t+Tell+The+Full+Story%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F17%2F&via=upscored 
[2016-02-05 16:21:30,497: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=When+Your+Job+Title+Doesn%27t+Tell+The+Full+Story%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F17%2F&via=upscored
[2016-02-05 16:21:30,497: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F17%2F 
[2016-02-05 16:21:30,497: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F17%2F
[2016-02-05 16:21:30,497: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=When+Your+Job+Title+Doesn%27t+Tell+The+Full+Story&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F17%2F 
[2016-02-05 16:21:30,497: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=When+Your+Job+Title+Doesn%27t+Tell+The+Full+Story&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F17%2F
[2016-02-05 16:21:30,498: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.prepary.com/ 
[2016-02-05 16:21:30,498: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.prepary.com/
[2016-02-05 16:21:30,498: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.prepary.com/resume-tip-job-title 
[2016-02-05 16:21:30,498: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.prepary.com/resume-tip-job-title
[2016-02-05 16:21:30,498: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:30,498: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:30,499: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:30,499: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:30,499: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:30,499: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:30,499: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:30,499: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:30,499: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:30,500: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:30,505: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:30,505: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:30,505: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:30,505: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:30,505: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/22/ 
[2016-02-05 16:21:30,506: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/22/
[2016-02-05 16:21:30,506: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:30,506: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:30,506: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:30,506: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:30,506: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:30,506: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:30,507: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=How+Hiring+Veterans+Can+Give+Your+Company+A+Competitive+Edge%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F22%2F&via=upscored 
[2016-02-05 16:21:30,507: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=How+Hiring+Veterans+Can+Give+Your+Company+A+Competitive+Edge%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F22%2F&via=upscored
[2016-02-05 16:21:30,507: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F22%2F 
[2016-02-05 16:21:30,507: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F22%2F
[2016-02-05 16:21:30,507: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=How+Hiring+Veterans+Can+Give+Your+Company+A+Competitive+Edge&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F22%2F 
[2016-02-05 16:21:30,507: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=How+Hiring+Veterans+Can+Give+Your+Company+A+Competitive+Edge&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F22%2F
[2016-02-05 16:21:30,508: DEBUG/Worker-4] Ignoring link (depth > 2): http://techcrunch.com/2015/11/03/twitter-engineering-manager-leslie-miley-leaves-company-because-of-diversity-issues/ 
[2016-02-05 16:21:30,508: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://techcrunch.com/2015/11/03/twitter-engineering-manager-leslie-miley-leaves-company-because-of-diversity-issues/
[2016-02-05 16:21:30,508: DEBUG/Worker-4] Ignoring link (depth > 2): http://hbswk.hbs.edu/item/in-venture-capital-birds-of-a-feather-lose-money-together 
[2016-02-05 16:21:30,508: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://hbswk.hbs.edu/item/in-venture-capital-birds-of-a-feather-lose-money-together
[2016-02-05 16:21:30,508: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.citigroup.com/citi/citizen/community/citisalutes/working-jobs.html 
[2016-02-05 16:21:30,508: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.citigroup.com/citi/citizen/community/citisalutes/working-jobs.html
[2016-02-05 16:21:30,508: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:30,509: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:30,509: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:30,509: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:30,509: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:30,509: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:30,509: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:30,509: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:30,510: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:30,510: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:30,515: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:30,515: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:30,515: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:30,515: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:30,516: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/25/ 
[2016-02-05 16:21:30,516: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/25/
[2016-02-05 16:21:30,516: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:30,516: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:30,516: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:30,516: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:30,517: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:30,517: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:30,517: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=So+You+Want+to+Work+in+Corporate+Strategy%3F+Learn+What+It+Takes%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F25%2F&via=upscored 
[2016-02-05 16:21:30,517: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=So+You+Want+to+Work+in+Corporate+Strategy%3F+Learn+What+It+Takes%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F25%2F&via=upscored
[2016-02-05 16:21:30,517: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F25%2F 
[2016-02-05 16:21:30,517: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F25%2F
[2016-02-05 16:21:30,518: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=So+You+Want+to+Work+in+Corporate+Strategy%3F+Learn+What+It+Takes&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F25%2F 
[2016-02-05 16:21:30,518: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=So+You+Want+to+Work+in+Corporate+Strategy%3F+Learn+What+It+Takes&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F25%2F
[2016-02-05 16:21:30,518: DEBUG/Worker-4] Ignoring link (depth > 2): http://womenforhire.com/uncategorized/so-you-want-to-work-in-corporate-strategy-learn-what-it-takes/ 
[2016-02-05 16:21:30,518: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://womenforhire.com/uncategorized/so-you-want-to-work-in-corporate-strategy-learn-what-it-takes/
[2016-02-05 16:21:30,518: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:30,518: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:30,518: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:30,519: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:30,519: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:30,519: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:30,519: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:30,519: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:30,519: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:30,519: WARNING/Worker-4] 2016-02-05 16:21:30 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:31,731: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/21/>
None
[2016-02-05 16:21:31,732: WARNING/Worker-4] 2016-02-05 16:21:31 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/21/>
None
[2016-02-05 16:21:32,677: DEBUG/Worker-4] STREAM IHDR 16 13
[2016-02-05 16:21:32,677: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM IHDR 16 13
[2016-02-05 16:21:32,677: DEBUG/Worker-4] STREAM bKGD 41 6
[2016-02-05 16:21:32,677: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM bKGD 41 6
[2016-02-05 16:21:32,678: DEBUG/Worker-4] bKGD 41 6 (unknown)
[2016-02-05 16:21:32,678: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: bKGD 41 6 (unknown)
[2016-02-05 16:21:32,678: DEBUG/Worker-4] STREAM pHYs 59 9
[2016-02-05 16:21:32,678: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM pHYs 59 9
[2016-02-05 16:21:32,678: DEBUG/Worker-4] STREAM tIME 80 7
[2016-02-05 16:21:32,678: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM tIME 80 7
[2016-02-05 16:21:32,679: DEBUG/Worker-4] tIME 80 7 (unknown)
[2016-02-05 16:21:32,679: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: tIME 80 7 (unknown)
[2016-02-05 16:21:32,679: DEBUG/Worker-4] STREAM IDAT 99 8192
[2016-02-05 16:21:32,679: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM IDAT 99 8192
[2016-02-05 16:21:32,680: DEBUG/Worker-4] STREAM IHDR 16 13
[2016-02-05 16:21:32,680: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM IHDR 16 13
[2016-02-05 16:21:32,680: DEBUG/Worker-4] STREAM bKGD 41 6
[2016-02-05 16:21:32,680: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM bKGD 41 6
[2016-02-05 16:21:32,680: DEBUG/Worker-4] bKGD 41 6 (unknown)
[2016-02-05 16:21:32,681: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: bKGD 41 6 (unknown)
[2016-02-05 16:21:32,681: DEBUG/Worker-4] STREAM pHYs 59 9
[2016-02-05 16:21:32,681: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM pHYs 59 9
[2016-02-05 16:21:32,681: DEBUG/Worker-4] STREAM tIME 80 7
[2016-02-05 16:21:32,681: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM tIME 80 7
[2016-02-05 16:21:32,681: DEBUG/Worker-4] tIME 80 7 (unknown)
[2016-02-05 16:21:32,681: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: tIME 80 7 (unknown)
[2016-02-05 16:21:32,681: DEBUG/Worker-4] STREAM IDAT 99 8192
[2016-02-05 16:21:32,682: WARNING/Worker-4] 2016-02-05 16:21:32 [PIL.PngImagePlugin] DEBUG: STREAM IDAT 99 8192
[2016-02-05 16:21:32,690: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/24/>
None
[2016-02-05 16:21:32,690: WARNING/Worker-4] 2016-02-05 16:21:32 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/24/>
None
[2016-02-05 16:21:32,795: DEBUG/Worker-4] Scraped from <200 http://upscored.com/accounts/login/?next=/js_account/>
None
[2016-02-05 16:21:32,795: WARNING/Worker-4] 2016-02-05 16:21:32 [scrapy] DEBUG: Scraped from <200 http://upscored.com/accounts/login/?next=/js_account/>
None
[2016-02-05 16:21:33,682: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/26/>
None
[2016-02-05 16:21:33,682: WARNING/Worker-4] 2016-02-05 16:21:33 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/26/>
None
[2016-02-05 16:21:33,758: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/accounts/login/>
None
[2016-02-05 16:21:33,759: WARNING/Worker-4] 2016-02-05 16:21:33 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/accounts/login/>
None
[2016-02-05 16:21:34,396: DEBUG/Worker-4] Scraped from <200 http://www.upscored.com/blogpage/15/>
None
[2016-02-05 16:21:34,397: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Scraped from <200 http://www.upscored.com/blogpage/15/>
None
[2016-02-05 16:21:34,403: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:34,404: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:34,404: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:34,404: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:34,404: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/21/ 
[2016-02-05 16:21:34,404: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/21/
[2016-02-05 16:21:34,404: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:34,405: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:34,405: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:34,405: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:34,405: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:34,405: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:34,405: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Leave+the+Job+Search+Behind%E2%80%A6+UpScored+is+Live%21%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F21%2F&via=upscored 
[2016-02-05 16:21:34,406: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Leave+the+Job+Search+Behind%E2%80%A6+UpScored+is+Live%21%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F21%2F&via=upscored
[2016-02-05 16:21:34,406: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F21%2F 
[2016-02-05 16:21:34,406: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F21%2F
[2016-02-05 16:21:34,406: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Leave+the+Job+Search+Behind%E2%80%A6+UpScored+is+Live%21&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F21%2F 
[2016-02-05 16:21:34,406: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Leave+the+Job+Search+Behind%E2%80%A6+UpScored+is+Live%21&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F21%2F
[2016-02-05 16:21:34,406: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/js_signup 
[2016-02-05 16:21:34,406: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/js_signup
[2016-02-05 16:21:34,407: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:34,407: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:34,407: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:34,407: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:34,407: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:34,407: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:34,407: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:34,408: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:34,408: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:34,408: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:34,414: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:34,415: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:34,415: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:34,415: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:34,415: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/24/ 
[2016-02-05 16:21:34,415: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/24/
[2016-02-05 16:21:34,416: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:34,416: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:34,416: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:34,416: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:34,416: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:34,416: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:34,417: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Capitalize+on+the+Most+Sought-After+Technical+Skills%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F24%2F&via=upscored 
[2016-02-05 16:21:34,417: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Capitalize+on+the+Most+Sought-After+Technical+Skills%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F24%2F&via=upscored
[2016-02-05 16:21:34,417: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F24%2F 
[2016-02-05 16:21:34,417: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F24%2F
[2016-02-05 16:21:34,417: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Capitalize+on+the+Most+Sought-After+Technical+Skills&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F24%2F 
[2016-02-05 16:21:34,417: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Capitalize+on+the+Most+Sought-After+Technical+Skills&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F24%2F
[2016-02-05 16:21:34,418: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.google.com/url?q=http%3A%2F%2Fwww.businessinsider.com%2Fmind-blowing-growth-and-power-of-big-data-2015-6&sa=D&usg=AFQjCNEKk9tnB5ivLU4bW8IwoeY7GYpR1Q 
[2016-02-05 16:21:34,418: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.google.com/url?q=http%3A%2F%2Fwww.businessinsider.com%2Fmind-blowing-growth-and-power-of-big-data-2015-6&sa=D&usg=AFQjCNEKk9tnB5ivLU4bW8IwoeY7GYpR1Q
[2016-02-05 16:21:34,418: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.thisismetis.com/data-science 
[2016-02-05 16:21:34,418: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.thisismetis.com/data-science
[2016-02-05 16:21:34,418: DEBUG/Worker-4] Ignoring link (depth > 2): https://generalassemb.ly/education/data-science 
[2016-02-05 16:21:34,418: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://generalassemb.ly/education/data-science
[2016-02-05 16:21:34,419: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.datasciencecentral.com/profiles/blogs/top-five-data-science-masters-programs 
[2016-02-05 16:21:34,419: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.datasciencecentral.com/profiles/blogs/top-five-data-science-masters-programs
[2016-02-05 16:21:34,419: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.coursera.org/course/datasci 
[2016-02-05 16:21:34,419: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.coursera.org/course/datasci
[2016-02-05 16:21:34,419: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617 
[2016-02-05 16:21:34,419: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617
[2016-02-05 16:21:34,419: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.edx.org/course/data-science-machine-learning-essentials-microsoft-dat203x-0 
[2016-02-05 16:21:34,420: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.edx.org/course/data-science-machine-learning-essentials-microsoft-dat203x-0
[2016-02-05 16:21:34,420: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.kaggle.com/competitions 
[2016-02-05 16:21:34,420: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.kaggle.com/competitions
[2016-02-05 16:21:34,420: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.google.com/publicdata/directory# 
[2016-02-05 16:21:34,420: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.google.com/publicdata/directory#
[2016-02-05 16:21:34,420: DEBUG/Worker-4] Ignoring link (depth > 2): http://archive.ics.uci.edu/ml/index.html 
[2016-02-05 16:21:34,421: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://archive.ics.uci.edu/ml/index.html
[2016-02-05 16:21:34,421: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup 
[2016-02-05 16:21:34,421: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup
[2016-02-05 16:21:34,421: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:34,421: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:34,421: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:34,421: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:34,422: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:34,422: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:34,422: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:34,422: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:34,422: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:34,422: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:34,425: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/ 
[2016-02-05 16:21:34,425: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/
[2016-02-05 16:21:34,425: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/blog/ 
[2016-02-05 16:21:34,425: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/blog/
[2016-02-05 16:21:34,425: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/accounts/login/?next=%2Fjs_account%2F 
[2016-02-05 16:21:34,425: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/accounts/login/?next=%2Fjs_account%2F
[2016-02-05 16:21:34,425: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/about/ 
[2016-02-05 16:21:34,426: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/about/
[2016-02-05 16:21:34,426: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/faq/ 
[2016-02-05 16:21:34,426: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/faq/
[2016-02-05 16:21:34,426: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/js_signup/ 
[2016-02-05 16:21:34,426: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/js_signup/
[2016-02-05 16:21:34,426: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/accounts/password/reset/ 
[2016-02-05 16:21:34,427: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/accounts/password/reset/
[2016-02-05 16:21:34,427: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:34,427: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:34,427: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:34,427: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:34,427: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:34,427: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:34,428: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/privacy 
[2016-02-05 16:21:34,428: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/privacy
[2016-02-05 16:21:34,428: DEBUG/Worker-4] Ignoring link (depth > 2): http://upscored.com/terms-of-use 
[2016-02-05 16:21:34,428: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://upscored.com/terms-of-use
[2016-02-05 16:21:34,434: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:34,434: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:34,434: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:34,434: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:34,434: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/26/ 
[2016-02-05 16:21:34,435: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/26/
[2016-02-05 16:21:34,435: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:34,435: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:34,435: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:34,435: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:34,435: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:34,435: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:34,436: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Career+Inspiration%3A+4+Books+To+Help+You+Chart+Your+Course+in+2016%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F26%2F&via=upscored 
[2016-02-05 16:21:34,436: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=Career+Inspiration%3A+4+Books+To+Help+You+Chart+Your+Course+in+2016%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F26%2F&via=upscored
[2016-02-05 16:21:34,436: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F26%2F 
[2016-02-05 16:21:34,436: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F26%2F
[2016-02-05 16:21:34,436: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Career+Inspiration%3A+4+Books+To+Help+You+Chart+Your+Course+in+2016&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F26%2F 
[2016-02-05 16:21:34,436: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=Career+Inspiration%3A+4+Books+To+Help+You+Chart+Your+Course+in+2016&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F26%2F
[2016-02-05 16:21:34,437: DEBUG/Worker-4] Ignoring link (depth > 2): http://carolinedowdhiggins.com/ 
[2016-02-05 16:21:34,437: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://carolinedowdhiggins.com/
[2016-02-05 16:21:34,437: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.amazon.com/This-Not-Career-Ordered-Empowering/dp/0982731817/ref=sr_1_1?ie=UTF8&keywords=This+Is+Not+The+Career+I+Ordered&qid=1451396861&sr=8-1 
[2016-02-05 16:21:34,437: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.amazon.com/This-Not-Career-Ordered-Empowering/dp/0982731817/ref=sr_1_1?ie=UTF8&keywords=This+Is+Not+The+Career+I+Ordered&qid=1451396861&sr=8-1
[2016-02-05 16:21:34,437: DEBUG/Worker-4] Ignoring link (depth > 2): http://carolinedowdhiggins.com/career-inspiration-4-books-to-help-you-chart-your-course-in-2016/ 
[2016-02-05 16:21:34,437: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://carolinedowdhiggins.com/career-inspiration-4-books-to-help-you-chart-your-course-in-2016/
[2016-02-05 16:21:34,438: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:34,438: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:34,438: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:34,438: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:34,438: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:34,438: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:34,438: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:34,439: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:34,439: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:34,439: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:34,443: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:34,443: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:34,443: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:34,443: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:34,443: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/accounts/login/ 
[2016-02-05 16:21:34,443: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/accounts/login/
[2016-02-05 16:21:34,444: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:34,444: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:34,444: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:34,444: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:34,444: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:34,444: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:34,445: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/accounts/password/reset/ 
[2016-02-05 16:21:34,445: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/accounts/password/reset/
[2016-02-05 16:21:34,445: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:34,445: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:34,445: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:34,445: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:34,445: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:34,446: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:34,446: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:34,446: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:34,446: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:34,446: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:34,452: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/ 
[2016-02-05 16:21:34,452: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/
[2016-02-05 16:21:34,452: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blog/ 
[2016-02-05 16:21:34,452: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blog/
[2016-02-05 16:21:34,453: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/blogpage/15/ 
[2016-02-05 16:21:34,453: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/blogpage/15/
[2016-02-05 16:21:34,453: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/about/ 
[2016-02-05 16:21:34,453: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/about/
[2016-02-05 16:21:34,453: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/faq/ 
[2016-02-05 16:21:34,453: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/faq/
[2016-02-05 16:21:34,453: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/js_signup/ 
[2016-02-05 16:21:34,454: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/js_signup/
[2016-02-05 16:21:34,454: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=UpScored%E2%80%99s+NYC+Neighborhood+Series%3A+Chelsea%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F15%2F&via=upscored 
[2016-02-05 16:21:34,454: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/intent/tweet?source=http%3A%2F%2Fwww.upscored.com&text=UpScored%E2%80%99s+NYC+Neighborhood+Series%3A+Chelsea%3A+http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F15%2F&via=upscored
[2016-02-05 16:21:34,454: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F15%2F 
[2016-02-05 16:21:34,454: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/sharer/sharer.php?u=www.upscored.com%2Fblogpage%2F15%2F
[2016-02-05 16:21:34,454: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=UpScored%E2%80%99s+NYC+Neighborhood+Series%3A+Chelsea&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F15%2F 
[2016-02-05 16:21:34,455: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.linkedin.com/shareArticle?mini=true&source=http%3A%2F%2Fwww.upscored.com&summary=&title=UpScored%E2%80%99s+NYC+Neighborhood+Series%3A+Chelsea&url=http%3A%2F%2Fwww.upscored.com%2Fblogpage%2F15%2F
[2016-02-05 16:21:34,455: DEBUG/Worker-4] Ignoring link (depth > 2): http://chelseagallerymap.com/ 
[2016-02-05 16:21:34,455: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://chelseagallerymap.com/
[2016-02-05 16:21:34,455: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.pushcartcoffee.com/locations/north-chelsea 
[2016-02-05 16:21:34,455: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.pushcartcoffee.com/locations/north-chelsea
[2016-02-05 16:21:34,455: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.yelp.com/biz/the-grey-dog-new-york-5 
[2016-02-05 16:21:34,455: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.yelp.com/biz/the-grey-dog-new-york-5
[2016-02-05 16:21:34,456: DEBUG/Worker-4] Ignoring link (depth > 2): http://thetippler.com/ 
[2016-02-05 16:21:34,456: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://thetippler.com/
[2016-02-05 16:21:34,456: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.bathtubginnyc.com/ 
[2016-02-05 16:21:34,456: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.bathtubginnyc.com/
[2016-02-05 16:21:34,456: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.thehighline.org/ 
[2016-02-05 16:21:34,456: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.thehighline.org/
[2016-02-05 16:21:34,457: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.chelseamarket.com/ 
[2016-02-05 16:21:34,457: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.chelseamarket.com/
[2016-02-05 16:21:34,457: DEBUG/Worker-4] Ignoring link (depth > 2): https://twitter.com/upscored 
[2016-02-05 16:21:34,457: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://twitter.com/upscored
[2016-02-05 16:21:34,457: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.linkedin.com/company/upscored 
[2016-02-05 16:21:34,457: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.linkedin.com/company/upscored
[2016-02-05 16:21:34,457: DEBUG/Worker-4] Ignoring link (depth > 2): https://www.facebook.com/upscored/ 
[2016-02-05 16:21:34,457: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.facebook.com/upscored/
[2016-02-05 16:21:34,458: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/privacy 
[2016-02-05 16:21:34,458: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/privacy
[2016-02-05 16:21:34,458: DEBUG/Worker-4] Ignoring link (depth > 2): http://www.upscored.com/terms-of-use 
[2016-02-05 16:21:34,458: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.upscored.com/terms-of-use
[2016-02-05 16:21:34,464: INFO/Worker-4] Closing spider (finished)
[2016-02-05 16:21:34,464: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] INFO: Closing spider (finished)
[2016-02-05 16:21:34,593: INFO/Worker-4] Dumping Scrapy stats:
{'downloader/request_bytes': 10109,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 120606,
 'downloader/response_count': 31,
 'downloader/response_status_count/200': 20,
 'downloader/response_status_count/301': 10,
 'downloader/response_status_count/302': 1,
 'dupefilter/filtered': 49,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 5, 16, 21, 34, 464835),
 'item_scraped_count': 19,
 'log_count/DEBUG': 316,
 'log_count/INFO': 7,
 'log_count/WARNING': 323,
 'offsite/domains': 4,
 'offsite/filtered': 31,
 'request_depth_max': 2,
 'response_received_count': 20,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'start_time': datetime.datetime(2016, 2, 5, 16, 21, 16, 364856)}
[2016-02-05 16:21:34,593: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 10109,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 120606,
 'downloader/response_count': 31,
 'downloader/response_status_count/200': 20,
 'downloader/response_status_count/301': 10,
 'downloader/response_status_count/302': 1,
 'dupefilter/filtered': 49,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 5, 16, 21, 34, 464835),
 'item_scraped_count': 19,
 'log_count/DEBUG': 316,
 'log_count/INFO': 7,
 'log_count/WARNING': 323,
 'offsite/domains': 4,
 'offsite/filtered': 31,
 'request_depth_max': 2,
 'response_received_count': 20,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'start_time': datetime.datetime(2016, 2, 5, 16, 21, 16, 364856)}
[2016-02-05 16:21:34,594: INFO/Worker-4] Spider closed (finished)
[2016-02-05 16:21:34,594: WARNING/Worker-4] 2016-02-05 16:21:34 [scrapy] INFO: Spider closed (finished)
[2016-02-05 16:21:34,604: WARNING/Worker-4] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-05 16:21:34,604: WARNING/Worker-4] 2016-02-05 16:21:34 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-05 16:21:34,605: CRITICAL/MainProcess] Task app.scrape[cff256af-9ad5-4bc9-8dfa-2ec620cff2f5] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-05 16:23:54,886: INFO/MainProcess] Received task: app.scrape[a3c71d95-4e8b-4122-bddd-e5cd8abec018]
[2016-02-05 16:23:54,893: INFO/Worker-2] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-05 16:23:54,893: WARNING/Worker-2] 2016-02-05 16:23:54 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-05 16:23:54,894: INFO/Worker-2] Optional features available: ssl, http11, boto
[2016-02-05 16:23:54,894: WARNING/Worker-2] 2016-02-05 16:23:54 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-05 16:23:54,894: INFO/Worker-2] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-05 16:23:54,894: WARNING/Worker-2] 2016-02-05 16:23:54 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-05 16:23:54,949: INFO/Worker-2] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-05 16:23:54,949: WARNING/Worker-2] 2016-02-05 16:23:54 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-05 16:23:54,986: INFO/Worker-2] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-05 16:23:54,987: WARNING/Worker-2] 2016-02-05 16:23:54 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-05 16:23:54,988: INFO/Worker-2] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-05 16:23:54,988: WARNING/Worker-2] 2016-02-05 16:23:54 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-05 16:23:55,202: INFO/Worker-2] Enabled item pipelines: UserInputPipeline
[2016-02-05 16:23:55,202: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-05 16:23:55,202: INFO/Worker-2] Spider opened
[2016-02-05 16:23:55,203: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] INFO: Spider opened
[2016-02-05 16:23:55,204: INFO/Worker-2] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-05 16:23:55,204: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-05 16:23:55,205: DEBUG/Worker-2] Telnet console listening on 127.0.0.1:6023
[2016-02-05 16:23:55,206: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
[2016-02-05 16:23:55,240: DEBUG/Worker-2] Redirecting (301) to <GET https://www.wayup.com/> from <GET http://www.wayup.com>
[2016-02-05 16:23:55,241: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] DEBUG: Redirecting (301) to <GET https://www.wayup.com/> from <GET http://www.wayup.com>
[2016-02-05 16:23:55,345: DEBUG/Worker-2] Crawled (200) <GET https://www.wayup.com/> (referer: None)
[2016-02-05 16:23:55,346: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] DEBUG: Crawled (200) <GET https://www.wayup.com/> (referer: None)
[2016-02-05 16:23:55,452: INFO/Worker-2] Closing spider (finished)
[2016-02-05 16:23:55,452: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] INFO: Closing spider (finished)
[2016-02-05 16:23:55,453: ERROR/Worker-2] Error caught on signal handler: <bound method ?.spider_closed of <app.vcspider.vcspider.pipelines.UserInputPipeline object at 0x7efd27af0f90>>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/xlib/pydispatch/robustapply.py", line 57, in robustApply
    return receiver(*arguments, **named)
  File "/home/ubuntu/programming/banker.ai/mcubed/app/vcspider/vcspider/pipelines.py", line 39, in spider_closed
    self.cur.execute(sql, sitekv[0]) # keeping tuple comprehension for if i do multiple sites
IndexError: list index out of range
[2016-02-05 16:23:55,456: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] ERROR: Error caught on signal handler: <bound method ?.spider_closed of <app.vcspider.vcspider.pipelines.UserInputPipeline object at 0x7efd27af0f90>>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 150, in maybeDeferred
    result = f(*args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/xlib/pydispatch/robustapply.py", line 57, in robustApply
    return receiver(*arguments, **named)
  File "/home/ubuntu/programming/banker.ai/mcubed/app/vcspider/vcspider/pipelines.py", line 39, in spider_closed
    self.cur.execute(sql, sitekv[0]) # keeping tuple comprehension for if i do multiple sites
IndexError: list index out of range
[2016-02-05 16:23:55,457: INFO/Worker-2] Dumping Scrapy stats:
{'downloader/request_bytes': 456,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 17572,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 5, 16, 23, 55, 453063),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'log_count/WARNING': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2016, 2, 5, 16, 23, 55, 204553)}
[2016-02-05 16:23:55,457: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 456,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 17572,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/301': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 5, 16, 23, 55, 453063),
 'log_count/DEBUG': 3,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'log_count/WARNING': 11,
 'response_received_count': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2016, 2, 5, 16, 23, 55, 204553)}
[2016-02-05 16:23:55,457: INFO/Worker-2] Spider closed (finished)
[2016-02-05 16:23:55,457: WARNING/Worker-2] 2016-02-05 16:23:55 [scrapy] INFO: Spider closed (finished)
[2016-02-05 16:23:55,463: WARNING/Worker-2] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-05 16:23:55,464: WARNING/Worker-2] 2016-02-05 16:23:55 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-05 16:23:55,465: CRITICAL/MainProcess] Task app.scrape[a3c71d95-4e8b-4122-bddd-e5cd8abec018] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-06 01:06:08,299: INFO/MainProcess] Received task: app.scrape[06bbfac9-2c08-45b6-b469-7e14caae8425]
[2016-02-06 01:06:08,306: INFO/Worker-1] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-06 01:06:08,306: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-06 01:06:08,306: INFO/Worker-1] Optional features available: ssl, http11, boto
[2016-02-06 01:06:08,306: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-06 01:06:08,307: INFO/Worker-1] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-06 01:06:08,307: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-06 01:06:08,361: INFO/Worker-1] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-06 01:06:08,361: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-06 01:06:08,399: INFO/Worker-1] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-06 01:06:08,399: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-06 01:06:08,400: INFO/Worker-1] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-06 01:06:08,400: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-06 01:06:08,615: INFO/Worker-1] Enabled item pipelines: UserInputPipeline
[2016-02-06 01:06:08,615: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-06 01:06:08,615: INFO/Worker-1] Spider opened
[2016-02-06 01:06:08,615: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Spider opened
[2016-02-06 01:06:08,616: INFO/Worker-1] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-06 01:06:08,617: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-06 01:06:08,618: DEBUG/Worker-1] Telnet console listening on 127.0.0.1:6023
[2016-02-06 01:06:08,618: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
[2016-02-06 01:06:08,955: DEBUG/Worker-1] Redirecting (301) to <GET https://www.picassolabs.com/> from <GET http://www.picassolabs.com>
[2016-02-06 01:06:08,955: WARNING/Worker-1] 2016-02-06 01:06:08 [scrapy] DEBUG: Redirecting (301) to <GET https://www.picassolabs.com/> from <GET http://www.picassolabs.com>
[2016-02-06 01:06:09,092: DEBUG/Worker-1] Crawled (200) <GET https://www.picassolabs.com/> (referer: None)
[2016-02-06 01:06:09,092: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Crawled (200) <GET https://www.picassolabs.com/> (referer: None)
[2016-02-06 01:06:09,268: DEBUG/Worker-1] Crawled (200) <GET https://www.picassolabs.com/privacy> (referer: https://www.picassolabs.com/)
[2016-02-06 01:06:09,268: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Crawled (200) <GET https://www.picassolabs.com/privacy> (referer: https://www.picassolabs.com/)
[2016-02-06 01:06:09,421: DEBUG/Worker-1] Scraped from <200 https://www.picassolabs.com/privacy>
None
[2016-02-06 01:06:09,421: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Scraped from <200 https://www.picassolabs.com/privacy>
None
[2016-02-06 01:06:09,423: DEBUG/Worker-1] Filtered duplicate request: <GET https://www.picassolabs.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
[2016-02-06 01:06:09,423: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Filtered duplicate request: <GET https://www.picassolabs.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
[2016-02-06 01:06:09,472: DEBUG/Worker-1] Crawled (200) <GET https://www.picassolabs.com/> (referer: https://www.picassolabs.com/)
[2016-02-06 01:06:09,472: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Crawled (200) <GET https://www.picassolabs.com/> (referer: https://www.picassolabs.com/)
[2016-02-06 01:06:09,474: DEBUG/Worker-1] Crawled (200) <GET https://www.picassolabs.com/users/sign_in> (referer: https://www.picassolabs.com/)
[2016-02-06 01:06:09,475: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Crawled (200) <GET https://www.picassolabs.com/users/sign_in> (referer: https://www.picassolabs.com/)
[2016-02-06 01:06:09,606: DEBUG/Worker-1] Scraped from <200 https://www.picassolabs.com/>
None
[2016-02-06 01:06:09,606: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Scraped from <200 https://www.picassolabs.com/>
None
[2016-02-06 01:06:09,641: DEBUG/Worker-1] Scraped from <200 https://www.picassolabs.com/users/sign_in>
None
[2016-02-06 01:06:09,641: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Scraped from <200 https://www.picassolabs.com/users/sign_in>
None
[2016-02-06 01:06:09,678: DEBUG/Worker-1] Crawled (200) <GET https://www.picassolabs.com/users/password/new> (referer: https://www.picassolabs.com/users/sign_in)
[2016-02-06 01:06:09,678: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Crawled (200) <GET https://www.picassolabs.com/users/password/new> (referer: https://www.picassolabs.com/users/sign_in)
[2016-02-06 01:06:09,809: DEBUG/Worker-1] Scraped from <200 https://www.picassolabs.com/users/password/new>
None
[2016-02-06 01:06:09,809: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Scraped from <200 https://www.picassolabs.com/users/password/new>
None
[2016-02-06 01:06:09,810: DEBUG/Worker-1] Ignoring link (depth > 2): https://www.picassolabs.com/ 
[2016-02-06 01:06:09,810: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.picassolabs.com/
[2016-02-06 01:06:09,810: DEBUG/Worker-1] Ignoring link (depth > 2): https://www.picassolabs.com/users/sign_in 
[2016-02-06 01:06:09,810: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.picassolabs.com/users/sign_in
[2016-02-06 01:06:09,811: DEBUG/Worker-1] Ignoring link (depth > 2): https://www.picassolabs.com/privacy 
[2016-02-06 01:06:09,811: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] DEBUG: Ignoring link (depth > 2): https://www.picassolabs.com/privacy
[2016-02-06 01:06:09,813: INFO/Worker-1] Closing spider (finished)
[2016-02-06 01:06:09,813: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] INFO: Closing spider (finished)
[2016-02-06 01:06:09,879: INFO/Worker-1] Dumping Scrapy stats:
{'downloader/request_bytes': 2855,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 31413,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 9,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 6, 1, 6, 9, 814350),
 'item_scraped_count': 4,
 'log_count/DEBUG': 15,
 'log_count/INFO': 7,
 'log_count/WARNING': 22,
 'request_depth_max': 2,
 'response_received_count': 5,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2016, 2, 6, 1, 6, 8, 617301)}
[2016-02-06 01:06:09,880: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2855,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 31413,
 'downloader/response_count': 6,
 'downloader/response_status_count/200': 5,
 'downloader/response_status_count/301': 1,
 'dupefilter/filtered': 9,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 2, 6, 1, 6, 9, 814350),
 'item_scraped_count': 4,
 'log_count/DEBUG': 15,
 'log_count/INFO': 7,
 'log_count/WARNING': 22,
 'request_depth_max': 2,
 'response_received_count': 5,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2016, 2, 6, 1, 6, 8, 617301)}
[2016-02-06 01:06:09,880: INFO/Worker-1] Spider closed (finished)
[2016-02-06 01:06:09,880: WARNING/Worker-1] 2016-02-06 01:06:09 [scrapy] INFO: Spider closed (finished)
[2016-02-06 01:06:09,894: WARNING/Worker-1] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-06 01:06:09,894: WARNING/Worker-1] 2016-02-06 01:06:09 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-06 01:06:09,896: CRITICAL/MainProcess] Task app.scrape[06bbfac9-2c08-45b6-b469-7e14caae8425] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-11 14:43:20,351: INFO/MainProcess] Received task: app.scrape[1f44a22d-7c23-4ef6-bd60-caff8a19a5e7]
[2016-02-11 14:43:20,358: INFO/Worker-3] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-11 14:43:20,358: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-11 14:43:20,359: INFO/Worker-3] Optional features available: ssl, http11, boto
[2016-02-11 14:43:20,359: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-11 14:43:20,359: INFO/Worker-3] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-11 14:43:20,359: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-11 14:43:20,418: INFO/Worker-3] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-11 14:43:20,418: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-11 14:43:20,456: INFO/Worker-3] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-11 14:43:20,456: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-11 14:43:20,458: INFO/Worker-3] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-11 14:43:20,458: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-11 14:43:20,672: INFO/Worker-3] Enabled item pipelines: UserInputPipeline
[2016-02-11 14:43:20,672: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-11 14:43:20,672: INFO/Worker-3] Spider opened
[2016-02-11 14:43:20,672: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Spider opened
[2016-02-11 14:43:20,673: INFO/Worker-3] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-11 14:43:20,673: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-11 14:43:20,675: DEBUG/Worker-3] Telnet console listening on 127.0.0.1:6023
[2016-02-11 14:43:20,675: WARNING/Worker-3] 2016-02-11 14:43:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
[2016-02-11 14:43:22,145: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com> (referer: None)
[2016-02-11 14:43:22,145: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com> (referer: None)
[2016-02-11 14:43:22,272: DEBUG/Worker-3] Filtered offsite request to 'devnetwork.com': <GET http://devnetwork.com/>
[2016-02-11 14:43:22,273: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'devnetwork.com': <GET http://devnetwork.com/>
[2016-02-11 14:43:22,273: DEBUG/Worker-3] Filtered offsite request to 'integratecon.com': <GET http://integratecon.com/>
[2016-02-11 14:43:22,273: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'integratecon.com': <GET http://integratecon.com/>
[2016-02-11 14:43:22,273: DEBUG/Worker-3] Filtered offsite request to 'dataweek.co': <GET http://dataweek.co/>
[2016-02-11 14:43:22,273: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'dataweek.co': <GET http://dataweek.co/>
[2016-02-11 14:43:22,274: DEBUG/Worker-3] Filtered offsite request to 'apiworld.co': <GET http://apiworld.co/>
[2016-02-11 14:43:22,274: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'apiworld.co': <GET http://apiworld.co/>
[2016-02-11 14:43:22,274: DEBUG/Worker-3] Filtered offsite request to 'hirepalooza.co': <GET http://hirepalooza.co/>
[2016-02-11 14:43:22,274: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'hirepalooza.co': <GET http://hirepalooza.co/>
[2016-02-11 14:43:22,275: DEBUG/Worker-3] Filtered offsite request to 'ctotalks.com': <GET http://ctotalks.com/>
[2016-02-11 14:43:22,275: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'ctotalks.com': <GET http://ctotalks.com/>
[2016-02-11 14:43:22,278: DEBUG/Worker-3] Filtered offsite request to 'www.ctoworldcongress.com': <GET http://www.ctoworldcongress.com/>
[2016-02-11 14:43:22,278: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.ctoworldcongress.com': <GET http://www.ctoworldcongress.com/>
[2016-02-11 14:43:22,279: DEBUG/Worker-3] Filtered offsite request to 'devnetwork.leadpages.co': <GET https://devnetwork.leadpages.co/developerweek16/>
[2016-02-11 14:43:22,280: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'devnetwork.leadpages.co': <GET https://devnetwork.leadpages.co/developerweek16/>
[2016-02-11 14:43:22,282: DEBUG/Worker-3] Filtered offsite request to 'www.facebook.com': <GET https://www.facebook.com/developerweek1/?fref=ts>
[2016-02-11 14:43:22,282: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.facebook.com': <GET https://www.facebook.com/developerweek1/?fref=ts>
[2016-02-11 14:43:22,283: DEBUG/Worker-3] Filtered offsite request to 'ctoworldcongress.com': <GET http://ctoworldcongress.com/>
[2016-02-11 14:43:22,283: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'ctoworldcongress.com': <GET http://ctoworldcongress.com/>
[2016-02-11 14:43:22,283: DEBUG/Worker-3] Filtered offsite request to 'monthofcode.com': <GET http://monthofcode.com/>
[2016-02-11 14:43:22,283: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'monthofcode.com': <GET http://monthofcode.com/>
[2016-02-11 14:43:22,283: DEBUG/Worker-3] Filtered offsite request to 'www.codingame.com': <GET https://www.codingame.com/hackathon/sf2442>
[2016-02-11 14:43:22,284: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.codingame.com': <GET https://www.codingame.com/hackathon/sf2442>
[2016-02-11 14:43:22,284: DEBUG/Worker-3] Filtered offsite request to 'programmableweb.com': <GET http://programmableweb.com/>
[2016-02-11 14:43:22,284: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'programmableweb.com': <GET http://programmableweb.com/>
[2016-02-11 14:43:22,284: DEBUG/Worker-3] Filtered offsite request to 'www.apiscience.com': <GET https://www.apiscience.com/>
[2016-02-11 14:43:22,284: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.apiscience.com': <GET https://www.apiscience.com/>
[2016-02-11 14:43:22,285: DEBUG/Worker-3] Filtered offsite request to 'technorati.com': <GET http://technorati.com/>
[2016-02-11 14:43:22,285: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'technorati.com': <GET http://technorati.com/>
[2016-02-11 14:43:22,285: DEBUG/Worker-3] Filtered offsite request to 'apidays.io': <GET http://apidays.io/>
[2016-02-11 14:43:22,285: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'apidays.io': <GET http://apidays.io/>
[2016-02-11 14:43:22,285: DEBUG/Worker-3] Filtered offsite request to 'oauth.io': <GET http://oauth.io/>
[2016-02-11 14:43:22,285: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'oauth.io': <GET http://oauth.io/>
[2016-02-11 14:43:22,286: DEBUG/Worker-3] Filtered offsite request to 'www.dnb.com': <GET http://www.dnb.com/>
[2016-02-11 14:43:22,286: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.dnb.com': <GET http://www.dnb.com/>
[2016-02-11 14:43:22,286: DEBUG/Worker-3] Filtered offsite request to 'www.capitalone.com': <GET http://www.capitalone.com/>
[2016-02-11 14:43:22,286: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.capitalone.com': <GET http://www.capitalone.com/>
[2016-02-11 14:43:22,286: DEBUG/Worker-3] Filtered offsite request to 'codeanywhere.com': <GET http://codeanywhere.com/>
[2016-02-11 14:43:22,286: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'codeanywhere.com': <GET http://codeanywhere.com/>
[2016-02-11 14:43:22,287: DEBUG/Worker-3] Filtered offsite request to 'live.gotoassist.com': <GET http://live.gotoassist.com/>
[2016-02-11 14:43:22,287: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'live.gotoassist.com': <GET http://live.gotoassist.com/>
[2016-02-11 14:43:22,287: DEBUG/Worker-3] Filtered offsite request to 'www.equinix.com': <GET http://www.equinix.com/>
[2016-02-11 14:43:22,287: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.equinix.com': <GET http://www.equinix.com/>
[2016-02-11 14:43:22,287: DEBUG/Worker-3] Filtered offsite request to 'galvanize.com': <GET http://galvanize.com/>
[2016-02-11 14:43:22,288: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'galvanize.com': <GET http://galvanize.com/>
[2016-02-11 14:43:22,288: DEBUG/Worker-3] Filtered offsite request to 'www.ibm.com': <GET http://www.ibm.com/developerworks/>
[2016-02-11 14:43:22,288: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.ibm.com': <GET http://www.ibm.com/developerworks/>
[2016-02-11 14:43:22,288: DEBUG/Worker-3] Filtered offsite request to 'justapis.com': <GET http://justapis.com/>
[2016-02-11 14:43:22,288: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'justapis.com': <GET http://justapis.com/>
[2016-02-11 14:43:22,288: DEBUG/Worker-3] Filtered offsite request to 'www.applause.com': <GET http://www.applause.com/>
[2016-02-11 14:43:22,289: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.applause.com': <GET http://www.applause.com/>
[2016-02-11 14:43:22,289: DEBUG/Worker-3] Filtered offsite request to 'www.cdw.com': <GET http://www.cdw.com/>
[2016-02-11 14:43:22,289: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.cdw.com': <GET http://www.cdw.com/>
[2016-02-11 14:43:22,289: DEBUG/Worker-3] Filtered offsite request to 'www.cloudbees.com': <GET http://www.cloudbees.com/>
[2016-02-11 14:43:22,289: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.cloudbees.com': <GET http://www.cloudbees.com/>
[2016-02-11 14:43:22,290: DEBUG/Worker-3] Filtered offsite request to 'www.cortical.io': <GET http://www.cortical.io/>
[2016-02-11 14:43:22,290: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.cortical.io': <GET http://www.cortical.io/>
[2016-02-11 14:43:22,290: DEBUG/Worker-3] Filtered offsite request to 'devbootcamp.com': <GET http://devbootcamp.com/>
[2016-02-11 14:43:22,290: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'devbootcamp.com': <GET http://devbootcamp.com/>
[2016-02-11 14:43:22,290: DEBUG/Worker-3] Filtered offsite request to 'dji.com': <GET http://dji.com/>
[2016-02-11 14:43:22,290: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'dji.com': <GET http://dji.com/>
[2016-02-11 14:43:22,291: DEBUG/Worker-3] Filtered offsite request to 'www.flowroute.com': <GET http://www.flowroute.com/>
[2016-02-11 14:43:22,291: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.flowroute.com': <GET http://www.flowroute.com/>
[2016-02-11 14:43:22,291: DEBUG/Worker-3] Filtered offsite request to 'www.predix.io': <GET https://www.predix.io/>
[2016-02-11 14:43:22,291: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.predix.io': <GET https://www.predix.io/>
[2016-02-11 14:43:22,291: DEBUG/Worker-3] Filtered offsite request to 'www.gupshup.io': <GET http://www.gupshup.io/>
[2016-02-11 14:43:22,291: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.gupshup.io': <GET http://www.gupshup.io/>
[2016-02-11 14:43:22,292: DEBUG/Worker-3] Filtered offsite request to 'dev.havenondemand.com': <GET https://dev.havenondemand.com/>
[2016-02-11 14:43:22,292: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'dev.havenondemand.com': <GET https://dev.havenondemand.com/>
[2016-02-11 14:43:22,292: DEBUG/Worker-3] Filtered offsite request to 'www.hyperwallet.com': <GET http://www.hyperwallet.com/>
[2016-02-11 14:43:22,292: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.hyperwallet.com': <GET http://www.hyperwallet.com/>
[2016-02-11 14:43:22,292: DEBUG/Worker-3] Filtered offsite request to 'developer.intuit.com': <GET https://developer.intuit.com/>
[2016-02-11 14:43:22,292: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'developer.intuit.com': <GET https://developer.intuit.com/>
[2016-02-11 14:43:22,293: DEBUG/Worker-3] Filtered offsite request to 'www.kony.com': <GET http://www.kony.com/>
[2016-02-11 14:43:22,293: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.kony.com': <GET http://www.kony.com/>
[2016-02-11 14:43:22,293: DEBUG/Worker-3] Filtered offsite request to 'www.magnet.com': <GET http://www.magnet.com/>
[2016-02-11 14:43:22,293: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.magnet.com': <GET http://www.magnet.com/>
[2016-02-11 14:43:22,293: DEBUG/Worker-3] Filtered offsite request to 'www.mertech.com': <GET http://www.mertech.com/>
[2016-02-11 14:43:22,293: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.mertech.com': <GET http://www.mertech.com/>
[2016-02-11 14:43:22,294: DEBUG/Worker-3] Filtered offsite request to 'microsoft.com': <GET http://microsoft.com/learning>
[2016-02-11 14:43:22,294: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'microsoft.com': <GET http://microsoft.com/learning>
[2016-02-11 14:43:22,294: DEBUG/Worker-3] Filtered offsite request to 'netapp.com': <GET http://netapp.com/>
[2016-02-11 14:43:22,294: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'netapp.com': <GET http://netapp.com/>
[2016-02-11 14:43:22,294: DEBUG/Worker-3] Filtered offsite request to 'oracle.com': <GET http://oracle.com/>
[2016-02-11 14:43:22,295: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'oracle.com': <GET http://oracle.com/>
[2016-02-11 14:43:22,295: DEBUG/Worker-3] Filtered offsite request to 'www.redislabs.com': <GET http://www.redislabs.com/>
[2016-02-11 14:43:22,295: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.redislabs.com': <GET http://www.redislabs.com/>
[2016-02-11 14:43:22,295: DEBUG/Worker-3] Filtered offsite request to 'theta360.com': <GET https://theta360.com/>
[2016-02-11 14:43:22,295: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'theta360.com': <GET https://theta360.com/>
[2016-02-11 14:43:22,295: DEBUG/Worker-3] Filtered offsite request to 'sparkpost.com': <GET http://sparkpost.com/>
[2016-02-11 14:43:22,296: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'sparkpost.com': <GET http://sparkpost.com/>
[2016-02-11 14:43:22,296: DEBUG/Worker-3] Filtered offsite request to 'www.workday.com': <GET http://www.workday.com/company/careers.php>
[2016-02-11 14:43:22,296: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.workday.com': <GET http://www.workday.com/company/careers.php>
[2016-02-11 14:43:22,296: DEBUG/Worker-3] Filtered offsite request to 'www.universe.com': <GET http://www.universe.com/selltickets>
[2016-02-11 14:43:22,296: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.universe.com': <GET http://www.universe.com/selltickets>
[2016-02-11 14:43:22,296: DEBUG/Worker-3] Filtered offsite request to 'www.usertesting.com': <GET http://www.usertesting.com/>
[2016-02-11 14:43:22,297: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.usertesting.com': <GET http://www.usertesting.com/>
[2016-02-11 14:43:22,297: DEBUG/Worker-3] Filtered offsite request to 'www.weebly.com': <GET http://www.weebly.com/>
[2016-02-11 14:43:22,297: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.weebly.com': <GET http://www.weebly.com/>
[2016-02-11 14:43:22,297: DEBUG/Worker-3] Filtered offsite request to 'www.zteusa.com': <GET https://www.zteusa.com/>
[2016-02-11 14:43:22,297: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.zteusa.com': <GET https://www.zteusa.com/>
[2016-02-11 14:43:22,297: DEBUG/Worker-3] Filtered offsite request to 'www.activestate.com': <GET http://www.activestate.com/>
[2016-02-11 14:43:22,298: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.activestate.com': <GET http://www.activestate.com/>
[2016-02-11 14:43:22,298: DEBUG/Worker-3] Filtered offsite request to 'www.chetu.com': <GET http://www.chetu.com/>
[2016-02-11 14:43:22,298: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.chetu.com': <GET http://www.chetu.com/>
[2016-02-11 14:43:22,298: DEBUG/Worker-3] Filtered offsite request to 'www.circleci.com': <GET http://www.circleci.com/>
[2016-02-11 14:43:22,298: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.circleci.com': <GET http://www.circleci.com/>
[2016-02-11 14:43:22,299: DEBUG/Worker-3] Filtered offsite request to 'www.codiscope.com': <GET http://www.codiscope.com/>
[2016-02-11 14:43:22,299: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.codiscope.com': <GET http://www.codiscope.com/>
[2016-02-11 14:43:22,299: DEBUG/Worker-3] Filtered offsite request to 'dato.com': <GET https://dato.com/>
[2016-02-11 14:43:22,299: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'dato.com': <GET https://dato.com/>
[2016-02-11 14:43:22,299: DEBUG/Worker-3] Filtered offsite request to 'www.digitalocean.com': <GET https://www.digitalocean.com/>
[2016-02-11 14:43:22,299: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.digitalocean.com': <GET https://www.digitalocean.com/>
[2016-02-11 14:43:22,300: DEBUG/Worker-3] Filtered offsite request to 'www.esurance.com': <GET https://www.esurance.com/>
[2016-02-11 14:43:22,300: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.esurance.com': <GET https://www.esurance.com/>
[2016-02-11 14:43:22,300: DEBUG/Worker-3] Filtered offsite request to 'www.experts-exchange.com': <GET http://www.experts-exchange.com/>
[2016-02-11 14:43:22,300: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.experts-exchange.com': <GET http://www.experts-exchange.com/>
[2016-02-11 14:43:22,300: DEBUG/Worker-3] Filtered offsite request to 'www.fanaticsinc.com': <GET http://www.fanaticsinc.com/>
[2016-02-11 14:43:22,300: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.fanaticsinc.com': <GET http://www.fanaticsinc.com/>
[2016-02-11 14:43:22,301: DEBUG/Worker-3] Filtered offsite request to 'www.filterdigital.com': <GET http://www.filterdigital.com/>
[2016-02-11 14:43:22,301: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.filterdigital.com': <GET http://www.filterdigital.com/>
[2016-02-11 14:43:22,301: DEBUG/Worker-3] Filtered offsite request to 'www.grindr.com': <GET http://www.grindr.com/jobs>
[2016-02-11 14:43:22,301: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.grindr.com': <GET http://www.grindr.com/jobs>
[2016-02-11 14:43:22,301: DEBUG/Worker-3] Filtered offsite request to 'helix.com': <GET http://helix.com/>
[2016-02-11 14:43:22,301: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'helix.com': <GET http://helix.com/>
[2016-02-11 14:43:22,302: DEBUG/Worker-3] Filtered offsite request to 'www.theinformation.com': <GET http://www.theinformation.com/>
[2016-02-11 14:43:22,302: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.theinformation.com': <GET http://www.theinformation.com/>
[2016-02-11 14:43:22,302: DEBUG/Worker-3] Filtered offsite request to 'www.theneura.com': <GET http://www.theneura.com/>
[2016-02-11 14:43:22,302: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.theneura.com': <GET http://www.theneura.com/>
[2016-02-11 14:43:22,302: DEBUG/Worker-3] Filtered offsite request to 'www.pushmote.com': <GET http://www.pushmote.com/>
[2016-02-11 14:43:22,302: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.pushmote.com': <GET http://www.pushmote.com/>
[2016-02-11 14:43:22,303: DEBUG/Worker-3] Filtered offsite request to 'raygun.io': <GET https://raygun.io/>
[2016-02-11 14:43:22,303: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'raygun.io': <GET https://raygun.io/>
[2016-02-11 14:43:22,303: DEBUG/Worker-3] Filtered offsite request to 'www.realself.com': <GET http://www.realself.com/about>
[2016-02-11 14:43:22,303: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.realself.com': <GET http://www.realself.com/about>
[2016-02-11 14:43:22,303: DEBUG/Worker-3] Filtered offsite request to 'www.rlsolutions.com': <GET http://www.rlsolutions.com/>
[2016-02-11 14:43:22,304: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.rlsolutions.com': <GET http://www.rlsolutions.com/>
[2016-02-11 14:43:22,304: DEBUG/Worker-3] Filtered offsite request to 'www.roberthalf.com': <GET http://www.roberthalf.com/san-francisco-bay-area/technology-it>
[2016-02-11 14:43:22,304: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.roberthalf.com': <GET http://www.roberthalf.com/san-francisco-bay-area/technology-it>
[2016-02-11 14:43:22,304: DEBUG/Worker-3] Filtered offsite request to 'goshippo.com': <GET https://goshippo.com/>
[2016-02-11 14:43:22,304: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'goshippo.com': <GET https://goshippo.com/>
[2016-02-11 14:43:22,304: DEBUG/Worker-3] Filtered offsite request to 'visa.com': <GET http://visa.com/>
[2016-02-11 14:43:22,305: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'visa.com': <GET http://visa.com/>
[2016-02-11 14:43:22,305: DEBUG/Worker-3] Filtered offsite request to 'developer.yodlee.com': <GET https://developer.yodlee.com/>
[2016-02-11 14:43:22,305: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'developer.yodlee.com': <GET https://developer.yodlee.com/>
[2016-02-11 14:43:22,305: DEBUG/Worker-3] Filtered offsite request to 'www.fluxx.io': <GET http://www.fluxx.io/>
[2016-02-11 14:43:22,305: WARNING/Worker-3] 2016-02-11 14:43:22 [scrapy] DEBUG: Filtered offsite request to 'www.fluxx.io': <GET http://www.fluxx.io/>
[2016-02-11 14:43:23,224: DEBUG/Worker-3] Redirecting (301) to <GET http://www.developerweek.com/host-an-event/> from <GET http://www.developerweek.com/events/host-an-event/>
[2016-02-11 14:43:23,224: WARNING/Worker-3] 2016-02-11 14:43:23 [scrapy] DEBUG: Redirecting (301) to <GET http://www.developerweek.com/host-an-event/> from <GET http://www.developerweek.com/events/host-an-event/>
[2016-02-11 14:43:23,225: DEBUG/Worker-3] Filtered duplicate request: <GET http://www.developerweek.com/host-an-event/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
[2016-02-11 14:43:23,225: WARNING/Worker-3] 2016-02-11 14:43:23 [scrapy] DEBUG: Filtered duplicate request: <GET http://www.developerweek.com/host-an-event/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
[2016-02-11 14:43:23,266: DEBUG/Worker-3] Redirecting (301) to <GET http://www.developerweek.com/media/media-partners/> from <GET http://developerweek.com/media-partners>
[2016-02-11 14:43:23,266: WARNING/Worker-3] 2016-02-11 14:43:23 [scrapy] DEBUG: Redirecting (301) to <GET http://www.developerweek.com/media/media-partners/> from <GET http://developerweek.com/media-partners>
[2016-02-11 14:43:23,361: DEBUG/Worker-3] Redirecting (301) to <GET http://www.developerweek.com/media/media-partners/> from <GET http://developerweek.com/2016/media-partners>
[2016-02-11 14:43:23,361: WARNING/Worker-3] 2016-02-11 14:43:23 [scrapy] DEBUG: Redirecting (301) to <GET http://www.developerweek.com/media/media-partners/> from <GET http://developerweek.com/2016/media-partners>
[2016-02-11 14:43:23,438: DEBUG/Worker-3] Redirecting (301) to <GET http://www.developerweek.com/register/> from <GET http://developerweek.com/register/>
[2016-02-11 14:43:23,439: WARNING/Worker-3] 2016-02-11 14:43:23 [scrapy] DEBUG: Redirecting (301) to <GET http://www.developerweek.com/register/> from <GET http://developerweek.com/register/>
[2016-02-11 14:43:23,538: DEBUG/Worker-3] Redirecting (301) to <GET http://www.developerweek.com/> from <GET http://developerweek.com/>
[2016-02-11 14:43:23,538: WARNING/Worker-3] 2016-02-11 14:43:23 [scrapy] DEBUG: Redirecting (301) to <GET http://www.developerweek.com/> from <GET http://developerweek.com/>
[2016-02-11 14:43:24,685: DEBUG/Worker-3] Redirecting (301) to <GET http://www.developerweek.com/hackathon/> from <GET http://developerweek.com/hackathon/>
[2016-02-11 14:43:24,686: WARNING/Worker-3] 2016-02-11 14:43:24 [scrapy] DEBUG: Redirecting (301) to <GET http://www.developerweek.com/hackathon/> from <GET http://developerweek.com/hackathon/>
[2016-02-11 14:43:25,491: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/register/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:25,491: WARNING/Worker-3] 2016-02-11 14:43:25 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/register/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,526: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/register/>
None
[2016-02-11 14:43:26,526: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/register/>
None
[2016-02-11 14:43:26,530: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/awards/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,530: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/awards/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,533: DEBUG/Worker-3] Crawled (404) <GET http://www.developerweek.com/twitter.com/developerweek> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,533: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (404) <GET http://www.developerweek.com/twitter.com/developerweek> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,537: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/media/press/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,537: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/media/press/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,541: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/media/promote/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,541: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/media/promote/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,544: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/hosted-lunch-with-oracle/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,544: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/hosted-lunch-with-oracle/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,548: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/awards/2016-award-recipients/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,548: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/awards/2016-award-recipients/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,552: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/media/media-partners/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,552: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/media/media-partners/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,555: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/host-an-event/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:26,555: WARNING/Worker-3] 2016-02-11 14:43:26 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/host-an-event/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:28,935: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/awards/>
None
[2016-02-11 14:43:28,935: WARNING/Worker-3] 2016-02-11 14:43:28 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/awards/>
None
[2016-02-11 14:43:28,937: DEBUG/Worker-3] Redirecting (301) to <GET http://www.developerweek.com/register/> from <GET http://www.developerweek.com/register>
[2016-02-11 14:43:28,937: WARNING/Worker-3] 2016-02-11 14:43:28 [scrapy] DEBUG: Redirecting (301) to <GET http://www.developerweek.com/register/> from <GET http://www.developerweek.com/register>
[2016-02-11 14:43:28,941: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/sponsors/2014-attendee-list/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:28,941: WARNING/Worker-3] 2016-02-11 14:43:28 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/sponsors/2014-attendee-list/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:28,944: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/terms-of-use/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:28,944: WARNING/Worker-3] 2016-02-11 14:43:28 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/terms-of-use/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:28,945: DEBUG/Worker-3] Ignoring response <404 http://www.developerweek.com/twitter.com/developerweek>: HTTP status code is not handled or not allowed
[2016-02-11 14:43:28,945: WARNING/Worker-3] 2016-02-11 14:43:28 [scrapy] DEBUG: Ignoring response <404 http://www.developerweek.com/twitter.com/developerweek>: HTTP status code is not handled or not allowed
[2016-02-11 14:43:30,665: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/media/press/>
None
[2016-02-11 14:43:30,665: WARNING/Worker-3] 2016-02-11 14:43:30 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/media/press/>
None
[2016-02-11 14:43:31,574: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/media/promote/>
None
[2016-02-11 14:43:31,574: WARNING/Worker-3] 2016-02-11 14:43:31 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/media/promote/>
None
[2016-02-11 14:43:32,380: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/hosted-lunch-with-oracle/>
None
[2016-02-11 14:43:32,380: WARNING/Worker-3] 2016-02-11 14:43:32 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/hosted-lunch-with-oracle/>
None
[2016-02-11 14:43:33,144: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/awards/2016-award-recipients/>
None
[2016-02-11 14:43:33,145: WARNING/Worker-3] 2016-02-11 14:43:33 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/awards/2016-award-recipients/>
None
[2016-02-11 14:43:34,438: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/media/media-partners/>
None
[2016-02-11 14:43:34,438: WARNING/Worker-3] 2016-02-11 14:43:34 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/media/media-partners/>
None
[2016-02-11 14:43:35,855: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/host-an-event/>
None
[2016-02-11 14:43:35,855: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/host-an-event/>
None
[2016-02-11 14:43:35,863: DEBUG/Worker-3] Filtered offsite request to 'data2.wufoo.com': <GET https://data2.wufoo.com/forms/developerweek-2016-award-nomination/>
[2016-02-11 14:43:35,863: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Filtered offsite request to 'data2.wufoo.com': <GET https://data2.wufoo.com/forms/developerweek-2016-award-nomination/>
[2016-02-11 14:43:35,863: DEBUG/Worker-3] Filtered offsite request to 'verticalize.me': <GET http://verticalize.me/technology/index>
[2016-02-11 14:43:35,863: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Filtered offsite request to 'verticalize.me': <GET http://verticalize.me/technology/index>
[2016-02-11 14:43:35,869: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/sponsors/demographics/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,869: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/sponsors/demographics/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,873: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/sponsors/sponsor-services/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,873: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/sponsors/sponsor-services/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,878: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/sponsors/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,879: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/sponsors/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,882: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/jobs/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,882: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/jobs/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,886: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/privacy-policy/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,886: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/privacy-policy/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,890: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/hiringmixer/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,890: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/hiringmixer/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,894: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/events/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,894: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/events/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,899: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/hackathon/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:35,900: WARNING/Worker-3] 2016-02-11 14:43:35 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/hackathon/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:38,470: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/sponsors/2014-attendee-list/>
None
[2016-02-11 14:43:38,470: WARNING/Worker-3] 2016-02-11 14:43:38 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/sponsors/2014-attendee-list/>
None
[2016-02-11 14:43:39,534: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/terms-of-use/>
None
[2016-02-11 14:43:39,534: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/terms-of-use/>
None
[2016-02-11 14:43:39,589: DEBUG/Worker-3] Filtered offsite request to 'techinmotionevents.com': <GET http://techinmotionevents.com/>
[2016-02-11 14:43:39,589: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'techinmotionevents.com': <GET http://techinmotionevents.com/>
[2016-02-11 14:43:39,590: DEBUG/Worker-3] Filtered offsite request to 'www.anyonecanlearntocode.com': <GET http://www.anyonecanlearntocode.com/>
[2016-02-11 14:43:39,590: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.anyonecanlearntocode.com': <GET http://www.anyonecanlearntocode.com/>
[2016-02-11 14:43:39,590: DEBUG/Worker-3] Filtered offsite request to 'www.appacademy.io': <GET http://www.appacademy.io/>
[2016-02-11 14:43:39,590: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.appacademy.io': <GET http://www.appacademy.io/>
[2016-02-11 14:43:39,590: DEBUG/Worker-3] Filtered offsite request to 'brandgarage.com': <GET http://brandgarage.com/>
[2016-02-11 14:43:39,590: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'brandgarage.com': <GET http://brandgarage.com/>
[2016-02-11 14:43:39,591: DEBUG/Worker-3] Filtered offsite request to 'codeselfstudy.com': <GET http://codeselfstudy.com/>
[2016-02-11 14:43:39,591: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'codeselfstudy.com': <GET http://codeselfstudy.com/>
[2016-02-11 14:43:39,591: DEBUG/Worker-3] Filtered offsite request to 'www.clocate.com': <GET http://www.clocate.com/conferences/2016/>
[2016-02-11 14:43:39,591: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.clocate.com': <GET http://www.clocate.com/conferences/2016/>
[2016-02-11 14:43:39,591: DEBUG/Worker-3] Filtered offsite request to 'clocate.com': <GET http://clocate.com/>
[2016-02-11 14:43:39,591: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'clocate.com': <GET http://clocate.com/>
[2016-02-11 14:43:39,592: DEBUG/Worker-3] Filtered offsite request to 'crowdreviews.com': <GET http://crowdreviews.com/>
[2016-02-11 14:43:39,592: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'crowdreviews.com': <GET http://crowdreviews.com/>
[2016-02-11 14:43:39,592: DEBUG/Worker-3] Filtered offsite request to 'www.dxpertise.com': <GET https://www.dxpertise.com/>
[2016-02-11 14:43:39,592: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.dxpertise.com': <GET https://www.dxpertise.com/>
[2016-02-11 14:43:39,592: DEBUG/Worker-3] Filtered offsite request to 'www.f6s.com': <GET http://www.f6s.com/>
[2016-02-11 14:43:39,593: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.f6s.com': <GET http://www.f6s.com/>
[2016-02-11 14:43:39,593: DEBUG/Worker-3] Filtered offsite request to 'www.marketresearchmedia.com': <GET http://www.marketresearchmedia.com/>
[2016-02-11 14:43:39,593: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.marketresearchmedia.com': <GET http://www.marketresearchmedia.com/>
[2016-02-11 14:43:39,593: DEBUG/Worker-3] Filtered offsite request to 'www.marketanalysis.com': <GET http://www.marketanalysis.com/>
[2016-02-11 14:43:39,593: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.marketanalysis.com': <GET http://www.marketanalysis.com/>
[2016-02-11 14:43:39,593: DEBUG/Worker-3] Filtered offsite request to 'www.matter.vc': <GET http://www.matter.vc/>
[2016-02-11 14:43:39,594: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.matter.vc': <GET http://www.matter.vc/>
[2016-02-11 14:43:39,594: DEBUG/Worker-3] Filtered offsite request to 'www.methodsandtools.com': <GET http://www.methodsandtools.com/>
[2016-02-11 14:43:39,594: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.methodsandtools.com': <GET http://www.methodsandtools.com/>
[2016-02-11 14:43:39,594: DEBUG/Worker-3] Filtered offsite request to 'mobilewebdevconference.com': <GET http://mobilewebdevconference.com/>
[2016-02-11 14:43:39,594: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'mobilewebdevconference.com': <GET http://mobilewebdevconference.com/>
[2016-02-11 14:43:39,595: DEBUG/Worker-3] Filtered offsite request to 'www.openmobilealliance.org': <GET http://www.openmobilealliance.org/>
[2016-02-11 14:43:39,595: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.openmobilealliance.org': <GET http://www.openmobilealliance.org/>
[2016-02-11 14:43:39,595: DEBUG/Worker-3] Filtered offsite request to 'www.sandboxsuites.com': <GET http://www.sandboxsuites.com/>
[2016-02-11 14:43:39,595: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.sandboxsuites.com': <GET http://www.sandboxsuites.com/>
[2016-02-11 14:43:39,595: DEBUG/Worker-3] Filtered offsite request to 'www.wisher.com': <GET http://www.wisher.com/>
[2016-02-11 14:43:39,595: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.wisher.com': <GET http://www.wisher.com/>
[2016-02-11 14:43:39,596: DEBUG/Worker-3] Filtered offsite request to 'www.womenwhocode.com': <GET https://www.womenwhocode.com/>
[2016-02-11 14:43:39,596: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Filtered offsite request to 'www.womenwhocode.com': <GET https://www.womenwhocode.com/>
[2016-02-11 14:43:39,609: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/justification-letter/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,609: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/justification-letter/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,613: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/expo/exhibiting-companies/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,614: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/expo/exhibiting-companies/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,617: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/expo/exhibitor-resources/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,617: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/expo/exhibitor-resources/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,620: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/expo/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,620: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/expo/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,624: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/dev-evangelist-forum/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,624: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/dev-evangelist-forum/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,627: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/api-microservices-summit/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,627: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/api-microservices-summit/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,631: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/javascript-conference/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,631: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/javascript-conference/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,635: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/breakthrough-2016/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:39,635: WARNING/Worker-3] 2016-02-11 14:43:39 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/breakthrough-2016/> (referer: http://www.developerweek.com)
[2016-02-11 14:43:40,534: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/sponsors/demographics/>
None
[2016-02-11 14:43:40,535: WARNING/Worker-3] 2016-02-11 14:43:40 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/sponsors/demographics/>
None
[2016-02-11 14:43:41,462: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/sponsors/sponsor-services/>
None
[2016-02-11 14:43:41,462: WARNING/Worker-3] 2016-02-11 14:43:41 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/sponsors/sponsor-services/>
None
[2016-02-11 14:43:43,492: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/sponsors/>
None
[2016-02-11 14:43:43,492: WARNING/Worker-3] 2016-02-11 14:43:43 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/sponsors/>
None
[2016-02-11 14:43:44,501: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/jobs/>
None
[2016-02-11 14:43:44,501: WARNING/Worker-3] 2016-02-11 14:43:44 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/jobs/>
None
[2016-02-11 14:43:45,357: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/privacy-policy/>
None
[2016-02-11 14:43:45,358: WARNING/Worker-3] 2016-02-11 14:43:45 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/privacy-policy/>
None
[2016-02-11 14:43:46,920: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/hiringmixer/>
None
[2016-02-11 14:43:46,920: WARNING/Worker-3] 2016-02-11 14:43:46 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/hiringmixer/>
None
[2016-02-11 14:43:48,302: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/events/>
None
[2016-02-11 14:43:48,302: WARNING/Worker-3] 2016-02-11 14:43:48 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/events/>
None
[2016-02-11 14:43:50,384: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/hackathon/>
None
[2016-02-11 14:43:50,384: WARNING/Worker-3] 2016-02-11 14:43:50 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/hackathon/>
None
[2016-02-11 14:43:50,405: DEBUG/Worker-3] Filtered offsite request to 'lcweb.loc.gov': <GET http://lcweb.loc.gov/copyright/>
[2016-02-11 14:43:50,406: WARNING/Worker-3] 2016-02-11 14:43:50 [scrapy] DEBUG: Filtered offsite request to 'lcweb.loc.gov': <GET http://lcweb.loc.gov/copyright/>
[2016-02-11 14:43:53,579: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/justification-letter/>
None
[2016-02-11 14:43:53,580: WARNING/Worker-3] 2016-02-11 14:43:53 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/justification-letter/>
None
[2016-02-11 14:43:55,240: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/expo/exhibiting-companies/>
None
[2016-02-11 14:43:55,241: WARNING/Worker-3] 2016-02-11 14:43:55 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/expo/exhibiting-companies/>
None
[2016-02-11 14:43:56,261: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/expo/exhibitor-resources/>
None
[2016-02-11 14:43:56,261: WARNING/Worker-3] 2016-02-11 14:43:56 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/expo/exhibitor-resources/>
None
[2016-02-11 14:43:57,078: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/expo/>
None
[2016-02-11 14:43:57,079: WARNING/Worker-3] 2016-02-11 14:43:57 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/expo/>
None
[2016-02-11 14:43:58,105: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/dev-evangelist-forum/>
None
[2016-02-11 14:43:58,105: WARNING/Worker-3] 2016-02-11 14:43:58 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/dev-evangelist-forum/>
None
[2016-02-11 14:43:59,183: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/api-microservices-summit/>
None
[2016-02-11 14:43:59,184: WARNING/Worker-3] 2016-02-11 14:43:59 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/api-microservices-summit/>
None
[2016-02-11 14:44:00,422: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/javascript-conference/>
None
[2016-02-11 14:44:00,422: WARNING/Worker-3] 2016-02-11 14:44:00 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/javascript-conference/>
None
[2016-02-11 14:44:01,460: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/breakthrough-2016/>
None
[2016-02-11 14:44:01,460: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/breakthrough-2016/>
None
[2016-02-11 14:44:01,489: DEBUG/Worker-3] Filtered offsite request to 'bit.ly': <GET http://bit.ly/HackathonResources>
[2016-02-11 14:44:01,489: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'bit.ly': <GET http://bit.ly/HackathonResources>
[2016-02-11 14:44:01,489: DEBUG/Worker-3] Filtered offsite request to 'www.netapp.com': <GET http://www.netapp.com/>
[2016-02-11 14:44:01,490: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.netapp.com': <GET http://www.netapp.com/>
[2016-02-11 14:44:01,490: DEBUG/Worker-3] Filtered offsite request to 'circleci.com': <GET http://circleci.com/>
[2016-02-11 14:44:01,491: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'circleci.com': <GET http://circleci.com/>
[2016-02-11 14:44:01,491: DEBUG/Worker-3] Filtered offsite request to 'data-science-summit.org': <GET http://data-science-summit.org/>
[2016-02-11 14:44:01,491: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'data-science-summit.org': <GET http://data-science-summit.org/>
[2016-02-11 14:44:01,492: DEBUG/Worker-3] Filtered offsite request to 'www.fanaticsauthentic.com': <GET http://www.fanaticsauthentic.com/>
[2016-02-11 14:44:01,492: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.fanaticsauthentic.com': <GET http://www.fanaticsauthentic.com/>
[2016-02-11 14:44:01,494: DEBUG/Worker-3] Filtered offsite request to 'www.enginethemes.com': <GET http://www.enginethemes.com/themes/jobengine>
[2016-02-11 14:44:01,494: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.enginethemes.com': <GET http://www.enginethemes.com/themes/jobengine>
[2016-02-11 14:44:01,495: DEBUG/Worker-3] Filtered offsite request to 'wordpress.org': <GET http://wordpress.org/>
[2016-02-11 14:44:01,495: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'wordpress.org': <GET http://wordpress.org/>
[2016-02-11 14:44:01,505: DEBUG/Worker-3] Filtered offsite request to 'www.google.com': <GET http://www.google.com/ads/preferences/>
[2016-02-11 14:44:01,505: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.google.com': <GET http://www.google.com/ads/preferences/>
[2016-02-11 14:44:01,505: DEBUG/Worker-3] Filtered offsite request to 'www.networkadvertising.org': <GET http://www.networkadvertising.org/choices/>
[2016-02-11 14:44:01,505: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.networkadvertising.org': <GET http://www.networkadvertising.org/choices/>
[2016-02-11 14:44:01,506: DEBUG/Worker-3] Filtered offsite request to 'www.aboutads.info': <GET http://www.aboutads.info/choices/>
[2016-02-11 14:44:01,506: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.aboutads.info': <GET http://www.aboutads.info/choices/>
[2016-02-11 14:44:01,515: DEBUG/Worker-3] Filtered offsite request to 'mkt.com': <GET https://mkt.com/devnetwork>
[2016-02-11 14:44:01,515: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'mkt.com': <GET https://mkt.com/devnetwork>
[2016-02-11 14:44:01,524: DEBUG/Worker-3] Filtered offsite request to 'usertestingmobilelaunch.eventbrite.com': <GET https://usertestingmobilelaunch.eventbrite.com/>
[2016-02-11 14:44:01,525: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'usertestingmobilelaunch.eventbrite.com': <GET https://usertestingmobilelaunch.eventbrite.com/>
[2016-02-11 14:44:01,525: DEBUG/Worker-3] Filtered offsite request to 'dontplaygameswithyourcode.eventbrite.com': <GET https://dontplaygameswithyourcode.eventbrite.com/>
[2016-02-11 14:44:01,525: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'dontplaygameswithyourcode.eventbrite.com': <GET https://dontplaygameswithyourcode.eventbrite.com/>
[2016-02-11 14:44:01,525: DEBUG/Worker-3] Filtered offsite request to 'www.meetup.com': <GET http://www.meetup.com/San-Francisco-Redis-Meetup/events/228620961/>
[2016-02-11 14:44:01,525: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.meetup.com': <GET http://www.meetup.com/San-Francisco-Redis-Meetup/events/228620961/>
[2016-02-11 14:44:01,526: DEBUG/Worker-3] Filtered offsite request to 'www.eventbrite.com': <GET https://www.eventbrite.com/e/developer-open-house-registration-21356774688>
[2016-02-11 14:44:01,526: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.eventbrite.com': <GET https://www.eventbrite.com/e/developer-open-house-registration-21356774688>
[2016-02-11 14:44:01,526: DEBUG/Worker-3] Filtered offsite request to 'clouddbdaysf.eventbrite.com': <GET http://clouddbdaysf.eventbrite.com/?aff=devweek>
[2016-02-11 14:44:01,526: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'clouddbdaysf.eventbrite.com': <GET http://clouddbdaysf.eventbrite.com/?aff=devweek>
[2016-02-11 14:44:01,534: DEBUG/Worker-3] Filtered offsite request to 'accelerate.im': <GET http://accelerate.im/Challenges/>
[2016-02-11 14:44:01,535: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'accelerate.im': <GET http://accelerate.im/Challenges/>
[2016-02-11 14:44:01,535: DEBUG/Worker-3] Filtered offsite request to 'www.dji.com': <GET http://www.dji.com/>
[2016-02-11 14:44:01,535: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.dji.com': <GET http://www.dji.com/>
[2016-02-11 14:44:01,536: DEBUG/Worker-3] Filtered offsite request to 'www.gupshup.me': <GET http://www.gupshup.me/>
[2016-02-11 14:44:01,536: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.gupshup.me': <GET http://www.gupshup.me/>
[2016-02-11 14:44:01,537: DEBUG/Worker-3] Filtered offsite request to 'hackcodeofconduct.org': <GET http://hackcodeofconduct.org/>
[2016-02-11 14:44:01,537: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'hackcodeofconduct.org': <GET http://hackcodeofconduct.org/>
[2016-02-11 14:44:01,541: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/conference-schedule/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,542: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/conference-schedule/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,545: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/live-coding-competition/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,545: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/live-coding-competition/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,548: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/conference-tracks/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,548: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/conference-tracks/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,552: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/about/volunteer/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,552: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/about/volunteer/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,555: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/pass-types/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,555: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/pass-types/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,558: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/speakers/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,558: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/speakers/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,561: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/apply-to-speak/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,562: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/apply-to-speak/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,566: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/conference/speaker-services/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,566: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/conference/speaker-services/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:01,566: INFO/Worker-3] Closing spider (closespider_timeout)
[2016-02-11 14:44:01,566: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] INFO: Closing spider (closespider_timeout)
[2016-02-11 14:44:01,702: DEBUG/Worker-3] Filtered offsite request to 'www.ringcentral.com': <GET http://www.ringcentral.com/office/developer-platform.html>
[2016-02-11 14:44:01,703: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.ringcentral.com': <GET http://www.ringcentral.com/office/developer-platform.html>
[2016-02-11 14:44:01,703: DEBUG/Worker-3] Filtered offsite request to 'ringcentral.com': <GET http://ringcentral.com/>
[2016-02-11 14:44:01,703: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'ringcentral.com': <GET http://ringcentral.com/>
[2016-02-11 14:44:01,710: DEBUG/Worker-3] Filtered offsite request to 'thetownkitchen.typeform.com': <GET https://thetownkitchen.typeform.com/to/rTeQxu>
[2016-02-11 14:44:01,710: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'thetownkitchen.typeform.com': <GET https://thetownkitchen.typeform.com/to/rTeQxu>
[2016-02-11 14:44:01,771: DEBUG/Worker-3] Filtered offsite request to 'getalkanet.com': <GET http://getalkanet.com/>
[2016-02-11 14:44:01,771: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'getalkanet.com': <GET http://getalkanet.com/>
[2016-02-11 14:44:01,772: DEBUG/Worker-3] Filtered offsite request to 'alkanet': <GET http://alkanet/>
[2016-02-11 14:44:01,772: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'alkanet': <GET http://alkanet/>
[2016-02-11 14:44:01,772: DEBUG/Worker-3] Filtered offsite request to 'www.cocooncam.com': <GET http://www.cocooncam.com/>
[2016-02-11 14:44:01,772: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.cocooncam.com': <GET http://www.cocooncam.com/>
[2016-02-11 14:44:01,772: DEBUG/Worker-3] Filtered offsite request to 'www.dropdrone.co': <GET http://www.dropdrone.co/>
[2016-02-11 14:44:01,772: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.dropdrone.co': <GET http://www.dropdrone.co/>
[2016-02-11 14:44:01,773: DEBUG/Worker-3] Filtered offsite request to 'freeskies.co': <GET http://freeskies.co/>
[2016-02-11 14:44:01,773: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'freeskies.co': <GET http://freeskies.co/>
[2016-02-11 14:44:01,773: DEBUG/Worker-3] Filtered offsite request to 'hyperdrive.me': <GET http://hyperdrive.me/>
[2016-02-11 14:44:01,773: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'hyperdrive.me': <GET http://hyperdrive.me/>
[2016-02-11 14:44:01,773: DEBUG/Worker-3] Filtered offsite request to 'www.ondavia.com': <GET http://www.ondavia.com/>
[2016-02-11 14:44:01,774: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.ondavia.com': <GET http://www.ondavia.com/>
[2016-02-11 14:44:01,774: DEBUG/Worker-3] Filtered offsite request to 'playpiper.com': <GET http://playpiper.com/>
[2016-02-11 14:44:01,774: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'playpiper.com': <GET http://playpiper.com/>
[2016-02-11 14:44:01,774: DEBUG/Worker-3] Filtered offsite request to 'www.saaspass.com': <GET http://www.saaspass.com/>
[2016-02-11 14:44:01,774: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.saaspass.com': <GET http://www.saaspass.com/>
[2016-02-11 14:44:01,774: DEBUG/Worker-3] Filtered offsite request to 'www.soundfit.co': <GET http://www.soundfit.co/>
[2016-02-11 14:44:01,775: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'www.soundfit.co': <GET http://www.soundfit.co/>
[2016-02-11 14:44:01,775: DEBUG/Worker-3] Filtered offsite request to 'stamplay.com': <GET https://stamplay.com/>
[2016-02-11 14:44:01,775: WARNING/Worker-3] 2016-02-11 14:44:01 [scrapy] DEBUG: Filtered offsite request to 'stamplay.com': <GET https://stamplay.com/>
[2016-02-11 14:44:04,884: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/conference-schedule/>
None
[2016-02-11 14:44:04,885: WARNING/Worker-3] 2016-02-11 14:44:04 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/conference-schedule/>
None
[2016-02-11 14:44:05,895: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/live-coding-competition/>
None
[2016-02-11 14:44:05,895: WARNING/Worker-3] 2016-02-11 14:44:05 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/live-coding-competition/>
None
[2016-02-11 14:44:06,798: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/conference-tracks/>
None
[2016-02-11 14:44:06,799: WARNING/Worker-3] 2016-02-11 14:44:06 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/conference-tracks/>
None
[2016-02-11 14:44:07,596: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/about/volunteer/>
None
[2016-02-11 14:44:07,597: WARNING/Worker-3] 2016-02-11 14:44:07 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/about/volunteer/>
None
[2016-02-11 14:44:08,362: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/pass-types/>
None
[2016-02-11 14:44:08,363: WARNING/Worker-3] 2016-02-11 14:44:08 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/pass-types/>
None
[2016-02-11 14:44:09,233: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/speakers/>
None
[2016-02-11 14:44:09,233: WARNING/Worker-3] 2016-02-11 14:44:09 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/speakers/>
None
[2016-02-11 14:44:10,028: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/apply-to-speak/>
None
[2016-02-11 14:44:10,028: WARNING/Worker-3] 2016-02-11 14:44:10 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/apply-to-speak/>
None
[2016-02-11 14:44:11,038: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/conference/speaker-services/>
None
[2016-02-11 14:44:11,038: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/conference/speaker-services/>
None
[2016-02-11 14:44:11,045: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/about/contact/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,046: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/about/contact/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,049: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/about/venue/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,049: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/about/venue/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,052: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/about/2016-attendee-guide/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,053: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/about/2016-attendee-guide/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,056: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/about/travel/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,056: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/about/travel/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,059: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/about/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,059: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/about/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,062: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/jobs/?job_type=internship> (referer: http://www.developerweek.com/jobs/)
[2016-02-11 14:44:11,062: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/jobs/?job_type=internship> (referer: http://www.developerweek.com/jobs/)
[2016-02-11 14:44:11,065: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/about/team/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,066: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/about/team/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,070: DEBUG/Worker-3] Crawled (200) <GET http://www.developerweek.com/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:11,070: WARNING/Worker-3] 2016-02-11 14:44:11 [scrapy] DEBUG: Crawled (200) <GET http://www.developerweek.com/> (referer: http://www.developerweek.com)
[2016-02-11 14:44:12,070: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/about/contact/>
None
[2016-02-11 14:44:12,071: WARNING/Worker-3] 2016-02-11 14:44:12 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/about/contact/>
None
[2016-02-11 14:44:12,959: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/about/venue/>
None
[2016-02-11 14:44:12,959: WARNING/Worker-3] 2016-02-11 14:44:12 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/about/venue/>
None
[2016-02-11 14:44:13,870: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/about/2016-attendee-guide/>
None
[2016-02-11 14:44:13,870: WARNING/Worker-3] 2016-02-11 14:44:13 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/about/2016-attendee-guide/>
None
[2016-02-11 14:44:14,686: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/about/travel/>
None
[2016-02-11 14:44:14,686: WARNING/Worker-3] 2016-02-11 14:44:14 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/about/travel/>
None
[2016-02-11 14:44:15,702: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/about/>
None
[2016-02-11 14:44:15,702: WARNING/Worker-3] 2016-02-11 14:44:15 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/about/>
None
[2016-02-11 14:44:16,425: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/jobs/?job_type=internship>
None
[2016-02-11 14:44:16,426: WARNING/Worker-3] 2016-02-11 14:44:16 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/jobs/?job_type=internship>
None
[2016-02-11 14:44:17,358: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/about/team/>
None
[2016-02-11 14:44:17,358: WARNING/Worker-3] 2016-02-11 14:44:17 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/about/team/>
None
[2016-02-11 14:44:19,205: DEBUG/Worker-3] Scraped from <200 http://www.developerweek.com/>
None
[2016-02-11 14:44:19,205: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Scraped from <200 http://www.developerweek.com/>
None
[2016-02-11 14:44:19,270: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs 
[2016-02-11 14:44:19,270: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs
[2016-02-11 14:44:19,270: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?post_type=job 
[2016-02-11 14:44:19,270: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?post_type=job
[2016-02-11 14:44:19,271: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=6 
[2016-02-11 14:44:19,271: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=6
[2016-02-11 14:44:19,271: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?post_type=resume 
[2016-02-11 14:44:19,271: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?post_type=resume
[2016-02-11 14:44:19,271: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=8 
[2016-02-11 14:44:19,271: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=8
[2016-02-11 14:44:19,271: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=internship 
[2016-02-11 14:44:19,272: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=internship
[2016-02-11 14:44:19,272: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=full-time 
[2016-02-11 14:44:19,272: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=full-time
[2016-02-11 14:44:19,272: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=part-time 
[2016-02-11 14:44:19,272: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=part-time
[2016-02-11 14:44:19,272: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=contract 
[2016-02-11 14:44:19,273: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_type=contract
[2016-02-11 14:44:19,273: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=full-time 
[2016-02-11 14:44:19,273: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=full-time
[2016-02-11 14:44:19,273: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=part-time 
[2016-02-11 14:44:19,273: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=part-time
[2016-02-11 14:44:19,273: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=development 
[2016-02-11 14:44:19,273: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=development
[2016-02-11 14:44:19,274: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=contract 
[2016-02-11 14:44:19,274: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=contract
[2016-02-11 14:44:19,274: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=sales 
[2016-02-11 14:44:19,274: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=sales
[2016-02-11 14:44:19,274: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=marketing 
[2016-02-11 14:44:19,274: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?job_category=marketing
[2016-02-11 14:44:19,274: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?action=twitterauth 
[2016-02-11 14:44:19,275: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?action=twitterauth
[2016-02-11 14:44:19,275: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=14 
[2016-02-11 14:44:19,275: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=14
[2016-02-11 14:44:19,275: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.enginethemes.com/themes/jobengine 
[2016-02-11 14:44:19,275: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.enginethemes.com/themes/jobengine
[2016-02-11 14:44:19,275: DEBUG/Worker-3] Ignoring link (depth > 2): http://wordpress.org/ 
[2016-02-11 14:44:19,276: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://wordpress.org/
[2016-02-11 14:44:19,276: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?feed=rss2 
[2016-02-11 14:44:19,276: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?feed=rss2
[2016-02-11 14:44:19,276: DEBUG/Worker-3] Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=18 
[2016-02-11 14:44:19,276: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Ignoring link (depth > 2): http://www.developerweek.com/jobs/?page_id=18
[2016-02-11 14:44:19,345: DEBUG/Worker-3] Filtered offsite request to 'www.viceroyhotelsandresorts.com': <GET http://www.viceroyhotelsandresorts.com/en/zetta>
[2016-02-11 14:44:19,345: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.viceroyhotelsandresorts.com': <GET http://www.viceroyhotelsandresorts.com/en/zetta>
[2016-02-11 14:44:19,346: DEBUG/Worker-3] Filtered offsite request to 'gc.synxis.com': <GET https://gc.synxis.com/rez.aspx?Dest=VUR&_ga=1.138602225.352904058.1415044359&adult=1&arrive=11%2F11%2F2014&chain=1003&child=0&depart=11%2F12%2F2014&hotel=56806&lang=1&rooms=1&shell=UR_Flex2&start=availresults&template=UR_Flex>
[2016-02-11 14:44:19,346: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'gc.synxis.com': <GET https://gc.synxis.com/rez.aspx?Dest=VUR&_ga=1.138602225.352904058.1415044359&adult=1&arrive=11%2F11%2F2014&chain=1003&child=0&depart=11%2F12%2F2014&hotel=56806&lang=1&rooms=1&shell=UR_Flex2&start=availresults&template=UR_Flex>
[2016-02-11 14:44:19,346: DEBUG/Worker-3] Filtered offsite request to 'sanfranciscoregency.hyatt.com': <GET http://sanfranciscoregency.hyatt.com/en/hotel/home.html>
[2016-02-11 14:44:19,346: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'sanfranciscoregency.hyatt.com': <GET http://sanfranciscoregency.hyatt.com/en/hotel/home.html>
[2016-02-11 14:44:19,346: DEBUG/Worker-3] Filtered offsite request to 'www3.hilton.com': <GET http://www3.hilton.com/en/hotels/california/hilton-san-francisco-union-square-SFOFHHH/index.html>
[2016-02-11 14:44:19,346: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www3.hilton.com': <GET http://www3.hilton.com/en/hotels/california/hilton-san-francisco-union-square-SFOFHHH/index.html>
[2016-02-11 14:44:19,347: DEBUG/Worker-3] Filtered offsite request to 'www.wsanfrancisco.com': <GET http://www.wsanfrancisco.com/>
[2016-02-11 14:44:19,347: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.wsanfrancisco.com': <GET http://www.wsanfrancisco.com/>
[2016-02-11 14:44:19,347: DEBUG/Worker-3] Filtered offsite request to 'www.fourseasons.com': <GET http://www.fourseasons.com/sanfrancisco/>
[2016-02-11 14:44:19,347: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.fourseasons.com': <GET http://www.fourseasons.com/sanfrancisco/>
[2016-02-11 14:44:19,347: DEBUG/Worker-3] Filtered offsite request to 'www.hotelnikkosf.com': <GET http://www.hotelnikkosf.com/>
[2016-02-11 14:44:19,348: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.hotelnikkosf.com': <GET http://www.hotelnikkosf.com/>
[2016-02-11 14:44:19,348: DEBUG/Worker-3] Filtered offsite request to 'www.sheratonatthewharf.com': <GET http://www.sheratonatthewharf.com/>
[2016-02-11 14:44:19,348: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.sheratonatthewharf.com': <GET http://www.sheratonatthewharf.com/>
[2016-02-11 14:44:19,348: DEBUG/Worker-3] Filtered offsite request to 'travel.state.gov': <GET http://travel.state.gov/content/visas/en/business/business-visa-center.html>
[2016-02-11 14:44:19,348: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'travel.state.gov': <GET http://travel.state.gov/content/visas/en/business/business-visa-center.html>
[2016-02-11 14:44:19,349: DEBUG/Worker-3] Filtered offsite request to 'www.usembassy.gov': <GET http://www.usembassy.gov/>
[2016-02-11 14:44:19,349: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.usembassy.gov': <GET http://www.usembassy.gov/>
[2016-02-11 14:44:19,349: DEBUG/Worker-3] Filtered offsite request to 'www.ustraveldocs.com': <GET http://www.ustraveldocs.com/in/in-gen-faq.asp>
[2016-02-11 14:44:19,349: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.ustraveldocs.com': <GET http://www.ustraveldocs.com/in/in-gen-faq.asp>
[2016-02-11 14:44:19,356: DEBUG/Worker-3] Filtered offsite request to 'google.com': <GET http://google.com/>
[2016-02-11 14:44:19,356: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'google.com': <GET http://google.com/>
[2016-02-11 14:44:19,357: DEBUG/Worker-3] Filtered offsite request to 'www.yelp.com': <GET http://www.yelp.com/>
[2016-02-11 14:44:19,357: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'www.yelp.com': <GET http://www.yelp.com/>
[2016-02-11 14:44:19,357: DEBUG/Worker-3] Filtered offsite request to 'rackspace.com': <GET http://rackspace.com/>
[2016-02-11 14:44:19,357: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'rackspace.com': <GET http://rackspace.com/>
[2016-02-11 14:44:19,357: DEBUG/Worker-3] Filtered offsite request to 'ibm.com': <GET http://ibm.com/>
[2016-02-11 14:44:19,358: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'ibm.com': <GET http://ibm.com/>
[2016-02-11 14:44:19,358: DEBUG/Worker-3] Filtered offsite request to 'cloudera.com': <GET http://cloudera.com/>
[2016-02-11 14:44:19,358: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'cloudera.com': <GET http://cloudera.com/>
[2016-02-11 14:44:19,358: DEBUG/Worker-3] Filtered offsite request to 'redhat.com': <GET http://redhat.com/>
[2016-02-11 14:44:19,358: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'redhat.com': <GET http://redhat.com/>
[2016-02-11 14:44:19,358: DEBUG/Worker-3] Filtered offsite request to 'optimizely.com': <GET http://optimizely.com/>
[2016-02-11 14:44:19,359: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'optimizely.com': <GET http://optimizely.com/>
[2016-02-11 14:44:19,359: DEBUG/Worker-3] Filtered offsite request to 'sendgrid.com': <GET http://sendgrid.com/>
[2016-02-11 14:44:19,359: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'sendgrid.com': <GET http://sendgrid.com/>
[2016-02-11 14:44:19,359: DEBUG/Worker-3] Filtered offsite request to 'blackberry.com': <GET http://blackberry.com/>
[2016-02-11 14:44:19,359: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'blackberry.com': <GET http://blackberry.com/>
[2016-02-11 14:44:19,360: DEBUG/Worker-3] Filtered offsite request to 'neotechnology.com': <GET http://neotechnology.com/>
[2016-02-11 14:44:19,360: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'neotechnology.com': <GET http://neotechnology.com/>
[2016-02-11 14:44:19,360: DEBUG/Worker-3] Filtered offsite request to 'eventbrite.com': <GET http://eventbrite.com/>
[2016-02-11 14:44:19,360: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'eventbrite.com': <GET http://eventbrite.com/>
[2016-02-11 14:44:19,360: DEBUG/Worker-3] Filtered offsite request to 'klout.com': <GET http://klout.com/>
[2016-02-11 14:44:19,360: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'klout.com': <GET http://klout.com/>
[2016-02-11 14:44:19,361: DEBUG/Worker-3] Filtered offsite request to 'built.io': <GET http://built.io/>
[2016-02-11 14:44:19,361: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'built.io': <GET http://built.io/>
[2016-02-11 14:44:19,361: DEBUG/Worker-3] Filtered offsite request to 'ripple.com': <GET http://ripple.com/>
[2016-02-11 14:44:19,361: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'ripple.com': <GET http://ripple.com/>
[2016-02-11 14:44:19,361: DEBUG/Worker-3] Filtered offsite request to 'gnip.com': <GET http://gnip.com/>
[2016-02-11 14:44:19,362: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'gnip.com': <GET http://gnip.com/>
[2016-02-11 14:44:19,362: DEBUG/Worker-3] Filtered offsite request to 'tagged.com': <GET http://tagged.com/>
[2016-02-11 14:44:19,362: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'tagged.com': <GET http://tagged.com/>
[2016-02-11 14:44:19,362: DEBUG/Worker-3] Filtered offsite request to 'hackreactor.com': <GET http://hackreactor.com/>
[2016-02-11 14:44:19,362: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'hackreactor.com': <GET http://hackreactor.com/>
[2016-02-11 14:44:19,372: DEBUG/Worker-3] Filtered offsite request to 'twitter.com': <GET http://twitter.com/geoffdomo>
[2016-02-11 14:44:19,372: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'twitter.com': <GET http://twitter.com/geoffdomo>
[2016-02-11 14:44:19,372: DEBUG/Worker-3] Filtered offsite request to 'techweek.com': <GET http://techweek.com/>
[2016-02-11 14:44:19,372: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] DEBUG: Filtered offsite request to 'techweek.com': <GET http://techweek.com/>
[2016-02-11 14:44:19,527: INFO/Worker-3] Dumping Scrapy stats:
{'downloader/request_bytes': 17944,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 914677,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 43,
 'downloader/response_status_count/301': 7,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 1859,
 'finish_reason': 'closespider_timeout',
 'finish_time': datetime.datetime(2016, 2, 11, 14, 44, 19, 398841),
 'item_scraped_count': 42,
 'log_count/DEBUG': 277,
 'log_count/INFO': 7,
 'log_count/WARNING': 284,
 'offsite/domains': 160,
 'offsite/filtered': 764,
 'request_depth_max': 2,
 'response_received_count': 44,
 'scheduler/dequeued': 51,
 'scheduler/dequeued/memory': 51,
 'scheduler/enqueued': 93,
 'scheduler/enqueued/memory': 93,
 'start_time': datetime.datetime(2016, 2, 11, 14, 43, 20, 673911)}
[2016-02-11 14:44:19,527: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 17944,
 'downloader/request_count': 51,
 'downloader/request_method_count/GET': 51,
 'downloader/response_bytes': 914677,
 'downloader/response_count': 51,
 'downloader/response_status_count/200': 43,
 'downloader/response_status_count/301': 7,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 1859,
 'finish_reason': 'closespider_timeout',
 'finish_time': datetime.datetime(2016, 2, 11, 14, 44, 19, 398841),
 'item_scraped_count': 42,
 'log_count/DEBUG': 277,
 'log_count/INFO': 7,
 'log_count/WARNING': 284,
 'offsite/domains': 160,
 'offsite/filtered': 764,
 'request_depth_max': 2,
 'response_received_count': 44,
 'scheduler/dequeued': 51,
 'scheduler/dequeued/memory': 51,
 'scheduler/enqueued': 93,
 'scheduler/enqueued/memory': 93,
 'start_time': datetime.datetime(2016, 2, 11, 14, 43, 20, 673911)}
[2016-02-11 14:44:19,527: INFO/Worker-3] Spider closed (closespider_timeout)
[2016-02-11 14:44:19,527: WARNING/Worker-3] 2016-02-11 14:44:19 [scrapy] INFO: Spider closed (closespider_timeout)
[2016-02-11 14:44:19,531: WARNING/Worker-3] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-11 14:44:19,532: WARNING/Worker-3] 2016-02-11 14:44:19 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-11 14:44:19,533: CRITICAL/MainProcess] Task app.scrape[1f44a22d-7c23-4ef6-bd60-caff8a19a5e7] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-12 11:13:49,527: INFO/MainProcess] Received task: app.scrape[616c0377-1c1d-4220-9bc8-b454747ea4fe]
[2016-02-12 11:13:49,533: INFO/Worker-4] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-12 11:13:49,533: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-12 11:13:49,533: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-12 11:13:49,533: INFO/Worker-4] Optional features available: ssl, http11, boto
[2016-02-12 11:13:49,533: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-12 11:13:49,534: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-12 11:13:49,534: INFO/Worker-4] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-12 11:13:49,534: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-12 11:13:49,534: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-12 11:13:49,539: INFO/Worker-4] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-12 11:13:49,539: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-12 11:13:49,539: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-12 11:13:49,541: INFO/Worker-4] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-12 11:13:49,541: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-12 11:13:49,541: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-12 11:13:49,542: INFO/Worker-4] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-12 11:13:49,542: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-12 11:13:49,542: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-12 11:13:49,762: INFO/Worker-4] Enabled item pipelines: UserInputPipeline
[2016-02-12 11:13:49,763: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-12 11:13:49,763: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-12 11:13:49,763: INFO/Worker-4] Spider opened
[2016-02-12 11:13:49,763: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Spider opened
[2016-02-12 11:13:49,763: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Spider opened
[2016-02-12 11:13:49,764: INFO/Worker-4] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-12 11:13:49,765: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-12 11:13:49,765: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-12 11:13:49,766: DEBUG/Worker-4] Telnet console listening on 127.0.0.1:6023
[2016-02-12 11:13:49,766: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
[2016-02-12 11:13:49,766: WARNING/Worker-4] 2016-02-12 11:13:49 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
[2016-02-12 11:13:49,770: WARNING/Worker-4] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-12 11:13:49,770: WARNING/Worker-4] 2016-02-12 11:13:49 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-12 11:13:49,770: WARNING/Worker-4] 2016-02-12 11:13:49 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-12 11:13:49,772: CRITICAL/MainProcess] Task app.scrape[616c0377-1c1d-4220-9bc8-b454747ea4fe] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-17 14:49:29,201: INFO/MainProcess] Received task: app.scrape[740330e8-cb1f-48c1-bbdc-8e45ee399c0e]
[2016-02-17 14:49:29,206: INFO/Worker-2] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-17 14:49:29,206: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-17 14:49:29,207: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-17 14:49:29,207: INFO/Worker-2] Optional features available: ssl, http11, boto
[2016-02-17 14:49:29,207: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-17 14:49:29,207: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-17 14:49:29,207: INFO/Worker-2] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-17 14:49:29,208: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-17 14:49:29,208: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-17 14:49:29,212: INFO/Worker-2] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-17 14:49:29,212: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-17 14:49:29,212: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-17 14:49:29,213: INFO/Worker-2] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-17 14:49:29,214: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-17 14:49:29,214: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-17 14:49:29,214: INFO/Worker-2] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-17 14:49:29,214: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-17 14:49:29,215: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-17 14:49:29,430: INFO/Worker-2] Enabled item pipelines: UserInputPipeline
[2016-02-17 14:49:29,430: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-17 14:49:29,431: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-17 14:49:29,431: INFO/Worker-2] Spider opened
[2016-02-17 14:49:29,431: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Spider opened
[2016-02-17 14:49:29,431: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Spider opened
[2016-02-17 14:49:29,432: INFO/Worker-2] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-17 14:49:29,432: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-17 14:49:29,432: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-17 14:49:29,433: DEBUG/Worker-2] Telnet console listening on 127.0.0.1:6024
[2016-02-17 14:49:29,433: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024
[2016-02-17 14:49:29,433: WARNING/Worker-2] 2016-02-17 14:49:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6024
[2016-02-17 14:49:29,435: WARNING/Worker-2] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-17 14:49:29,435: WARNING/Worker-2] 2016-02-17 14:49:29 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-17 14:49:29,436: WARNING/Worker-2] 2016-02-17 14:49:29 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-17 14:49:29,437: CRITICAL/MainProcess] Task app.scrape[740330e8-cb1f-48c1-bbdc-8e45ee399c0e] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-17 14:52:18,615: INFO/MainProcess] Received task: app.scrape[0813dc43-b481-4f43-84b6-36daf1217368]
[2016-02-17 14:52:18,620: INFO/Worker-1] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-17 14:52:18,620: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-17 14:52:18,621: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-17 14:52:18,621: INFO/Worker-1] Optional features available: ssl, http11, boto
[2016-02-17 14:52:18,621: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-17 14:52:18,621: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-17 14:52:18,621: INFO/Worker-1] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-17 14:52:18,621: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-17 14:52:18,622: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-17 14:52:18,626: INFO/Worker-1] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-17 14:52:18,626: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-17 14:52:18,626: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-17 14:52:18,627: INFO/Worker-1] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-17 14:52:18,628: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-17 14:52:18,628: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-17 14:52:18,628: INFO/Worker-1] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-17 14:52:18,629: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-17 14:52:18,629: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-17 14:52:18,846: INFO/Worker-1] Enabled item pipelines: UserInputPipeline
[2016-02-17 14:52:18,846: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-17 14:52:18,846: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-17 14:52:18,847: INFO/Worker-1] Spider opened
[2016-02-17 14:52:18,847: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Spider opened
[2016-02-17 14:52:18,847: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Spider opened
[2016-02-17 14:52:18,847: INFO/Worker-1] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-17 14:52:18,848: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-17 14:52:18,848: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-17 14:52:18,849: DEBUG/Worker-1] Telnet console listening on 127.0.0.1:6025
[2016-02-17 14:52:18,849: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
[2016-02-17 14:52:18,849: WARNING/Worker-1] 2016-02-17 14:52:18 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6025
[2016-02-17 14:52:18,851: WARNING/Worker-1] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-17 14:52:18,851: WARNING/Worker-1] 2016-02-17 14:52:18 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-17 14:52:18,852: WARNING/Worker-1] 2016-02-17 14:52:18 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-17 14:52:18,853: CRITICAL/MainProcess] Task app.scrape[0813dc43-b481-4f43-84b6-36daf1217368] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-19 14:56:31,505: INFO/MainProcess] Received task: app.scrape[1b77bdfd-2605-43bf-af21-fa5fb94b5758]
[2016-02-19 14:56:31,511: INFO/Worker-3] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-19 14:56:31,511: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-19 14:56:31,511: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-19 14:56:31,511: INFO/Worker-3] Optional features available: ssl, http11, boto
[2016-02-19 14:56:31,511: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-19 14:56:31,512: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-19 14:56:31,512: INFO/Worker-3] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-19 14:56:31,512: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-19 14:56:31,512: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-19 14:56:31,517: INFO/Worker-3] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-19 14:56:31,518: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-19 14:56:31,518: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-19 14:56:31,519: INFO/Worker-3] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-19 14:56:31,519: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-19 14:56:31,519: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-19 14:56:31,520: INFO/Worker-3] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-19 14:56:31,520: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-19 14:56:31,520: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-19 14:56:31,729: INFO/Worker-3] Enabled item pipelines: UserInputPipeline
[2016-02-19 14:56:31,729: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-19 14:56:31,729: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-19 14:56:31,730: INFO/Worker-3] Spider opened
[2016-02-19 14:56:31,730: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Spider opened
[2016-02-19 14:56:31,730: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Spider opened
[2016-02-19 14:56:31,731: INFO/Worker-3] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-19 14:56:31,731: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-19 14:56:31,731: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-19 14:56:31,732: DEBUG/Worker-3] Telnet console listening on 127.0.0.1:6026
[2016-02-19 14:56:31,732: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6026
[2016-02-19 14:56:31,733: WARNING/Worker-3] 2016-02-19 14:56:31 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6026
[2016-02-19 14:56:31,734: WARNING/Worker-3] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2016-02-19 14:56:31,734: WARNING/Worker-3] 2016-02-19 14:56:31 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-19 14:56:31,735: WARNING/Worker-3] 2016-02-19 14:56:31 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2016-02-19 14:56:31,736: CRITICAL/MainProcess] Task app.scrape[1b77bdfd-2605-43bf-af21-fa5fb94b5758] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-20 21:15:19,922: INFO/MainProcess] Received task: app.scrape[073664f7-33a2-4b47-977b-38c2a54df9e4]
[2016-02-20 21:15:19,927: INFO/Worker-4] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 21:15:19,927: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 21:15:19,928: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 21:15:19,928: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 21:15:19,928: INFO/Worker-4] Optional features available: ssl, http11, boto
[2016-02-20 21:15:19,928: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 21:15:19,928: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 21:15:19,929: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 21:15:19,929: INFO/Worker-4] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 21:15:19,929: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 21:15:19,929: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 21:15:19,930: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 21:15:19,934: INFO/Worker-4] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 21:15:19,934: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 21:15:19,934: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 21:15:19,935: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 21:15:19,936: INFO/Worker-4] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 21:15:19,936: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 21:15:19,936: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 21:15:19,936: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 21:15:19,937: INFO/Worker-4] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 21:15:19,937: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 21:15:19,937: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 21:15:19,938: WARNING/Worker-4] 2016-02-20 21:15:19 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 21:15:20,155: INFO/Worker-4] Enabled item pipelines: UserInputPipeline
[2016-02-20 21:15:20,155: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 21:15:20,155: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 21:15:20,155: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 21:15:20,156: INFO/Worker-4] Spider opened
[2016-02-20 21:15:20,156: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Spider opened
[2016-02-20 21:15:20,156: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Spider opened
[2016-02-20 21:15:20,156: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Spider opened
[2016-02-20 21:15:20,157: INFO/Worker-4] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 21:15:20,157: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 21:15:20,157: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 21:15:20,157: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 21:15:20,159: DEBUG/Worker-4] Telnet console listening on 127.0.0.1:6027
[2016-02-20 21:15:20,159: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6027
[2016-02-20 21:15:20,159: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6027
[2016-02-20 21:15:20,159: WARNING/Worker-4] 2016-02-20 21:15:20 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6027
[2016-02-20 21:15:20,162: CRITICAL/MainProcess] Task app.scrape[073664f7-33a2-4b47-977b-38c2a54df9e4] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-20 22:45:17,060: INFO/MainProcess] Received task: app.scrape[4977fa7d-82ad-47a0-810b-0f62beb9a5a5]
[2016-02-20 22:45:17,065: INFO/Worker-2] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:45:17,065: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:45:17,065: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:45:17,066: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:45:17,066: INFO/Worker-2] Optional features available: ssl, http11, boto
[2016-02-20 22:45:17,066: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:45:17,066: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:45:17,066: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:45:17,067: INFO/Worker-2] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:45:17,067: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:45:17,067: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:45:17,067: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:45:17,072: INFO/Worker-2] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:45:17,072: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:45:17,072: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:45:17,072: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:45:17,074: INFO/Worker-2] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:45:17,074: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:45:17,074: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:45:17,074: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:45:17,075: INFO/Worker-2] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:45:17,075: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:45:17,075: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:45:17,075: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:45:17,287: INFO/Worker-2] Enabled item pipelines: UserInputPipeline
[2016-02-20 22:45:17,287: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:45:17,287: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:45:17,287: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:45:17,287: INFO/Worker-2] Spider opened
[2016-02-20 22:45:17,288: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Spider opened
[2016-02-20 22:45:17,288: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Spider opened
[2016-02-20 22:45:17,288: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Spider opened
[2016-02-20 22:45:17,288: INFO/Worker-2] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:45:17,289: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:45:17,289: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:45:17,289: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:45:17,291: DEBUG/Worker-2] Telnet console listening on 127.0.0.1:6028
[2016-02-20 22:45:17,291: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6028
[2016-02-20 22:45:17,291: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6028
[2016-02-20 22:45:17,291: WARNING/Worker-2] 2016-02-20 22:45:17 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6028
[2016-02-20 22:45:17,293: CRITICAL/MainProcess] Task app.scrape[4977fa7d-82ad-47a0-810b-0f62beb9a5a5] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-20 22:46:58,619: INFO/MainProcess] Received task: app.scrape[dfbac124-a9f5-49f2-871a-b60897ff54d2]
[2016-02-20 22:46:58,624: INFO/Worker-1] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:46:58,624: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:46:58,624: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:46:58,624: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:46:58,625: INFO/Worker-1] Optional features available: ssl, http11, boto
[2016-02-20 22:46:58,625: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:46:58,625: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:46:58,625: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:46:58,626: INFO/Worker-1] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:46:58,626: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:46:58,626: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:46:58,626: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:46:58,631: INFO/Worker-1] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:46:58,631: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:46:58,631: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:46:58,631: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:46:58,633: INFO/Worker-1] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:46:58,633: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:46:58,633: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:46:58,633: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:46:58,634: INFO/Worker-1] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:46:58,634: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:46:58,634: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:46:58,634: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:46:58,843: INFO/Worker-1] Enabled item pipelines: UserInputPipeline
[2016-02-20 22:46:58,843: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:46:58,843: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:46:58,844: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:46:58,844: INFO/Worker-1] Spider opened
[2016-02-20 22:46:58,844: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Spider opened
[2016-02-20 22:46:58,844: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Spider opened
[2016-02-20 22:46:58,844: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Spider opened
[2016-02-20 22:46:58,845: INFO/Worker-1] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:46:58,845: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:46:58,845: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:46:58,846: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:46:58,847: DEBUG/Worker-1] Telnet console listening on 127.0.0.1:6029
[2016-02-20 22:46:58,847: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6029
[2016-02-20 22:46:58,848: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6029
[2016-02-20 22:46:58,848: WARNING/Worker-1] 2016-02-20 22:46:58 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6029
[2016-02-20 22:46:58,850: CRITICAL/MainProcess] Task app.scrape[dfbac124-a9f5-49f2-871a-b60897ff54d2] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-20 22:51:34,325: INFO/MainProcess] Received task: app.scrape[88b57a28-3c0d-46cb-ba39-88a8b893d74e]
[2016-02-20 22:51:34,330: INFO/Worker-3] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:51:34,330: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:51:34,330: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:51:34,331: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:51:34,331: INFO/Worker-3] Optional features available: ssl, http11, boto
[2016-02-20 22:51:34,331: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:51:34,331: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:51:34,331: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:51:34,332: INFO/Worker-3] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:51:34,332: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:51:34,332: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:51:34,332: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:51:34,337: INFO/Worker-3] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:51:34,337: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:51:34,337: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:51:34,337: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:51:34,339: INFO/Worker-3] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:51:34,339: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:51:34,339: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:51:34,339: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:51:34,340: INFO/Worker-3] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:51:34,340: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:51:34,340: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:51:34,340: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:51:34,555: INFO/Worker-3] Enabled item pipelines: UserInputPipeline
[2016-02-20 22:51:34,555: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:51:34,555: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:51:34,555: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:51:34,556: INFO/Worker-3] Spider opened
[2016-02-20 22:51:34,556: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Spider opened
[2016-02-20 22:51:34,556: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Spider opened
[2016-02-20 22:51:34,556: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Spider opened
[2016-02-20 22:51:34,557: INFO/Worker-3] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:51:34,557: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:51:34,557: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:51:34,557: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:51:34,559: DEBUG/Worker-3] Telnet console listening on 127.0.0.1:6030
[2016-02-20 22:51:34,559: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6030
[2016-02-20 22:51:34,559: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6030
[2016-02-20 22:51:34,560: WARNING/Worker-3] 2016-02-20 22:51:34 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6030
[2016-02-20 22:51:34,562: CRITICAL/MainProcess] Task app.scrape[88b57a28-3c0d-46cb-ba39-88a8b893d74e] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-20 22:52:35,939: INFO/MainProcess] Received task: app.scrape[01d5f515-726a-420a-8abc-be7d50461646]
[2016-02-20 22:52:35,945: INFO/Worker-4] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:52:35,945: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:52:35,945: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:52:35,946: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:52:35,946: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 22:52:35,946: INFO/Worker-4] Optional features available: ssl, http11, boto
[2016-02-20 22:52:35,946: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:52:35,946: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:52:35,947: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:52:35,947: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 22:52:35,947: INFO/Worker-4] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:52:35,948: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:52:35,948: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:52:35,948: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:52:35,948: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 22:52:35,953: INFO/Worker-4] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:52:35,953: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:52:35,953: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:52:35,953: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:52:35,954: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 22:52:35,955: INFO/Worker-4] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:52:35,955: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:52:35,955: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:52:35,956: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:52:35,956: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 22:52:35,957: INFO/Worker-4] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:52:35,957: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:52:35,957: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:52:35,957: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:52:35,957: WARNING/Worker-4] 2016-02-20 22:52:35 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 22:52:36,170: INFO/Worker-4] Enabled item pipelines: UserInputPipeline
[2016-02-20 22:52:36,170: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:52:36,170: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:52:36,171: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:52:36,171: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 22:52:36,171: INFO/Worker-4] Spider opened
[2016-02-20 22:52:36,171: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Spider opened
[2016-02-20 22:52:36,171: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Spider opened
[2016-02-20 22:52:36,172: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Spider opened
[2016-02-20 22:52:36,172: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Spider opened
[2016-02-20 22:52:36,172: INFO/Worker-4] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:52:36,173: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:52:36,173: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:52:36,173: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:52:36,173: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 22:52:36,175: DEBUG/Worker-4] Telnet console listening on 127.0.0.1:6031
[2016-02-20 22:52:36,175: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6031
[2016-02-20 22:52:36,176: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6031
[2016-02-20 22:52:36,176: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6031
[2016-02-20 22:52:36,176: WARNING/Worker-4] 2016-02-20 22:52:36 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6031
[2016-02-20 22:52:36,178: CRITICAL/MainProcess] Task app.scrape[01d5f515-726a-420a-8abc-be7d50461646] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2016-02-20 23:12:14,998: INFO/MainProcess] Received task: app.scrape[d36052db-219d-40c0-aff3-b37eee3b7164]
[2016-02-20 23:12:15,003: INFO/Worker-2] Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 23:12:15,004: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 23:12:15,004: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 23:12:15,004: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 23:12:15,004: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2016-02-20 23:12:15,005: INFO/Worker-2] Optional features available: ssl, http11, boto
[2016-02-20 23:12:15,005: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 23:12:15,005: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 23:12:15,005: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 23:12:15,005: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Optional features available: ssl, http11, boto
[2016-02-20 23:12:15,006: INFO/Worker-2] Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 23:12:15,006: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 23:12:15,006: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 23:12:15,006: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 23:12:15,007: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 30, 'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2016-02-20 23:12:15,011: INFO/Worker-2] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 23:12:15,011: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 23:12:15,011: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 23:12:15,012: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 23:12:15,012: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2016-02-20 23:12:15,013: INFO/Worker-2] Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 23:12:15,013: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 23:12:15,014: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 23:12:15,014: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 23:12:15,014: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
[2016-02-20 23:12:15,015: INFO/Worker-2] Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 23:12:15,015: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 23:12:15,015: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 23:12:15,015: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 23:12:15,016: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
[2016-02-20 23:12:15,231: INFO/Worker-2] Enabled item pipelines: UserInputPipeline
[2016-02-20 23:12:15,231: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 23:12:15,231: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 23:12:15,232: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 23:12:15,232: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Enabled item pipelines: UserInputPipeline
[2016-02-20 23:12:15,232: INFO/Worker-2] Spider opened
[2016-02-20 23:12:15,232: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Spider opened
[2016-02-20 23:12:15,233: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Spider opened
[2016-02-20 23:12:15,233: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Spider opened
[2016-02-20 23:12:15,233: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Spider opened
[2016-02-20 23:12:15,234: INFO/Worker-2] Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 23:12:15,234: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 23:12:15,234: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 23:12:15,234: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 23:12:15,235: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
[2016-02-20 23:12:15,236: DEBUG/Worker-2] Telnet console listening on 127.0.0.1:6032
[2016-02-20 23:12:15,236: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6032
[2016-02-20 23:12:15,237: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6032
[2016-02-20 23:12:15,237: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6032
[2016-02-20 23:12:15,237: WARNING/Worker-2] 2016-02-20 23:12:15 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6032
[2016-02-20 23:12:15,240: CRITICAL/MainProcess] Task app.scrape[d36052db-219d-40c0-aff3-b37eee3b7164] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 253, in trace_task
    I, R, state, retval = on_error(task_request, exc, uuid)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 201, in on_error
    R = I.handle_error_state(task, eager=eager)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 85, in handle_error_state
    }[self.state](task, store_errors=store_errors)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 118, in handle_failure
    req.id, exc, einfo.traceback, request=req,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 121, in mark_as_failure
    traceback=traceback, request=request)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
