/home/ubuntu/programming/banker.ai/mcubed/app/vcspider/vcspider/pipelines.py:3: ScrapyDeprecationWarning: Module `scrapy.conf` is deprecated, use `crawler.settings` attribute instead
  from scrapy.conf import settings
[2015-12-05 23:16:26,613: WARNING/MainProcess] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/apps/worker.py:161: CDeprecationWarning: 
Starting from version 3.2 Celery will refuse to accept pickle by default.

The pickle serializer is a security concern as it may give attackers
the ability to execute any command.  It's important to secure
your broker from unauthorized access when using pickle, so we think
that enabling pickle should require a deliberate action and not be
the default choice.

If you depend on pickle then you should set a setting to disable this
warning and to be sure that everything will continue working
when you upgrade to Celery 3.2::

    CELERY_ACCEPT_CONTENT = ['pickle', 'json', 'msgpack', 'yaml']

You must only enable the serializers that you will actually use.


  warnings.warn(CDeprecationWarning(W_PICKLE_DEPRECATED))
[2015-12-05 23:16:26,659: INFO/MainProcess] Connected to redis://localhost:6379//
[2015-12-05 23:16:26,667: INFO/MainProcess] mingle: searching for neighbors
[2015-12-05 23:16:27,672: INFO/MainProcess] mingle: all alone
[2015-12-05 23:16:27,680: WARNING/MainProcess] celery@ip-172-31-59-92 ready.
[2015-12-06 01:42:43,806: INFO/MainProcess] Received task: app.scrape[a9a2e55b-ea09-4aff-91c9-94e6df45307e]
[2015-12-06 01:42:43,814: INFO/Worker-7] Scrapy 1.0.3 started (bot: scrapybot)
[2015-12-06 01:42:43,814: WARNING/Worker-7] 2015-12-06 01:42:43 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2015-12-06 01:42:43,814: INFO/Worker-7] Optional features available: ssl, http11, boto
[2015-12-06 01:42:43,814: WARNING/Worker-7] 2015-12-06 01:42:43 [scrapy] INFO: Optional features available: ssl, http11, boto
[2015-12-06 01:42:43,815: INFO/Worker-7] Overridden settings: {'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2015-12-06 01:42:43,815: WARNING/Worker-7] 2015-12-06 01:42:43 [scrapy] INFO: Overridden settings: {'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2015-12-06 01:42:43,881: INFO/Worker-7] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2015-12-06 01:42:43,881: WARNING/Worker-7] 2015-12-06 01:42:43 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
Unhandled error in Deferred:
[2015-12-06 01:42:43,885: CRITICAL/Worker-7] Unhandled error in Deferred:
[2015-12-06 01:42:43,885: WARNING/Worker-7] 2015-12-06 01:42:43 [twisted] CRITICAL: Unhandled error in Deferred:


Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 438, in __protected_call__
    return self.run(*args, **kwargs)
  File "/home/ubuntu/programming/banker.ai/mcubed/app/__init__.py", line 87, in scrape
    process.crawl(solo.SoloSpider, domain = siteurl)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/crawler.py", line 153, in crawl
    d = crawler.crawl(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 1274, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 1128, in _inlineCallbacks
    result = g.send(result)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/crawler.py", line 70, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/crawler.py", line 80, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 91, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/home/ubuntu/programming/banker.ai/mcubed/app/vcspider/vcspider/spiders/solo.py", line 21, in __init__
    self.start_urls = ['http://www.' + url]
exceptions.TypeError: cannot concatenate 'str' and 'NoneType' objects
[2015-12-06 01:42:43,886: CRITICAL/Worker-7] 
[2015-12-06 01:42:43,887: WARNING/Worker-7] 2015-12-06 01:42:43 [twisted] CRITICAL:
[2015-12-06 01:42:43,889: WARNING/Worker-7] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2015-12-06 01:42:43,889: WARNING/Worker-7] 2015-12-06 01:42:43 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2015-12-06 01:42:43,890: CRITICAL/MainProcess] Task app.scrape[a9a2e55b-ea09-4aff-91c9-94e6df45307e] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
[2015-12-06 01:50:18,277: INFO/MainProcess] Received task: app.scrape[43c4b807-ce92-451e-8afc-9e4593461f09]
[2015-12-06 01:50:18,285: INFO/Worker-3] Scrapy 1.0.3 started (bot: scrapybot)
[2015-12-06 01:50:18,286: WARNING/Worker-3] 2015-12-06 01:50:18 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)
[2015-12-06 01:50:18,286: INFO/Worker-3] Optional features available: ssl, http11, boto
[2015-12-06 01:50:18,286: WARNING/Worker-3] 2015-12-06 01:50:18 [scrapy] INFO: Optional features available: ssl, http11, boto
[2015-12-06 01:50:18,287: INFO/Worker-3] Overridden settings: {'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2015-12-06 01:50:18,287: WARNING/Worker-3] 2015-12-06 01:50:18 [scrapy] INFO: Overridden settings: {'DEPTH_LIMIT': 2, 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}
[2015-12-06 01:50:18,343: INFO/Worker-3] Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
[2015-12-06 01:50:18,344: WARNING/Worker-3] 2015-12-06 01:50:18 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
Unhandled error in Deferred:
[2015-12-06 01:50:18,347: CRITICAL/Worker-3] Unhandled error in Deferred:
[2015-12-06 01:50:18,347: WARNING/Worker-3] 2015-12-06 01:50:18 [twisted] CRITICAL: Unhandled error in Deferred:


Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 438, in __protected_call__
    return self.run(*args, **kwargs)
  File "/home/ubuntu/programming/banker.ai/mcubed/app/__init__.py", line 87, in scrape
    process.crawl(solo.SoloSpider, domain = siteurl)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/crawler.py", line 153, in crawl
    d = crawler.crawl(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 1274, in unwindGenerator
    return _inlineCallbacks(None, gen, Deferred())
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 1128, in _inlineCallbacks
    result = g.send(result)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/crawler.py", line 70, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/crawler.py", line 80, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 91, in from_crawler
    spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/__init__.py", line 50, in from_crawler
    spider = cls(*args, **kwargs)
  File "/home/ubuntu/programming/banker.ai/mcubed/app/vcspider/vcspider/spiders/solo.py", line 21, in __init__
    self.start_urls = ['http://www.' + url]
exceptions.TypeError: cannot concatenate 'str' and 'NoneType' objects
[2015-12-06 01:50:18,348: CRITICAL/Worker-3] 
[2015-12-06 01:50:18,348: WARNING/Worker-3] 2015-12-06 01:50:18 [twisted] CRITICAL:
[2015-12-06 01:50:18,350: WARNING/Worker-3] /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))

[2015-12-06 01:50:18,351: WARNING/Worker-3] 2015-12-06 01:50:18 [py.warnings] WARNING: /home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py:365: RuntimeWarning: Exception raised outside body: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",):
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'

  exc, exc_info.traceback)))
[2015-12-06 01:50:18,352: CRITICAL/MainProcess] Task app.scrape[43c4b807-ce92-451e-8afc-9e4593461f09] INTERNAL ERROR: TypeError("__init__() got an unexpected keyword argument 'socket_connect_timeout'",)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/app/trace.py", line 283, in trace_task
    uuid, retval, SUCCESS, request=task_request,
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 257, in store_result
    request=request, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/base.py", line 491, in _store_result
    self.set(self.get_key_for_task(task_id), self.encode(meta))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 160, in set
    return self.ensure(self._set, (key, value), **retry_policy)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 149, in ensure
    **retry_policy
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/kombu/utils/__init__.py", line 243, in retry_over_time
    return fun(*args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/celery/backends/redis.py", line 169, in _set
    pipe.execute()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/client.py", line 2149, in execute
    self.shard_hint)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 455, in get_connection
    connection = self.make_connection()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/redis/connection.py", line 464, in make_connection
    return self.connection_class(**self.connection_kwargs)
TypeError: __init__() got an unexpected keyword argument 'socket_connect_timeout'
