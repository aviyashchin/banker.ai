2015-11-02 00:16:01 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-02 00:16:01 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-02 00:16:01 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 1}
2015-11-02 00:16:01 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-02 00:16:01 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-02 00:16:01 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-02 00:16:02 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-02 00:16:02 [scrapy] INFO: Spider opened
2015-11-02 00:16:02 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-02 00:17:05 [scrapy] INFO: Crawled 50 pages (at 50 pages/min), scraped 9 items (at 9 items/min)
2015-11-02 00:18:12 [scrapy] INFO: Crawled 112 pages (at 62 pages/min), scraped 27 items (at 18 items/min)
2015-11-02 00:20:30 [scrapy] ERROR: Spider error processing <GET http://www.1006.tv/center/boombeach> (referer: http://www.1006.tv/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-02 00:23:25 [scrapy] ERROR: Spider error processing <GET http://www.1006.tv/center/dtcq> (referer: http://www.1006.tv/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-02 00:23:54 [scrapy] INFO: Crawled 154 pages (at 42 pages/min), scraped 55 items (at 28 items/min)
2015-11-02 00:24:52 [scrapy] ERROR: Spider error processing <GET http://www.1006.tv/news/291182> (referer: http://www.1006.tv/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-02 00:25:47 [scrapy] ERROR: Spider error processing <GET http://www.1006.tv/news/291180> (referer: http://www.1006.tv/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-02 00:26:27 [scrapy] INFO: Crawled 154 pages (at 0 pages/min), scraped 62 items (at 7 items/min)
2015-11-02 00:28:10 [scrapy] INFO: Crawled 215 pages (at 61 pages/min), scraped 114 items (at 52 items/min)
2015-11-02 00:28:52 [scrapy] ERROR: Spider error processing <GET http://www.1006.tv/news/291189> (referer: http://www.1006.tv/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-02 00:29:08 [scrapy] ERROR: Error downloading <GET https://1000corks.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-02 00:29:08 [scrapy] INFO: Crawled 222 pages (at 7 pages/min), scraped 123 items (at 9 items/min)
2015-11-02 00:31:54 [scrapy] ERROR: Spider error processing <GET http://www.1006.tv/news/286008> (referer: http://www.1006.tv/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-02 00:31:57 [scrapy] INFO: Crawled 222 pages (at 0 pages/min), scraped 131 items (at 8 items/min)
2015-11-02 00:33:06 [scrapy] INFO: Crawled 222 pages (at 0 pages/min), scraped 133 items (at 2 items/min)
2015-11-02 00:33:06 [scrapy] ERROR: Error downloading <GET https://www.pinterest.com/140proof/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-02 00:34:31 [scrapy] INFO: Crawled 273 pages (at 51 pages/min), scraped 176 items (at 43 items/min)
2015-11-02 00:35:05 [scrapy] INFO: Crawled 297 pages (at 24 pages/min), scraped 208 items (at 32 items/min)
2015-11-02 00:36:31 [scrapy] ERROR: Spider error processing <GET http://www.1006.tv/news/266361> (referer: http://www.1006.tv/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/Users/johnmontroy/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-02 00:36:31 [scrapy] INFO: Crawled 347 pages (at 50 pages/min), scraped 249 items (at 41 items/min)
2015-11-02 00:37:02 [scrapy] INFO: Crawled 372 pages (at 25 pages/min), scraped 281 items (at 32 items/min)
