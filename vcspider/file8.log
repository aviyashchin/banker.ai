usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
2015-11-04 00:31:06 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 00:31:06 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 00:31:06 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 00:31:06 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 00:31:06 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 00:31:06 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 00:31:06 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 00:31:06 [scrapy] INFO: Spider opened
2015-11-04 00:31:06 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 00:31:28 [scrapy] ERROR: Spider error processing <GET http://www.sproutling.com/parentage> (referer: http://www.sproutling.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 868: htmlParseEntityRef: expecting ';'
2015-11-04 00:32:24 [scrapy] INFO: Crawled 167 pages (at 167 pages/min), scraped 65 items (at 65 items/min)
2015-11-04 00:33:32 [scrapy] INFO: Crawled 193 pages (at 26 pages/min), scraped 88 items (at 23 items/min)
2015-11-04 00:34:25 [scrapy] INFO: Crawled 205 pages (at 12 pages/min), scraped 106 items (at 18 items/min)
2015-11-04 00:34:26 [scrapy] ERROR: Error downloading <GET https://frontapp.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:35:48 [scrapy] INFO: Crawled 230 pages (at 25 pages/min), scraped 129 items (at 23 items/min)
2015-11-04 00:36:38 [scrapy] ERROR: Error downloading <GET https://rocket.la/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:36:38 [scrapy] INFO: Crawled 240 pages (at 10 pages/min), scraped 141 items (at 12 items/min)
2015-11-04 00:37:25 [scrapy] INFO: Crawled 250 pages (at 10 pages/min), scraped 151 items (at 10 items/min)
2015-11-04 00:38:13 [scrapy] INFO: Crawled 259 pages (at 9 pages/min), scraped 161 items (at 10 items/min)
2015-11-04 00:39:11 [scrapy] INFO: Crawled 267 pages (at 8 pages/min), scraped 162 items (at 1 items/min)
2015-11-04 00:40:07 [scrapy] INFO: Crawled 274 pages (at 7 pages/min), scraped 178 items (at 16 items/min)
2015-11-04 00:41:15 [scrapy] INFO: Crawled 299 pages (at 25 pages/min), scraped 188 items (at 10 items/min)
2015-11-04 00:43:15 [scrapy] ERROR: Error downloading <GET http://sports.letv.com/video/23909281.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 00:43:15 [scrapy] ERROR: Error downloading <GET http://sports.letv.com/video/23910709.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 00:43:15 [scrapy] INFO: Crawled 307 pages (at 8 pages/min), scraped 210 items (at 22 items/min)
2015-11-04 00:43:37 [scrapy] ERROR: Error downloading <GET http://sports.letv.com/video/23917323.html>: An error occurred while connecting: 32: Broken pipe.
2015-11-04 00:44:54 [scrapy] INFO: Crawled 316 pages (at 9 pages/min), scraped 219 items (at 9 items/min)
2015-11-04 00:46:11 [scrapy] INFO: Crawled 324 pages (at 8 pages/min), scraped 226 items (at 7 items/min)
2015-11-04 00:47:34 [scrapy] INFO: Crawled 332 pages (at 8 pages/min), scraped 234 items (at 8 items/min)
2015-11-04 00:48:27 [scrapy] INFO: Crawled 339 pages (at 7 pages/min), scraped 242 items (at 8 items/min)
2015-11-04 00:49:26 [scrapy] INFO: Crawled 354 pages (at 15 pages/min), scraped 250 items (at 8 items/min)
2015-11-04 00:50:35 [scrapy] INFO: Crawled 362 pages (at 8 pages/min), scraped 265 items (at 15 items/min)
2015-11-04 00:51:09 [scrapy] INFO: Crawled 370 pages (at 8 pages/min), scraped 273 items (at 8 items/min)
2015-11-04 00:52:09 [scrapy] INFO: Crawled 386 pages (at 16 pages/min), scraped 289 items (at 16 items/min)
2015-11-04 00:53:20 [scrapy] INFO: Crawled 410 pages (at 24 pages/min), scraped 313 items (at 24 items/min)
2015-11-04 00:54:08 [scrapy] INFO: Crawled 426 pages (at 16 pages/min), scraped 329 items (at 16 items/min)
2015-11-04 00:55:34 [scrapy] INFO: Crawled 450 pages (at 24 pages/min), scraped 353 items (at 24 items/min)
2015-11-04 00:56:28 [scrapy] INFO: Crawled 466 pages (at 16 pages/min), scraped 369 items (at 16 items/min)
2015-11-04 00:57:20 [scrapy] INFO: Crawled 482 pages (at 16 pages/min), scraped 385 items (at 16 items/min)
2015-11-04 00:58:15 [scrapy] INFO: Crawled 498 pages (at 16 pages/min), scraped 401 items (at 16 items/min)
2015-11-04 00:59:27 [scrapy] INFO: Crawled 523 pages (at 25 pages/min), scraped 426 items (at 25 items/min)
2015-11-04 01:00:13 [scrapy] INFO: Crawled 539 pages (at 16 pages/min), scraped 442 items (at 16 items/min)
2015-11-04 01:01:28 [scrapy] INFO: Crawled 563 pages (at 24 pages/min), scraped 466 items (at 24 items/min)
2015-11-04 01:02:23 [scrapy] INFO: Crawled 579 pages (at 16 pages/min), scraped 482 items (at 16 items/min)
2015-11-04 01:03:11 [scrapy] INFO: Crawled 586 pages (at 7 pages/min), scraped 490 items (at 8 items/min)
2015-11-04 01:04:13 [scrapy] INFO: Crawled 609 pages (at 23 pages/min), scraped 507 items (at 17 items/min)
2015-11-04 01:05:10 [scrapy] INFO: Crawled 625 pages (at 16 pages/min), scraped 528 items (at 21 items/min)
2015-11-04 01:06:09 [scrapy] INFO: Crawled 659 pages (at 34 pages/min), scraped 554 items (at 26 items/min)
2015-11-04 01:08:10 [scrapy] INFO: Crawled 675 pages (at 16 pages/min), scraped 578 items (at 24 items/min)
2015-11-04 01:09:22 [scrapy] INFO: Crawled 706 pages (at 31 pages/min), scraped 610 items (at 32 items/min)
2015-11-04 01:10:22 [scrapy] INFO: Crawled 723 pages (at 17 pages/min), scraped 626 items (at 16 items/min)
2015-11-04 01:11:23 [scrapy] INFO: Crawled 747 pages (at 24 pages/min), scraped 650 items (at 24 items/min)
2015-11-04 01:12:13 [scrapy] INFO: Crawled 763 pages (at 16 pages/min), scraped 666 items (at 16 items/min)
2015-11-04 01:13:25 [scrapy] INFO: Crawled 779 pages (at 16 pages/min), scraped 681 items (at 15 items/min)
2015-11-04 01:14:14 [scrapy] INFO: Crawled 795 pages (at 16 pages/min), scraped 698 items (at 17 items/min)
2015-11-04 01:15:21 [scrapy] INFO: Crawled 822 pages (at 27 pages/min), scraped 725 items (at 27 items/min)
2015-11-04 01:16:10 [scrapy] INFO: Crawled 838 pages (at 16 pages/min), scraped 741 items (at 16 items/min)
2015-11-04 01:17:17 [scrapy] INFO: Crawled 868 pages (at 30 pages/min), scraped 763 items (at 22 items/min)
2015-11-04 01:18:22 [scrapy] INFO: Crawled 878 pages (at 10 pages/min), scraped 779 items (at 16 items/min)
2015-11-04 01:19:40 [scrapy] INFO: Crawled 900 pages (at 22 pages/min), scraped 798 items (at 19 items/min)
2015-11-04 01:20:15 [scrapy] INFO: Crawled 905 pages (at 5 pages/min), scraped 811 items (at 13 items/min)
2015-11-04 01:21:24 [scrapy] INFO: Crawled 938 pages (at 33 pages/min), scraped 838 items (at 27 items/min)
2015-11-04 01:22:23 [scrapy] INFO: Crawled 961 pages (at 23 pages/min), scraped 860 items (at 22 items/min)
2015-11-04 01:24:19 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/tv/10007215.html> (referer: http://list.letv.com/listn/cs2_o9_p.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:24:23 [scrapy] INFO: Crawled 972 pages (at 11 pages/min), scraped 873 items (at 13 items/min)
2015-11-04 01:25:36 [scrapy] INFO: Crawled 995 pages (at 23 pages/min), scraped 893 items (at 20 items/min)
2015-11-04 01:27:23 [scrapy] INFO: Crawled 1008 pages (at 13 pages/min), scraped 905 items (at 12 items/min)
2015-11-04 01:29:30 [scrapy] INFO: Crawled 1008 pages (at 0 pages/min), scraped 918 items (at 13 items/min)
2015-11-04 01:30:51 [scrapy] INFO: Crawled 1054 pages (at 46 pages/min), scraped 927 items (at 9 items/min)
2015-11-04 01:33:38 [scrapy] INFO: Crawled 1054 pages (at 0 pages/min), scraped 956 items (at 29 items/min)
2015-11-04 01:35:19 [scrapy] INFO: Crawled 1054 pages (at 0 pages/min), scraped 964 items (at 8 items/min)
2015-11-04 01:36:11 [scrapy] INFO: Crawled 1067 pages (at 13 pages/min), scraped 973 items (at 9 items/min)
2015-11-04 01:37:12 [scrapy] INFO: Crawled 1075 pages (at 8 pages/min), scraped 982 items (at 9 items/min)
2015-11-04 01:38:16 [scrapy] INFO: Crawled 1080 pages (at 5 pages/min), scraped 990 items (at 8 items/min)
2015-11-04 01:39:29 [scrapy] INFO: Crawled 1092 pages (at 12 pages/min), scraped 998 items (at 8 items/min)
2015-11-04 01:40:11 [scrapy] INFO: Crawled 1099 pages (at 7 pages/min), scraped 1003 items (at 5 items/min)
2015-11-04 01:41:36 [scrapy] INFO: Crawled 1108 pages (at 9 pages/min), scraped 1016 items (at 13 items/min)
2015-11-04 01:42:07 [scrapy] INFO: Crawled 1119 pages (at 11 pages/min), scraped 1022 items (at 6 items/min)
2015-11-04 01:43:25 [scrapy] INFO: Crawled 1137 pages (at 18 pages/min), scraped 1038 items (at 16 items/min)
2015-11-04 01:44:13 [scrapy] INFO: Crawled 1146 pages (at 9 pages/min), scraped 1050 items (at 12 items/min)
2015-11-04 01:45:30 [scrapy] INFO: Crawled 1154 pages (at 8 pages/min), scraped 1063 items (at 13 items/min)
2015-11-04 01:46:07 [scrapy] INFO: Crawled 1161 pages (at 7 pages/min), scraped 1068 items (at 5 items/min)
2015-11-04 01:47:12 [scrapy] INFO: Crawled 1169 pages (at 8 pages/min), scraped 1076 items (at 8 items/min)
2015-11-04 01:48:20 [scrapy] INFO: Crawled 1177 pages (at 8 pages/min), scraped 1084 items (at 8 items/min)
2015-11-04 01:49:25 [scrapy] INFO: Crawled 1185 pages (at 8 pages/min), scraped 1092 items (at 8 items/min)
2015-11-04 01:50:27 [scrapy] INFO: Crawled 1193 pages (at 8 pages/min), scraped 1100 items (at 8 items/min)
2015-11-04 01:51:38 [scrapy] INFO: Crawled 1208 pages (at 15 pages/min), scraped 1111 items (at 11 items/min)
2015-11-04 01:52:14 [scrapy] INFO: Crawled 1209 pages (at 1 pages/min), scraped 1118 items (at 7 items/min)
2015-11-04 01:53:47 [scrapy] INFO: Crawled 1232 pages (at 23 pages/min), scraped 1135 items (at 17 items/min)
2015-11-04 01:54:09 [scrapy] INFO: Crawled 1237 pages (at 5 pages/min), scraped 1142 items (at 7 items/min)
2015-11-04 01:55:10 [scrapy] INFO: Crawled 1258 pages (at 21 pages/min), scraped 1155 items (at 13 items/min)
2015-11-04 01:57:35 [scrapy] INFO: Crawled 1283 pages (at 25 pages/min), scraped 1176 items (at 21 items/min)
2015-11-04 01:58:34 [scrapy] INFO: Crawled 1283 pages (at 0 pages/min), scraped 1185 items (at 9 items/min)
2015-11-04 01:59:48 [scrapy] INFO: Crawled 1303 pages (at 20 pages/min), scraped 1205 items (at 20 items/min)
2015-11-04 02:00:37 [scrapy] INFO: Crawled 1316 pages (at 13 pages/min), scraped 1217 items (at 12 items/min)
2015-11-04 02:01:12 [scrapy] INFO: Crawled 1327 pages (at 11 pages/min), scraped 1226 items (at 9 items/min)
2015-11-04 02:03:06 [scrapy] INFO: Crawled 1348 pages (at 21 pages/min), scraped 1249 items (at 23 items/min)
2015-11-04 02:03:07 [scrapy] INFO: Crawled 1348 pages (at 0 pages/min), scraped 1249 items (at 0 items/min)
2015-11-04 02:04:41 [scrapy] INFO: Crawled 1371 pages (at 23 pages/min), scraped 1269 items (at 20 items/min)
2015-11-04 02:05:51 [scrapy] INFO: Crawled 1380 pages (at 9 pages/min), scraped 1281 items (at 12 items/min)
2015-11-04 02:06:39 [scrapy] INFO: Crawled 1391 pages (at 11 pages/min), scraped 1290 items (at 9 items/min)
2015-11-04 02:07:29 [scrapy] INFO: Crawled 1403 pages (at 12 pages/min), scraped 1301 items (at 11 items/min)
2015-11-04 02:08:42 [scrapy] INFO: Crawled 1412 pages (at 9 pages/min), scraped 1313 items (at 12 items/min)
2015-11-04 02:09:25 [scrapy] INFO: Crawled 1423 pages (at 11 pages/min), scraped 1322 items (at 9 items/min)
2015-11-04 02:10:12 [scrapy] INFO: Crawled 1435 pages (at 12 pages/min), scraped 1333 items (at 11 items/min)
2015-11-04 02:11:50 [scrapy] INFO: Crawled 1436 pages (at 1 pages/min), scraped 1345 items (at 12 items/min)
2015-11-04 02:12:26 [scrapy] INFO: Crawled 1444 pages (at 8 pages/min), scraped 1349 items (at 4 items/min)
2015-11-04 02:13:12 [scrapy] INFO: Crawled 1457 pages (at 13 pages/min), scraped 1354 items (at 5 items/min)
2015-11-04 02:14:08 [scrapy] INFO: Crawled 1460 pages (at 3 pages/min), scraped 1367 items (at 13 items/min)
2015-11-04 02:15:23 [scrapy] INFO: Crawled 1481 pages (at 21 pages/min), scraped 1378 items (at 11 items/min)
2015-11-04 02:16:17 [scrapy] INFO: Crawled 1484 pages (at 3 pages/min), scraped 1391 items (at 13 items/min)
2015-11-04 02:17:10 [scrapy] INFO: Crawled 1498 pages (at 14 pages/min), scraped 1395 items (at 4 items/min)
2015-11-04 02:18:17 [scrapy] INFO: Crawled 1509 pages (at 11 pages/min), scraped 1408 items (at 13 items/min)
2015-11-04 02:19:28 [scrapy] INFO: Crawled 1509 pages (at 0 pages/min), scraped 1419 items (at 11 items/min)
2015-11-04 02:20:09 [scrapy] INFO: Crawled 1527 pages (at 18 pages/min), scraped 1424 items (at 5 items/min)
2015-11-04 02:21:08 [scrapy] INFO: Crawled 1535 pages (at 8 pages/min), scraped 1437 items (at 13 items/min)
2015-11-04 02:22:26 [scrapy] INFO: Crawled 1538 pages (at 3 pages/min), scraped 1445 items (at 8 items/min)
2015-11-04 02:24:05 [scrapy] INFO: Crawled 1562 pages (at 24 pages/min), scraped 1464 items (at 19 items/min)
2015-11-04 02:24:25 [scrapy] INFO: Crawled 1562 pages (at 0 pages/min), scraped 1472 items (at 8 items/min)
2015-11-04 02:25:35 [scrapy] INFO: Crawled 1585 pages (at 23 pages/min), scraped 1487 items (at 15 items/min)
2015-11-04 02:26:32 [scrapy] INFO: Crawled 1590 pages (at 5 pages/min), scraped 1495 items (at 8 items/min)
2015-11-04 02:27:38 [scrapy] INFO: Crawled 1604 pages (at 14 pages/min), scraped 1506 items (at 11 items/min)
2015-11-04 02:28:14 [scrapy] INFO: Crawled 1620 pages (at 16 pages/min), scraped 1516 items (at 10 items/min)
2015-11-04 02:29:24 [scrapy] INFO: Crawled 1628 pages (at 8 pages/min), scraped 1530 items (at 14 items/min)
2015-11-04 02:31:05 [scrapy] INFO: Crawled 1652 pages (at 24 pages/min), scraped 1555 items (at 25 items/min)
2015-11-04 02:31:31 [scrapy] INFO: Crawled 1653 pages (at 1 pages/min), scraped 1561 items (at 6 items/min)
2015-11-04 02:32:08 [scrapy] INFO: Crawled 1664 pages (at 11 pages/min), scraped 1565 items (at 4 items/min)
2015-11-04 02:33:24 [scrapy] INFO: Crawled 1672 pages (at 8 pages/min), scraped 1573 items (at 8 items/min)
2015-11-04 02:35:09 [scrapy] INFO: Crawled 1696 pages (at 24 pages/min), scraped 1597 items (at 24 items/min)
2015-11-04 02:36:49 [scrapy] INFO: Crawled 1721 pages (at 25 pages/min), scraped 1622 items (at 25 items/min)
2015-11-04 02:37:08 [scrapy] INFO: Crawled 1723 pages (at 2 pages/min), scraped 1630 items (at 8 items/min)
2015-11-04 02:38:53 [scrapy] INFO: Crawled 1743 pages (at 20 pages/min), scraped 1645 items (at 15 items/min)
2015-11-04 02:39:12 [scrapy] INFO: Crawled 1745 pages (at 2 pages/min), scraped 1652 items (at 7 items/min)
2015-11-04 02:40:31 [scrapy] INFO: Crawled 1765 pages (at 20 pages/min), scraped 1666 items (at 14 items/min)
2015-11-04 02:41:08 [scrapy] INFO: Crawled 1781 pages (at 16 pages/min), scraped 1675 items (at 9 items/min)
2015-11-04 02:42:25 [scrapy] INFO: Crawled 1789 pages (at 8 pages/min), scraped 1697 items (at 22 items/min)
2015-11-04 02:43:39 [scrapy] INFO: Crawled 1823 pages (at 34 pages/min), scraped 1715 items (at 18 items/min)
2015-11-04 02:44:47 [scrapy] INFO: Crawled 1823 pages (at 0 pages/min), scraped 1731 items (at 16 items/min)
2015-11-04 02:45:16 [scrapy] INFO: Crawled 1848 pages (at 25 pages/min), scraped 1741 items (at 10 items/min)
2015-11-04 02:47:10 [scrapy] INFO: Crawled 1864 pages (at 16 pages/min), scraped 1766 items (at 25 items/min)
2015-11-04 02:48:43 [scrapy] INFO: Crawled 1886 pages (at 22 pages/min), scraped 1778 items (at 12 items/min)
2015-11-04 02:49:30 [scrapy] INFO: Crawled 1886 pages (at 0 pages/min), scraped 1786 items (at 8 items/min)
2015-11-04 02:50:08 [scrapy] INFO: Crawled 1903 pages (at 17 pages/min), scraped 1796 items (at 10 items/min)
2015-11-04 02:51:19 [scrapy] INFO: Crawled 1917 pages (at 14 pages/min), scraped 1818 items (at 22 items/min)
2015-11-04 02:52:22 [scrapy] INFO: Crawled 1939 pages (at 22 pages/min), scraped 1833 items (at 15 items/min)
2015-11-04 02:53:08 [scrapy] INFO: Crawled 1958 pages (at 19 pages/min), scraped 1849 items (at 16 items/min)
2015-11-04 02:54:30 [scrapy] INFO: Crawled 1976 pages (at 18 pages/min), scraped 1877 items (at 28 items/min)
2015-11-04 02:55:12 [scrapy] INFO: Crawled 2014 pages (at 38 pages/min), scraped 1895 items (at 18 items/min)
2015-11-04 02:56:24 [scrapy] INFO: Crawled 2023 pages (at 9 pages/min), scraped 1907 items (at 12 items/min)
2015-11-04 02:58:46 [scrapy] INFO: Crawled 2036 pages (at 13 pages/min), scraped 1938 items (at 31 items/min)
2015-11-04 02:59:50 [scrapy] INFO: Crawled 2036 pages (at 0 pages/min), scraped 1944 items (at 6 items/min)
2015-11-04 03:01:14 [scrapy] INFO: Crawled 2064 pages (at 28 pages/min), scraped 1963 items (at 19 items/min)
2015-11-04 03:02:39 [scrapy] INFO: Crawled 2070 pages (at 6 pages/min), scraped 1973 items (at 10 items/min)
2015-11-04 03:03:12 [scrapy] INFO: Crawled 2098 pages (at 28 pages/min), scraped 1982 items (at 9 items/min)
2015-11-04 03:07:03 [scrapy] INFO: Crawled 2103 pages (at 5 pages/min), scraped 2006 items (at 24 items/min)
2015-11-04 03:07:12 [scrapy] INFO: Crawled 2103 pages (at 0 pages/min), scraped 2007 items (at 1 items/min)
2015-11-04 03:08:34 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-973706-1.html> (referer: http://www.letv.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:08:34 [scrapy] INFO: Crawled 2103 pages (at 0 pages/min), scraped 2010 items (at 3 items/min)
2015-11-04 03:09:09 [scrapy] INFO: Crawled 2130 pages (at 27 pages/min), scraped 2019 items (at 9 items/min)
2015-11-04 03:09:29 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1NzE2M3wwNTBhOTc0NXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:09:34 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1OTA4NnwyOGYzZDJjYnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:09:41 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1OTA4OXw4NjNhNDVhMnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:10:00 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1OTEwN3w1N2EwNmM4NXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:10:12 [scrapy] INFO: Crawled 2137 pages (at 7 pages/min), scraped 2032 items (at 13 items/min)
2015-11-04 03:11:09 [scrapy] INFO: Crawled 2143 pages (at 6 pages/min), scraped 2035 items (at 3 items/min)
2015-11-04 03:13:29 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTEzMzUyMXxjZTJmYmIxZXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:13:38 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEzMnw5ZGNlYmYyMnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:13:38 [scrapy] INFO: Crawled 2148 pages (at 5 pages/min), scraped 2043 items (at 8 items/min)
2015-11-04 03:14:42 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEzM3wxOTZlYTlhYnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:14:56 [scrapy] INFO: Crawled 2149 pages (at 1 pages/min), scraped 2046 items (at 3 items/min)
2015-11-04 03:15:58 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEzNXw0ODBjNTIxYnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:15:58 [scrapy] INFO: Crawled 2149 pages (at 0 pages/min), scraped 2046 items (at 0 items/min)
2015-11-04 03:16:08 [scrapy] INFO: Crawled 2168 pages (at 19 pages/min), scraped 2049 items (at 3 items/min)
2015-11-04 03:17:22 [scrapy] INFO: Crawled 2172 pages (at 4 pages/min), scraped 2065 items (at 16 items/min)
2015-11-04 03:18:04 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-944633-2.html> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:18:49 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3Njc4MHxhYzAwNGUxNHwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:18:50 [scrapy] INFO: Crawled 2172 pages (at 0 pages/min), scraped 2067 items (at 2 items/min)
2015-11-04 03:20:49 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/space-uid-131018327.html> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:21:51 [scrapy] INFO: Crawled 2201 pages (at 29 pages/min), scraped 2078 items (at 11 items/min)
2015-11-04 03:22:19 [scrapy] INFO: Crawled 2205 pages (at 4 pages/min), scraped 2085 items (at 7 items/min)
2015-11-04 03:23:54 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEyOXxmY2Y2YjZjOHwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:25:02 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEzNHw1ZjRmZWFjYXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:25:27 [scrapy] INFO: Crawled 2205 pages (at 0 pages/min), scraped 2093 items (at 8 items/min)
2015-11-04 03:25:48 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2Mzc1OXwyZTRjODFlNHwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:27:18 [scrapy] INFO: Crawled 2231 pages (at 26 pages/min), scraped 2102 items (at 9 items/min)
2015-11-04 03:29:28 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/space-uid-136344823.html> (referer: http://bbs.letv.com/thread-933534-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:29:32 [scrapy] INFO: Crawled 2231 pages (at 0 pages/min), scraped 2117 items (at 15 items/min)
2015-11-04 03:30:42 [scrapy] INFO: Crawled 2259 pages (at 28 pages/min), scraped 2133 items (at 16 items/min)
2015-11-04 03:32:14 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTAzMjU0MXw1YjExMzYxOHwxNDQ2NjA1ODUzfDB8OTMzNTM0&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-933534-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:32:16 [scrapy] INFO: Crawled 2267 pages (at 8 pages/min), scraped 2143 items (at 10 items/min)
2015-11-04 03:33:09 [scrapy] INFO: Crawled 2293 pages (at 26 pages/min), scraped 2149 items (at 6 items/min)
2015-11-04 03:34:37 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTAzMDI3M3w0Nzk3MjEwMnwxNDQ2NjA1ODUzfDB8OTMzNTM0&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-933534-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:34:58 [scrapy] INFO: Crawled 2303 pages (at 10 pages/min), scraped 2160 items (at 11 items/min)
2015-11-04 03:35:53 [scrapy] INFO: Crawled 2306 pages (at 3 pages/min), scraped 2172 items (at 12 items/min)
2015-11-04 03:36:28 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/space-uid-139610455.html> (referer: http://bbs.letv.com/thread-977435-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:37:14 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/space-uid-124350399.html> (referer: http://bbs.letv.com/thread-977435-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 661, in _read_chunked
    self._safe_read(2)      # toss the CRLF at the end of the chunk
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:39:46 [scrapy] INFO: Crawled 2306 pages (at 0 pages/min), scraped 2182 items (at 10 items/min)
2015-11-04 03:40:09 [scrapy] INFO: Crawled 2333 pages (at 27 pages/min), scraped 2192 items (at 10 items/min)
2015-11-04 03:41:17 [scrapy] INFO: Crawled 2341 pages (at 8 pages/min), scraped 2209 items (at 17 items/min)
2015-11-04 03:42:14 [scrapy] INFO: Crawled 2367 pages (at 26 pages/min), scraped 2227 items (at 18 items/min)
2015-11-04 03:43:08 [scrapy] INFO: Crawled 2375 pages (at 8 pages/min), scraped 2243 items (at 16 items/min)
2015-11-04 03:45:40 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/s?from=play&wd=%E8%B5%B5%E8%96%87> (referer: http://www.letv.com/ptv/vplay/1056153.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:46:01 [scrapy] INFO: Crawled 2386 pages (at 11 pages/min), scraped 2259 items (at 16 items/min)
2015-11-04 03:46:07 [scrapy] INFO: Crawled 2386 pages (at 0 pages/min), scraped 2261 items (at 2 items/min)
2015-11-04 03:47:34 [scrapy] INFO: Crawled 2427 pages (at 41 pages/min), scraped 2277 items (at 16 items/min)
2015-11-04 03:48:26 [scrapy] INFO: Crawled 2433 pages (at 6 pages/min), scraped 2290 items (at 13 items/min)
2015-11-04 03:50:27 [scrapy] INFO: Crawled 2433 pages (at 0 pages/min), scraped 2306 items (at 16 items/min)
2015-11-04 03:51:52 [scrapy] INFO: Crawled 2462 pages (at 29 pages/min), scraped 2325 items (at 19 items/min)
2015-11-04 03:53:53 [scrapy] INFO: Crawled 2470 pages (at 8 pages/min), scraped 2334 items (at 9 items/min)
2015-11-04 03:54:27 [scrapy] INFO: Crawled 2470 pages (at 0 pages/min), scraped 2342 items (at 8 items/min)
2015-11-04 03:55:09 [scrapy] INFO: Crawled 2499 pages (at 29 pages/min), scraped 2344 items (at 2 items/min)
2015-11-04 03:56:46 [scrapy] INFO: Crawled 2508 pages (at 9 pages/min), scraped 2360 items (at 16 items/min)
2015-11-04 03:56:56 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3Njc0MnwzMTA4MDFmZXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:57:25 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3Njc3NnxkYzI5OGEwZXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:58:35 [scrapy] INFO: Crawled 2508 pages (at 0 pages/min), scraped 2368 items (at 8 items/min)
2015-11-04 03:58:44 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3Njc2N3w1NmRhZWRiYXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:58:54 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3NjczNXw5NmU0OTUxY3wxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:59:00 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3NjczNHw2M2JmNTVhZHwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:59:42 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3Njc2MHw3ZDA3MGUxNHwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:59:42 [scrapy] INFO: Crawled 2508 pages (at 0 pages/min), scraped 2372 items (at 4 items/min)
2015-11-04 04:00:10 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3NjU4MHw2MGVjNTVkOHwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:01:16 [scrapy] INFO: Crawled 2530 pages (at 22 pages/min), scraped 2384 items (at 12 items/min)
2015-11-04 04:02:01 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3NjcxNXw3ZGUzYTg0MHwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:02:42 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA3NjcxN3w3MTkwMDY1NnwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:02:57 [scrapy] INFO: Crawled 2530 pages (at 0 pages/min), scraped 2390 items (at 6 items/min)
2015-11-04 04:03:10 [scrapy] INFO: Crawled 2537 pages (at 7 pages/min), scraped 2392 items (at 2 items/min)
2015-11-04 04:05:13 [scrapy] INFO: Crawled 2545 pages (at 8 pages/min), scraped 2398 items (at 6 items/min)
2015-11-04 04:06:14 [scrapy] INFO: Crawled 2545 pages (at 0 pages/min), scraped 2406 items (at 8 items/min)
2015-11-04 04:07:07 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTE0NDU3OHxhNTVjOTk5Y3wxNDQ2NjA2MTg0fDB8OTc0MTU5&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-974159-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:08:46 [scrapy] INFO: Crawled 2582 pages (at 37 pages/min), scraped 2419 items (at 13 items/min)
2015-11-04 04:09:42 [scrapy] INFO: Crawled 2582 pages (at 0 pages/min), scraped 2430 items (at 11 items/min)
2015-11-04 04:10:17 [scrapy] ERROR: Spider error processing <GET http://mobile.letv.com/lesync/> (referer: http://pc.letv.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 630, in _read_chunked
    line = self.fp.readline(_MAXLINE + 1)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 480, in readline
    data = self._sock.recv(self._rbufsize)
timeout: timed out
2015-11-04 04:11:14 [scrapy] ERROR: Error downloading <GET http://cloud.letv.com/down/tvapk/201502/v1.1.8/Ledisk_lecloud_main.apk>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://cloud.letv.com/down/tvapk/201502/v1.1.8/Ledisk_lecloud_main.apk took longer than 180.0 seconds..
2015-11-04 04:11:14 [scrapy] ERROR: Error downloading <GET http://cloud.letv.com/down/setup/201509/letvsetup.exe>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://cloud.letv.com/down/setup/201509/letvsetup.exe took longer than 180.0 seconds..
2015-11-04 04:11:14 [scrapy] ERROR: Error downloading <GET http://cloud.letv.com/down/apk/201504/v2.2.2/LetvCloudV2.2.2_lecloud_main.apk>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://cloud.letv.com/down/apk/201504/v2.2.2/LetvCloudV2.2.2_lecloud_main.apk took longer than 180.0 seconds..
2015-11-04 04:11:14 [scrapy] INFO: Crawled 2582 pages (at 0 pages/min), scraped 2441 items (at 11 items/min)
2015-11-04 04:12:33 [scrapy] INFO: Crawled 2584 pages (at 2 pages/min), scraped 2442 items (at 1 items/min)
2015-11-04 04:13:43 [scrapy] INFO: Crawled 2589 pages (at 5 pages/min), scraped 2443 items (at 1 items/min)
2015-11-04 04:15:32 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEyNnw1OTVlNTAwYXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:15:32 [scrapy] INFO: Crawled 2592 pages (at 3 pages/min), scraped 2443 items (at 0 items/min)
2015-11-04 04:16:46 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEyN3w0ZTcyM2Q5OXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:17:17 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEzMHxjNmFiNTJkZXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:18:05 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEyOHwzMTViZmNjZHwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:18:38 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEyM3wzN2ZkOTk5YnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:19:59 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEzMXw5MjkyOGQwZXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:21:34 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEyNHxiZWNhNTY0NXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:22:35 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MzEyNXw0ODQzNTY2NHwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:22:35 [scrapy] INFO: Crawled 2592 pages (at 0 pages/min), scraped 2443 items (at 0 items/min)
2015-11-04 04:23:44 [scrapy] INFO: Crawled 2606 pages (at 14 pages/min), scraped 2450 items (at 7 items/min)
2015-11-04 04:24:54 [scrapy] INFO: Crawled 2610 pages (at 4 pages/min), scraped 2452 items (at 2 items/min)
2015-11-04 04:27:24 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/space-uid-94496989.html> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:27:39 [scrapy] INFO: Crawled 2610 pages (at 0 pages/min), scraped 2456 items (at 4 items/min)
2015-11-04 04:27:46 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1NzE5MXw5NTYwMGEwMXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:27:47 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1NzE4NnwzZGNmZjEwNXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:27:56 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1NzE2OXwxY2UwNjhhZnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:27:58 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1NzE2N3w2NjhhNmU5N3wxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:28:02 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1NzE3MXw0ZTI1ZDc2YXwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:28:17 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1NzE3N3wwZjFhNzA1YnwxNDQ2NjA2MTgxfDB8OTQ0NjMz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:28:17 [scrapy] INFO: Crawled 2624 pages (at 14 pages/min), scraped 2461 items (at 5 items/min)
2015-11-04 04:29:52 [scrapy] INFO: Crawled 2627 pages (at 3 pages/min), scraped 2464 items (at 3 items/min)
2015-11-04 04:31:23 [scrapy] INFO: Crawled 2627 pages (at 0 pages/min), scraped 2468 items (at 4 items/min)
2015-11-04 04:32:26 [scrapy] INFO: Crawled 2627 pages (at 0 pages/min), scraped 2471 items (at 3 items/min)
2015-11-04 04:33:29 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-938402-1.html> (referer: http://bbs.letv.com/thread-977435-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:34:06 [scrapy] INFO: Crawled 2646 pages (at 19 pages/min), scraped 2478 items (at 7 items/min)
2015-11-04 04:34:52 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-331840-1.html> (referer: http://bbs.letv.com/thread-977435-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:35:26 [scrapy] INFO: Crawled 2646 pages (at 0 pages/min), scraped 2479 items (at 1 items/min)
2015-11-04 04:36:18 [scrapy] INFO: Crawled 2653 pages (at 7 pages/min), scraped 2481 items (at 2 items/min)
2015-11-04 04:36:33 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTEzMzc1M3xmOGJiNDRmZXwxNDQ2NjA2MTg0fDB8OTc3NDM1&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-977435-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:37:20 [scrapy] INFO: Crawled 2659 pages (at 6 pages/min), scraped 2485 items (at 4 items/min)
2015-11-04 04:38:42 [scrapy] INFO: Crawled 2663 pages (at 4 pages/min), scraped 2488 items (at 3 items/min)
2015-11-04 04:39:10 [scrapy] INFO: Crawled 2666 pages (at 3 pages/min), scraped 2493 items (at 5 items/min)
2015-11-04 04:39:25 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODU0MnwwNjk2ZjhjNXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:39:59 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODU0MXwwYTRkNjk5OXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:40:05 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODUxMXwwYzI5ZTE5ZnwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:40:12 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODUyNnxmMjc4MDlmMnwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:40:14 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODUyOHxhZmM1NWIwZHwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:40:14 [scrapy] INFO: Crawled 2676 pages (at 10 pages/min), scraped 2496 items (at 3 items/min)
2015-11-04 04:41:52 [scrapy] INFO: Crawled 2679 pages (at 3 pages/min), scraped 2500 items (at 4 items/min)
2015-11-04 04:43:17 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODUxN3xmNWU1MWM2YXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:43:17 [scrapy] INFO: Crawled 2680 pages (at 1 pages/min), scraped 2500 items (at 0 items/min)
2015-11-04 04:43:25 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODUxOHxhZmVhYzMyMXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:43:34 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODUxNXwwOGEwYjUyMnwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:44:22 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA1ODUxM3xhOGQzZGZmZXwxNDQ2NjA1ODg3fDB8OTUyNTQz&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:44:22 [scrapy] INFO: Crawled 2680 pages (at 0 pages/min), scraped 2500 items (at 0 items/min)
2015-11-04 04:45:29 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-952543-7.html?extra=page%3D1> (referer: http://bbs.letv.com/thread-952543-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:45:29 [scrapy] INFO: Crawled 2689 pages (at 9 pages/min), scraped 2502 items (at 2 items/min)
2015-11-04 04:46:15 [scrapy] INFO: Crawled 2693 pages (at 4 pages/min), scraped 2505 items (at 3 items/min)
2015-11-04 04:48:03 [scrapy] INFO: Crawled 2695 pages (at 2 pages/min), scraped 2508 items (at 3 items/min)
2015-11-04 04:48:39 [scrapy] INFO: Crawled 2695 pages (at 0 pages/min), scraped 2512 items (at 4 items/min)
2015-11-04 04:49:12 [scrapy] INFO: Crawled 2706 pages (at 11 pages/min), scraped 2516 items (at 4 items/min)
2015-11-04 04:49:57 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?action=reply&extra=page%3D1&fid=319&mod=post&page=1&repquote=19641597&tid=974159> (referer: http://bbs.letv.com/thread-974159-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:50:56 [scrapy] INFO: Crawled 2719 pages (at 13 pages/min), scraped 2524 items (at 8 items/min)
2015-11-04 04:52:34 [scrapy] INFO: Crawled 2724 pages (at 5 pages/min), scraped 2533 items (at 9 items/min)
2015-11-04 04:53:25 [scrapy] INFO: Crawled 2724 pages (at 0 pages/min), scraped 2537 items (at 4 items/min)
2015-11-04 04:54:55 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-877457-1.html> (referer: http://bbs.letv.com/thread-955370-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:55:39 [scrapy] INFO: Crawled 2724 pages (at 0 pages/min), scraped 2541 items (at 4 items/min)
2015-11-04 04:56:11 [scrapy] INFO: Crawled 2736 pages (at 12 pages/min), scraped 2544 items (at 3 items/min)
2015-11-04 04:58:02 [scrapy] INFO: Crawled 2740 pages (at 4 pages/min), scraped 2550 items (at 6 items/min)
2015-11-04 04:58:35 [scrapy] INFO: Crawled 2740 pages (at 0 pages/min), scraped 2553 items (at 3 items/min)
2015-11-04 04:59:44 [scrapy] INFO: Crawled 2740 pages (at 0 pages/min), scraped 2557 items (at 4 items/min)
2015-11-04 05:00:26 [scrapy] INFO: Crawled 2757 pages (at 17 pages/min), scraped 2559 items (at 2 items/min)
2015-11-04 05:02:40 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-960224-1.html> (referer: http://bbs.letv.com/thread-933534-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:02:57 [scrapy] INFO: Crawled 2764 pages (at 7 pages/min), scraped 2574 items (at 15 items/min)
2015-11-04 05:03:45 [scrapy] INFO: Crawled 2764 pages (at 0 pages/min), scraped 2580 items (at 6 items/min)
2015-11-04 05:05:28 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-867289-1.html> (referer: http://bbs.letv.com/thread-944633-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:06:04 [scrapy] INFO: Crawled 2804 pages (at 40 pages/min), scraped 2598 items (at 18 items/min)
2015-11-04 05:06:51 [scrapy] INFO: Crawled 2805 pages (at 1 pages/min), scraped 2608 items (at 10 items/min)
2015-11-04 05:09:05 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/space-uid-47378424.html> (referer: http://bbs.letv.com/thread-955370-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:09:12 [scrapy] INFO: Crawled 2807 pages (at 2 pages/min), scraped 2618 items (at 10 items/min)
2015-11-04 05:10:10 [scrapy] INFO: Crawled 2809 pages (at 2 pages/min), scraped 2620 items (at 2 items/min)
2015-11-04 05:12:12 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/home.php?mod=space&uid=129899421> (referer: http://bbs.letv.com/thread-955370-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:12:37 [scrapy] INFO: Crawled 2848 pages (at 39 pages/min), scraped 2637 items (at 17 items/min)
2015-11-04 05:13:55 [scrapy] INFO: Crawled 2849 pages (at 1 pages/min), scraped 2649 items (at 12 items/min)
2015-11-04 05:14:00 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2Mjc0N3w3ZWFkOWE1ZHwxNDQ2NjA2MTgxfDB8OTU1Mzcw&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-955370-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:14:07 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/forum.php?aid=MTA2MjcyNHw1NWNlNDFlZHwxNDQ2NjA2MTgxfDB8OTU1Mzcw&mod=attachment&nothumb=yes> (referer: http://bbs.letv.com/thread-955370-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:14:32 [scrapy] INFO: Crawled 2849 pages (at 0 pages/min), scraped 2658 items (at 9 items/min)
2015-11-04 05:16:23 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-955370-2.html?extra=page%3D1> (referer: http://bbs.letv.com/thread-955370-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:16:23 [scrapy] INFO: Crawled 2879 pages (at 30 pages/min), scraped 2668 items (at 10 items/min)
2015-11-04 05:17:14 [scrapy] INFO: Crawled 2888 pages (at 9 pages/min), scraped 2676 items (at 8 items/min)
2015-11-04 05:20:47 [scrapy] INFO: Crawled 2891 pages (at 3 pages/min), scraped 2689 items (at 13 items/min)
2015-11-04 05:21:47 [scrapy] INFO: Crawled 2891 pages (at 0 pages/min), scraped 2699 items (at 10 items/min)
2015-11-04 05:22:40 [scrapy] INFO: Crawled 2930 pages (at 39 pages/min), scraped 2708 items (at 9 items/min)
2015-11-04 05:26:16 [scrapy] INFO: Crawled 2938 pages (at 8 pages/min), scraped 2722 items (at 14 items/min)
2015-11-04 05:27:14 [scrapy] ERROR: Spider error processing <GET http://bbs.letv.com/thread-922761-1.html> (referer: http://bbs.letv.com/thread-955370-1.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:28:40 [scrapy] INFO: Crawled 2938 pages (at 0 pages/min), scraped 2737 items (at 15 items/min)
2015-11-04 05:29:35 [scrapy] INFO: Crawled 2938 pages (at 0 pages/min), scraped 2745 items (at 8 items/min)
2015-11-04 05:31:10 [scrapy] INFO: Crawled 2987 pages (at 49 pages/min), scraped 2769 items (at 24 items/min)
2015-11-04 05:34:37 [scrapy] INFO: Crawled 2995 pages (at 8 pages/min), scraped 2799 items (at 30 items/min)
2015-11-04 05:35:07 [scrapy] INFO: Crawled 3005 pages (at 10 pages/min), scraped 2803 items (at 4 items/min)
2015-11-04 05:37:07 [scrapy] INFO: Crawled 3026 pages (at 21 pages/min), scraped 2826 items (at 23 items/min)
2015-11-04 05:38:39 [scrapy] INFO: Crawled 3045 pages (at 19 pages/min), scraped 2840 items (at 14 items/min)
2015-11-04 05:39:44 [scrapy] INFO: Crawled 3056 pages (at 11 pages/min), scraped 2852 items (at 12 items/min)
2015-11-04 05:40:53 [scrapy] INFO: Crawled 3057 pages (at 1 pages/min), scraped 2863 items (at 11 items/min)
2015-11-04 05:41:14 [scrapy] INFO: Crawled 3067 pages (at 10 pages/min), scraped 2867 items (at 4 items/min)
2015-11-04 05:42:15 [scrapy] INFO: Crawled 3089 pages (at 22 pages/min), scraped 2874 items (at 7 items/min)
2015-11-04 05:43:20 [scrapy] INFO: Crawled 3094 pages (at 5 pages/min), scraped 2885 items (at 11 items/min)
2015-11-04 05:44:47 [scrapy] INFO: Crawled 3094 pages (at 0 pages/min), scraped 2900 items (at 15 items/min)
2015-11-04 05:45:20 [scrapy] INFO: Crawled 3123 pages (at 29 pages/min), scraped 2908 items (at 8 items/min)
2015-11-04 05:46:20 [scrapy] INFO: Crawled 3131 pages (at 8 pages/min), scraped 2919 items (at 11 items/min)
2015-11-04 05:47:32 [scrapy] INFO: Crawled 3131 pages (at 0 pages/min), scraped 2932 items (at 13 items/min)
2015-11-04 05:48:10 [scrapy] INFO: Crawled 3144 pages (at 13 pages/min), scraped 2937 items (at 5 items/min)
2015-11-04 05:49:19 [scrapy] INFO: Crawled 3152 pages (at 8 pages/min), scraped 2946 items (at 9 items/min)
2015-11-04 05:50:09 [scrapy] INFO: Crawled 3158 pages (at 6 pages/min), scraped 2954 items (at 8 items/min)
2015-11-04 05:51:20 [scrapy] INFO: Crawled 3167 pages (at 9 pages/min), scraped 2964 items (at 10 items/min)
2015-11-04 05:53:09 [scrapy] INFO: Crawled 3183 pages (at 16 pages/min), scraped 2979 items (at 15 items/min)
2015-11-04 05:54:13 [scrapy] INFO: Crawled 3189 pages (at 6 pages/min), scraped 2986 items (at 7 items/min)
2015-11-04 05:55:30 [scrapy] INFO: Crawled 3213 pages (at 24 pages/min), scraped 2997 items (at 11 items/min)
2015-11-04 05:56:08 [scrapy] INFO: Crawled 3214 pages (at 1 pages/min), scraped 3005 items (at 8 items/min)
2015-11-04 05:57:08 [scrapy] INFO: Crawled 3237 pages (at 23 pages/min), scraped 3018 items (at 13 items/min)
2015-11-04 05:58:09 [scrapy] INFO: Crawled 3245 pages (at 8 pages/min), scraped 3031 items (at 13 items/min)
2015-11-04 05:59:08 [scrapy] INFO: Crawled 3251 pages (at 6 pages/min), scraped 3040 items (at 9 items/min)
2015-11-04 06:00:55 [scrapy] INFO: Crawled 3276 pages (at 25 pages/min), scraped 3058 items (at 18 items/min)
2015-11-04 06:02:22 [scrapy] INFO: Crawled 3276 pages (at 0 pages/min), scraped 3070 items (at 12 items/min)
2015-11-04 06:03:55 [scrapy] INFO: Crawled 3318 pages (at 42 pages/min), scraped 3087 items (at 17 items/min)
2015-11-04 06:06:05 [scrapy] INFO: Crawled 3321 pages (at 3 pages/min), scraped 3104 items (at 17 items/min)
2015-11-04 06:06:39 [scrapy] ERROR: Spider error processing <GET http://mobile.letv.com/?ref=3032> (referer: http://so.letv.com/s?from=pc&ref=click&wd=%E6%85%88%E5%96%84)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 06:07:00 [scrapy] INFO: Crawled 3322 pages (at 1 pages/min), scraped 3110 items (at 6 items/min)
2015-11-04 06:07:16 [scrapy] INFO: Crawled 3322 pages (at 0 pages/min), scraped 3113 items (at 3 items/min)
2015-11-04 06:08:35 [scrapy] INFO: Crawled 3349 pages (at 27 pages/min), scraped 3123 items (at 10 items/min)
2015-11-04 06:09:18 [scrapy] INFO: Crawled 3349 pages (at 0 pages/min), scraped 3132 items (at 9 items/min)
2015-11-04 06:10:22 [scrapy] INFO: Crawled 3362 pages (at 13 pages/min), scraped 3141 items (at 9 items/min)
2015-11-04 06:11:17 [scrapy] INFO: Crawled 3369 pages (at 7 pages/min), scraped 3156 items (at 15 items/min)
2015-11-04 06:12:22 [scrapy] INFO: Crawled 3383 pages (at 14 pages/min), scraped 3164 items (at 8 items/min)
2015-11-04 06:13:17 [scrapy] INFO: Crawled 3385 pages (at 2 pages/min), scraped 3174 items (at 10 items/min)
2015-11-04 06:14:36 [scrapy] INFO: Crawled 3412 pages (at 27 pages/min), scraped 3195 items (at 21 items/min)
2015-11-04 06:15:28 [scrapy] INFO: Crawled 3412 pages (at 0 pages/min), scraped 3202 items (at 7 items/min)
2015-11-04 06:16:34 [scrapy] INFO: Crawled 3429 pages (at 17 pages/min), scraped 3212 items (at 10 items/min)
2015-11-04 06:17:18 [scrapy] INFO: Crawled 3437 pages (at 8 pages/min), scraped 3222 items (at 10 items/min)
2015-11-04 06:18:52 [scrapy] INFO: Crawled 3460 pages (at 23 pages/min), scraped 3242 items (at 20 items/min)
2015-11-04 06:19:27 [scrapy] INFO: Crawled 3460 pages (at 0 pages/min), scraped 3250 items (at 8 items/min)
2015-11-04 06:20:21 [scrapy] INFO: Crawled 3493 pages (at 33 pages/min), scraped 3267 items (at 17 items/min)
2015-11-04 06:21:32 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/ptv/vplay/2068458.html> (referer: http://www.letv.com/comic/92063.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 06:21:47 [scrapy] INFO: Crawled 3493 pages (at 0 pages/min), scraped 3282 items (at 15 items/min)
2015-11-04 06:23:00 [scrapy] INFO: Crawled 3517 pages (at 24 pages/min), scraped 3292 items (at 10 items/min)
2015-11-04 06:23:24 [scrapy] INFO: Crawled 3517 pages (at 0 pages/min), scraped 3299 items (at 7 items/min)
2015-11-04 06:24:28 [scrapy] INFO: Crawled 3535 pages (at 18 pages/min), scraped 3316 items (at 17 items/min)
2015-11-04 06:26:04 [scrapy] INFO: Crawled 3543 pages (at 8 pages/min), scraped 3324 items (at 8 items/min)
2015-11-04 06:27:14 [scrapy] INFO: Crawled 3543 pages (at 0 pages/min), scraped 3332 items (at 8 items/min)
2015-11-04 06:28:22 [scrapy] INFO: Crawled 3565 pages (at 22 pages/min), scraped 3354 items (at 22 items/min)
2015-11-04 06:29:20 [scrapy] INFO: Crawled 3591 pages (at 26 pages/min), scraped 3372 items (at 18 items/min)
2015-11-04 06:30:10 [scrapy] INFO: Crawled 3613 pages (at 22 pages/min), scraped 3388 items (at 16 items/min)
2015-11-04 06:31:19 [scrapy] INFO: Crawled 3620 pages (at 7 pages/min), scraped 3409 items (at 21 items/min)
2015-11-04 06:32:24 [scrapy] INFO: Crawled 3645 pages (at 25 pages/min), scraped 3418 items (at 9 items/min)
2015-11-04 06:33:46 [scrapy] INFO: Crawled 3645 pages (at 0 pages/min), scraped 3426 items (at 8 items/min)
2015-11-04 06:34:13 [scrapy] INFO: Crawled 3645 pages (at 0 pages/min), scraped 3434 items (at 8 items/min)
2015-11-04 06:35:30 [scrapy] INFO: Crawled 3672 pages (at 27 pages/min), scraped 3453 items (at 19 items/min)
2015-11-04 06:36:09 [scrapy] INFO: Crawled 3692 pages (at 20 pages/min), scraped 3465 items (at 12 items/min)
2015-11-04 06:37:31 [scrapy] INFO: Crawled 3700 pages (at 8 pages/min), scraped 3481 items (at 16 items/min)
2015-11-04 06:38:57 [scrapy] INFO: Crawled 3724 pages (at 24 pages/min), scraped 3497 items (at 16 items/min)
2015-11-04 06:39:21 [scrapy] INFO: Crawled 3724 pages (at 0 pages/min), scraped 3505 items (at 8 items/min)
2015-11-04 06:40:31 [scrapy] INFO: Crawled 3744 pages (at 20 pages/min), scraped 3519 items (at 14 items/min)
2015-11-04 06:41:07 [scrapy] INFO: Crawled 3751 pages (at 7 pages/min), scraped 3526 items (at 7 items/min)
2015-11-04 06:42:14 [scrapy] INFO: Crawled 3751 pages (at 0 pages/min), scraped 3540 items (at 14 items/min)
2015-11-04 06:43:18 [scrapy] INFO: Crawled 3778 pages (at 27 pages/min), scraped 3559 items (at 19 items/min)
2015-11-04 06:44:31 [scrapy] INFO: Crawled 3802 pages (at 24 pages/min), scraped 3576 items (at 17 items/min)
2015-11-04 06:45:22 [scrapy] INFO: Crawled 3802 pages (at 0 pages/min), scraped 3591 items (at 15 items/min)
2015-11-04 06:46:45 [scrapy] INFO: Crawled 3824 pages (at 22 pages/min), scraped 3600 items (at 9 items/min)
2015-11-04 06:47:08 [scrapy] INFO: Crawled 3824 pages (at 0 pages/min), scraped 3606 items (at 6 items/min)
2015-11-04 06:48:32 [scrapy] INFO: Crawled 3851 pages (at 27 pages/min), scraped 3633 items (at 27 items/min)
2015-11-04 06:49:53 [scrapy] INFO: Crawled 3877 pages (at 26 pages/min), scraped 3650 items (at 17 items/min)
2015-11-04 06:50:57 [scrapy] INFO: Crawled 3877 pages (at 0 pages/min), scraped 3658 items (at 8 items/min)
2015-11-04 06:51:27 [scrapy] INFO: Crawled 3877 pages (at 0 pages/min), scraped 3666 items (at 8 items/min)
2015-11-04 06:52:30 [scrapy] INFO: Crawled 3899 pages (at 22 pages/min), scraped 3675 items (at 9 items/min)
2015-11-04 06:53:13 [scrapy] INFO: Crawled 3899 pages (at 0 pages/min), scraped 3688 items (at 13 items/min)
2015-11-04 06:54:49 [scrapy] INFO: Crawled 3925 pages (at 26 pages/min), scraped 3698 items (at 10 items/min)
2015-11-04 06:55:19 [scrapy] INFO: Crawled 3925 pages (at 0 pages/min), scraped 3706 items (at 8 items/min)
2015-11-04 06:56:19 [scrapy] INFO: Crawled 3943 pages (at 18 pages/min), scraped 3721 items (at 15 items/min)
2015-11-04 06:57:24 [scrapy] INFO: Crawled 3957 pages (at 14 pages/min), scraped 3739 items (at 18 items/min)
2015-11-04 06:58:21 [scrapy] INFO: Crawled 3983 pages (at 26 pages/min), scraped 3756 items (at 17 items/min)
2015-11-04 06:59:44 [scrapy] INFO: Crawled 3991 pages (at 8 pages/min), scraped 3772 items (at 16 items/min)
2015-11-04 07:00:11 [scrapy] INFO: Crawled 3991 pages (at 0 pages/min), scraped 3780 items (at 8 items/min)
2015-11-04 07:01:07 [scrapy] INFO: Crawled 4023 pages (at 32 pages/min), scraped 3797 items (at 17 items/min)
2015-11-04 07:02:24 [scrapy] INFO: Crawled 4031 pages (at 8 pages/min), scraped 3813 items (at 16 items/min)
2015-11-04 07:03:32 [scrapy] INFO: Crawled 4039 pages (at 8 pages/min), scraped 3827 items (at 14 items/min)
2015-11-04 07:04:18 [scrapy] INFO: Crawled 4039 pages (at 0 pages/min), scraped 3828 items (at 1 items/min)
2015-11-04 07:05:27 [scrapy] INFO: Crawled 4068 pages (at 29 pages/min), scraped 3841 items (at 13 items/min)
2015-11-04 07:06:30 [scrapy] INFO: Crawled 4068 pages (at 0 pages/min), scraped 3857 items (at 16 items/min)
2015-11-04 07:07:12 [scrapy] INFO: Crawled 4085 pages (at 17 pages/min), scraped 3859 items (at 2 items/min)
2015-11-04 07:08:52 [scrapy] INFO: Crawled 4085 pages (at 0 pages/min), scraped 3874 items (at 15 items/min)
2015-11-04 07:09:31 [scrapy] INFO: Crawled 4111 pages (at 26 pages/min), scraped 3885 items (at 11 items/min)
2015-11-04 07:10:12 [scrapy] INFO: Crawled 4111 pages (at 0 pages/min), scraped 3900 items (at 15 items/min)
2015-11-04 07:11:10 [scrapy] INFO: Crawled 4136 pages (at 25 pages/min), scraped 3917 items (at 17 items/min)
2015-11-04 07:12:27 [scrapy] INFO: Crawled 4161 pages (at 25 pages/min), scraped 3942 items (at 25 items/min)
2015-11-04 07:13:07 [scrapy] INFO: Crawled 4178 pages (at 17 pages/min), scraped 3952 items (at 10 items/min)
2015-11-04 07:14:49 [scrapy] INFO: Crawled 4186 pages (at 8 pages/min), scraped 3967 items (at 15 items/min)
2015-11-04 07:15:46 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/ptv/vplay/2154254.html> (referer: http://www.letv.com/comic/94075.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 07:15:46 [scrapy] INFO: Crawled 4186 pages (at 0 pages/min), scraped 3974 items (at 7 items/min)
2015-11-04 07:16:19 [scrapy] INFO: Crawled 4215 pages (at 29 pages/min), scraped 3987 items (at 13 items/min)
2015-11-04 07:17:48 [scrapy] INFO: Crawled 4216 pages (at 1 pages/min), scraped 4003 items (at 16 items/min)
2015-11-04 07:18:08 [scrapy] INFO: Crawled 4227 pages (at 11 pages/min), scraped 4007 items (at 4 items/min)
2015-11-04 07:19:26 [scrapy] INFO: Crawled 4257 pages (at 30 pages/min), scraped 4029 items (at 22 items/min)
2015-11-04 07:20:14 [scrapy] INFO: Crawled 4257 pages (at 0 pages/min), scraped 4044 items (at 15 items/min)
2015-11-04 07:21:09 [scrapy] INFO: Crawled 4293 pages (at 36 pages/min), scraped 4061 items (at 17 items/min)
2015-11-04 07:22:21 [scrapy] INFO: Crawled 4294 pages (at 1 pages/min), scraped 4073 items (at 12 items/min)
2015-11-04 07:23:11 [scrapy] INFO: Crawled 4321 pages (at 27 pages/min), scraped 4087 items (at 14 items/min)
2015-11-04 07:25:35 [scrapy] INFO: Crawled 4336 pages (at 15 pages/min), scraped 4115 items (at 28 items/min)
2015-11-04 07:26:20 [scrapy] INFO: Crawled 4336 pages (at 0 pages/min), scraped 4124 items (at 9 items/min)
2015-11-04 07:27:24 [scrapy] INFO: Crawled 4370 pages (at 34 pages/min), scraped 4142 items (at 18 items/min)
2015-11-04 07:29:45 [scrapy] INFO: Crawled 4376 pages (at 6 pages/min), scraped 4164 items (at 22 items/min)
2015-11-04 07:30:44 [scrapy] INFO: Crawled 4410 pages (at 34 pages/min), scraped 4181 items (at 17 items/min)
2015-11-04 07:31:16 [scrapy] INFO: Crawled 4410 pages (at 0 pages/min), scraped 4190 items (at 9 items/min)
2015-11-04 07:32:45 [scrapy] INFO: Crawled 4438 pages (at 28 pages/min), scraped 4210 items (at 20 items/min)
2015-11-04 07:33:18 [scrapy] INFO: Crawled 4438 pages (at 0 pages/min), scraped 4218 items (at 8 items/min)
2015-11-04 07:34:29 [scrapy] INFO: Crawled 4438 pages (at 0 pages/min), scraped 4226 items (at 8 items/min)
2015-11-04 07:35:31 [scrapy] INFO: Crawled 4477 pages (at 39 pages/min), scraped 4241 items (at 15 items/min)
2015-11-04 07:36:14 [scrapy] INFO: Crawled 4477 pages (at 0 pages/min), scraped 4252 items (at 11 items/min)
2015-11-04 07:37:18 [scrapy] INFO: Crawled 4517 pages (at 40 pages/min), scraped 4278 items (at 26 items/min)
2015-11-04 07:39:16 [scrapy] INFO: Crawled 4519 pages (at 2 pages/min), scraped 4304 items (at 26 items/min)
2015-11-04 07:40:01 [scrapy] ERROR: Spider error processing <GET http://music.letv.com/izt/langlang/index.html> (referer: http://music.letv.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 630, in _read_chunked
    line = self.fp.readline(_MAXLINE + 1)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 480, in readline
    data = self._sock.recv(self._rbufsize)
timeout: timed out
2015-11-04 07:40:47 [scrapy] INFO: Crawled 4553 pages (at 34 pages/min), scraped 4327 items (at 23 items/min)
2015-11-04 07:41:27 [scrapy] INFO: Crawled 4553 pages (at 0 pages/min), scraped 4339 items (at 12 items/min)
2015-11-04 07:42:11 [scrapy] INFO: Crawled 4586 pages (at 33 pages/min), scraped 4350 items (at 11 items/min)
2015-11-04 07:44:01 [scrapy] INFO: Crawled 4588 pages (at 2 pages/min), scraped 4374 items (at 24 items/min)
2015-11-04 07:44:09 [scrapy] INFO: Crawled 4604 pages (at 16 pages/min), scraped 4377 items (at 3 items/min)
2015-11-04 07:45:34 [scrapy] INFO: Crawled 4612 pages (at 8 pages/min), scraped 4398 items (at 21 items/min)
2015-11-04 07:47:11 [scrapy] INFO: Crawled 4641 pages (at 29 pages/min), scraped 4419 items (at 21 items/min)
2015-11-04 07:48:18 [scrapy] INFO: Crawled 4678 pages (at 37 pages/min), scraped 4445 items (at 26 items/min)
2015-11-04 07:49:18 [scrapy] INFO: Crawled 4679 pages (at 1 pages/min), scraped 4465 items (at 20 items/min)
2015-11-04 07:51:36 [scrapy] INFO: Crawled 4724 pages (at 45 pages/min), scraped 4482 items (at 17 items/min)
2015-11-04 07:53:42 [scrapy] INFO: Crawled 4724 pages (at 0 pages/min), scraped 4497 items (at 15 items/min)
2015-11-04 07:54:18 [scrapy] INFO: Crawled 4724 pages (at 0 pages/min), scraped 4510 items (at 13 items/min)
2015-11-04 07:56:11 [scrapy] INFO: Crawled 4760 pages (at 36 pages/min), scraped 4527 items (at 17 items/min)
2015-11-04 07:57:36 [scrapy] ERROR: Spider error processing <GET http://list.letv.com/listn/c5_t30055_a50071_y-1_vt-1_f-1_s1_lg-1_st-1_md_o9_d1_p.html> (referer: http://list.letv.com/listn/c5_t-1_a50071_y-1_vt-1_f-1_s1_lg-1_st-1_md_o9_d1_p.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 07:57:54 [scrapy] INFO: Crawled 4763 pages (at 3 pages/min), scraped 4548 items (at 21 items/min)
2015-11-04 08:00:20 [scrapy] INFO: Crawled 4791 pages (at 28 pages/min), scraped 4566 items (at 18 items/min)
2015-11-04 08:01:29 [scrapy] INFO: Crawled 4801 pages (at 10 pages/min), scraped 4581 items (at 15 items/min)
2015-11-04 08:02:18 [scrapy] INFO: Crawled 4803 pages (at 2 pages/min), scraped 4585 items (at 4 items/min)
2015-11-04 08:03:46 [scrapy] INFO: Crawled 4836 pages (at 33 pages/min), scraped 4609 items (at 24 items/min)
2015-11-04 08:04:24 [scrapy] INFO: Crawled 4841 pages (at 5 pages/min), scraped 4620 items (at 11 items/min)
2015-11-04 08:05:18 [scrapy] INFO: Crawled 4875 pages (at 34 pages/min), scraped 4631 items (at 11 items/min)
2015-11-04 08:07:45 [scrapy] INFO: Crawled 4879 pages (at 4 pages/min), scraped 4651 items (at 20 items/min)
2015-11-04 08:08:33 [scrapy] INFO: Crawled 4888 pages (at 9 pages/min), scraped 4663 items (at 12 items/min)
2015-11-04 08:09:06 [scrapy] INFO: Crawled 4888 pages (at 0 pages/min), scraped 4672 items (at 9 items/min)
2015-11-04 08:10:21 [scrapy] INFO: Crawled 4941 pages (at 53 pages/min), scraped 4693 items (at 21 items/min)
2015-11-04 08:11:24 [scrapy] INFO: Crawled 4941 pages (at 0 pages/min), scraped 4710 items (at 17 items/min)
2015-11-04 08:12:22 [scrapy] INFO: Crawled 4941 pages (at 0 pages/min), scraped 4725 items (at 15 items/min)
2015-11-04 08:13:55 [scrapy] INFO: Crawled 4969 pages (at 28 pages/min), scraped 4745 items (at 20 items/min)
2015-11-04 08:14:27 [scrapy] INFO: Crawled 4969 pages (at 0 pages/min), scraped 4753 items (at 8 items/min)
2015-11-04 08:15:20 [scrapy] INFO: Crawled 4993 pages (at 24 pages/min), scraped 4761 items (at 8 items/min)
2015-11-04 08:16:15 [scrapy] INFO: Crawled 4993 pages (at 0 pages/min), scraped 4769 items (at 8 items/min)
2015-11-04 08:17:10 [scrapy] INFO: Crawled 5013 pages (at 20 pages/min), scraped 4781 items (at 12 items/min)
2015-11-04 08:18:20 [scrapy] INFO: Crawled 5024 pages (at 11 pages/min), scraped 4797 items (at 16 items/min)
2015-11-04 08:19:18 [scrapy] INFO: Crawled 5025 pages (at 1 pages/min), scraped 4808 items (at 11 items/min)
2015-11-04 08:20:26 [scrapy] INFO: Crawled 5034 pages (at 9 pages/min), scraped 4815 items (at 7 items/min)
2015-11-04 08:21:17 [scrapy] INFO: Crawled 5045 pages (at 11 pages/min), scraped 4823 items (at 8 items/min)
2015-11-04 08:22:12 [scrapy] INFO: Crawled 5049 pages (at 4 pages/min), scraped 4830 items (at 7 items/min)
2015-11-04 08:24:06 [scrapy] INFO: Crawled 5071 pages (at 22 pages/min), scraped 4846 items (at 16 items/min)
2015-11-04 08:25:03 [scrapy] INFO: Crawled 5072 pages (at 1 pages/min), scraped 4855 items (at 9 items/min)
2015-11-04 08:25:13 [scrapy] INFO: Crawled 5077 pages (at 5 pages/min), scraped 4856 items (at 1 items/min)
2015-11-04 08:26:09 [scrapy] INFO: Crawled 5089 pages (at 12 pages/min), scraped 4862 items (at 6 items/min)
2015-11-04 08:26:47 [scrapy] ERROR: Error downloading <GET http://www.letv.com/comic/84595.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:26:55 [scrapy] ERROR: Error downloading <GET http://www.letv.com/comic/85353.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:26:55 [scrapy] ERROR: Error downloading <GET http://www.letv.com/comic/84496.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:27:08 [scrapy] INFO: Crawled 5107 pages (at 18 pages/min), scraped 4876 items (at 14 items/min)
2015-11-04 08:28:29 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/comic/84461.html> (referer: http://comic.letv.com/zt/nick/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 08:28:29 [scrapy] INFO: Crawled 5107 pages (at 0 pages/min), scraped 4884 items (at 8 items/min)
2015-11-04 08:30:50 [scrapy] INFO: Crawled 5130 pages (at 23 pages/min), scraped 4902 items (at 18 items/min)
2015-11-04 08:30:50 [scrapy] ERROR: Error downloading <GET http://www.letv.com/comic/85352.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:31:09 [scrapy] INFO: Crawled 5167 pages (at 37 pages/min), scraped 4907 items (at 5 items/min)
2015-11-04 08:33:10 [scrapy] INFO: Crawled 5170 pages (at 3 pages/min), scraped 4940 items (at 33 items/min)
2015-11-04 08:34:29 [scrapy] INFO: Crawled 5196 pages (at 26 pages/min), scraped 4957 items (at 17 items/min)
2015-11-04 08:36:15 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/s?wd=%E5%88%98%E9%A0%94> (referer: http://list.letv.com/listn/c1_t-1_a-1_y-1_s2_lg-1_ph-1_md_o1_d1_p.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 08:36:36 [scrapy] INFO: Crawled 5206 pages (at 10 pages/min), scraped 4966 items (at 9 items/min)
2015-11-04 08:37:11 [scrapy] ERROR: Error downloading <GET http://sports.letv.com/video/23349820.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:37:11 [scrapy] ERROR: Error downloading <GET http://sports.letv.com/video/23347493.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:37:12 [scrapy] INFO: Crawled 5206 pages (at 0 pages/min), scraped 4978 items (at 12 items/min)
2015-11-04 08:39:04 [scrapy] INFO: Crawled 5242 pages (at 36 pages/min), scraped 5002 items (at 24 items/min)
2015-11-04 08:39:53 [scrapy] INFO: Crawled 5242 pages (at 0 pages/min), scraped 5014 items (at 12 items/min)
2015-11-04 08:40:11 [scrapy] INFO: Crawled 5273 pages (at 31 pages/min), scraped 5020 items (at 6 items/min)
2015-11-04 08:41:51 [scrapy] INFO: Crawled 5281 pages (at 8 pages/min), scraped 5049 items (at 29 items/min)
2015-11-04 08:42:20 [scrapy] INFO: Crawled 5289 pages (at 8 pages/min), scraped 5053 items (at 4 items/min)
2015-11-04 08:43:07 [scrapy] INFO: Crawled 5306 pages (at 17 pages/min), scraped 5066 items (at 13 items/min)
2015-11-04 08:44:49 [scrapy] INFO: Crawled 5346 pages (at 40 pages/min), scraped 5083 items (at 17 items/min)
2015-11-04 08:46:25 [scrapy] INFO: Crawled 5346 pages (at 0 pages/min), scraped 5104 items (at 21 items/min)
2015-11-04 08:47:25 [scrapy] INFO: Crawled 5346 pages (at 0 pages/min), scraped 5118 items (at 14 items/min)
2015-11-04 08:48:05 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/s?from=play&wd=%E7%AE%A1%E6%99%93%E6%9D%B0> (referer: http://www.letv.com/ptv/vplay/23106334.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 08:48:13 [scrapy] INFO: Crawled 5362 pages (at 16 pages/min), scraped 5120 items (at 2 items/min)
2015-11-04 08:50:03 [scrapy] INFO: Crawled 5362 pages (at 0 pages/min), scraped 5133 items (at 13 items/min)
2015-11-04 08:50:26 [scrapy] INFO: Crawled 5376 pages (at 14 pages/min), scraped 5134 items (at 1 items/min)
2015-11-04 08:51:12 [scrapy] INFO: Crawled 5400 pages (at 24 pages/min), scraped 5147 items (at 13 items/min)
2015-11-04 08:53:09 [scrapy] INFO: Crawled 5400 pages (at 0 pages/min), scraped 5171 items (at 24 items/min)
2015-11-04 08:54:48 [scrapy] INFO: Crawled 5437 pages (at 37 pages/min), scraped 5191 items (at 20 items/min)
2015-11-04 08:56:05 [scrapy] INFO: Crawled 5439 pages (at 2 pages/min), scraped 5205 items (at 14 items/min)
2015-11-04 08:56:34 [scrapy] INFO: Crawled 5441 pages (at 2 pages/min), scraped 5210 items (at 5 items/min)
2015-11-04 08:58:27 [scrapy] INFO: Crawled 5473 pages (at 32 pages/min), scraped 5225 items (at 15 items/min)
2015-11-04 08:59:15 [scrapy] INFO: Crawled 5478 pages (at 5 pages/min), scraped 5236 items (at 11 items/min)
2015-11-04 09:00:13 [scrapy] INFO: Crawled 5483 pages (at 5 pages/min), scraped 5251 items (at 15 items/min)
2015-11-04 09:01:43 [scrapy] INFO: Crawled 5515 pages (at 32 pages/min), scraped 5271 items (at 20 items/min)
2015-11-04 09:01:43 [scrapy] ERROR: Error downloading <GET http://so.letv.com/s?wd=%E7%8E%8B%E5%AE%9B%E6%99%A8>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 09:02:57 [scrapy] INFO: Crawled 5517 pages (at 2 pages/min), scraped 5286 items (at 15 items/min)
2015-11-04 09:03:08 [scrapy] INFO: Crawled 5533 pages (at 16 pages/min), scraped 5289 items (at 3 items/min)
2015-11-04 09:04:08 [scrapy] INFO: Crawled 5553 pages (at 20 pages/min), scraped 5303 items (at 14 items/min)
2015-11-04 09:05:16 [scrapy] INFO: Crawled 5554 pages (at 1 pages/min), scraped 5312 items (at 9 items/min)
2015-11-04 09:06:13 [scrapy] INFO: Crawled 5596 pages (at 42 pages/min), scraped 5328 items (at 16 items/min)
2015-11-04 09:08:39 [scrapy] INFO: Crawled 5604 pages (at 8 pages/min), scraped 5342 items (at 14 items/min)
2015-11-04 09:09:08 [scrapy] INFO: Crawled 5604 pages (at 0 pages/min), scraped 5350 items (at 8 items/min)
2015-11-04 09:10:32 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/comic/84464.html> (referer: http://comic.letv.com/zt/nick/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 09:10:51 [scrapy] INFO: Crawled 5646 pages (at 42 pages/min), scraped 5361 items (at 11 items/min)
2015-11-04 09:12:41 [scrapy] INFO: Crawled 5650 pages (at 4 pages/min), scraped 5376 items (at 15 items/min)
2015-11-04 09:13:52 [scrapy] INFO: Crawled 5650 pages (at 0 pages/min), scraped 5395 items (at 19 items/min)
2015-11-04 09:16:27 [scrapy] INFO: Crawled 5699 pages (at 49 pages/min), scraped 5422 items (at 27 items/min)
2015-11-04 09:17:39 [scrapy] INFO: Crawled 5707 pages (at 8 pages/min), scraped 5443 items (at 21 items/min)
2015-11-04 09:19:13 [scrapy] INFO: Crawled 5707 pages (at 0 pages/min), scraped 5451 items (at 8 items/min)
2015-11-04 09:21:01 [scrapy] INFO: Crawled 5727 pages (at 20 pages/min), scraped 5460 items (at 9 items/min)
2015-11-04 09:22:17 [scrapy] INFO: Crawled 5733 pages (at 6 pages/min), scraped 5471 items (at 11 items/min)
2015-11-04 09:23:46 [scrapy] INFO: Crawled 5762 pages (at 29 pages/min), scraped 5485 items (at 14 items/min)
2015-11-04 09:24:37 [scrapy] INFO: Crawled 5767 pages (at 5 pages/min), scraped 5497 items (at 12 items/min)
2015-11-04 09:25:21 [scrapy] INFO: Crawled 5776 pages (at 9 pages/min), scraped 5509 items (at 12 items/min)
2015-11-04 09:27:19 [scrapy] INFO: Crawled 5776 pages (at 0 pages/min), scraped 5520 items (at 11 items/min)
2015-11-04 09:29:26 [scrapy] INFO: Crawled 5824 pages (at 48 pages/min), scraped 5551 items (at 31 items/min)
2015-11-04 09:31:42 [scrapy] INFO: Crawled 5831 pages (at 7 pages/min), scraped 5568 items (at 17 items/min)
2015-11-04 09:32:33 [scrapy] INFO: Crawled 5831 pages (at 0 pages/min), scraped 5575 items (at 7 items/min)
2015-11-04 09:34:17 [scrapy] INFO: Crawled 5857 pages (at 26 pages/min), scraped 5587 items (at 12 items/min)
2015-11-04 09:36:11 [scrapy] INFO: Crawled 5857 pages (at 0 pages/min), scraped 5601 items (at 14 items/min)
2015-11-04 09:38:01 [scrapy] INFO: Crawled 5900 pages (at 43 pages/min), scraped 5617 items (at 16 items/min)
2015-11-04 09:38:59 [scrapy] INFO: Crawled 5900 pages (at 0 pages/min), scraped 5632 items (at 15 items/min)
2015-11-04 09:39:40 [scrapy] INFO: Crawled 5900 pages (at 0 pages/min), scraped 5644 items (at 12 items/min)
2015-11-04 09:40:41 [scrapy] INFO: Crawled 5924 pages (at 24 pages/min), scraped 5660 items (at 16 items/min)
2015-11-04 09:41:12 [scrapy] INFO: Crawled 5924 pages (at 0 pages/min), scraped 5668 items (at 8 items/min)
2015-11-04 09:42:20 [scrapy] INFO: Crawled 5948 pages (at 24 pages/min), scraped 5686 items (at 18 items/min)
2015-11-04 09:43:28 [scrapy] INFO: Crawled 5969 pages (at 21 pages/min), scraped 5706 items (at 20 items/min)
2015-11-04 09:45:07 [scrapy] INFO: Crawled 5977 pages (at 8 pages/min), scraped 5713 items (at 7 items/min)
2015-11-04 09:46:16 [scrapy] INFO: Crawled 5998 pages (at 21 pages/min), scraped 5728 items (at 15 items/min)
2015-11-04 09:47:23 [scrapy] INFO: Crawled 6005 pages (at 7 pages/min), scraped 5742 items (at 14 items/min)
2015-11-04 09:48:07 [scrapy] INFO: Crawled 6019 pages (at 14 pages/min), scraped 5756 items (at 14 items/min)
2015-11-04 09:49:24 [scrapy] INFO: Crawled 6026 pages (at 7 pages/min), scraped 5770 items (at 14 items/min)
2015-11-04 09:50:32 [scrapy] INFO: Crawled 6053 pages (at 27 pages/min), scraped 5781 items (at 11 items/min)
2015-11-04 09:51:09 [scrapy] INFO: Crawled 6053 pages (at 0 pages/min), scraped 5789 items (at 8 items/min)
2015-11-04 09:52:06 [scrapy] INFO: Crawled 6053 pages (at 0 pages/min), scraped 5797 items (at 8 items/min)
2015-11-04 09:53:11 [scrapy] INFO: Crawled 6082 pages (at 29 pages/min), scraped 5819 items (at 22 items/min)
2015-11-04 09:54:14 [scrapy] INFO: Crawled 6106 pages (at 24 pages/min), scraped 5836 items (at 17 items/min)
2015-11-04 09:55:13 [scrapy] INFO: Crawled 6106 pages (at 0 pages/min), scraped 5843 items (at 7 items/min)
2015-11-04 09:56:12 [scrapy] INFO: Crawled 6110 pages (at 4 pages/min), scraped 5853 items (at 10 items/min)
2015-11-04 09:58:03 [scrapy] INFO: Crawled 6154 pages (at 44 pages/min), scraped 5881 items (at 28 items/min)
2015-11-04 09:58:37 [scrapy] INFO: Crawled 6154 pages (at 0 pages/min), scraped 5890 items (at 9 items/min)
2015-11-04 09:59:18 [scrapy] INFO: Crawled 6168 pages (at 14 pages/min), scraped 5904 items (at 14 items/min)
2015-11-04 10:00:45 [scrapy] INFO: Crawled 6181 pages (at 13 pages/min), scraped 5925 items (at 21 items/min)
2015-11-04 10:01:44 [scrapy] INFO: Crawled 6222 pages (at 41 pages/min), scraped 5941 items (at 16 items/min)
2015-11-04 10:02:50 [scrapy] INFO: Crawled 6222 pages (at 0 pages/min), scraped 5954 items (at 13 items/min)
2015-11-04 10:03:32 [scrapy] INFO: Crawled 6222 pages (at 0 pages/min), scraped 5966 items (at 12 items/min)
2015-11-04 10:04:34 [scrapy] INFO: Crawled 6253 pages (at 31 pages/min), scraped 5982 items (at 16 items/min)
2015-11-04 10:05:14 [scrapy] INFO: Crawled 6253 pages (at 0 pages/min), scraped 5997 items (at 15 items/min)
2015-11-04 10:06:20 [scrapy] INFO: Crawled 6290 pages (at 37 pages/min), scraped 6012 items (at 15 items/min)
2015-11-04 10:08:00 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/star?from=list&wd=%E8%A5%BF%E6%81%A9%C2%B7%E6%BD%98> (referer: http://list.letv.com/listn/c1_t-1_a50071_y-1_s1_lg-1_ph-1_md_o9_d1_p.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 10:08:29 [scrapy] INFO: Crawled 6290 pages (at 0 pages/min), scraped 6025 items (at 13 items/min)
2015-11-04 10:09:45 [scrapy] INFO: Crawled 6318 pages (at 28 pages/min), scraped 6049 items (at 24 items/min)
2015-11-04 10:10:53 [scrapy] INFO: Crawled 6318 pages (at 0 pages/min), scraped 6061 items (at 12 items/min)
2015-11-04 10:11:38 [scrapy] INFO: Crawled 6351 pages (at 33 pages/min), scraped 6072 items (at 11 items/min)
2015-11-04 10:13:17 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/star?from=list&wd=%E6%89%98%E5%B0%BC%C2%B7%E8%B4%BE> (referer: http://list.letv.com/listn/c1_t-1_a50001_y-1_s1_lg-1_ph-1_md_o9_d1_p.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 10:13:42 [scrapy] INFO: Crawled 6351 pages (at 0 pages/min), scraped 6081 items (at 9 items/min)
2015-11-04 10:14:23 [scrapy] INFO: Crawled 6351 pages (at 0 pages/min), scraped 6093 items (at 12 items/min)
2015-11-04 10:15:23 [scrapy] INFO: Crawled 6370 pages (at 19 pages/min), scraped 6109 items (at 16 items/min)
2015-11-04 10:16:07 [scrapy] INFO: Crawled 6385 pages (at 15 pages/min), scraped 6119 items (at 10 items/min)
2015-11-04 10:17:09 [scrapy] INFO: Crawled 6413 pages (at 28 pages/min), scraped 6139 items (at 20 items/min)
2015-11-04 10:18:31 [scrapy] INFO: Crawled 6413 pages (at 0 pages/min), scraped 6155 items (at 16 items/min)
2015-11-04 10:19:31 [scrapy] INFO: Crawled 6428 pages (at 15 pages/min), scraped 6170 items (at 15 items/min)
2015-11-04 10:20:20 [scrapy] INFO: Crawled 6451 pages (at 23 pages/min), scraped 6189 items (at 19 items/min)
2015-11-04 10:21:06 [scrapy] INFO: Crawled 6462 pages (at 11 pages/min), scraped 6204 items (at 15 items/min)
2015-11-04 10:22:08 [scrapy] INFO: Crawled 6482 pages (at 20 pages/min), scraped 6218 items (at 14 items/min)
2015-11-04 10:23:28 [scrapy] INFO: Crawled 6512 pages (at 30 pages/min), scraped 6240 items (at 22 items/min)
2015-11-04 10:24:15 [scrapy] INFO: Crawled 6513 pages (at 1 pages/min), scraped 6254 items (at 14 items/min)
2015-11-04 10:25:49 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/star?from=list&wd=%E6%9D%8E%E6%B2%A7%E4%B8%9C> (referer: http://list.letv.com/listn/c1_t-1_a50042_y-1_s1_lg-1_ph-1_md_o9_d1_p.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 10:26:08 [scrapy] INFO: Crawled 6546 pages (at 33 pages/min), scraped 6278 items (at 24 items/min)
2015-11-04 10:27:46 [scrapy] INFO: Crawled 6582 pages (at 36 pages/min), scraped 6304 items (at 26 items/min)
2015-11-04 10:28:10 [scrapy] INFO: Crawled 6584 pages (at 2 pages/min), scraped 6312 items (at 8 items/min)
2015-11-04 10:29:19 [scrapy] INFO: Crawled 6610 pages (at 26 pages/min), scraped 6336 items (at 24 items/min)
2015-11-04 10:31:20 [scrapy] INFO: Crawled 6615 pages (at 5 pages/min), scraped 6353 items (at 17 items/min)
2015-11-04 10:32:57 [scrapy] INFO: Crawled 6656 pages (at 41 pages/min), scraped 6376 items (at 23 items/min)
2015-11-04 10:33:59 [scrapy] INFO: Crawled 6657 pages (at 1 pages/min), scraped 6392 items (at 16 items/min)
2015-11-04 10:34:13 [scrapy] INFO: Crawled 6657 pages (at 0 pages/min), scraped 6397 items (at 5 items/min)
2015-11-04 10:35:18 [scrapy] INFO: Crawled 6686 pages (at 29 pages/min), scraped 6417 items (at 20 items/min)
2015-11-04 10:36:26 [scrapy] INFO: Crawled 6706 pages (at 20 pages/min), scraped 6436 items (at 19 items/min)
2015-11-04 10:37:37 [scrapy] INFO: Crawled 6713 pages (at 7 pages/min), scraped 6453 items (at 17 items/min)
2015-11-04 10:38:45 [scrapy] INFO: Crawled 6743 pages (at 30 pages/min), scraped 6472 items (at 19 items/min)
2015-11-04 10:39:19 [scrapy] INFO: Crawled 6747 pages (at 4 pages/min), scraped 6482 items (at 10 items/min)
2015-11-04 10:40:33 [scrapy] INFO: Crawled 6760 pages (at 13 pages/min), scraped 6494 items (at 12 items/min)
2015-11-04 10:41:28 [scrapy] INFO: Crawled 6773 pages (at 13 pages/min), scraped 6504 items (at 10 items/min)
2015-11-04 10:42:08 [scrapy] INFO: Crawled 6776 pages (at 3 pages/min), scraped 6514 items (at 10 items/min)
2015-11-04 10:44:42 [scrapy] INFO: Crawled 6836 pages (at 60 pages/min), scraped 6539 items (at 25 items/min)
2015-11-04 10:46:03 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/s?from=play&wd=%E9%99%88%E6%99%93> (referer: http://www.letv.com/ptv/vplay/23796170.html?ref=ym0203)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 10:48:12 [scrapy] INFO: Crawled 6838 pages (at 2 pages/min), scraped 6556 items (at 17 items/min)
2015-11-04 10:50:05 [scrapy] ERROR: Error downloading <GET http://list.letv.com/list/c1_t_a_y_f_at_o_p.html>: An error occurred while connecting: 32: Broken pipe.
2015-11-04 10:50:05 [scrapy] INFO: Crawled 6838 pages (at 0 pages/min), scraped 6577 items (at 21 items/min)
2015-11-04 10:50:52 [scrapy] INFO: Crawled 6854 pages (at 16 pages/min), scraped 6578 items (at 1 items/min)
2015-11-04 10:52:39 [scrapy] INFO: Crawled 6868 pages (at 14 pages/min), scraped 6597 items (at 19 items/min)
2015-11-04 10:53:09 [scrapy] INFO: Crawled 6869 pages (at 1 pages/min), scraped 6601 items (at 4 items/min)
2015-11-04 10:54:15 [scrapy] INFO: Crawled 6889 pages (at 20 pages/min), scraped 6611 items (at 10 items/min)
2015-11-04 10:56:29 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/s?from=play&wd=%E5%AE%89%E5%BE%BD%E5%8D%AB%E8%A7%86> (referer: http://www.letv.com/ptv/vplay/23881195.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 10:57:20 [scrapy] INFO: Crawled 6899 pages (at 10 pages/min), scraped 6627 items (at 16 items/min)
2015-11-04 10:58:15 [scrapy] INFO: Crawled 6899 pages (at 0 pages/min), scraped 6637 items (at 10 items/min)
2015-11-04 10:59:52 [scrapy] INFO: Crawled 6946 pages (at 47 pages/min), scraped 6654 items (at 17 items/min)
2015-11-04 11:00:59 [scrapy] INFO: Crawled 6949 pages (at 3 pages/min), scraped 6670 items (at 16 items/min)
2015-11-04 11:02:35 [scrapy] INFO: Crawled 6953 pages (at 4 pages/min), scraped 6686 items (at 16 items/min)
2015-11-04 11:03:17 [scrapy] INFO: Crawled 6953 pages (at 0 pages/min), scraped 6690 items (at 4 items/min)
2015-11-04 11:04:20 [scrapy] INFO: Crawled 6981 pages (at 28 pages/min), scraped 6707 items (at 17 items/min)
2015-11-04 11:05:29 [scrapy] INFO: Crawled 7002 pages (at 21 pages/min), scraped 6728 items (at 21 items/min)
2015-11-04 11:06:17 [scrapy] INFO: Crawled 7008 pages (at 6 pages/min), scraped 6744 items (at 16 items/min)
2015-11-04 11:07:14 [scrapy] INFO: Crawled 7028 pages (at 20 pages/min), scraped 6751 items (at 7 items/min)
2015-11-04 11:08:17 [scrapy] INFO: Crawled 7060 pages (at 32 pages/min), scraped 6770 items (at 19 items/min)
2015-11-04 11:09:41 [scrapy] INFO: Crawled 7070 pages (at 10 pages/min), scraped 6784 items (at 14 items/min)
2015-11-04 11:10:25 [scrapy] INFO: Crawled 7075 pages (at 5 pages/min), scraped 6799 items (at 15 items/min)
2015-11-04 11:11:16 [scrapy] INFO: Crawled 7076 pages (at 1 pages/min), scraped 6812 items (at 13 items/min)
2015-11-04 11:12:12 [scrapy] INFO: Crawled 7107 pages (at 31 pages/min), scraped 6821 items (at 9 items/min)
2015-11-04 11:13:45 [scrapy] INFO: Crawled 7107 pages (at 0 pages/min), scraped 6844 items (at 23 items/min)
2015-11-04 11:14:27 [scrapy] INFO: Crawled 7139 pages (at 32 pages/min), scraped 6857 items (at 13 items/min)
2015-11-04 11:15:10 [scrapy] INFO: Crawled 7140 pages (at 1 pages/min), scraped 6866 items (at 9 items/min)
2015-11-04 11:16:13 [scrapy] INFO: Crawled 7140 pages (at 0 pages/min), scraped 6877 items (at 11 items/min)
2015-11-04 11:17:08 [scrapy] INFO: Crawled 7173 pages (at 33 pages/min), scraped 6893 items (at 16 items/min)
2015-11-04 11:18:07 [scrapy] INFO: Crawled 7179 pages (at 6 pages/min), scraped 6904 items (at 11 items/min)
2015-11-04 11:19:25 [scrapy] INFO: Crawled 7206 pages (at 27 pages/min), scraped 6919 items (at 15 items/min)
2015-11-04 11:20:11 [scrapy] INFO: Crawled 7220 pages (at 14 pages/min), scraped 6933 items (at 14 items/min)
2015-11-04 11:23:24 [scrapy] INFO: Crawled 7220 pages (at 0 pages/min), scraped 6957 items (at 24 items/min)
2015-11-04 11:24:53 [scrapy] INFO: Crawled 7265 pages (at 45 pages/min), scraped 6986 items (at 29 items/min)
2015-11-04 11:26:54 [scrapy] INFO: Crawled 7265 pages (at 0 pages/min), scraped 7002 items (at 16 items/min)
2015-11-04 11:27:41 [scrapy] INFO: Crawled 7297 pages (at 32 pages/min), scraped 7014 items (at 12 items/min)
2015-11-04 11:28:22 [scrapy] INFO: Crawled 7300 pages (at 3 pages/min), scraped 7023 items (at 9 items/min)
2015-11-04 11:29:41 [scrapy] INFO: Crawled 7326 pages (at 26 pages/min), scraped 7042 items (at 19 items/min)
2015-11-04 11:31:09 [scrapy] INFO: Crawled 7332 pages (at 6 pages/min), scraped 7055 items (at 13 items/min)
2015-11-04 11:33:23 [scrapy] ERROR: Spider error processing <GET http://list.letv.com/listn/c11_t30376_a-1_s1_tv35_md_o9_d2_p.html> (referer: http://list.letv.com/listn/c11_t-1_a-1_s1_tv35_md_o9_d2_p.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:33:37 [scrapy] INFO: Crawled 7370 pages (at 38 pages/min), scraped 7095 items (at 40 items/min)
2015-11-04 11:34:25 [scrapy] INFO: Crawled 7374 pages (at 4 pages/min), scraped 7106 items (at 11 items/min)
2015-11-04 11:35:28 [scrapy] INFO: Crawled 7396 pages (at 22 pages/min), scraped 7121 items (at 15 items/min)
2015-11-04 11:36:13 [scrapy] INFO: Crawled 7397 pages (at 1 pages/min), scraped 7133 items (at 12 items/min)
2015-11-04 11:37:19 [scrapy] INFO: Crawled 7420 pages (at 23 pages/min), scraped 7153 items (at 20 items/min)
2015-11-04 11:38:28 [scrapy] INFO: Crawled 7441 pages (at 21 pages/min), scraped 7171 items (at 18 items/min)
2015-11-04 11:39:39 [scrapy] INFO: Crawled 7458 pages (at 17 pages/min), scraped 7184 items (at 13 items/min)
2015-11-04 11:40:30 [scrapy] INFO: Crawled 7458 pages (at 0 pages/min), scraped 7194 items (at 10 items/min)
2015-11-04 11:41:30 [scrapy] INFO: Crawled 7487 pages (at 29 pages/min), scraped 7211 items (at 17 items/min)
2015-11-04 11:42:14 [scrapy] INFO: Crawled 7492 pages (at 5 pages/min), scraped 7225 items (at 14 items/min)
2015-11-04 11:43:27 [scrapy] INFO: Crawled 7514 pages (at 22 pages/min), scraped 7242 items (at 17 items/min)
2015-11-04 11:45:19 [scrapy] INFO: Crawled 7569 pages (at 55 pages/min), scraped 7270 items (at 28 items/min)
2015-11-04 11:46:56 [scrapy] INFO: Crawled 7582 pages (at 13 pages/min), scraped 7304 items (at 34 items/min)
2015-11-04 11:47:45 [scrapy] INFO: Crawled 7591 pages (at 9 pages/min), scraped 7310 items (at 6 items/min)
2015-11-04 11:48:55 [scrapy] INFO: Crawled 7615 pages (at 24 pages/min), scraped 7335 items (at 25 items/min)
2015-11-04 11:49:43 [scrapy] INFO: Crawled 7627 pages (at 12 pages/min), scraped 7350 items (at 15 items/min)
2015-11-04 11:51:59 [scrapy] INFO: Crawled 7627 pages (at 0 pages/min), scraped 7362 items (at 12 items/min)
2015-11-04 11:52:08 [scrapy] INFO: Crawled 7650 pages (at 23 pages/min), scraped 7364 items (at 2 items/min)
2015-11-04 11:54:07 [scrapy] INFO: Crawled 7665 pages (at 15 pages/min), scraped 7387 items (at 23 items/min)
2015-11-04 11:55:52 [scrapy] ERROR: Spider error processing <GET http://list.letv.com/listn/c11_t30083_a-1_s3_tv-1_md_o9_d1_p.html> (referer: http://www.letv.com/ptv/vplay/23903470.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:56:06 [scrapy] INFO: Crawled 7668 pages (at 3 pages/min), scraped 7399 items (at 12 items/min)
2015-11-04 11:56:09 [scrapy] INFO: Crawled 7668 pages (at 0 pages/min), scraped 7400 items (at 1 items/min)
2015-11-04 11:58:21 [scrapy] INFO: Crawled 7728 pages (at 60 pages/min), scraped 7437 items (at 37 items/min)
2015-11-04 12:00:13 [scrapy] INFO: Crawled 7728 pages (at 0 pages/min), scraped 7460 items (at 23 items/min)
2015-11-04 12:01:14 [scrapy] INFO: Crawled 7764 pages (at 36 pages/min), scraped 7466 items (at 6 items/min)
2015-11-04 12:02:34 [scrapy] INFO: Crawled 7764 pages (at 0 pages/min), scraped 7482 items (at 16 items/min)
2015-11-04 12:03:32 [scrapy] ERROR: Spider error processing <GET http://sports.letv.com/video/23799901.html> (referer: http://sports.letv.com/match/116869003/tlive.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:04:00 [scrapy] INFO: Crawled 7764 pages (at 0 pages/min), scraped 7493 items (at 11 items/min)
2015-11-04 12:04:29 [scrapy] INFO: Crawled 7788 pages (at 24 pages/min), scraped 7501 items (at 8 items/min)
2015-11-04 12:06:47 [scrapy] INFO: Crawled 7791 pages (at 3 pages/min), scraped 7516 items (at 15 items/min)
2015-11-04 12:07:22 [scrapy] INFO: Crawled 7796 pages (at 5 pages/min), scraped 7524 items (at 8 items/min)
2015-11-04 12:08:14 [scrapy] INFO: Crawled 7811 pages (at 15 pages/min), scraped 7531 items (at 7 items/min)
2015-11-04 12:09:36 [scrapy] INFO: Crawled 7815 pages (at 4 pages/min), scraped 7542 items (at 11 items/min)
2015-11-04 12:11:28 [scrapy] INFO: Crawled 7832 pages (at 17 pages/min), scraped 7553 items (at 11 items/min)
2015-11-04 12:12:17 [scrapy] INFO: Crawled 7837 pages (at 5 pages/min), scraped 7557 items (at 4 items/min)
2015-11-04 12:13:19 [scrapy] INFO: Crawled 7838 pages (at 1 pages/min), scraped 7559 items (at 2 items/min)
2015-11-04 12:14:11 [scrapy] INFO: Crawled 7850 pages (at 12 pages/min), scraped 7564 items (at 5 items/min)
2015-11-04 12:15:28 [scrapy] INFO: Crawled 7863 pages (at 13 pages/min), scraped 7577 items (at 13 items/min)
2015-11-04 12:16:49 [scrapy] INFO: Crawled 7871 pages (at 8 pages/min), scraped 7592 items (at 15 items/min)
2015-11-04 12:18:39 [scrapy] INFO: Crawled 7905 pages (at 34 pages/min), scraped 7609 items (at 17 items/min)
2015-11-04 12:21:11 [scrapy] ERROR: Spider error processing <GET http://so.letv.com/s?from=suggest&ref=click&wd=%E4%B9%90%E5%B0%9A%E6%92%AD%E6%8A%A5> (referer: http://www.letv.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:22:00 [scrapy] INFO: Crawled 7916 pages (at 11 pages/min), scraped 7632 items (at 23 items/min)
2015-11-04 12:22:27 [scrapy] INFO: Crawled 7924 pages (at 8 pages/min), scraped 7634 items (at 2 items/min)
2015-11-04 12:23:31 [scrapy] INFO: Crawled 7937 pages (at 13 pages/min), scraped 7642 items (at 8 items/min)
2015-11-04 12:24:43 [scrapy] INFO: Crawled 7939 pages (at 2 pages/min), scraped 7655 items (at 13 items/min)
2015-11-04 12:25:07 [scrapy] INFO: Crawled 7940 pages (at 1 pages/min), scraped 7657 items (at 2 items/min)
2015-11-04 12:26:14 [scrapy] INFO: Crawled 7974 pages (at 34 pages/min), scraped 7670 items (at 13 items/min)
2015-11-04 12:28:34 [scrapy] INFO: Crawled 7976 pages (at 2 pages/min), scraped 7690 items (at 20 items/min)
2015-11-04 12:29:23 [scrapy] INFO: Crawled 8001 pages (at 25 pages/min), scraped 7701 items (at 11 items/min)
2015-11-04 12:30:15 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/ptv/vplay/21634708.html> (referer: http://www.letv.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:30:30 [scrapy] INFO: Crawled 8001 pages (at 0 pages/min), scraped 7708 items (at 7 items/min)
2015-11-04 12:31:32 [scrapy] INFO: Crawled 8001 pages (at 0 pages/min), scraped 7716 items (at 8 items/min)
2015-11-04 12:32:08 [scrapy] INFO: Crawled 8006 pages (at 5 pages/min), scraped 7717 items (at 1 items/min)
2015-11-04 12:34:09 [scrapy] INFO: Crawled 8030 pages (at 24 pages/min), scraped 7737 items (at 20 items/min)
2015-11-04 12:35:26 [scrapy] INFO: Crawled 8048 pages (at 18 pages/min), scraped 7756 items (at 19 items/min)
2015-11-04 12:36:23 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/ptv/vplay/23573045.html> (referer: http://fashion.letv.com/izt/2016ssfashionweek/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:36:33 [scrapy] INFO: Crawled 8049 pages (at 1 pages/min), scraped 7762 items (at 6 items/min)
2015-11-04 12:37:10 [scrapy] INFO: Crawled 8068 pages (at 19 pages/min), scraped 7773 items (at 11 items/min)
2015-11-04 12:38:10 [scrapy] INFO: Crawled 8070 pages (at 2 pages/min), scraped 7782 items (at 9 items/min)
2015-11-04 12:39:19 [scrapy] INFO: Crawled 8094 pages (at 24 pages/min), scraped 7794 items (at 12 items/min)
2015-11-04 12:40:44 [scrapy] INFO: Crawled 8094 pages (at 0 pages/min), scraped 7801 items (at 7 items/min)
2015-11-04 12:41:28 [scrapy] INFO: Crawled 8094 pages (at 0 pages/min), scraped 7808 items (at 7 items/min)
2015-11-04 12:42:27 [scrapy] INFO: Crawled 8119 pages (at 25 pages/min), scraped 7824 items (at 16 items/min)
2015-11-04 12:43:39 [scrapy] INFO: Crawled 8119 pages (at 0 pages/min), scraped 7832 items (at 8 items/min)
2015-11-04 12:44:19 [scrapy] INFO: Crawled 8138 pages (at 19 pages/min), scraped 7838 items (at 6 items/min)
2015-11-04 12:46:13 [scrapy] INFO: Crawled 8145 pages (at 7 pages/min), scraped 7851 items (at 13 items/min)
2015-11-04 12:47:18 [scrapy] INFO: Crawled 8145 pages (at 0 pages/min), scraped 7858 items (at 7 items/min)
2015-11-04 12:48:08 [scrapy] INFO: Crawled 8161 pages (at 16 pages/min), scraped 7864 items (at 6 items/min)
2015-11-04 12:49:11 [scrapy] INFO: Crawled 8161 pages (at 0 pages/min), scraped 7869 items (at 5 items/min)
2015-11-04 12:50:22 [scrapy] INFO: Crawled 8181 pages (at 20 pages/min), scraped 7881 items (at 12 items/min)
2015-11-04 12:51:20 [scrapy] INFO: Crawled 8182 pages (at 1 pages/min), scraped 7886 items (at 5 items/min)
2015-11-04 12:52:43 [scrapy] INFO: Crawled 8207 pages (at 25 pages/min), scraped 7900 items (at 14 items/min)
2015-11-04 12:53:32 [scrapy] INFO: Crawled 8207 pages (at 0 pages/min), scraped 7907 items (at 7 items/min)
2015-11-04 12:54:19 [scrapy] INFO: Crawled 8207 pages (at 0 pages/min), scraped 7915 items (at 8 items/min)
2015-11-04 12:55:33 [scrapy] INFO: Crawled 8235 pages (at 28 pages/min), scraped 7929 items (at 14 items/min)
2015-11-04 12:58:07 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/ptv/vplay/22947372.html> (referer: http://www.letv.com/ptv/vplay/22973028.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:58:07 [scrapy] INFO: Crawled 8235 pages (at 0 pages/min), scraped 7935 items (at 6 items/min)
2015-11-04 12:59:39 [scrapy] ERROR: Spider error processing <GET http://www.letv.com/ptv/vplay/23095207.html> (referer: http://www.letv.com/ptv/vplay/22973028.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:59:39 [scrapy] INFO: Crawled 8243 pages (at 8 pages/min), scraped 7942 items (at 7 items/min)
2015-11-04 13:00:26 [scrapy] INFO: Crawled 8250 pages (at 7 pages/min), scraped 7949 items (at 7 items/min)
2015-11-04 13:01:38 [scrapy] INFO: Crawled 8251 pages (at 1 pages/min), scraped 7955 items (at 6 items/min)
2015-11-04 13:02:14 [scrapy] INFO: Crawled 8266 pages (at 15 pages/min), scraped 7962 items (at 7 items/min)
2015-11-04 13:03:22 [scrapy] INFO: Crawled 8280 pages (at 14 pages/min), scraped 7975 items (at 13 items/min)
2015-11-04 13:04:55 [scrapy] INFO: Crawled 8300 pages (at 20 pages/min), scraped 7989 items (at 14 items/min)
2015-11-04 13:06:59 [scrapy] INFO: Crawled 8300 pages (at 0 pages/min), scraped 7997 items (at 8 items/min)
2015-11-04 13:07:09 [scrapy] INFO: Crawled 8300 pages (at 0 pages/min), scraped 7998 items (at 1 items/min)
2015-11-04 13:08:14 [scrapy] INFO: Crawled 8324 pages (at 24 pages/min), scraped 8006 items (at 8 items/min)
2015-11-04 13:09:54 [scrapy] INFO: Crawled 8324 pages (at 0 pages/min), scraped 8022 items (at 16 items/min)
2015-11-04 13:10:36 [scrapy] INFO: Crawled 8343 pages (at 19 pages/min), scraped 8032 items (at 10 items/min)
2015-11-04 13:11:16 [scrapy] INFO: Crawled 8345 pages (at 2 pages/min), scraped 8035 items (at 3 items/min)
2015-11-04 13:12:24 [scrapy] INFO: Crawled 8361 pages (at 16 pages/min), scraped 8046 items (at 11 items/min)
2015-11-04 13:13:18 [scrapy] INFO: Crawled 8369 pages (at 8 pages/min), scraped 8058 items (at 12 items/min)
2015-11-04 13:14:08 [scrapy] INFO: Crawled 8381 pages (at 12 pages/min), scraped 8064 items (at 6 items/min)
2015-11-04 13:15:28 [scrapy] INFO: Crawled 8389 pages (at 8 pages/min), scraped 8074 items (at 10 items/min)
2015-11-04 13:16:08 [scrapy] INFO: Crawled 8408 pages (at 19 pages/min), scraped 8084 items (at 10 items/min)
2015-11-04 13:17:40 [scrapy] INFO: Crawled 8414 pages (at 6 pages/min), scraped 8103 items (at 19 items/min)
2015-11-04 13:18:38 [scrapy] INFO: Crawled 8423 pages (at 9 pages/min), scraped 8114 items (at 11 items/min)
2015-11-04 13:19:09 [scrapy] INFO: Crawled 8429 pages (at 6 pages/min), scraped 8118 items (at 4 items/min)
2015-11-04 13:20:32 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=423279> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:20:32 [scrapy] INFO: Crawled 8439 pages (at 10 pages/min), scraped 8122 items (at 4 items/min)
2015-11-04 13:21:30 [scrapy] INFO: Crawled 8446 pages (at 7 pages/min), scraped 8130 items (at 8 items/min)
2015-11-04 13:22:38 [scrapy] INFO: Crawled 8446 pages (at 0 pages/min), scraped 8137 items (at 7 items/min)
2015-11-04 13:23:20 [scrapy] INFO: Crawled 8477 pages (at 31 pages/min), scraped 8148 items (at 11 items/min)
2015-11-04 13:24:57 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=416014> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:25:33 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=433817> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:25:40 [scrapy] INFO: Crawled 8479 pages (at 2 pages/min), scraped 8163 items (at 15 items/min)
2015-11-04 13:26:30 [scrapy] INFO: Crawled 8504 pages (at 25 pages/min), scraped 8179 items (at 16 items/min)
2015-11-04 13:27:10 [scrapy] INFO: Crawled 8506 pages (at 2 pages/min), scraped 8190 items (at 11 items/min)
2015-11-04 13:28:28 [scrapy] INFO: Crawled 8535 pages (at 29 pages/min), scraped 8201 items (at 11 items/min)
2015-11-04 13:30:14 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=428373> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:30:54 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=424556> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:31:40 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=425949> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:32:29 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=437751> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:33:04 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=432519> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:33:37 [scrapy] ERROR: Spider error processing <GET http://vod.kankan.com/v/85/85316.shtml?subid=434746> (referer: http://so.letv.com/s?from=suggest&ref=click&wd=%E8%80%81%E5%AE%89%E8%AF%9D%E8%BD%A6%E4%BA%8B)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:33:37 [scrapy] INFO: Crawled 8535 pages (at 0 pages/min), scraped 8214 items (at 13 items/min)
2015-11-04 13:33:37 [scrapy] ERROR: Error downloading <GET http://vod.kankan.com/v/85/85316.shtml?subid=429466>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 13:33:37 [scrapy] ERROR: Error downloading <GET http://vod.kankan.com/v/85/85316.shtml?subid=420726>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://vod.kankan.com/v/85/85316.shtml?subid=420726 took longer than 180.0 seconds..
2015-11-04 13:34:38 [scrapy] INFO: Crawled 8563 pages (at 28 pages/min), scraped 8223 items (at 9 items/min)
2015-11-04 13:35:32 [scrapy] INFO: Crawled 8571 pages (at 8 pages/min), scraped 8232 items (at 9 items/min)
2015-11-04 13:36:10 [scrapy] INFO: Crawled 8571 pages (at 0 pages/min), scraped 8242 items (at 10 items/min)
2015-11-04 13:37:19 [scrapy] ERROR: Error downloading <GET http://vod.kankan.com/v/85/85316.shtml?subid=421953>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://vod.kankan.com/v/85/85316.shtml?subid=421953 took longer than 180.0 seconds..
2015-11-04 13:37:19 [scrapy] INFO: Crawled 8597 pages (at 26 pages/min), scraped 8257 items (at 15 items/min)
2015-11-04 13:38:52 [scrapy] INFO: Crawled 8605 pages (at 8 pages/min), scraped 8273 items (at 16 items/min)
2015-11-04 13:39:34 [scrapy] INFO: Crawled 8605 pages (at 0 pages/min), scraped 8276 items (at 3 items/min)
2015-11-04 13:40:19 [scrapy] INFO: Crawled 8607 pages (at 2 pages/min), scraped 8284 items (at 8 items/min)
2015-11-04 13:40:41 [scrapy] ERROR: Spider error processing <GET http://show.letv.com/> (referer: http://www.letv.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 13:41:38 [scrapy] INFO: Crawled 8638 pages (at 31 pages/min), scraped 8301 items (at 17 items/min)
2015-11-04 13:42:50 [scrapy] INFO: Crawled 8646 pages (at 8 pages/min), scraped 8316 items (at 15 items/min)
2015-11-04 13:43:41 [scrapy] ERROR: Spider error processing <GET http://hot.letv.com/kzt/vplay/30_2929_5163_23111707.html> (referer: http://hot.letv.com/kzt/30_2015gcw/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:44:05 [scrapy] INFO: Crawled 8646 pages (at 0 pages/min), scraped 8323 items (at 7 items/min)
2015-11-04 13:44:21 [scrapy] INFO: Crawled 8661 pages (at 15 pages/min), scraped 8324 items (at 1 items/min)
2015-11-04 13:45:48 [scrapy] INFO: Crawled 8684 pages (at 23 pages/min), scraped 8338 items (at 14 items/min)
2015-11-04 13:47:03 [scrapy] INFO: Crawled 8685 pages (at 1 pages/min), scraped 8346 items (at 8 items/min)
2015-11-04 13:48:31 [scrapy] INFO: Crawled 8685 pages (at 0 pages/min), scraped 8361 items (at 15 items/min)
2015-11-04 13:49:08 [scrapy] INFO: Crawled 8711 pages (at 26 pages/min), scraped 8364 items (at 3 items/min)
2015-11-04 13:52:17 [scrapy] INFO: Crawled 8718 pages (at 7 pages/min), scraped 8387 items (at 23 items/min)
2015-11-04 13:53:06 [scrapy] INFO: Crawled 8718 pages (at 0 pages/min), scraped 8392 items (at 5 items/min)
2015-11-04 13:54:38 [scrapy] INFO: Crawled 8756 pages (at 38 pages/min), scraped 8408 items (at 16 items/min)
2015-11-04 13:55:15 [scrapy] INFO: Crawled 8758 pages (at 2 pages/min), scraped 8417 items (at 9 items/min)
2015-11-04 13:56:14 [scrapy] INFO: Crawled 8758 pages (at 0 pages/min), scraped 8430 items (at 13 items/min)
2015-11-04 13:57:29 [scrapy] INFO: Crawled 8786 pages (at 28 pages/min), scraped 8446 items (at 16 items/min)
2015-11-04 13:58:30 [scrapy] INFO: Crawled 8791 pages (at 5 pages/min), scraped 8462 items (at 16 items/min)
2015-11-04 13:59:33 [scrapy] INFO: Crawled 8814 pages (at 23 pages/min), scraped 8476 items (at 14 items/min)
2015-11-04 14:00:06 [scrapy] INFO: Crawled 8814 pages (at 0 pages/min), scraped 8485 items (at 9 items/min)
2015-11-04 14:01:18 [scrapy] INFO: Crawled 8839 pages (at 25 pages/min), scraped 8503 items (at 18 items/min)
2015-11-04 14:02:47 [scrapy] INFO: Crawled 8862 pages (at 23 pages/min), scraped 8520 items (at 17 items/min)
2015-11-04 14:04:05 [scrapy] INFO: Crawled 8869 pages (at 7 pages/min), scraped 8527 items (at 7 items/min)
2015-11-04 14:04:32 [scrapy] INFO: Crawled 8869 pages (at 0 pages/min), scraped 8533 items (at 6 items/min)
2015-11-04 14:05:42 [scrapy] ERROR: Spider error processing <GET http://hot.letv.com/kzt/vplay/30_1734_3057_22167455.html> (referer: http://hot.letv.com/kzt/30_fcac/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:06:19 [scrapy] INFO: Crawled 8902 pages (at 33 pages/min), scraped 8548 items (at 15 items/min)
2015-11-04 14:08:04 [scrapy] ERROR: Spider error processing <GET http://hot.letv.com/kzt/vplay/30_1734_3058_22134120.html> (referer: http://hot.letv.com/kzt/30_fcac/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:08:23 [scrapy] INFO: Crawled 8904 pages (at 2 pages/min), scraped 8565 items (at 17 items/min)
2015-11-04 14:10:00 [scrapy] INFO: Crawled 8926 pages (at 22 pages/min), scraped 8572 items (at 7 items/min)
2015-11-04 14:10:26 [scrapy] INFO: Crawled 8927 pages (at 1 pages/min), scraped 8577 items (at 5 items/min)
2015-11-04 14:12:28 [scrapy] INFO: Crawled 8928 pages (at 1 pages/min), scraped 8589 items (at 12 items/min)
2015-11-04 14:14:13 [scrapy] INFO: Crawled 8983 pages (at 55 pages/min), scraped 8607 items (at 18 items/min)
2015-11-04 14:15:19 [scrapy] INFO: Crawled 8991 pages (at 8 pages/min), scraped 8622 items (at 15 items/min)
2015-11-04 14:17:37 [scrapy] ERROR: Spider error processing <GET http://hot.letv.com/kzt/vplay/30_1436_2320_20615771.html> (referer: http://hot.letv.com/kzt/30_mcmb/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:17:59 [scrapy] INFO: Crawled 8991 pages (at 0 pages/min), scraped 8644 items (at 22 items/min)
2015-11-04 14:18:39 [scrapy] INFO: Crawled 8991 pages (at 0 pages/min), scraped 8652 items (at 8 items/min)
2015-11-04 14:19:46 [scrapy] INFO: Crawled 9049 pages (at 58 pages/min), scraped 8669 items (at 17 items/min)
2015-11-04 14:22:02 [scrapy] INFO: Crawled 9052 pages (at 3 pages/min), scraped 8689 items (at 20 items/min)
2015-11-04 14:24:19 [scrapy] ERROR: Error downloading <GET https://penneo.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://penneo.com/ took longer than 180.0 seconds..
2015-11-04 14:24:19 [scrapy] ERROR: Error downloading <GET https://inovancetech.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://inovancetech.com/ took longer than 180.0 seconds..
2015-11-04 14:24:19 [scrapy] INFO: Crawled 9052 pages (at 0 pages/min), scraped 8711 items (at 22 items/min)
2015-11-04 14:26:02 [scrapy] INFO: Crawled 9088 pages (at 36 pages/min), scraped 8734 items (at 23 items/min)
2015-11-04 14:26:18 [scrapy] INFO: Crawled 9092 pages (at 4 pages/min), scraped 8740 items (at 6 items/min)
2015-11-04 14:27:27 [scrapy] INFO: Crawled 9114 pages (at 22 pages/min), scraped 8759 items (at 19 items/min)
2015-11-04 14:28:10 [scrapy] INFO: Crawled 9126 pages (at 12 pages/min), scraped 8779 items (at 20 items/min)
2015-11-04 14:29:15 [scrapy] INFO: Crawled 9168 pages (at 42 pages/min), scraped 8800 items (at 21 items/min)
2015-11-04 14:30:46 [scrapy] INFO: Crawled 9181 pages (at 13 pages/min), scraped 8822 items (at 22 items/min)
2015-11-04 14:30:49 [scrapy] ERROR: Spider error processing <GET https://www.linemetrics.com/profil/florian/> (referer: https://www.linemetrics.com/de/demo/anlagenkennzahlen-in-metallverarbeitender-industrie/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 14:31:09 [scrapy] ERROR: Spider error processing <GET https://www.linemetrics.com/profil/david/> (referer: https://www.linemetrics.com/de/produkte/box/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 14:31:37 [scrapy] INFO: Crawled 9198 pages (at 17 pages/min), scraped 8836 items (at 14 items/min)
2015-11-04 14:32:25 [scrapy] INFO: Crawled 9216 pages (at 18 pages/min), scraped 8852 items (at 16 items/min)
2015-11-04 14:33:07 [scrapy] INFO: Crawled 9236 pages (at 20 pages/min), scraped 8869 items (at 17 items/min)
2015-11-04 14:34:07 [scrapy] INFO: Crawled 9254 pages (at 18 pages/min), scraped 8890 items (at 21 items/min)
2015-11-04 14:35:23 [scrapy] INFO: Crawled 9287 pages (at 33 pages/min), scraped 8920 items (at 30 items/min)
2015-11-04 14:36:10 [scrapy] INFO: Crawled 9301 pages (at 14 pages/min), scraped 8938 items (at 18 items/min)
2015-11-04 14:37:21 [scrapy] INFO: Crawled 9319 pages (at 18 pages/min), scraped 8962 items (at 24 items/min)
2015-11-04 14:38:15 [scrapy] INFO: Crawled 9353 pages (at 34 pages/min), scraped 8981 items (at 19 items/min)
2015-11-04 14:39:33 [scrapy] INFO: Crawled 9373 pages (at 20 pages/min), scraped 9010 items (at 29 items/min)
2015-11-04 14:40:15 [scrapy] INFO: Crawled 9387 pages (at 14 pages/min), scraped 9027 items (at 17 items/min)
2015-11-04 14:41:17 [scrapy] INFO: Crawled 9410 pages (at 23 pages/min), scraped 9046 items (at 19 items/min)
2015-11-04 14:42:29 [scrapy] INFO: Crawled 9432 pages (at 22 pages/min), scraped 9072 items (at 26 items/min)
2015-11-04 14:43:10 [scrapy] INFO: Crawled 9455 pages (at 23 pages/min), scraped 9089 items (at 17 items/min)
2015-11-04 14:44:12 [scrapy] INFO: Crawled 9475 pages (at 20 pages/min), scraped 9113 items (at 24 items/min)
2015-11-04 14:45:13 [scrapy] INFO: Crawled 9504 pages (at 29 pages/min), scraped 9136 items (at 23 items/min)
2015-11-04 14:45:37 [scrapy] ERROR: Spider error processing <GET https://www.linemetrics.com/profil/thomaslinemetrics-com/> (referer: https://www.linemetrics.com/de/forums/forum/lmsupport/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 14:46:06 [scrapy] ERROR: Spider error processing <GET https://www.linemetrics.com/profil/jerry2044/> (referer: https://www.linemetrics.com/de/forums/forum/lmsupport/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 14:46:33 [scrapy] INFO: Crawled 9538 pages (at 34 pages/min), scraped 9168 items (at 32 items/min)
2015-11-04 14:47:14 [scrapy] INFO: Crawled 9547 pages (at 9 pages/min), scraped 9182 items (at 14 items/min)
2015-11-04 14:48:18 [scrapy] INFO: Crawled 9571 pages (at 24 pages/min), scraped 9207 items (at 25 items/min)
2015-11-04 14:49:14 [scrapy] INFO: Crawled 9585 pages (at 14 pages/min), scraped 9229 items (at 22 items/min)
2015-11-04 14:51:20 [scrapy] INFO: Crawled 9640 pages (at 55 pages/min), scraped 9260 items (at 31 items/min)
2015-11-04 14:52:47 [scrapy] INFO: Crawled 9663 pages (at 23 pages/min), scraped 9296 items (at 36 items/min)
2015-11-04 14:53:10 [scrapy] INFO: Crawled 9688 pages (at 25 pages/min), scraped 9314 items (at 18 items/min)
2015-11-04 14:54:25 [scrapy] INFO: Crawled 9721 pages (at 33 pages/min), scraped 9331 items (at 17 items/min)
2015-11-04 14:55:29 [scrapy] ERROR: Spider error processing <GET http://www.huatusoft.com/culture.html> (referer: http://www.huatusoft.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 14:57:34 [scrapy] INFO: Crawled 9746 pages (at 25 pages/min), scraped 9363 items (at 32 items/min)
2015-11-04 14:58:58 [scrapy] INFO: Crawled 9746 pages (at 0 pages/min), scraped 9387 items (at 24 items/min)
2015-11-04 14:58:59 [scrapy] ERROR: Spider error processing <GET http://blog.konnektid.com/> (referer: https://www.konnektid.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 14:59:57 [scrapy] INFO: Crawled 9811 pages (at 65 pages/min), scraped 9414 items (at 27 items/min)
2015-11-04 15:01:13 [scrapy] INFO: Crawled 9827 pages (at 16 pages/min), scraped 9451 items (at 37 items/min)
2015-11-04 15:02:18 [scrapy] INFO: Crawled 9847 pages (at 20 pages/min), scraped 9467 items (at 16 items/min)
2015-11-04 15:03:49 [scrapy] INFO: Crawled 9851 pages (at 4 pages/min), scraped 9480 items (at 13 items/min)
2015-11-04 15:04:18 [scrapy] INFO: Crawled 9859 pages (at 8 pages/min), scraped 9487 items (at 7 items/min)
2015-11-04 15:05:17 [scrapy] INFO: Crawled 9874 pages (at 15 pages/min), scraped 9499 items (at 12 items/min)
2015-11-04 15:06:50 [scrapy] INFO: Crawled 9913 pages (at 39 pages/min), scraped 9527 items (at 28 items/min)
2015-11-04 15:08:19 [scrapy] INFO: Crawled 9934 pages (at 21 pages/min), scraped 9552 items (at 25 items/min)
2015-11-04 15:10:15 [scrapy] INFO: Crawled 9942 pages (at 8 pages/min), scraped 9573 items (at 21 items/min)
2015-11-04 15:11:11 [scrapy] INFO: Crawled 9950 pages (at 8 pages/min), scraped 9579 items (at 6 items/min)
2015-11-04 15:13:01 [scrapy] INFO: Crawled 9987 pages (at 37 pages/min), scraped 9607 items (at 28 items/min)
2015-11-04 15:14:08 [scrapy] INFO: Crawled 10005 pages (at 18 pages/min), scraped 9623 items (at 16 items/min)
2015-11-04 15:15:09 [scrapy] INFO: Crawled 10050 pages (at 45 pages/min), scraped 9657 items (at 34 items/min)
2015-11-04 15:16:54 [scrapy] INFO: Crawled 10092 pages (at 42 pages/min), scraped 9707 items (at 50 items/min)
2015-11-04 15:16:58 [scrapy] WARNING: Expected response size (35761629) larger than download warn size (33554432).
2015-11-04 15:17:10 [scrapy] INFO: Crawled 10119 pages (at 27 pages/min), scraped 9716 items (at 9 items/min)
2015-11-04 15:18:25 [scrapy] INFO: Crawled 10148 pages (at 29 pages/min), scraped 9751 items (at 35 items/min)
2015-11-04 15:19:26 [scrapy] INFO: Crawled 10155 pages (at 7 pages/min), scraped 9763 items (at 12 items/min)
2015-11-04 15:21:01 [scrapy] INFO: Crawled 10164 pages (at 9 pages/min), scraped 9779 items (at 16 items/min)
2015-11-04 15:22:11 [scrapy] INFO: Crawled 10173 pages (at 9 pages/min), scraped 9788 items (at 9 items/min)
2015-11-04 15:24:09 [scrapy] INFO: Crawled 10194 pages (at 21 pages/min), scraped 9810 items (at 22 items/min)
2015-11-04 15:25:35 [scrapy] INFO: Crawled 10204 pages (at 10 pages/min), scraped 9818 items (at 8 items/min)
2015-11-04 15:26:40 [scrapy] INFO: Crawled 10213 pages (at 9 pages/min), scraped 9828 items (at 10 items/min)
2015-11-04 15:27:10 [scrapy] INFO: Crawled 10222 pages (at 9 pages/min), scraped 9837 items (at 9 items/min)
2015-11-04 15:29:10 [scrapy] INFO: Crawled 10245 pages (at 23 pages/min), scraped 9860 items (at 23 items/min)
2015-11-04 15:30:34 [scrapy] INFO: Crawled 10260 pages (at 15 pages/min), scraped 9868 items (at 8 items/min)
2015-11-04 15:31:40 [scrapy] INFO: Crawled 10262 pages (at 2 pages/min), scraped 9884 items (at 16 items/min)
2015-11-04 15:32:10 [scrapy] INFO: Crawled 10286 pages (at 24 pages/min), scraped 9902 items (at 18 items/min)
2015-11-04 15:33:19 [scrapy] INFO: Crawled 10313 pages (at 27 pages/min), scraped 9936 items (at 34 items/min)
2015-11-04 15:34:16 [scrapy] INFO: Crawled 10345 pages (at 32 pages/min), scraped 9961 items (at 25 items/min)
2015-11-04 15:36:11 [scrapy] INFO: Crawled 10385 pages (at 40 pages/min), scraped 9993 items (at 32 items/min)
2015-11-04 15:37:22 [scrapy] INFO: Crawled 10394 pages (at 9 pages/min), scraped 10009 items (at 16 items/min)
2015-11-04 15:38:34 [scrapy] INFO: Crawled 10403 pages (at 9 pages/min), scraped 10018 items (at 9 items/min)
2015-11-04 15:39:50 [scrapy] INFO: Crawled 10419 pages (at 16 pages/min), scraped 10042 items (at 24 items/min)
2015-11-04 15:40:08 [scrapy] INFO: Crawled 10441 pages (at 22 pages/min), scraped 10056 items (at 14 items/min)
2015-11-04 15:41:24 [scrapy] INFO: Crawled 10498 pages (at 57 pages/min), scraped 10105 items (at 49 items/min)
2015-11-04 15:42:24 [scrapy] INFO: Crawled 10530 pages (at 32 pages/min), scraped 10139 items (at 34 items/min)
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:45 [scrapy] INFO: Closing spider (shutdown)
2015-11-04 15:43:12 [scrapy] INFO: Crawled 10558 pages (at 28 pages/min), scraped 10171 items (at 32 items/min)
