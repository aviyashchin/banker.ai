2015-11-02 00:07:43 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-02 00:07:43 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-02 00:07:43 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['vcspider.spiders'], 'DEPTH_LIMIT': 1, 'BOT_NAME': 'vcspider'}
2015-11-02 00:07:43 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-02 00:07:44 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-02 00:07:44 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-02 00:07:44 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-02 00:07:44 [scrapy] INFO: Spider opened
2015-11-02 00:07:44 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-02 00:08:27 [scrapy] ERROR: Error processing {'pageurl': ['https://hfs-webshare.jpmorgan.com/'],
 'siteurl': ['hfs-webshare.jpmorgan.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/pipelines.py", line 39, in process_item
    self.cur.execute(sql, dc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 507, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 722, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 640, in _handle_result
    raise errors.get_exception(packet)
ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%(pagetitle)s, '', 'https://hfs-webshare.jpmorgan.com/', 'hfs-webshare.jpmorgan.' at line 1
2015-11-02 00:08:44 [scrapy] INFO: Crawled 124 pages (at 124 pages/min), scraped 89 items (at 89 items/min)
2015-11-02 00:09:47 [scrapy] INFO: Crawled 253 pages (at 129 pages/min), scraped 186 items (at 97 items/min)
2015-11-02 00:09:51 [scrapy] ERROR: Spider error processing <GET http://www.bpfunds.com/files/2014/2540/4191/Hedge_Funds_Review_Deconstructing_Alpha.pdf> (referer: http://www.bpfunds.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:09:52 [scrapy] ERROR: Spider error processing <GET http://www.bpfunds.com/files/3814/2540/3709/Real_Alpha_Process_general_public_version.pdf> (referer: http://www.bpfunds.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:10:02 [scrapy] ERROR: Spider error processing <GET http://www.lonestarfunds.com/> (referer: http://www.lonestarfunds.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-02 00:10:38 [scrapy] ERROR: Error processing {'pageurl': ['https://naveview.equinoxeais.com'],
 'siteurl': ['naveview.equinoxeais.co'],
 'text': ['']}
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/pipelines.py", line 39, in process_item
    self.cur.execute(sql, dc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 507, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 722, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 640, in _handle_result
    raise errors.get_exception(packet)
ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%(pagetitle)s, '', 'https://naveview.equinoxeais.com', 'naveview.equinoxeais.co'' at line 1
2015-11-02 00:10:45 [scrapy] INFO: Crawled 359 pages (at 106 pages/min), scraped 295 items (at 109 items/min)
2015-11-02 00:10:56 [scrapy] ERROR: Error downloading <GET http://www.sandsbros.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-02 00:11:57 [scrapy] INFO: Crawled 454 pages (at 95 pages/min), scraped 375 items (at 80 items/min)
2015-11-02 00:12:45 [scrapy] INFO: Crawled 492 pages (at 38 pages/min), scraped 415 items (at 40 items/min)
2015-11-02 00:13:55 [scrapy] INFO: Crawled 547 pages (at 55 pages/min), scraped 469 items (at 54 items/min)
2015-11-02 00:14:41 [scrapy] ERROR: Spider error processing <GET http://www.ofiglobal.com/insights-what-the-fed-choice-means-for-2015-and-beyond.html> (referer: http://www.ofiglobal.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:14:44 [scrapy] INFO: Crawled 660 pages (at 113 pages/min), scraped 554 items (at 85 items/min)
2015-11-02 00:15:05 [scrapy] ERROR: Spider error processing <GET https://www.caxton.com/Download2.aspx?ID=8a04922a-3.pdf&Name=Caxton_New_York_directions.pdf> (referer: https://www.Caxton.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:15:11 [scrapy] ERROR: Spider error processing <GET http://www.eigpartners.com/investments/our-investments> (referer: http://www.eigpartners.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 99, in get_title
    return self.clean_title(title)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-02 00:15:17 [scrapy] ERROR: Spider error processing <GET https://www.caxton.com/Download2.aspx?ID=249dc393-a.pdf&Name=Caxton_New_Jersey_directions.pdf> (referer: https://www.Caxton.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:15:17 [scrapy] ERROR: Spider error processing <GET https://www.caxton.com/Download2.aspx?ID=71afadac-e.pdf&Name=Caxton_London_directions.pdf> (referer: https://www.Caxton.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:15:24 [scrapy] ERROR: Spider error processing <GET http://www.eigpartners.com/team> (referer: http://www.eigpartners.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 99, in get_title
    return self.clean_title(title)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-02 00:15:47 [scrapy] INFO: Crawled 758 pages (at 98 pages/min), scraped 641 items (at 87 items/min)
2015-11-02 00:15:52 [scrapy] ERROR: Spider error processing <GET http://www.ofiglobal.com/insights-importing-deflation-from-china.html> (referer: http://www.ofiglobal.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:15:57 [scrapy] ERROR: Error downloading <GET http://www.glaxisllc.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-02 00:16:44 [scrapy] INFO: Crawled 882 pages (at 124 pages/min), scraped 759 items (at 118 items/min)
2015-11-02 00:17:53 [scrapy] INFO: Crawled 978 pages (at 96 pages/min), scraped 841 items (at 82 items/min)
2015-11-02 00:18:44 [scrapy] INFO: Crawled 1059 pages (at 81 pages/min), scraped 917 items (at 76 items/min)
2015-11-02 00:19:41 [scrapy] ERROR: Spider error processing <GET https://www.zcap.net/contact/> (referer: https://www.zcap.net/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:19:43 [scrapy] ERROR: Spider error processing <GET https://www.zcap.net/company/> (referer: https://www.zcap.net/)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/Users/johnmontroy/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-02 00:19:48 [scrapy] INFO: Crawled 1148 pages (at 89 pages/min), scraped 994 items (at 77 items/min)
2015-11-02 00:19:52 [scrapy] ERROR: Error processing {'pageurl': ['http://eightfoldcapital.com/?_escaped_fragment_=%2Fcontact-us%2F'],
 'siteurl': ['eightfoldcapital.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/pipelines.py", line 39, in process_item
    self.cur.execute(sql, dc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 507, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 722, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 640, in _handle_result
    raise errors.get_exception(packet)
ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%(pagetitle)s, '', 'http://eightfoldcapital.com/?_escaped_fragment_=%2Fcontact-u' at line 1
2015-11-02 00:20:56 [scrapy] INFO: Crawled 1252 pages (at 104 pages/min), scraped 1079 items (at 85 items/min)
2015-11-02 00:21:45 [scrapy] INFO: Crawled 1297 pages (at 45 pages/min), scraped 1134 items (at 55 items/min)
2015-11-02 00:21:56 [scrapy] ERROR: Error downloading <GET http://www.oxforddevelopmentcompany.com%20(shown%20as%20affiliate)>: DNS lookup failed: address 'www.oxforddevelopmentcompany.com%20(shown%20as%20affiliate)' not found: [Errno 8] nodename nor servname provided, or not known.
2015-11-02 00:22:47 [scrapy] INFO: Crawled 1378 pages (at 81 pages/min), scraped 1191 items (at 57 items/min)
2015-11-02 00:23:45 [scrapy] INFO: Crawled 1431 pages (at 53 pages/min), scraped 1254 items (at 63 items/min)
2015-11-02 00:24:48 [scrapy] INFO: Crawled 1537 pages (at 106 pages/min), scraped 1336 items (at 82 items/min)
2015-11-02 00:25:46 [scrapy] INFO: Crawled 1575 pages (at 38 pages/min), scraped 1387 items (at 51 items/min)
2015-11-02 00:26:47 [scrapy] INFO: Crawled 1654 pages (at 79 pages/min), scraped 1460 items (at 73 items/min)
2015-11-02 00:27:50 [scrapy] INFO: Crawled 1756 pages (at 102 pages/min), scraped 1537 items (at 77 items/min)
2015-11-02 00:28:47 [scrapy] INFO: Crawled 1840 pages (at 84 pages/min), scraped 1616 items (at 79 items/min)
2015-11-02 00:28:55 [scrapy] ERROR: Error downloading <GET http://www.permalgroup.com>: TCP connection timed out: 60: Operation timed out.
2015-11-02 00:28:59 [scrapy] ERROR: Spider error processing <GET http://www.ranieripartners.com/LiteratureRetrieve.aspx?ID=192075> (referer: http://www.ranieripartners.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-02 00:29:00 [scrapy] ERROR: Spider error processing <GET http://www.ranieripartners.com/LiteratureRetrieve.aspx?ID=215366> (referer: http://www.ranieripartners.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-02 00:29:13 [scrapy] ERROR: Error downloading <GET http://www.bluesprucelp.com>: DNS lookup failed: address 'www.bluesprucelp.com' not found: [Errno 8] nodename nor servname provided, or not known.
2015-11-02 00:29:27 [scrapy] ERROR: Error processing {'pageurl': ['https://investor.criterionmgt.com/'],
 'siteurl': ['investor.criterionmgt.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/pipelines.py", line 39, in process_item
    self.cur.execute(sql, dc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 507, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 722, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 640, in _handle_result
    raise errors.get_exception(packet)
ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%(pagetitle)s, '', 'https://investor.criterionmgt.com/', 'investor.criterionmgt.' at line 1
2015-11-02 00:29:53 [scrapy] INFO: Crawled 1934 pages (at 94 pages/min), scraped 1699 items (at 83 items/min)
2015-11-02 00:30:17 [scrapy] ERROR: Error downloading <GET http://www.intrepidcap.com>: TCP connection timed out: 60: Operation timed out.
2015-11-02 00:30:48 [scrapy] INFO: Crawled 2004 pages (at 70 pages/min), scraped 1759 items (at 60 items/min)
2015-11-02 00:31:44 [scrapy] INFO: Crawled 2071 pages (at 67 pages/min), scraped 1816 items (at 57 items/min)
2015-11-02 00:32:15 [scrapy] ERROR: Spider error processing <GET http://www.bhgrp.com/> (referer: http://www.bhgrp.com)
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/spiders/vcs.py", line 36, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/goose_extractor-1.0.25-py2.7.egg/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-02 00:32:44 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-02 00:32:46 [scrapy] INFO: Crawled 2125 pages (at 54 pages/min), scraped 1869 items (at 53 items/min)
2015-11-02 00:33:02 [scrapy] ERROR: Error processing {'pageurl': ['https://www.mwam.com/'], 'siteurl': ['mwam.com'], 'text': ['']}
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/pipelines.py", line 39, in process_item
    self.cur.execute(sql, dc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 507, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 722, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 640, in _handle_result
    raise errors.get_exception(packet)
ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%(pagetitle)s, '', 'https://www.mwam.com/', 'mwam.com')' at line 1
2015-11-02 00:33:47 [scrapy] INFO: Crawled 2211 pages (at 86 pages/min), scraped 1931 items (at 62 items/min)
2015-11-02 00:33:56 [scrapy] ERROR: Error downloading <GET http://www.emffp.com>: TCP connection timed out: 60: Operation timed out.
2015-11-02 00:34:45 [scrapy] INFO: Crawled 2298 pages (at 87 pages/min), scraped 2006 items (at 75 items/min)
2015-11-02 00:35:33 [scrapy] ERROR: Error downloading <GET http://www.formulainvesting.com/>: Connection was refused by other side: 61: Connection refused.
2015-11-02 00:35:45 [scrapy] ERROR: Error processing {'pageurl': ['http://www.southpointcapital.com/default.asp?S=692639'],
 'siteurl': ['southpointcapital.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/johnmontroy/Documents/Learning/NYC-DSA/Project 3/banker.ai/vcspider/vcspider/pipelines.py", line 39, in process_item
    self.cur.execute(sql, dc)
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 507, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 722, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/Users/johnmontroy/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 640, in _handle_result
    raise errors.get_exception(packet)
ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '%(pagetitle)s, '', 'http://www.southpointcapital.com/default.asp?S=692639', 'sou' at line 1
2015-11-02 00:35:45 [scrapy] INFO: Crawled 2397 pages (at 99 pages/min), scraped 2085 items (at 79 items/min)
2015-11-02 00:36:45 [scrapy] INFO: Crawled 2433 pages (at 36 pages/min), scraped 2117 items (at 32 items/min)
