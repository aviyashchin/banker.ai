usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
2015-11-04 00:31:01 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 00:31:01 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 00:31:01 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 00:31:01 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 00:31:01 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 00:31:01 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 00:31:02 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 00:31:02 [scrapy] INFO: Spider opened
2015-11-04 00:31:02 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 00:31:02 [scrapy] ERROR: Error downloading <GET http://www.bigappleinsurancesolutions.com>: DNS lookup failed: address 'www.bigappleinsurancesolutions.com' not found: [Errno -2] Name or service not known.
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocol2015-11-04 00:31:41 [scrapy] INFO: Crawled 283 pages (at 283 pages/min), scraped 150 items (at 150 items/min)
2015-11-04 00:32:10 [scrapy] ERROR: Error downloading <GET https://www.new-vernon.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:32:43 [scrapy] INFO: Crawled 334 pages (at 51 pages/min), scraped 222 items (at 72 items/min)
2015-11-04 00:33:33 [scrapy] INFO: Crawled 401 pages (at 67 pages/min), scraped 297 items (at 75 items/min)
2015-11-04 00:34:32 [scrapy] INFO: Crawled 510 pages (at 109 pages/min), scraped 390 items (at 93 items/min)
2015-11-04 00:35:32 [scrapy] INFO: Crawled 595 pages (at 85 pages/min), scraped 483 items (at 93 items/min)
2015-11-04 00:36:33 [scrapy] INFO: Crawled 691 pages (at 96 pages/min), scraped 587 items (at 104 items/min)
2015-11-04 00:37:31 [scrapy] INFO: Crawled 754 pages (at 63 pages/min), scraped 648 items (at 61 items/min)
2015-11-04 00:38:17 [scrapy] ERROR: Error downloading <GET https://www.northgate.com/profile/our-team.php>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:38:27 [scrapy] ERROR: Error downloading <GET https://www.northgate.com/login/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:38:35 [scrapy] INFO: Crawled 848 pages (at 94 pages/min), scraped 745 items (at 97 items/min)
2015-11-04 00:39:14 [scrapy] ERROR: Spider error processing <GET http://www.marketbridgecapital.com/contact_en> (referer: http://www.marketbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 127: Unexpected end tag : p
2015-11-04 00:39:46 [scrapy] INFO: Crawled 967 pages (at 119 pages/min), scraped 858 items (at 113 items/min)
2015-11-04 00:40:13 [scrapy] ERROR: Spider error processing <GET http://www.marketbridgecapital.com/les_fonds/10/fiche_fond/market_bridge_global_macro_strategy_ii_ltd> (referer: http://www.marketbridgecapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/2015-11-04 00:40:31 [scrapy] INFO: Crawled 450 pages (at 45 pages/min), scraped 4012015-11-04 00:40:34 [scra2015-11-04 00:41:11 [scrapy] INFO: Crawled 467 pages (at 17 pages/min), scraped 423 2015-11-04 00:41:39 [scr2015-11-04 00:42:38 [scrapy] INFO: Crawled 475 pages (at 8 pages/min), scraped 439 items (at 16 items/min)
2015-11-04 00:43:08 [scrapy] INFO: Crawled 488 pages (at 13 pages/min), scraped 444 items (at2015-11-04 00:2015-11-04 00:44:06 [scrapy] INFO: Crawled 506 pages (at 18 pages/min), scraped 463 items (at 19 i2015-11-042015-11-04 00:45:29 [scrapy] INFO: Crawled 529 pages (at 23 pages/min), scraped 485 items (at 22 items/min)
2015-11-04 00:46:09 [scrapy] INFO: Crawled 529 pages (at 0 pages/min), scraped 493 items (at 8 items/min)
2015-11-04 00:47:16 [scrapy] INFO: Crawled 546 pages (at 17 pages/min), scraped 509 items (at 16 items/min)
2015-11-04 00:48:03 [scrapy] INFO: Crawled 570 pages (at 24 pages/min), scraped 526 items (at 17 items/min)
2015-11-04 00:49:11 [scrapy] INFO: Crawled 586 pages (at 16 pages/min), scraped 542 items (at 16 items/min)
2015-11-04 00:50:05 [scrapy] INFO: Crawled 586 pages (at 0 pages/min), scraped 550 items (at 8 items/min)
2015-11-04 00:51:21 [scrapy] INFO: Crawled 609 pages (at 23 pages/min), scraped 565 items (at 15 items/min)
2015-11-04 00:52:13 [scrapy] INFO: Crawled 627 pages (at 18 pages/min), scraped 583 items (at 18 items/min)
2015-11-04 00:53:29 [scrapy] INFO: Crawled 658 pages (at 31 pages/min), scraped 614 items (at 31 items/min)
2015-11-04 00:54:09 [scrapy] INFO: Crawled 666 pages (at 8 pages/min), scraped 622 items (at 8 items/min)
2015-11-04 00:55:18 [scrapy] INFO: Crawled 680 pages (at 14 pages/min), scraped 636 items (at 14 items/min)
2015-11-04 00:56:02 [scrapy] INFO: Crawled 688 pages (at 8 pages/min), scraped 644 items (at 8 items/min)
2015-11-04 00:57:08 [scrapy] INFO: Crawled 704 pages (at 16 pages/min), scraped 660 items (at 16 items/min)
2015-11-04 00:58:14 [scrapy] INFO: Crawled 712 pages (at 8 pages/min), scraped 676 items (at 16 items/min)
2015-11-04 00:59:12 [scrapy] INFO: Crawled 727 pages (at 15 pages/min), scraped 684 items (at 8 items/min)
2015-11-04 01:00:05 [scrapy] INFO: Crawled 735 pages (at 8 pages/min), scraped 691 items (at 7 items/min)
2015-11-04 01:01:02 [scrapy] INFO: Crawled 735 pages (at 0 pages/min), scraped 699 items (at 8 items/min)
2015-11-04 01:02:02 [scrapy] INFO: Crawled 749 pages (at 14 pages/min), scraped 708 items (at 9 items/min)
2015-11-04 01:03:17 [scrapy] INFO: Crawled 773 pages (at 24 pages/min), scraped 729 items (at 21 items/min)
2015-11-04 01:04:18 [scrapy] INFO: Crawled 789 pages (at 16 pages/min), scraped 745 items (at 16 items/min)
2015-11-04 01:05:08 [scrapy] INFO: Crawled 813 pages (at 24 pages/min), scraped 769 items (at 24 items/min)
2015-11-04 01:06:04 [scrapy] INFO: Crawled 849 pages (at 36 pages/min), scraped 799 items (at 30 items/min)
2015-11-04 01:07:12 [scrapy] INFO: Crawled 881 pages (at 32 pages/min), scraped 837 items (at 38 items/min)
2015-11-04 01:08:12 [scrapy] INFO: Crawled 913 pages (at 32 pages/min), scraped 869 items (at 32 items/min)
2015-11-04 01:09:11 [scrapy] INFO: Crawled 937 pages (at 24 pages/min), scraped 893 items (at 24 items/min)
2015-11-04 01:10:22 [scrapy] INFO: Crawled 945 pages (at 8 pages/min), scraped 909 items (at 16 items/min)
2015-11-04 01:11:36 [scrapy] INFO: Crawled 969 pages (at 24 pages/min), scraped 925 items (at 16 items/min)
2015-11-04 01:12:17 [scrapy] INFO: Crawled 969 pages (at 0 pages/min), scraped 933 items (at 8 items/min)
2015-11-04 01:13:05 [scrapy] INFO: Crawled 986 pages (at 17 pages/min), scraped 942 items (at 9 items/min)
2015-11-04 01:14:04 [scrapy] INFO: Crawled 1008 pages (at 22 pages/min), scraped 958 items (at 16 items/min)
2015-11-04 01:15:03 [scrapy] INFO: Crawled 1018 pages (at 10 pages/min), scraped 974 items (at 16 items/min)
2015-11-04 01:16:25 [scrapy] INFO: Crawled 1042 pages (at 24 pages/min), scraped 998 items (at 24 items/min)
2015-11-04 01:17:18 [scrapy] INFO: Crawled 1058 pages (at 16 pages/min), scraped 1014 items (at 16 items/min)
2015-11-04 01:18:21 [scrapy] INFO: Crawled 1074 pages (at 16 pages/min), scraped 1030 items (at 16 items/min)
2015-11-04 01:19:32 [scrapy] INFO: Crawled 1093 pages (at 19 pages/min), scraped 1049 items (at 19 items/min)
2015-11-04 01:20:09 [scrapy] INFO: Crawled 1097 pages (at 4 pages/min), scraped 1057 items (at 8 items/min)
2015-11-04 01:21:23 [scrapy] INFO: Crawled 1122 pages (at 25 pages/min), scraped 1078 items (at 21 items/min)
2015-11-04 01:22:12 [scrapy] INFO: Crawled 1137 pages (at 15 pages/min), scraped 1094 items (at 16 items/min)
2015-11-04 01:23:09 [scrapy] INFO: Crawled 1161 pages (at 24 pages/min), scraped 1117 items (at 23 items/min)
2015-11-04 01:24:14 [scrapy] INFO: Crawled 1199 pages (at 38 pages/min), scraped 1155 items (at 38 items/min)
2015-11-04 01:25:12 [scrapy] INFO: Crawled 1232 pages (at 33 pages/min), scraped 1188 items (at 33 items/min)
2015-11-04 01:26:04 [scrapy] INFO: Crawled 1256 pages (at 24 pages/min), scraped 1212 items (at 24 items/min)
2015-11-04 01:27:05 [scrapy] INFO: Crawled 1286 pages (at 30 pages/min), scraped 1242 items (at 30 items/min)
2015-11-04 01:28:04 [scrapy] INFO: Crawled 1309 pages (at 23 pages/min), scraped 1262 items (at 20 items/min)
2015-11-04 01:29:17 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/iflight/jc-BKK.html> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:29:27 [scrapy] INFO: Crawled 1309 pages (at 0 pages/min), scraped 1267 items (at 5 items/min)
2015-11-04 01:31:48 [scrapy] INFO: Crawled 1309 pages (at 0 pages/min), scraped 1272 items (at 5 items/min)
2015-11-04 01:32:05 [scrapy] INFO: Crawled 1315 pages (at 6 pages/min), scraped 1274 items (at 2 items/min)
2015-11-04 01:34:06 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/iflight/routes_TNA-HKG.html> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:34:06 [scrapy] INFO: Crawled 1317 pages (at 2 pages/min), scraped 1276 items (at 2 items/min)
2015-11-04 01:35:02 [scrapy] INFO: Crawled 1348 pages (at 31 pages/min), scraped 1308 items (at 32 items/min)
2015-11-04 01:35:44 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fjc-FRA.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:37:23 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fjc-HKT.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:37:23 [scrapy] INFO: Crawled 1361 pages (at 13 pages/min), scraped 1313 items (at 5 items/min)
2015-11-04 01:38:28 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fjc-SIN.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:39:41 [scrapy] INFO: Crawled 1361 pages (at 0 pages/min), scraped 1320 items (at 7 items/min)
2015-11-04 01:41:00 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_HKG-BJS.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:41:00 [scrapy] INFO: Crawled 1372 pages (at 11 pages/min), scraped 1321 items (at 1 items/min)
2015-11-04 01:41:35 [scrapy] INFO: Crawled 1372 pages (at 0 pages/min), scraped 1327 items (at 6 items/min)
2015-11-04 01:42:15 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_HKG-CAN.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:42:23 [scrapy] INFO: Crawled 1373 pages (at 1 pages/min), scraped 1329 items (at 2 items/min)
2015-11-04 01:43:43 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_DLC-HKG.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:44:27 [scrapy] INFO: Crawled 1386 pages (at 13 pages/min), scraped 1335 items (at 6 items/min)
2015-11-04 01:45:19 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_SHA-YVR.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 01:45:34 [scrapy] INFO: Crawled 1387 pages (at 1 pages/min), scraped 1341 items (at 6 items/min)
2015-11-04 01:47:12 [scrapy] INFO: Crawled 1401 pages (at 14 pages/min), scraped 1347 items (at 6 items/min)
2015-11-04 01:48:04 [scrapy] INFO: Crawled 1409 pages (at 8 pages/min), scraped 1357 items (at 10 items/min)
2015-11-04 01:49:19 [scrapy] INFO: Crawled 1425 pages (at 16 pages/min), scraped 1372 items (at 15 items/min)
2015-11-04 01:50:06 [scrapy] INFO: Crawled 1425 pages (at 0 pages/min), scraped 1380 items (at 8 items/min)
2015-11-04 01:51:14 [scrapy] INFO: Crawled 1441 pages (at 16 pages/min), scraped 1392 items (at 12 items/min)
2015-11-04 01:52:38 [scrapy] INFO: Crawled 1459 pages (at 18 pages/min), scraped 1407 items (at 15 items/min)
2015-11-04 01:53:29 [scrapy] INFO: Crawled 1462 pages (at 3 pages/min), scraped 1414 items (at 7 items/min)
2015-11-04 01:54:06 [scrapy] INFO: Crawled 1470 pages (at 8 pages/min), scraped 1418 items (at 4 items/min)
2015-11-04 01:55:07 [scrapy] INFO: Crawled 1489 pages (at 19 pages/min), scraped 1438 items (at 20 items/min)
2015-11-04 01:56:32 [scrapy] INFO: Crawled 1504 pages (at 15 pages/min), scraped 1459 items (at 21 items/min)
2015-11-04 01:58:53 [scrapy] INFO: Crawled 1522 pages (at 18 pages/min), scraped 1469 items (at 10 items/min)
2015-11-04 01:59:38 [scrapy] INFO: Crawled 1522 pages (at 0 pages/min), scraped 1477 items (at 8 items/min)
2015-11-04 02:00:29 [scrapy] INFO: Crawled 1543 pages (at 21 pages/min), scraped 1486 items (at 9 items/min)
2015-11-04 02:01:24 [scrapy] INFO: Crawled 1543 pages (at 0 pages/min), scraped 1498 items (at 12 items/min)
2015-11-04 02:02:17 [scrapy] INFO: Crawled 1557 pages (at 14 pages/min), scraped 1505 items (at 7 items/min)
2015-11-04 02:02:27 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/hotel/maplist.aspx?txtCityId=321> (referer: http://www.ly.com/hotel/shanghai321/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:03:21 [scrapy] INFO: Crawled 1575 pages (at 18 pages/min), scraped 1521 items (at 16 items/min)
2015-11-04 02:04:09 [scrapy] INFO: Crawled 1591 pages (at 16 pages/min), scraped 1537 items (at 16 items/min)
2015-11-04 02:05:10 [scrapy] INFO: Crawled 1607 pages (at 16 pages/min), scraped 1553 items (at 16 items/min)
2015-11-04 02:06:04 [scrapy] INFO: Crawled 1622 pages (at 15 pages/min), scraped 1568 items (at 15 items/min)
2015-11-04 02:07:03 [scrapy] INFO: Crawled 1638 pages (at 16 pages/min), scraped 1584 items (at 16 items/min)
2015-11-04 02:09:05 [scrapy] INFO: Crawled 1646 pages (at 8 pages/min), scraped 1600 items (at 16 items/min)
2015-11-04 02:10:13 [scrapy] INFO: Crawled 1677 pages (at 31 pages/min), scraped 1615 items (at 15 items/min)
2015-11-04 02:11:16 [scrapy] INFO: Crawled 1677 pages (at 0 pages/min), scraped 1631 items (at 16 items/min)
2015-11-04 02:12:06 [scrapy] INFO: Crawled 1697 pages (at 20 pages/min), scraped 1643 items (at 12 items/min)
2015-11-04 02:13:05 [scrapy] INFO: Crawled 1710 pages (at 13 pages/min), scraped 1658 items (at 15 items/min)
2015-11-04 02:14:36 [scrapy] INFO: Crawled 1733 pages (at 23 pages/min), scraped 1680 items (at 22 items/min)
2015-11-04 02:15:08 [scrapy] INFO: Crawled 1733 pages (at 0 pages/min), scraped 1687 items (at 7 items/min)
2015-11-04 02:16:26 [scrapy] INFO: Crawled 1762 pages (at 29 pages/min), scraped 1708 items (at 21 items/min)
2015-11-04 02:17:03 [scrapy] INFO: Crawled 1775 pages (at 13 pages/min), scraped 1718 items (at 10 items/min)
2015-11-04 02:19:19 [scrapy] INFO: Crawled 1790 pages (at 15 pages/min), scraped 1744 items (at 26 items/min)
2015-11-04 02:20:05 [scrapy] INFO: Crawled 1819 pages (at 29 pages/min), scraped 1766 items (at 22 items/min)
2015-11-04 02:21:48 [scrapy] INFO: Crawled 1842 pages (at 23 pages/min), scraped 1784 items (at 18 items/min)
2015-11-04 02:22:08 [scrapy] INFO: Crawled 1842 pages (at 0 pages/min), scraped 1796 items (at 12 items/min)
2015-11-04 02:23:10 [scrapy] INFO: Crawled 1877 pages (at 35 pages/min), scraped 1820 items (at 24 items/min)
2015-11-04 02:24:05 [scrapy] INFO: Crawled 1885 pages (at 8 pages/min), scraped 1833 items (at 13 items/min)
2015-11-04 02:25:13 [scrapy] INFO: Crawled 1898 pages (at 13 pages/min), scraped 1841 items (at 8 items/min)
2015-11-04 02:26:37 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fairline-AF.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:26:38 [scrapy] INFO: Crawled 1901 pages (at 3 pages/min), scraped 1846 items (at 5 items/min)
2015-11-04 02:27:51 [scrapy] INFO: Crawled 1901 pages (at 0 pages/min), scraped 1851 items (at 5 items/min)
2015-11-04 02:28:36 [scrapy] INFO: Crawled 1901 pages (at 0 pages/min), scraped 1854 items (at 3 items/min)
2015-11-04 02:29:05 [scrapy] INFO: Crawled 1916 pages (at 15 pages/min), scraped 1860 items (at 6 items/min)
2015-11-04 02:30:28 [scrapy] INFO: Crawled 1929 pages (at 13 pages/min), scraped 1876 items (at 16 items/min)
2015-11-04 02:31:09 [scrapy] INFO: Crawled 1956 pages (at 27 pages/min), scraped 1893 items (at 17 items/min)
2015-11-04 02:32:24 [scrapy] INFO: Crawled 1964 pages (at 8 pages/min), scraped 1917 items (at 24 items/min)
2015-11-04 02:33:44 [scrapy] INFO: Crawled 1985 pages (at 21 pages/min), scraped 1933 items (at 16 items/min)
2015-11-04 02:36:14 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/HotelComments-179646-1.html> (referer: http://www.ly.com/hotel/suzhou226/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:36:36 [scrapy] INFO: Crawled 1988 pages (at 3 pages/min), scraped 1937 items (at 4 items/min)
2015-11-04 02:37:09 [scrapy] INFO: Crawled 1988 pages (at 0 pages/min), scraped 1940 items (at 3 items/min)
2015-11-04 02:38:57 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/HotelComments-6457-1.html> (referer: http://www.ly.com/hotel/suzhou226/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:38:57 [scrapy] INFO: Crawled 2003 pages (at 15 pages/min), scraped 1952 items (at 12 items/min)
2015-11-04 02:39:07 [scrapy] INFO: Crawled 2004 pages (at 1 pages/min), scraped 1954 items (at 2 items/min)
2015-11-04 02:40:33 [scrapy] INFO: Crawled 2023 pages (at 19 pages/min), scraped 1964 items (at 10 items/min)
2015-11-04 02:41:28 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/HotelComments-46539-1.html> (referer: http://www.ly.com/hotel/hangzhou383/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:42:24 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/HotelComments-10211-1.html> (referer: http://www.ly.com/hotel/hangzhou383/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:42:29 [scrapy] INFO: Crawled 2023 pages (at 0 pages/min), scraped 1972 items (at 8 items/min)
2015-11-04 02:43:27 [scrapy] INFO: Crawled 2030 pages (at 7 pages/min), scraped 1973 items (at 1 items/min)
2015-11-04 02:43:31 [scrapy] ERROR: Error downloading <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fairline-SQ.html&ac=873966249>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 02:45:11 [scrapy] INFO: Crawled 2042 pages (at 12 pages/min), scraped 1986 items (at 13 items/min)
2015-11-04 02:46:21 [scrapy] INFO: Crawled 2068 pages (at 26 pages/min), scraped 1998 items (at 12 items/min)
2015-11-04 02:47:29 [scrapy] INFO: Crawled 2081 pages (at 13 pages/min), scraped 2017 items (at 19 items/min)
2015-11-04 02:48:44 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsChannel%3dhotel%26TcPmsUniqueKey%3dcnhotelcommentsnew1200%26id%3d14053%26Flag%3d1%26page%3d1&ac=873966249> (referer: http://www.ly.com/hotel/hangzhou383/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:50:24 [scrapy] INFO: Crawled 2082 pages (at 1 pages/min), scraped 2027 items (at 10 items/min)
2015-11-04 02:50:37 [scrapy] ERROR: Error downloading <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fairline-BR.html&ac=873966249>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 02:51:07 [scrapy] INFO: Crawled 2091 pages (at 9 pages/min), scraped 2032 items (at 5 items/min)
2015-11-04 02:51:59 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/HotelInfo-321192.html> (referer: http://www.ly.com/hotel/nanjing224/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:52:49 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/HotelInfo-2387.html> (referer: http://www.ly.com/hotel/nanjing224/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:52:49 [scrapy] INFO: Crawled 2097 pages (at 6 pages/min), scraped 2033 items (at 1 items/min)
2015-11-04 02:53:27 [scrapy] INFO: Crawled 2097 pages (at 0 pages/min), scraped 2037 items (at 4 items/min)
2015-11-04 02:55:18 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsUniqueKey%3dcnhotelinfo1200%26TcPmsChannel%3dhotel%26id%3d1861&ac=873966249> (referer: http://www.ly.com/hotel/hangzhou383/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 02:55:18 [scrapy] INFO: Crawled 2099 pages (at 2 pages/min), scraped 2042 items (at 5 items/min)
2015-11-04 02:56:31 [scrapy] INFO: Crawled 2115 pages (at 16 pages/min), scraped 2050 items (at 8 items/min)
2015-11-04 02:57:51 [scrapy] INFO: Crawled 2116 pages (at 1 pages/min), scraped 2055 items (at 5 items/min)
2015-11-04 02:58:31 [scrapy] INFO: Crawled 2116 pages (at 0 pages/min), scraped 2060 items (at 5 items/min)
2015-11-04 03:00:02 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsUniqueKey%3dcnhotelinfo1200%26TcPmsChannel%3dhotel%26id%3d81607&ac=873966249> (referer: http://www.ly.com/hotel/guangzhou80/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:00:02 [scrapy] INFO: Crawled 2116 pages (at 0 pages/min), scraped 2060 items (at 0 items/min)
2015-11-04 03:01:26 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsChannel%3dhotel%26TcPmsUniqueKey%3dcnhotelcommentsnew1200%26id%3d53085%26Flag%3d1%26page%3d1&ac=873966249> (referer: http://www.ly.com/hotel/guangzhou80/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:01:27 [scrapy] INFO: Crawled 2132 pages (at 16 pages/min), scraped 2070 items (at 10 items/min)
2015-11-04 03:02:14 [scrapy] INFO: Crawled 2133 pages (at 1 pages/min), scraped 2075 items (at 5 items/min)
2015-11-04 03:03:29 [scrapy] INFO: Crawled 2160 pages (at 27 pages/min), scraped 2100 items (at 25 items/min)
2015-11-04 03:04:19 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsChannel%3dhotel%26TcPmsUniqueKey%3dcnhotelcommentsnew1200%26id%3d121401%26Flag%3d1%26page%3d1&ac=873966249> (referer: http://www.ly.com/hotel/hangzhou383/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:04:19 [scrapy] INFO: Crawled 2161 pages (at 1 pages/min), scraped 2102 items (at 2 items/min)
2015-11-04 03:05:28 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsChannel%3dhotel%26TcPmsUniqueKey%3dcnhotelcommentsnew1200%26id%3d510138%26Flag%3d1%26page%3d1&ac=873966249> (referer: http://www.ly.com/hotel/nanjing224/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:05:28 [scrapy] INFO: Crawled 2161 pages (at 0 pages/min), scraped 2102 items (at 0 items/min)
2015-11-04 03:06:05 [scrapy] INFO: Crawled 2176 pages (at 15 pages/min), scraped 2104 items (at 2 items/min)
2015-11-04 03:07:08 [scrapy] INFO: Crawled 2182 pages (at 6 pages/min), scraped 2117 items (at 13 items/min)
2015-11-04 03:08:07 [scrapy] INFO: Crawled 2209 pages (at 27 pages/min), scraped 2143 items (at 26 items/min)
2015-11-04 03:08:55 [scrapy] ERROR: Error downloading <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsChannel%3dhotel%26TcPmsUniqueKey%3dcnhotelcommentsnew1200%26id%3d28657%26Flag%3d1%26page%3d1&ac=873966249>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 03:09:21 [scrapy] INFO: Crawled 2214 pages (at 5 pages/min), scraped 2154 items (at 11 items/min)
2015-11-04 03:10:50 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fairline-DL.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:11:22 [scrapy] INFO: Crawled 2231 pages (at 17 pages/min), scraped 2159 items (at 5 items/min)
2015-11-04 03:12:05 [scrapy] INFO: Crawled 2231 pages (at 0 pages/min), scraped 2165 items (at 6 items/min)
2015-11-04 03:13:22 [scrapy] INFO: Crawled 2246 pages (at 15 pages/min), scraped 2180 items (at 15 items/min)
2015-11-04 03:16:58 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fcity_SEL.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:16:58 [scrapy] INFO: Crawled 2247 pages (at 1 pages/min), scraped 2185 items (at 5 items/min)
2015-11-04 03:17:13 [scrapy] INFO: Crawled 2248 pages (at 1 pages/min), scraped 2186 items (at 1 items/min)
2015-11-04 03:18:33 [scrapy] INFO: Crawled 2298 pages (at 50 pages/min), scraped 2219 items (at 33 items/min)
2015-11-04 03:19:20 [scrapy] INFO: Crawled 2308 pages (at 10 pages/min), scraped 2234 items (at 15 items/min)
2015-11-04 03:21:29 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fvirtual.ly.com%2fhotel%2fPageHandler.ashx%3fTcPmsUniqueKey%3dcnhotelinfo1200%26TcPmsChannel%3dhotel%26id%3d1983&ac=873966249> (referer: http://www.ly.com/hotel/guangzhou80/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:21:54 [scrapy] INFO: Crawled 2341 pages (at 33 pages/min), scraped 2268 items (at 34 items/min)
2015-11-04 03:22:28 [scrapy] INFO: Crawled 2344 pages (at 3 pages/min), scraped 2274 items (at 6 items/min)
2015-11-04 03:23:34 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fcomments%2f&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:24:28 [scrapy] INFO: Crawled 2346 pages (at 2 pages/min), scraped 2276 items (at 2 items/min)
2015-11-04 03:25:05 [scrapy] INFO: Crawled 2386 pages (at 40 pages/min), scraped 2293 items (at 17 items/min)
2015-11-04 03:26:55 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fcity_CEB.html&ac=873966249> (referer: http://flights.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:26:55 [scrapy] INFO: Crawled 2390 pages (at 4 pages/min), scraped 2315 items (at 22 items/min)
2015-11-04 03:27:15 [scrapy] INFO: Crawled 2390 pages (at 0 pages/min), scraped 2317 items (at 2 items/min)
2015-11-04 03:28:09 [scrapy] INFO: Crawled 2436 pages (at 46 pages/min), scraped 2336 items (at 19 items/min)
2015-11-04 03:29:14 [scrapy] INFO: Crawled 2450 pages (at 14 pages/min), scraped 2365 items (at 29 items/min)
2015-11-04 03:30:05 [scrapy] ERROR: Spider error processing <GET http://union.ly.com/NewsDetails_227.html> (referer: http://union.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:30:25 [scrapy] INFO: Crawled 2469 pages (at 19 pages/min), scraped 2383 items (at 18 items/min)
2015-11-04 03:31:33 [scrapy] INFO: Crawled 2503 pages (at 34 pages/min), scraped 2411 items (at 28 items/min)
2015-11-04 03:32:25 [scrapy] INFO: Crawled 2503 pages (at 0 pages/min), scraped 2430 items (at 19 items/min)
2015-11-04 03:33:07 [scrapy] INFO: Crawled 2533 pages (at 30 pages/min), scraped 2444 items (at 14 items/min)
2015-11-04 03:42:44 [scrapy] INFO: Crawled 2565 pages (at 32 pages/min), scraped 2476 items (at 32 items/min)
2015-11-04 03:42:55 [scrapy] ERROR: Error downloading <GET http://passport.ly.com?pageurl=http%3a%2f%2fmember.ly.com%2fDefault.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 03:43:09 [scrapy] INFO: Crawled 2565 pages (at 0 pages/min), scraped 2492 items (at 16 items/min)
2015-11-04 03:44:11 [scrapy] INFO: Crawled 2590 pages (at 25 pages/min), scraped 2515 items (at 23 items/min)
2015-11-04 03:45:24 [scrapy] INFO: Crawled 2645 pages (at 55 pages/min), scraped 2559 items (at 44 items/min)
2015-11-04 03:48:04 [scrapy] INFO: Crawled 2661 pages (at 16 pages/min), scraped 2572 items (at 13 items/min)
2015-11-04 03:49:41 [scrapy] INFO: Crawled 2704 pages (at 43 pages/min), scraped 2619 items (at 47 items/min)
2015-11-04 03:50:07 [scrapy] INFO: Crawled 2727 pages (at 23 pages/min), scraped 2637 items (at 18 items/min)
2015-11-04 03:51:10 [scrapy] INFO: Crawled 2777 pages (at 50 pages/min), scraped 2682 items (at 45 items/min)
2015-11-04 03:53:05 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_CAN-SYD.html&ac=873966249> (referer: http://www.ly.com/city_ticket_80.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 03:53:30 [scrapy] INFO: Crawled 2819 pages (at 42 pages/min), scraped 2729 items (at 47 items/min)
2015-11-04 03:54:05 [scrapy] INFO: Crawled 2872 pages (at 53 pages/min), scraped 2765 items (at 36 items/min)
2015-11-04 03:55:16 [scrapy] INFO: Crawled 2929 pages (at 57 pages/min), scraped 2838 items (at 73 items/min)
2015-11-04 03:56:07 [scrapy] INFO: Crawled 2976 pages (at 47 pages/min), scraped 2883 items (at 45 items/min)
2015-11-04 03:56:19 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.petcube.android&&referrer=utm_content%3D8acf800b-d090-41af-8bdc-e6d9e470241b%26utm_medium%3Dad-analytics%26utm_source%3Dflurry%26utm_campaign%3DPetcube.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:56:53 [scrapy] ERROR: Spider error processing <GET https://petcube.com/user_guide.pdf> (referer: https://petcube.com/support/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:57:04 [scrapy] INFO: Crawled 3036 pages (at 60 pages/min), scraped 2937 items (at 54 items/min)
2015-11-04 03:58:01 [scrapy] ERROR: Error downloading <GET http://www.storic.pl>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 03:58:21 [scrapy] INFO: Crawled 3112 pages (at 76 pages/min), scraped 3019 items (at 82 items/min)
2015-11-04 04:00:06 [scrapy] INFO: Crawled 3144 pages (at 32 pages/min), scraped 3050 items (at 31 items/min)
2015-11-04 04:01:18 [scrapy] INFO: Crawled 3189 pages (at 45 pages/min), scraped 3083 items (at 33 items/min)
2015-11-04 04:03:32 [scrapy] INFO: Crawled 3202 pages (at 13 pages/min), scraped 3099 items (at 16 items/min)
2015-11-04 04:05:59 [scrapy] INFO: Crawled 3202 pages (at 0 pages/min), scraped 3115 items (at 16 items/min)
2015-11-04 04:07:00 [scrapy] INFO: Crawled 3202 pages (at 0 pages/min), scraped 3124 items (at 9 items/min)
2015-11-04 04:07:02 [scrapy] INFO: Crawled 3227 pages (at 25 pages/min), scraped 3126 items (at 2 items/min)
2015-11-04 04:09:13 [scrapy] INFO: Crawled 3268 pages (at 41 pages/min), scraped 3155 items (at 29 items/min)
2015-11-04 04:11:42 [scrapy] INFO: Crawled 3276 pages (at 8 pages/min), scraped 3164 items (at 9 items/min)
2015-11-04 04:12:34 [scrapy] INFO: Crawled 3276 pages (at 0 pages/min), scraped 3171 items (at 7 items/min)
2015-11-04 04:14:25 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_nanjing.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 04:15:01 [scrapy] INFO: Crawled 3277 pages (at 1 pages/min), scraped 3178 items (at 7 items/min)
2015-11-04 04:15:04 [scrapy] INFO: Crawled 3280 pages (at 3 pages/min), scraped 3179 items (at 1 items/min)
2015-11-04 04:17:44 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_zhengzhou.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:17:57 [scrapy] INFO: Crawled 3322 pages (at 42 pages/min), scraped 3189 items (at 10 items/min)
2015-11-04 04:19:07 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_nanning_shenzhen.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:20:28 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_dalian_shenzhen.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:21:00 [scrapy] INFO: Crawled 3322 pages (at 0 pages/min), scraped 3197 items (at 8 items/min)
2015-11-04 04:21:00 [scrapy] ERROR: Error downloading <GET https://passport.ly.com/?pageurl=http%3a%2f%2fmember.ly.com%2fyoulun%2fyoulunorderlist.aspx>: An error occurred while connecting: 32: Broken pipe.
2015-11-04 04:21:00 [scrapy] ERROR: Error downloading <GET https://passport.ly.com/?pageurl=http%3a%2f%2fmember.ly.com%2fguoneiyou%2fguoneiyouorderlist.aspx>: An error occurred while connecting: 32: Broken pipe.
2015-11-04 04:21:00 [scrapy] ERROR: Error downloading <GET https://passport.ly.com/?pageurl=http%3a%2f%2fmember.ly.com%2fdujia%2fdujiaorderlist.aspx>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://passport.ly.com/?pageurl=http%3a%2f%2fmember.ly.com%2fdujia%2fdujiaorderlist.aspx took longer than 180.0 seconds..
2015-11-04 04:21:02 [scrapy] INFO: Crawled 3322 pages (at 0 pages/min), scraped 3197 items (at 0 items/min)
2015-11-04 04:22:47 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_tianjin_shenzhen.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:23:55 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_shenzhen_nanning.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:23:55 [scrapy] INFO: Crawled 3347 pages (at 25 pages/min), scraped 3204 items (at 7 items/min)
2015-11-04 04:24:47 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_shenzhen_guilin.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:26:36 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_shenzhen_fuzhou.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:26:36 [scrapy] INFO: Crawled 3347 pages (at 0 pages/min), scraped 3210 items (at 6 items/min)
2015-11-04 04:28:37 [scrapy] INFO: Crawled 3347 pages (at 0 pages/min), scraped 3218 items (at 8 items/min)
2015-11-04 04:30:13 [scrapy] INFO: Crawled 3380 pages (at 33 pages/min), scraped 3230 items (at 12 items/min)
2015-11-04 04:32:55 [scrapy] INFO: Crawled 3388 pages (at 8 pages/min), scraped 3249 items (at 19 items/min)
2015-11-04 04:34:47 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_chongqing_ningbo.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:35:01 [scrapy] INFO: Crawled 3388 pages (at 0 pages/min), scraped 3255 items (at 6 items/min)
2015-11-04 04:35:12 [scrapy] INFO: Crawled 3388 pages (at 0 pages/min), scraped 3256 items (at 1 items/min)
2015-11-04 04:38:07 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_jinan_chengdu.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:38:18 [scrapy] INFO: Crawled 3436 pages (at 48 pages/min), scraped 3271 items (at 15 items/min)
2015-11-04 04:39:35 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_changsha_chengdu.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:41:43 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_wulumuqi_chengdu.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:41:44 [scrapy] INFO: Crawled 3436 pages (at 0 pages/min), scraped 3283 items (at 12 items/min)
2015-11-04 04:43:25 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_chengdu_shijiazhuang.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:44:22 [scrapy] INFO: Crawled 3436 pages (at 0 pages/min), scraped 3297 items (at 14 items/min)
2015-11-04 04:45:28 [scrapy] INFO: Crawled 3464 pages (at 28 pages/min), scraped 3302 items (at 5 items/min)
2015-11-04 04:48:08 [scrapy] INFO: Crawled 3472 pages (at 8 pages/min), scraped 3315 items (at 13 items/min)
2015-11-04 04:50:07 [scrapy] INFO: Crawled 3472 pages (at 0 pages/min), scraped 3325 items (at 10 items/min)
2015-11-04 04:52:28 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_qingdao_wuhan.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:52:29 [scrapy] INFO: Crawled 3474 pages (at 2 pages/min), scraped 3332 items (at 7 items/min)
2015-11-04 04:54:29 [scrapy] INFO: Crawled 3527 pages (at 53 pages/min), scraped 3356 items (at 24 items/min)
2015-11-04 04:55:47 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_wuhan_chengdu.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:56:57 [scrapy] INFO: Crawled 3527 pages (at 0 pages/min), scraped 3370 items (at 14 items/min)
2015-11-04 04:59:12 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_wulumuqi_hangzhou.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:59:40 [scrapy] INFO: Crawled 3527 pages (at 0 pages/min), scraped 3385 items (at 15 items/min)
2015-11-04 05:00:42 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_qingdao_hangzhou.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:00:45 [scrapy] INFO: Crawled 3562 pages (at 35 pages/min), scraped 3403 items (at 18 items/min)
2015-11-04 05:03:14 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_zhengzhou_hangzhou.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:03:31 [scrapy] INFO: Crawled 3570 pages (at 8 pages/min), scraped 3416 items (at 13 items/min)
2015-11-04 05:05:15 [scrapy] INFO: Crawled 3570 pages (at 0 pages/min), scraped 3424 items (at 8 items/min)
2015-11-04 05:07:05 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_hangzhou_kunming.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:07:36 [scrapy] INFO: Crawled 3604 pages (at 34 pages/min), scraped 3437 items (at 13 items/min)
2015-11-04 05:08:34 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_hangzhou_chongqing.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:09:11 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_hangzhou_xiamen.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:10:49 [scrapy] INFO: Crawled 3604 pages (at 0 pages/min), scraped 3453 items (at 16 items/min)
2015-11-04 05:11:08 [scrapy] INFO: Crawled 3630 pages (at 26 pages/min), scraped 3455 items (at 2 items/min)
2015-11-04 05:12:22 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_xian_guangzhou.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:12:46 [scrapy] INFO: Crawled 3641 pages (at 11 pages/min), scraped 3462 items (at 7 items/min)
2015-11-04 05:13:59 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_guangzhou_guilin.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:15:57 [scrapy] INFO: Crawled 3641 pages (at 0 pages/min), scraped 3477 items (at 15 items/min)
2015-11-04 05:18:28 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_guangzhou_sanya.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:18:39 [scrapy] INFO: Crawled 3641 pages (at 0 pages/min), scraped 3487 items (at 10 items/min)
2015-11-04 05:19:02 [scrapy] INFO: Crawled 3676 pages (at 35 pages/min), scraped 3497 items (at 10 items/min)
2015-11-04 05:20:46 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_guiyang_shanghai.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:21:12 [scrapy] INFO: Crawled 3680 pages (at 4 pages/min), scraped 3509 items (at 12 items/min)
2015-11-04 05:21:56 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_kunming_shanghai.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:22:57 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_zhengzhou_shanghai.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:24:41 [scrapy] INFO: Crawled 3680 pages (at 0 pages/min), scraped 3523 items (at 14 items/min)
2015-11-04 05:26:23 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_CTU-SIN.html&ac=873966249> (referer: http://www.ly.com/city_ticket_324.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:26:26 [scrapy] INFO: Crawled 3708 pages (at 28 pages/min), scraped 3527 items (at 4 items/min)
2015-11-04 05:27:18 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_shanghai_changchun.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:28:57 [scrapy] INFO: Crawled 3708 pages (at 0 pages/min), scraped 3536 items (at 9 items/min)
2015-11-04 05:31:17 [scrapy] INFO: Crawled 3708 pages (at 0 pages/min), scraped 3549 items (at 13 items/min)
2015-11-04 05:32:06 [scrapy] INFO: Crawled 3750 pages (at 42 pages/min), scraped 3583 items (at 34 items/min)
2015-11-04 05:33:02 [scrapy] INFO: Crawled 3784 pages (at 34 pages/min), scraped 3617 items (at 34 items/min)
2015-11-04 05:34:07 [scrapy] INFO: Crawled 3820 pages (at 36 pages/min), scraped 3657 items (at 40 items/min)
2015-11-04 05:35:05 [scrapy] INFO: Crawled 3838 pages (at 18 pages/min), scraped 3666 items (at 9 items/min)
2015-11-04 05:37:15 [scrapy] INFO: Crawled 3846 pages (at 8 pages/min), scraped 3677 items (at 11 items/min)
2015-11-04 05:39:31 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_beijing_wenzhou.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:39:31 [scrapy] INFO: Crawled 3846 pages (at 0 pages/min), scraped 3684 items (at 7 items/min)
2015-11-04 05:41:09 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_hangzhou_shenzhen.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:41:09 [scrapy] INFO: Crawled 3876 pages (at 30 pages/min), scraped 3691 items (at 7 items/min)
2015-11-04 05:42:16 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_xian_shanghai.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:42:26 [scrapy] INFO: Crawled 3876 pages (at 0 pages/min), scraped 3697 items (at 6 items/min)
2015-11-04 05:43:06 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_shanghai_xian.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:43:47 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_chongqing_beijing.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:45:22 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_beijing_ningbo.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:46:28 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_yantai_shanghai.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:46:54 [scrapy] INFO: Crawled 3876 pages (at 0 pages/min), scraped 3706 items (at 9 items/min)
2015-11-04 05:48:50 [scrapy] INFO: Crawled 3906 pages (at 30 pages/min), scraped 3718 items (at 12 items/min)
2015-11-04 05:50:31 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_shanghai_dalian.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:51:34 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_nanjing_beijing.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:53:06 [scrapy] INFO: Crawled 3916 pages (at 10 pages/min), scraped 3732 items (at 14 items/min)
2015-11-04 05:55:50 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_xiamen_shanghai.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:56:42 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_tianjin_shanghai.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:57:25 [scrapy] ERROR: Spider error processing <GET http://jipiao.ly.com/city_guangzhou_nanjing.html> (referer: http://jipiao.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:57:25 [scrapy] INFO: Crawled 3916 pages (at 0 pages/min), scraped 3739 items (at 7 items/min)
2015-11-04 05:59:26 [scrapy] INFO: Crawled 3927 pages (at 11 pages/min), scraped 3748 items (at 9 items/min)
2015-11-04 06:00:02 [scrapy] INFO: Crawled 3939 pages (at 12 pages/min), scraped 3752 items (at 4 items/min)
2015-11-04 06:01:09 [scrapy] INFO: Crawled 3972 pages (at 33 pages/min), scraped 3793 items (at 41 items/min)
2015-11-04 06:03:01 [scrapy] INFO: Crawled 3996 pages (at 24 pages/min), scraped 3808 items (at 15 items/min)
2015-11-04 06:03:17 [scrapy] INFO: Crawled 3996 pages (at 0 pages/min), scraped 3814 items (at 6 items/min)
2015-11-04 06:04:06 [scrapy] INFO: Crawled 4014 pages (at 18 pages/min), scraped 3827 items (at 13 items/min)
2015-11-04 06:05:10 [scrapy] INFO: Crawled 4046 pages (at 32 pages/min), scraped 3853 items (at 26 items/min)
2015-11-04 06:06:28 [scrapy] INFO: Crawled 4079 pages (at 33 pages/min), scraped 3887 items (at 34 items/min)
2015-11-04 06:09:00 [scrapy] INFO: Crawled 4087 pages (at 8 pages/min), scraped 3901 items (at 14 items/min)
2015-11-04 06:09:59 [scrapy] INFO: Crawled 4087 pages (at 0 pages/min), scraped 3909 items (at 8 items/min)
2015-11-04 06:10:06 [scrapy] INFO: Crawled 4094 pages (at 7 pages/min), scraped 3910 items (at 1 items/min)
2015-11-04 06:11:10 [scrapy] INFO: Crawled 4121 pages (at 27 pages/min), scraped 3934 items (at 24 items/min)
2015-11-04 06:12:08 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2fcity_WUH.html&ac=873966249> (referer: http://www.ly.com/city_ticket_192.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 06:12:08 [scrapy] INFO: Crawled 4128 pages (at 7 pages/min), scraped 3942 items (at 8 items/min)
2015-11-04 06:13:03 [scrapy] INFO: Crawled 4161 pages (at 33 pages/min), scraped 3958 items (at 16 items/min)
2015-11-04 06:14:14 [scrapy] INFO: Crawled 4161 pages (at 0 pages/min), scraped 3974 items (at 16 items/min)
2015-11-04 06:15:47 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_TAO-OSA.html&ac=873966249> (referer: http://www.ly.com/city_ticket_292.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 06:15:48 [scrapy] INFO: Crawled 4204 pages (at 43 pages/min), scraped 4004 items (at 30 items/min)
2015-11-04 06:17:02 [scrapy] INFO: Crawled 4204 pages (at 0 pages/min), scraped 4017 items (at 13 items/min)
2015-11-04 06:18:11 [scrapy] INFO: Crawled 4235 pages (at 31 pages/min), scraped 4037 items (at 20 items/min)
2015-11-04 06:19:20 [scrapy] INFO: Crawled 4257 pages (at 22 pages/min), scraped 4063 items (at 26 items/min)
2015-11-04 06:21:35 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2froutes_URC-SIN.html&ac=873966249> (referer: http://www.ly.com/city_ticket_364.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 06:21:35 [scrapy] INFO: Crawled 4257 pages (at 0 pages/min), scraped 4074 items (at 11 items/min)
2015-11-04 06:22:22 [scrapy] INFO: Crawled 4300 pages (at 43 pages/min), scraped 4091 items (at 17 items/min)
2015-11-04 06:23:10 [scrapy] INFO: Crawled 4302 pages (at 2 pages/min), scraped 4115 items (at 24 items/min)
2015-11-04 06:24:58 [scrapy] INFO: Crawled 4360 pages (at 58 pages/min), scraped 4160 items (at 45 items/min)
2015-11-04 06:25:27 [scrapy] INFO: Crawled 4368 pages (at 8 pages/min), scraped 4175 items (at 15 items/min)
2015-11-04 06:26:04 [scrapy] INFO: Crawled 4379 pages (at 11 pages/min), scraped 4188 items (at 13 items/min)
2015-11-04 06:27:22 [scrapy] INFO: Crawled 4399 pages (at 20 pages/min), scraped 4206 items (at 18 items/min)
2015-11-04 06:28:16 [scrapy] INFO: Crawled 4400 pages (at 1 pages/min), scraped 4214 items (at 8 items/min)
2015-11-04 06:29:04 [scrapy] INFO: Crawled 4414 pages (at 14 pages/min), scraped 4221 items (at 7 items/min)
2015-11-04 06:30:06 [scrapy] INFO: Crawled 4422 pages (at 8 pages/min), scraped 4232 items (at 11 items/min)
2015-11-04 06:31:03 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/youlun/AjaxCall.aspx?cid=2847&lineid=71067&ptid=177&type=getCruiseVisaPageHtml> (referer: http://www.ly.com/youlun/tours-71067.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:31:31 [scrapy] INFO: Crawled 4443 pages (at 21 pages/min), scraped 4254 items (at 22 items/min)
2015-11-04 06:31:41 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/youlun/AjaxCall.aspx?cid=2847&lineid=72647&ptid=177&type=getCruiseVisaPageHtml> (referer: http://www.ly.com/youlun/tours-72647.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:32:05 [scrapy] INFO: Crawled 4465 pages (at 22 pages/min), scraped 4265 items (at 11 items/min)
2015-11-04 06:32:07 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/youlun/AjaxCall.aspx?cid=2932&lineid=86200&ptid=177&type=getCruiseVisaPageHtml> (referer: http://www.ly.com/youlun/tours-86200.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:32:08 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/youlun/AjaxCall.aspx?cid=2847&lineid=86200&ptid=177&type=getCruiseVisaPageHtml> (referer: http://www.ly.com/youlun/tours-86200.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:32:19 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/youlun/AjaxCall.aspx?cid=2847&lineid=72682&ptid=170&type=getCruiseVisaPageHtml> (referer: http://www.ly.com/youlun/tours-72682.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:32:21 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/youlun/AjaxCall.aspx?cid=2847&lineid=72682&ptid=177&type=getCruiseVisaPageHtml> (referer: http://www.ly.com/youlun/tours-72682.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:34:54 [scrapy] INFO: Crawled 4493 pages (at 28 pages/min), scraped 4291 items (at 26 items/min)
2015-11-04 06:35:59 [scrapy] INFO: Crawled 4493 pages (at 0 pages/min), scraped 4301 items (at 10 items/min)
2015-11-04 06:36:02 [scrapy] ERROR: Spider error processing <GET http://www.ly.com/youlun/AjaxCall.aspx?cid=2847&lineid=86203&ptid=177&type=getCruiseVisaPageHtml> (referer: http://www.ly.com/youlun/tours-86203.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:36:02 [scrapy] INFO: Crawled 4508 pages (at 15 pages/min), scraped 4304 items (at 3 items/min)
2015-11-04 06:37:02 [scrapy] INFO: Crawled 4531 pages (at 23 pages/min), scraped 4330 items (at 26 items/min)
2015-11-04 06:38:07 [scrapy] INFO: Crawled 4569 pages (at 38 pages/min), scraped 4363 items (at 33 items/min)
2015-11-04 06:39:34 [scrapy] INFO: Crawled 4591 pages (at 22 pages/min), scraped 4389 items (at 26 items/min)
2015-11-04 06:40:48 [scrapy] INFO: Crawled 4591 pages (at 0 pages/min), scraped 4398 items (at 9 items/min)
2015-11-04 06:41:07 [scrapy] INFO: Crawled 4614 pages (at 23 pages/min), scraped 4407 items (at 9 items/min)
2015-11-04 06:42:26 [scrapy] INFO: Crawled 4643 pages (at 29 pages/min), scraped 4439 items (at 32 items/min)
2015-11-04 06:43:07 [scrapy] INFO: Crawled 4657 pages (at 14 pages/min), scraped 4456 items (at 17 items/min)
2015-11-04 06:44:44 [scrapy] INFO: Crawled 4683 pages (at 26 pages/min), scraped 4482 items (at 26 items/min)
2015-11-04 06:45:14 [scrapy] INFO: Crawled 4688 pages (at 5 pages/min), scraped 4490 items (at 8 items/min)
2015-11-04 06:46:04 [scrapy] INFO: Crawled 4707 pages (at 19 pages/min), scraped 4511 items (at 21 items/min)
2015-11-04 06:47:12 [scrapy] INFO: Crawled 4748 pages (at 41 pages/min), scraped 4541 items (at 30 items/min)
2015-11-04 06:48:19 [scrapy] INFO: Crawled 4761 pages (at 13 pages/min), scraped 4561 items (at 20 items/min)
2015-11-04 06:49:11 [scrapy] INFO: Crawled 4770 pages (at 9 pages/min), scraped 4573 items (at 12 items/min)
2015-11-04 06:50:33 [scrapy] INFO: Crawled 4793 pages (at 23 pages/min), scraped 4591 items (at 18 items/min)
2015-11-04 06:51:07 [scrapy] INFO: Crawled 4794 pages (at 1 pages/min), scraped 4600 items (at 9 items/min)
2015-11-04 06:52:05 [scrapy] INFO: Crawled 4842 pages (at 48 pages/min), scraped 4638 items (at 38 items/min)
2015-11-04 06:53:11 [scrapy] INFO: Crawled 4876 pages (at 34 pages/min), scraped 4671 items (at 33 items/min)
2015-11-04 06:54:10 [scrapy] INFO: Crawled 4905 pages (at 29 pages/min), scraped 4698 items (at 27 items/min)
2015-11-04 06:55:05 [scrapy] INFO: Crawled 4943 pages (at 38 pages/min), scraped 4728 items (at 30 items/min)
2015-11-04 06:56:13 [scrapy] INFO: Crawled 4992 pages (at 49 pages/min), scraped 4775 items (at 47 items/min)
2015-11-04 06:57:09 [scrapy] INFO: Crawled 5032 pages (at 40 pages/min), scraped 4816 items (at 41 items/min)
2015-11-04 06:58:06 [scrapy] INFO: Crawled 5052 pages (at 20 pages/min), scraped 4848 items (at 32 items/min)
2015-11-04 06:59:10 [scrapy] INFO: Crawled 5060 pages (at 8 pages/min), scraped 4863 items (at 15 items/min)
2015-11-04 07:00:04 [scrapy] INFO: Crawled 5096 pages (at 36 pages/min), scraped 4891 items (at 28 items/min)
2015-11-04 07:01:32 [scrapy] INFO: Crawled 5132 pages (at 36 pages/min), scraped 4928 items (at 37 items/min)
2015-11-04 07:02:04 [scrapy] INFO: Crawled 5142 pages (at 10 pages/min), scraped 4939 items (at 11 items/min)
2015-11-04 07:03:03 [scrapy] INFO: Crawled 5163 pages (at 21 pages/min), scraped 4964 items (at 25 items/min)
2015-11-04 07:04:08 [scrapy] INFO: Crawled 5200 pages (at 37 pages/min), scraped 4996 items (at 32 items/min)
2015-11-04 07:05:21 [scrapy] INFO: Crawled 5226 pages (at 26 pages/min), scraped 5023 items (at 27 items/min)
2015-11-04 07:06:03 [scrapy] INFO: Crawled 5250 pages (at 24 pages/min), scraped 5047 items (at 24 items/min)
2015-11-04 07:07:12 [scrapy] INFO: Crawled 5282 pages (at 32 pages/min), scraped 5079 items (at 32 items/min)
2015-11-04 07:08:14 [scrapy] INFO: Crawled 5314 pages (at 32 pages/min), scraped 5111 items (at 32 items/min)
2015-11-04 07:09:03 [scrapy] INFO: Crawled 5338 pages (at 24 pages/min), scraped 5135 items (at 24 items/min)
2015-11-04 07:10:13 [scrapy] INFO: Crawled 5378 pages (at 40 pages/min), scraped 5175 items (at 40 items/min)
2015-11-04 07:11:14 [scrapy] INFO: Crawled 5418 pages (at 40 pages/min), scraped 5215 items (at 40 items/min)
2015-11-04 07:12:03 [scrapy] INFO: Crawled 5450 pages (at 32 pages/min), scraped 5243 items (at 28 items/min)
2015-11-04 07:14:35 [scrapy] INFO: Crawled 5487 pages (at 37 pages/min), scraped 5280 items (at 37 items/min)
2015-11-04 07:15:02 [scrapy] INFO: Crawled 5502 pages (at 15 pages/min), scraped 5293 items (at 13 items/min)
2015-11-04 07:16:14 [scrapy] INFO: Crawled 5533 pages (at 31 pages/min), scraped 5316 items (at 23 items/min)
2015-11-04 07:17:50 [scrapy] ERROR: Error downloading <GET http://techcrunch.com/?post_type=tc_video&p=1232884>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 07:17:51 [scrapy] INFO: Crawled 5533 pages (at 0 pages/min), scraped 5327 items (at 11 items/min)
2015-11-04 07:18:10 [scrapy] INFO: Crawled 5533 pages (at 0 pages/min), scraped 5338 items (at 11 items/min)
2015-11-04 07:19:34 [scrapy] INFO: Crawled 5579 pages (at 46 pages/min), scraped 5368 items (at 30 items/min)
2015-11-04 07:20:14 [scrapy] INFO: Crawled 5579 pages (at 0 pages/min), scraped 5384 items (at 16 items/min)
2015-11-04 07:21:25 [scrapy] INFO: Crawled 5620 pages (at 41 pages/min), scraped 5414 items (at 30 items/min)
2015-11-04 07:22:04 [scrapy] INFO: Crawled 5638 pages (at 18 pages/min), scraped 5426 items (at 12 items/min)
2015-11-04 07:23:17 [scrapy] INFO: Crawled 5651 pages (at 13 pages/min), scraped 5455 items (at 29 items/min)
2015-11-04 07:24:51 [scrapy] INFO: Crawled 5678 pages (at 27 pages/min), scraped 5471 items (at 16 items/min)
2015-11-04 07:26:49 [scrapy] INFO: Crawled 5682 pages (at 4 pages/min), scraped 5483 items (at 12 items/min)
2015-11-04 07:27:37 [scrapy] INFO: Crawled 5711 pages (at 29 pages/min), scraped 5490 items (at 7 items/min)
2015-11-04 07:28:16 [scrapy] INFO: Crawled 5711 pages (at 0 pages/min), scraped 5492 items (at 2 items/min)
2015-11-04 07:36:12 [scrapy] INFO: Crawled 5711 pages (at 0 pages/min), scraped 5516 items (at 24 items/min)
2015-11-04 07:37:22 [scrapy] INFO: Crawled 5760 pages (at 49 pages/min), scraped 5534 items (at 18 items/min)
2015-11-04 07:39:13 [scrapy] INFO: Crawled 5760 pages (at 0 pages/min), scraped 5550 items (at 16 items/min)
2015-11-04 07:41:12 [scrapy] INFO: Crawled 5760 pages (at 0 pages/min), scraped 5565 items (at 15 items/min)
2015-11-04 07:41:28 [scrapy] ERROR: Spider error processing <GET http://zby.ly.com/zizhuyou/ajax/linedetailajaxcall.aspx?action=getZZYResourceInfo&hid=0&htype=2&resid=141634> (referer: http://zby.ly.com/zizhuyou/hangzhou383/75729-xianlu/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 60: Unexpected end tag : h1
2015-11-04 07:42:26 [scrapy] INFO: Crawled 5794 pages (at 34 pages/min), scraped 5583 items (at 18 items/min)
2015-11-04 07:43:44 [scrapy] ERROR: Error downloading <GET http://zby.ly.com/zizhuyou/linesearchlist.aspx?days=2&ktype=9>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 07:43:45 [scrapy] INFO: Crawled 5794 pages (at 0 pages/min), scraped 5598 items (at 15 items/min)
2015-11-04 07:44:23 [scrapy] INFO: Crawled 5835 pages (at 41 pages/min), scraped 5612 items (at 14 items/min)
2015-11-04 07:49:07 [scrapy] INFO: Crawled 5835 pages (at 0 pages/min), scraped 5627 items (at 15 items/min)
2015-11-04 07:52:34 [scrapy] INFO: Crawled 5857 pages (at 22 pages/min), scraped 5653 items (at 26 items/min)
2015-11-04 07:53:51 [scrapy] INFO: Crawled 5857 pages (at 0 pages/min), scraped 5661 items (at 8 items/min)
2015-11-04 07:54:06 [scrapy] INFO: Crawled 5873 pages (at 16 pages/min), scraped 5665 items (at 4 items/min)
2015-11-04 07:55:11 [scrapy] INFO: Crawled 5899 pages (at 26 pages/min), scraped 5695 items (at 30 items/min)
2015-11-04 07:56:07 [scrapy] INFO: Crawled 5931 pages (at 32 pages/min), scraped 5727 items (at 32 items/min)
2015-11-04 07:57:13 [scrapy] INFO: Crawled 5971 pages (at 40 pages/min), scraped 5767 items (at 40 items/min)
2015-11-04 07:58:08 [scrapy] INFO: Crawled 6003 pages (at 32 pages/min), scraped 5799 items (at 32 items/min)
2015-11-04 07:59:06 [scrapy] INFO: Crawled 6035 pages (at 32 pages/min), scraped 5831 items (at 32 items/min)
2015-11-04 08:00:09 [scrapy] INFO: Crawled 6075 pages (at 40 pages/min), scraped 5871 items (at 40 items/min)
2015-11-04 08:01:03 [scrapy] INFO: Crawled 6107 pages (at 32 pages/min), scraped 5903 items (at 32 items/min)
2015-11-04 08:02:11 [scrapy] INFO: Crawled 6147 pages (at 40 pages/min), scraped 5943 items (at 40 items/min)
2015-11-04 08:03:12 [scrapy] INFO: Crawled 6155 pages (at 8 pages/min), scraped 5951 items (at 8 items/min)
2015-11-04 08:04:08 [scrapy] INFO: Crawled 6190 pages (at 35 pages/min), scraped 5982 items (at 31 items/min)
2015-11-04 08:05:11 [scrapy] INFO: Crawled 6222 pages (at 32 pages/min), scraped 6018 items (at 36 items/min)
2015-11-04 08:06:36 [scrapy] INFO: Crawled 6230 pages (at 8 pages/min), scraped 6026 items (at 8 items/min)
2015-11-04 08:07:15 [scrapy] INFO: Crawled 6251 pages (at 21 pages/min), scraped 6043 items (at 17 items/min)
2015-11-04 08:08:05 [scrapy] INFO: Crawled 6275 pages (at 24 pages/min), scraped 6071 items (at 28 items/min)
2015-11-04 08:09:14 [scrapy] INFO: Crawled 6314 pages (at 39 pages/min), scraped 6110 items (at 39 items/min)
2015-11-04 08:10:02 [scrapy] INFO: Crawled 6346 pages (at 32 pages/min), scraped 6142 items (at 32 items/min)
2015-11-04 08:11:15 [scrapy] INFO: Crawled 6386 pages (at 40 pages/min), scraped 6182 items (at 40 items/min)
2015-11-04 08:12:11 [scrapy] INFO: Crawled 6418 pages (at 32 pages/min), scraped 6214 items (at 32 items/min)
2015-11-04 08:13:07 [scrapy] INFO: Crawled 6449 pages (at 31 pages/min), scraped 6245 items (at 31 items/min)
2015-11-04 08:15:11 [scrapy] INFO: Crawled 6466 pages (at 17 pages/min), scraped 6269 items (at 24 items/min)
2015-11-04 08:16:06 [scrapy] INFO: Crawled 6475 pages (at 9 pages/min), scraped 6271 items (at 2 items/min)
2015-11-04 08:17:38 [scrapy] INFO: Crawled 6498 pages (at 23 pages/min), scraped 6294 items (at 23 items/min)
2015-11-04 08:18:09 [scrapy] INFO: Crawled 6498 pages (at 0 pages/min), scraped 6302 items (at 8 items/min)
2015-11-04 08:19:14 [scrapy] INFO: Crawled 6528 pages (at 30 pages/min), scraped 6324 items (at 22 items/min)
2015-11-04 08:20:06 [scrapy] INFO: Crawled 6536 pages (at 8 pages/min), scraped 6334 items (at 10 items/min)
2015-11-04 08:21:13 [scrapy] INFO: Crawled 6550 pages (at 14 pages/min), scraped 6352 items (at 18 items/min)
2015-11-04 08:22:09 [scrapy] INFO: Crawled 6585 pages (at 35 pages/min), scraped 6381 items (at 29 items/min)
2015-11-04 08:23:18 [scrapy] INFO: Crawled 6626 pages (at 41 pages/min), scraped 6415 items (at 34 items/min)
2015-11-04 08:24:07 [scrapy] INFO: Crawled 6634 pages (at 8 pages/min), scraped 6438 items (at 23 items/min)
2015-11-04 08:25:10 [scrapy] INFO: Crawled 6682 pages (at 48 pages/min), scraped 6471 items (at 33 items/min)
2015-11-04 08:26:16 [scrapy] INFO: Crawled 6690 pages (at 8 pages/min), scraped 6494 items (at 23 items/min)
2015-11-04 08:27:06 [scrapy] INFO: Crawled 6722 pages (at 32 pages/min), scraped 6518 items (at 24 items/min)
2015-11-04 08:28:13 [scrapy] INFO: Crawled 6757 pages (at 35 pages/min), scraped 6546 items (at 28 items/min)
2015-11-04 08:29:08 [scrapy] INFO: Crawled 6766 pages (at 9 pages/min), scraped 6563 items (at 17 items/min)
2015-11-04 08:30:09 [scrapy] INFO: Crawled 6798 pages (at 32 pages/min), scraped 6594 items (at 31 items/min)
2015-11-04 08:31:11 [scrapy] INFO: Crawled 6829 pages (at 31 pages/min), scraped 6625 items (at 31 items/min)
2015-11-04 08:32:08 [scrapy] INFO: Crawled 6853 pages (at 24 pages/min), scraped 6649 items (at 24 items/min)
2015-11-04 08:33:03 [scrapy] INFO: Crawled 6885 pages (at 32 pages/min), scraped 6681 items (at 32 items/min)
2015-11-04 08:34:15 [scrapy] INFO: Crawled 6925 pages (at 40 pages/min), scraped 6721 items (at 40 items/min)
2015-11-04 08:35:13 [scrapy] INFO: Crawled 6957 pages (at 32 pages/min), scraped 6753 items (at 32 items/min)
2015-11-04 08:36:47 [scrapy] INFO: Crawled 6999 pages (at 42 pages/min), scraped 6789 items (at 36 items/min)
2015-11-04 08:37:19 [scrapy] INFO: Crawled 6999 pages (at 0 pages/min), scraped 6803 items (at 14 items/min)
2015-11-04 08:38:10 [scrapy] INFO: Crawled 7030 pages (at 31 pages/min), scraped 6826 items (at 23 items/min)
2015-11-04 08:39:08 [scrapy] INFO: Crawled 7062 pages (at 32 pages/min), scraped 6858 items (at 32 items/min)
2015-11-04 08:40:08 [scrapy] INFO: Crawled 7094 pages (at 32 pages/min), scraped 6890 items (at 32 items/min)
2015-11-04 08:41:05 [scrapy] INFO: Crawled 7126 pages (at 32 pages/min), scraped 6922 items (at 32 items/min)
2015-11-04 08:42:10 [scrapy] INFO: Crawled 7166 pages (at 40 pages/min), scraped 6962 items (at 40 items/min)
2015-11-04 08:43:09 [scrapy] INFO: Crawled 7198 pages (at 32 pages/min), scraped 6994 items (at 32 items/min)
2015-11-04 08:44:13 [scrapy] INFO: Crawled 7237 pages (at 39 pages/min), scraped 7033 items (at 39 items/min)
2015-11-04 08:45:02 [scrapy] INFO: Crawled 7269 pages (at 32 pages/min), scraped 7065 items (at 32 items/min)
2015-11-04 08:46:10 [scrapy] INFO: Crawled 7309 pages (at 40 pages/min), scraped 7105 items (at 40 items/min)
2015-11-04 08:47:03 [scrapy] INFO: Crawled 7341 pages (at 32 pages/min), scraped 7137 items (at 32 items/min)
2015-11-04 08:48:07 [scrapy] INFO: Crawled 7371 pages (at 30 pages/min), scraped 7167 items (at 30 items/min)
2015-11-04 08:49:09 [scrapy] INFO: Crawled 7403 pages (at 32 pages/min), scraped 7199 items (at 32 items/min)
2015-11-04 08:50:16 [scrapy] INFO: Crawled 7438 pages (at 35 pages/min), scraped 7236 items (at 37 items/min)
2015-11-04 08:51:20 [scrapy] INFO: Crawled 7470 pages (at 32 pages/min), scraped 7266 items (at 30 items/min)
2015-11-04 08:52:05 [scrapy] INFO: Crawled 7494 pages (at 24 pages/min), scraped 7290 items (at 24 items/min)
2015-11-04 08:53:51 [scrapy] INFO: Crawled 7503 pages (at 9 pages/min), scraped 7306 items (at 16 items/min)
2015-11-04 08:54:26 [scrapy] INFO: Crawled 7542 pages (at 39 pages/min), scraped 7323 items (at 17 items/min)
2015-11-04 08:55:37 [scrapy] ERROR: Spider error processing <GET http://qianggou.ly.com/zizhuyou/detail_80705_20151104100000.html> (referer: http://zby.ly.com/zizhuyou/suzhou226/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 08:56:12 [scrapy] INFO: Crawled 7542 pages (at 0 pages/min), scraped 7345 items (at 22 items/min)
2015-11-04 08:57:24 [scrapy] INFO: Crawled 7584 pages (at 42 pages/min), scraped 7379 items (at 34 items/min)
2015-11-04 08:58:09 [scrapy] INFO: Crawled 7602 pages (at 18 pages/min), scraped 7396 items (at 17 items/min)
2015-11-04 08:59:02 [scrapy] INFO: Crawled 7624 pages (at 22 pages/min), scraped 7421 items (at 25 items/min)
2015-11-04 09:00:05 [scrapy] INFO: Crawled 7649 pages (at 25 pages/min), scraped 7449 items (at 28 items/min)
2015-11-04 09:01:37 [scrapy] INFO: Crawled 7690 pages (at 41 pages/min), scraped 7487 items (at 38 items/min)
2015-11-04 09:02:33 [scrapy] INFO: Crawled 7724 pages (at 34 pages/min), scraped 7511 items (at 24 items/min)
2015-11-04 09:03:05 [scrapy] INFO: Crawled 7725 pages (at 1 pages/min), scraped 7527 items (at 16 items/min)
2015-11-04 09:04:38 [scrapy] INFO: Crawled 7778 pages (at 53 pages/min), scraped 7566 items (at 39 items/min)
2015-11-04 09:05:15 [scrapy] INFO: Crawled 7778 pages (at 0 pages/min), scraped 7581 items (at 15 items/min)
2015-11-04 09:06:10 [scrapy] INFO: Crawled 7818 pages (at 40 pages/min), scraped 7606 items (at 25 items/min)
2015-11-04 09:07:11 [scrapy] INFO: Crawled 7859 pages (at 41 pages/min), scraped 7637 items (at 31 items/min)
2015-11-04 09:08:14 [scrapy] INFO: Crawled 7870 pages (at 11 pages/min), scraped 7669 items (at 32 items/min)
2015-11-04 09:09:18 [scrapy] INFO: Crawled 7906 pages (at 36 pages/min), scraped 7700 items (at 31 items/min)
2015-11-04 09:10:03 [scrapy] INFO: Crawled 7927 pages (at 21 pages/min), scraped 7722 items (at 22 items/min)
2015-11-04 09:11:02 [scrapy] INFO: Crawled 7959 pages (at 32 pages/min), scraped 7754 items (at 32 items/min)
2015-11-04 09:12:05 [scrapy] INFO: Crawled 7988 pages (at 29 pages/min), scraped 7785 items (at 31 items/min)
2015-11-04 09:13:08 [scrapy] INFO: Crawled 8016 pages (at 28 pages/min), scraped 7811 items (at 26 items/min)
2015-11-04 09:14:07 [scrapy] INFO: Crawled 8048 pages (at 32 pages/min), scraped 7843 items (at 32 items/min)
2015-11-04 09:15:05 [scrapy] INFO: Crawled 8080 pages (at 32 pages/min), scraped 7875 items (at 32 items/min)
2015-11-04 09:16:11 [scrapy] INFO: Crawled 8112 pages (at 32 pages/min), scraped 7907 items (at 32 items/min)
2015-11-04 09:17:09 [scrapy] INFO: Crawled 8144 pages (at 32 pages/min), scraped 7939 items (at 32 items/min)
2015-11-04 09:18:16 [scrapy] INFO: Crawled 8176 pages (at 32 pages/min), scraped 7971 items (at 32 items/min)
2015-11-04 09:19:08 [scrapy] INFO: Crawled 8200 pages (at 24 pages/min), scraped 7995 items (at 24 items/min)
2015-11-04 09:20:07 [scrapy] INFO: Crawled 8230 pages (at 30 pages/min), scraped 8025 items (at 30 items/min)
2015-11-04 09:21:02 [scrapy] INFO: Crawled 8256 pages (at 26 pages/min), scraped 8053 items (at 28 items/min)
2015-11-04 09:22:13 [scrapy] INFO: Crawled 8286 pages (at 30 pages/min), scraped 8081 items (at 28 items/min)
2015-11-04 09:23:08 [scrapy] INFO: Crawled 8308 pages (at 22 pages/min), scraped 8103 items (at 22 items/min)
2015-11-04 09:24:09 [scrapy] INFO: Crawled 8337 pages (at 29 pages/min), scraped 8135 items (at 32 items/min)
2015-11-04 09:25:08 [scrapy] INFO: Crawled 8368 pages (at 31 pages/min), scraped 8163 items (at 28 items/min)
2015-11-04 09:26:11 [scrapy] INFO: Crawled 8407 pages (at 39 pages/min), scraped 8195 items (at 32 items/min)
2015-11-04 09:27:29 [scrapy] INFO: Crawled 8423 pages (at 16 pages/min), scraped 8226 items (at 31 items/min)
2015-11-04 09:28:24 [scrapy] INFO: Crawled 8460 pages (at 37 pages/min), scraped 8247 items (at 21 items/min)
2015-11-04 09:29:06 [scrapy] INFO: Crawled 8460 pages (at 0 pages/min), scraped 8263 items (at 16 items/min)
2015-11-04 09:31:18 [scrapy] INFO: Crawled 8498 pages (at 38 pages/min), scraped 8296 items (at 33 items/min)
2015-11-04 09:32:03 [scrapy] INFO: Crawled 8512 pages (at 14 pages/min), scraped 8302 items (at 6 items/min)
2015-11-04 09:33:38 [scrapy] INFO: Crawled 8518 pages (at 6 pages/min), scraped 8315 items (at 13 items/min)
2015-11-04 09:34:26 [scrapy] INFO: Crawled 8518 pages (at 0 pages/min), scraped 8321 items (at 6 items/min)
2015-11-04 09:35:52 [scrapy] INFO: Crawled 8534 pages (at 16 pages/min), scraped 8330 items (at 9 items/min)
2015-11-04 09:37:21 [scrapy] INFO: Crawled 8534 pages (at 0 pages/min), scraped 8337 items (at 7 items/min)
2015-11-04 09:38:23 [scrapy] INFO: Crawled 8553 pages (at 19 pages/min), scraped 8356 items (at 19 items/min)
2015-11-04 09:39:49 [scrapy] INFO: Crawled 8570 pages (at 17 pages/min), scraped 8365 items (at 9 items/min)
2015-11-04 09:40:10 [scrapy] INFO: Crawled 8570 pages (at 0 pages/min), scraped 8373 items (at 8 items/min)
2015-11-04 09:41:04 [scrapy] INFO: Crawled 8596 pages (at 26 pages/min), scraped 8391 items (at 18 items/min)
2015-11-04 09:43:12 [scrapy] INFO: Crawled 8621 pages (at 25 pages/min), scraped 8416 items (at 25 items/min)
2015-11-04 09:44:05 [scrapy] INFO: Crawled 8621 pages (at 0 pages/min), scraped 8424 items (at 8 items/min)
2015-11-04 09:45:07 [scrapy] INFO: Crawled 8632 pages (at 11 pages/min), scraped 8435 items (at 11 items/min)
2015-11-04 09:46:18 [scrapy] INFO: Crawled 8652 pages (at 20 pages/min), scraped 8449 items (at 14 items/min)
2015-11-04 09:47:15 [scrapy] INFO: Crawled 8661 pages (at 9 pages/min), scraped 8456 items (at 7 items/min)
2015-11-04 09:48:36 [scrapy] INFO: Crawled 8665 pages (at 4 pages/min), scraped 8464 items (at 8 items/min)
2015-11-04 09:49:54 [scrapy] INFO: Crawled 8673 pages (at 8 pages/min), scraped 8472 items (at 8 items/min)
2015-11-04 09:50:06 [scrapy] INFO: Crawled 8673 pages (at 0 pages/min), scraped 8476 items (at 4 items/min)
2015-11-04 09:51:42 [scrapy] INFO: Crawled 8684 pages (at 11 pages/min), scraped 8487 items (at 11 items/min)
2015-11-04 09:52:46 [scrapy] INFO: Crawled 8698 pages (at 14 pages/min), scraped 8495 items (at 8 items/min)
2015-11-04 09:53:32 [scrapy] INFO: Crawled 8698 pages (at 0 pages/min), scraped 8501 items (at 6 items/min)
2015-11-04 09:54:22 [scrapy] INFO: Crawled 8723 pages (at 25 pages/min), scraped 8518 items (at 17 items/min)
2015-11-04 09:55:18 [scrapy] INFO: Crawled 8739 pages (at 16 pages/min), scraped 8534 items (at 16 items/min)
2015-11-04 09:56:12 [scrapy] INFO: Crawled 8755 pages (at 16 pages/min), scraped 8550 items (at 16 items/min)
2015-11-04 09:57:07 [scrapy] INFO: Crawled 8771 pages (at 16 pages/min), scraped 8566 items (at 16 items/min)
2015-11-04 09:58:14 [scrapy] INFO: Crawled 8779 pages (at 8 pages/min), scraped 8582 items (at 16 items/min)
2015-11-04 09:59:18 [scrapy] INFO: Crawled 8803 pages (at 24 pages/min), scraped 8599 items (at 17 items/min)
2015-11-04 10:00:13 [scrapy] INFO: Crawled 8811 pages (at 8 pages/min), scraped 8610 items (at 11 items/min)
2015-11-04 10:01:05 [scrapy] INFO: Crawled 8829 pages (at 18 pages/min), scraped 8624 items (at 14 items/min)
2015-11-04 10:02:11 [scrapy] INFO: Crawled 8837 pages (at 8 pages/min), scraped 8640 items (at 16 items/min)
2015-11-04 10:03:23 [scrapy] INFO: Crawled 8855 pages (at 18 pages/min), scraped 8658 items (at 18 items/min)
2015-11-04 10:04:30 [scrapy] INFO: Crawled 8880 pages (at 25 pages/min), scraped 8675 items (at 17 items/min)
2015-11-04 10:05:10 [scrapy] INFO: Crawled 8880 pages (at 0 pages/min), scraped 8683 items (at 8 items/min)
2015-11-04 10:06:11 [scrapy] INFO: Crawled 8901 pages (at 21 pages/min), scraped 8696 items (at 13 items/min)
2015-11-04 10:07:19 [scrapy] INFO: Crawled 8943 pages (at 42 pages/min), scraped 8716 items (at 20 items/min)
2015-11-04 10:08:43 [scrapy] INFO: Crawled 8951 pages (at 8 pages/min), scraped 8746 items (at 30 items/min)
2015-11-04 10:09:04 [scrapy] INFO: Crawled 8970 pages (at 19 pages/min), scraped 8755 items (at 9 items/min)
2015-11-04 10:10:23 [scrapy] INFO: Crawled 8986 pages (at 16 pages/min), scraped 8779 items (at 24 items/min)
2015-11-04 10:11:25 [scrapy] INFO: Crawled 8989 pages (at 3 pages/min), scraped 8789 items (at 10 items/min)
2015-11-04 10:12:20 [scrapy] INFO: Crawled 9020 pages (at 31 pages/min), scraped 8807 items (at 18 items/min)
2015-11-04 10:13:11 [scrapy] INFO: Crawled 9035 pages (at 15 pages/min), scraped 8823 items (at 16 items/min)
2015-11-04 10:14:03 [scrapy] INFO: Crawled 9042 pages (at 7 pages/min), scraped 8839 items (at 16 items/min)
2015-11-04 10:15:12 [scrapy] INFO: Crawled 9082 pages (at 40 pages/min), scraped 8866 items (at 27 items/min)
2015-11-04 10:16:03 [scrapy] INFO: Crawled 9100 pages (at 18 pages/min), scraped 8887 items (at 21 items/min)
2015-11-04 10:17:05 [scrapy] INFO: Crawled 9123 pages (at 23 pages/min), scraped 8910 items (at 23 items/min)
2015-11-04 10:18:04 [scrapy] INFO: Crawled 9138 pages (at 15 pages/min), scraped 8927 items (at 17 items/min)
2015-11-04 10:20:41 [scrapy] INFO: Crawled 9150 pages (at 12 pages/min), scraped 8941 items (at 14 items/min)
2015-11-04 10:23:01 [scrapy] INFO: Crawled 9151 pages (at 1 pages/min), scraped 8953 items (at 12 items/min)
2015-11-04 10:23:03 [scrapy] INFO: Crawled 9151 pages (at 0 pages/min), scraped 8954 items (at 1 items/min)
2015-11-04 10:25:12 [scrapy] INFO: Crawled 9205 pages (at 54 pages/min), scraped 8994 items (at 40 items/min)
2015-11-04 10:26:07 [scrapy] INFO: Crawled 9225 pages (at 20 pages/min), scraped 9020 items (at 26 items/min)
2015-11-04 10:27:09 [scrapy] INFO: Crawled 9237 pages (at 12 pages/min), scraped 9032 items (at 12 items/min)
2015-11-04 10:28:32 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-youji-fiji-2812/> (referer: http://go.ly.com/youji/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 10:28:32 [scrapy] INFO: Crawled 9273 pages (at 36 pages/min), scraped 9060 items (at 28 items/min)
2015-11-04 10:30:08 [scrapy] INFO: Crawled 9273 pages (at 0 pages/min), scraped 9068 items (at 8 items/min)
2015-11-04 10:31:31 [scrapy] INFO: Crawled 9293 pages (at 20 pages/min), scraped 9080 items (at 12 items/min)
2015-11-04 10:32:17 [scrapy] INFO: Crawled 9297 pages (at 4 pages/min), scraped 9092 items (at 12 items/min)
2015-11-04 10:32:55 [scrapy] ERROR: Error downloading <GET https://passport.ly.com/?pageurl=http%3a%2f%2fgo.ly.com%2fuser%2f>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:32:55 [scrapy] ERROR: Error downloading <GET https://passport.ly.com/?pageurl=http%3a%2f%2fgo.ly.com%2fyouji%2fedit%2f>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:33:18 [scrapy] INFO: Crawled 9335 pages (at 38 pages/min), scraped 9109 items (at 17 items/min)
2015-11-04 10:34:47 [scrapy] INFO: Crawled 9346 pages (at 11 pages/min), scraped 9134 items (at 25 items/min)
2015-11-04 10:35:11 [scrapy] INFO: Crawled 9346 pages (at 0 pages/min), scraped 9145 items (at 11 items/min)
2015-11-04 10:36:26 [scrapy] ERROR: Spider error processing <GET http://guard.ly.com/authcode.aspx?returnUrl=http%3a%2f%2fwww.ly.com%2fiflight%2frequireordernew.aspx%3ftype%3dtuan&ac=873966249> (referer: http://www.ly.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 10:36:58 [scrapy] INFO: Crawled 9371 pages (at 25 pages/min), scraped 9163 items (at 18 items/min)
2015-11-04 10:37:29 [scrapy] INFO: Crawled 9371 pages (at 0 pages/min), scraped 9169 items (at 6 items/min)
2015-11-04 10:38:20 [scrapy] INFO: Crawled 9377 pages (at 6 pages/min), scraped 9171 items (at 2 items/min)
2015-11-04 10:39:22 [scrapy] INFO: Crawled 9384 pages (at 7 pages/min), scraped 9174 items (at 3 items/min)
2015-11-04 10:41:56 [scrapy] INFO: Crawled 9384 pages (at 0 pages/min), scraped 9182 items (at 8 items/min)
2015-11-04 10:42:04 [scrapy] INFO: Crawled 9392 pages (at 8 pages/min), scraped 9184 items (at 2 items/min)
2015-11-04 10:43:53 [scrapy] INFO: Crawled 9404 pages (at 12 pages/min), scraped 9194 items (at 10 items/min)
2015-11-04 10:44:05 [scrapy] INFO: Crawled 9404 pages (at 0 pages/min), scraped 9199 items (at 5 items/min)
2015-11-04 10:45:20 [scrapy] INFO: Crawled 9408 pages (at 4 pages/min), scraped 9202 items (at 3 items/min)
2015-11-04 10:46:04 [scrapy] INFO: Crawled 9431 pages (at 23 pages/min), scraped 9217 items (at 15 items/min)
2015-11-04 10:47:03 [scrapy] INFO: Crawled 9438 pages (at 7 pages/min), scraped 9236 items (at 19 items/min)
2015-11-04 10:48:03 [scrapy] INFO: Crawled 9477 pages (at 39 pages/min), scraped 9269 items (at 33 items/min)
2015-11-04 10:49:17 [scrapy] INFO: Crawled 9513 pages (at 36 pages/min), scraped 9304 items (at 35 items/min)
2015-11-04 10:50:29 [scrapy] INFO: Crawled 9544 pages (at 31 pages/min), scraped 9334 items (at 30 items/min)
2015-11-04 10:51:10 [scrapy] INFO: Crawled 9560 pages (at 16 pages/min), scraped 9350 items (at 16 items/min)
2015-11-04 10:52:04 [scrapy] INFO: Crawled 9592 pages (at 32 pages/min), scraped 9382 items (at 32 items/min)
2015-11-04 10:53:10 [scrapy] INFO: Crawled 9640 pages (at 48 pages/min), scraped 9430 items (at 48 items/min)
2015-11-04 10:54:14 [scrapy] INFO: Crawled 9670 pages (at 30 pages/min), scraped 9461 items (at 31 items/min)
2015-11-04 10:55:11 [scrapy] INFO: Crawled 9702 pages (at 32 pages/min), scraped 9492 items (at 31 items/min)
2015-11-04 10:56:15 [scrapy] INFO: Crawled 9734 pages (at 32 pages/min), scraped 9524 items (at 32 items/min)
2015-11-04 10:57:07 [scrapy] INFO: Crawled 9759 pages (at 25 pages/min), scraped 9551 items (at 27 items/min)
2015-11-04 10:58:13 [scrapy] INFO: Crawled 9791 pages (at 32 pages/min), scraped 9589 items (at 38 items/min)
2015-11-04 10:59:13 [scrapy] INFO: Crawled 9831 pages (at 40 pages/min), scraped 9621 items (at 32 items/min)
2015-11-04 11:00:12 [scrapy] INFO: Crawled 9860 pages (at 29 pages/min), scraped 9650 items (at 29 items/min)
2015-11-04 11:01:12 [scrapy] INFO: Crawled 9893 pages (at 33 pages/min), scraped 9682 items (at 32 items/min)
2015-11-04 11:02:02 [scrapy] INFO: Crawled 9917 pages (at 24 pages/min), scraped 9707 items (at 25 items/min)
2015-11-04 11:03:13 [scrapy] INFO: Crawled 9941 pages (at 24 pages/min), scraped 9731 items (at 24 items/min)
2015-11-04 11:04:03 [scrapy] INFO: Crawled 9971 pages (at 30 pages/min), scraped 9760 items (at 29 items/min)
2015-11-04 11:05:12 [scrapy] INFO: Crawled 10003 pages (at 32 pages/min), scraped 9791 items (at 31 items/min)
2015-11-04 11:06:12 [scrapy] INFO: Crawled 10019 pages (at 16 pages/min), scraped 9809 items (at 18 items/min)
2015-11-04 11:07:14 [scrapy] INFO: Crawled 10037 pages (at 18 pages/min), scraped 9830 items (at 21 items/min)
2015-11-04 11:09:38 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/youji/2179589.html> (referer: http://www.ly.com/youlun/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:10:14 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/youji/2178587.html> (referer: http://www.ly.com/youlun/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:10:14 [scrapy] INFO: Crawled 10059 pages (at 22 pages/min), scraped 9848 items (at 18 items/min)
2015-11-04 11:11:05 [scrapy] INFO: Crawled 10092 pages (at 33 pages/min), scraped 9872 items (at 24 items/min)
2015-11-04 11:12:02 [scrapy] INFO: Crawled 10115 pages (at 23 pages/min), scraped 9905 items (at 33 items/min)
2015-11-04 11:13:13 [scrapy] INFO: Crawled 10131 pages (at 16 pages/min), scraped 9919 items (at 14 items/min)
2015-11-04 11:14:06 [scrapy] INFO: Crawled 10147 pages (at 16 pages/min), scraped 9935 items (at 16 items/min)
2015-11-04 11:15:13 [scrapy] INFO: Crawled 10183 pages (at 36 pages/min), scraped 9972 items (at 37 items/min)
2015-11-04 11:17:38 [scrapy] INFO: Crawled 10213 pages (at 30 pages/min), scraped 10001 items (at 29 items/min)
2015-11-04 11:18:21 [scrapy] INFO: Crawled 10221 pages (at 8 pages/min), scraped 10009 items (at 8 items/min)
2015-11-04 11:19:18 [scrapy] INFO: Crawled 10240 pages (at 19 pages/min), scraped 10035 items (at 26 items/min)
2015-11-04 11:20:42 [scrapy] INFO: Crawled 10269 pages (at 29 pages/min), scraped 10059 items (at 24 items/min)
2015-11-04 11:21:27 [scrapy] INFO: Crawled 10276 pages (at 7 pages/min), scraped 10065 items (at 6 items/min)
2015-11-04 11:22:08 [scrapy] INFO: Crawled 10294 pages (at 18 pages/min), scraped 10076 items (at 11 items/min)
2015-11-04 11:23:07 [scrapy] INFO: Crawled 10310 pages (at 16 pages/min), scraped 10098 items (at 22 items/min)
2015-11-04 11:24:07 [scrapy] INFO: Crawled 10344 pages (at 34 pages/min), scraped 10124 items (at 26 items/min)
2015-11-04 11:25:05 [scrapy] INFO: Crawled 10344 pages (at 0 pages/min), scraped 10140 items (at 16 items/min)
2015-11-04 11:26:10 [scrapy] INFO: Crawled 10376 pages (at 32 pages/min), scraped 10167 items (at 27 items/min)
2015-11-04 11:27:31 [scrapy] INFO: Crawled 10382 pages (at 6 pages/min), scraped 10172 items (at 5 items/min)
2015-11-04 11:28:35 [scrapy] INFO: Crawled 10383 pages (at 1 pages/min), scraped 10178 items (at 6 items/min)
2015-11-04 11:29:47 [scrapy] INFO: Crawled 10405 pages (at 22 pages/min), scraped 10189 items (at 11 items/min)
2015-11-04 11:30:06 [scrapy] INFO: Crawled 10405 pages (at 0 pages/min), scraped 10193 items (at 4 items/min)
2015-11-04 11:31:10 [scrapy] INFO: Crawled 10445 pages (at 40 pages/min), scraped 10206 items (at 13 items/min)
2015-11-04 11:33:48 [scrapy] INFO: Crawled 10449 pages (at 4 pages/min), scraped 10227 items (at 21 items/min)
2015-11-04 11:36:03 [scrapy] INFO: Crawled 10449 pages (at 0 pages/min), scraped 10241 items (at 14 items/min)
2015-11-04 11:38:00 [scrapy] INFO: Crawled 10464 pages (at 15 pages/min), scraped 10252 items (at 11 items/min)
2015-11-04 11:39:31 [scrapy] INFO: Crawled 10464 pages (at 0 pages/min), scraped 10260 items (at 8 items/min)
2015-11-04 11:43:05 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/youji/1774354.html> (referer: http://go.ly.com/youji/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:43:05 [scrapy] INFO: Crawled 10506 pages (at 42 pages/min), scraped 10286 items (at 26 items/min)
2015-11-04 11:44:19 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-xishuangbanna-5328/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:45:55 [scrapy] INFO: Crawled 10506 pages (at 0 pages/min), scraped 10298 items (at 12 items/min)
2015-11-04 11:46:03 [scrapy] INFO: Crawled 10506 pages (at 0 pages/min), scraped 10300 items (at 2 items/min)
2015-11-04 11:48:45 [scrapy] INFO: Crawled 10540 pages (at 34 pages/min), scraped 10323 items (at 23 items/min)
2015-11-04 11:51:18 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/jing-xixishidi-28434/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:51:57 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/jing-shouxihu-1390/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:51:57 [scrapy] INFO: Crawled 10540 pages (at 0 pages/min), scraped 10332 items (at 9 items/min)
2015-11-04 11:52:02 [scrapy] INFO: Crawled 10555 pages (at 15 pages/min), scraped 10333 items (at 1 items/min)
2015-11-04 11:53:14 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-zhoushan-3309/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:54:19 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-hangzhou-3301/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:54:58 [scrapy] INFO: Crawled 10568 pages (at 13 pages/min), scraped 10345 items (at 12 items/min)
2015-11-04 11:56:50 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-changzhou-3204/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:57:45 [scrapy] INFO: Crawled 10576 pages (at 8 pages/min), scraped 10357 items (at 12 items/min)
2015-11-04 11:59:42 [scrapy] INFO: Crawled 10576 pages (at 0 pages/min), scraped 10365 items (at 8 items/min)
2015-11-04 12:00:43 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-madagascar-2864/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:02:11 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-kenya-2850/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:02:23 [scrapy] INFO: Crawled 10606 pages (at 30 pages/min), scraped 10377 items (at 12 items/min)
2015-11-04 12:03:16 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-australia-2755/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:04:13 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-egypt-2804/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:05:16 [scrapy] INFO: Crawled 10606 pages (at 0 pages/min), scraped 10383 items (at 6 items/min)
2015-11-04 12:06:17 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-canada-2780/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:08:07 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-cuba-2795/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:08:25 [scrapy] INFO: Crawled 10606 pages (at 0 pages/min), scraped 10389 items (at 6 items/min)
2015-11-04 12:09:07 [scrapy] INFO: Crawled 10654 pages (at 48 pages/min), scraped 10405 items (at 16 items/min)
2015-11-04 12:10:10 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-germany-2822/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:12:23 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-malaysia-2866/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:12:32 [scrapy] INFO: Crawled 10654 pages (at 0 pages/min), scraped 10419 items (at 14 items/min)
2015-11-04 12:13:37 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-laos-2854/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:15:41 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/city-venice-3145/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:15:56 [scrapy] INFO: Crawled 10654 pages (at 0 pages/min), scraped 10433 items (at 14 items/min)
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12be229758>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12bdbd67d0>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12fa7988c0>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12dc714cf8>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12bd19f2a8>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12be012c80>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12fa9335f0>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 12:15:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f12be015d70>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 12:16:18 [scrapy] INFO: Crawled 10711 pages (at 57 pages/min), scraped 10462 items (at 29 items/min)
2015-11-04 12:17:49 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/user/478f2201788ea6b5dbecda745b8faf2a> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:18:36 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/user/1a89c3c3f5042e3dec00011d6663c84a> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:19:20 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/user/9963989c0b03ca114ceec1d942d88758> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:20:14 [scrapy] INFO: Crawled 10746 pages (at 35 pages/min), scraped 10485 items (at 23 items/min)
2015-11-04 12:21:24 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/youji/2181920.html> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:23:40 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/youji/2185536.html> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:25:26 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/youji/2163100.html> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:28:12 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/youji/2162588.html> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:30:00 [scrapy] INFO: Crawled 10750 pages (at 4 pages/min), scraped 10516 items (at 31 items/min)
2015-11-04 12:30:03 [scrapy] INFO: Crawled 10750 pages (at 0 pages/min), scraped 10520 items (at 4 items/min)
2015-11-04 12:32:11 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-youji-lijiang-5307/> (referer: http://go.ly.com/youji/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:33:36 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-korea-2932/> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:33:36 [scrapy] INFO: Crawled 10813 pages (at 63 pages/min), scraped 10558 items (at 38 items/min)
2015-11-04 12:35:29 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-huangshan-3410/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:36:29 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-greece-2825/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:36:43 [scrapy] INFO: Crawled 10815 pages (at 2 pages/min), scraped 10579 items (at 21 items/min)
2015-11-04 12:37:22 [scrapy] ERROR: Spider error processing <GET http://blog.gonitro.com/> (referer: https://www.gonitro.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 12:37:34 [scrapy] INFO: Crawled 10887 pages (at 72 pages/min), scraped 10617 items (at 38 items/min)
2015-11-04 12:38:38 [scrapy] INFO: Crawled 10907 pages (at 20 pages/min), scraped 10657 items (at 40 items/min)
2015-11-04 12:39:23 [scrapy] INFO: Crawled 10942 pages (at 35 pages/min), scraped 10690 items (at 33 items/min)
2015-11-04 12:42:12 [scrapy] INFO: Crawled 10973 pages (at 31 pages/min), scraped 10720 items (at 30 items/min)
2015-11-04 12:43:07 [scrapy] INFO: Crawled 11021 pages (at 48 pages/min), scraped 10763 items (at 43 items/min)
2015-11-04 12:44:08 [scrapy] INFO: Crawled 11043 pages (at 22 pages/min), scraped 10799 items (at 36 items/min)
2015-11-04 12:45:06 [scrapy] INFO: Crawled 11078 pages (at 35 pages/min), scraped 10836 items (at 37 items/min)
2015-11-04 12:46:02 [scrapy] INFO: Crawled 11142 pages (at 64 pages/min), scraped 10887 items (at 51 items/min)
2015-11-04 12:47:09 [scrapy] INFO: Crawled 11205 pages (at 63 pages/min), scraped 10948 items (at 61 items/min)
2015-11-04 12:48:04 [scrapy] INFO: Crawled 11271 pages (at 66 pages/min), scraped 10997 items (at 49 items/min)
2015-11-04 12:49:20 [scrapy] INFO: Crawled 11329 pages (at 58 pages/min), scraped 11070 items (at 73 items/min)
2015-11-04 12:50:21 [scrapy] INFO: Crawled 11377 pages (at 48 pages/min), scraped 11110 items (at 40 items/min)
2015-11-04 12:53:04 [scrapy] INFO: Crawled 11395 pages (at 18 pages/min), scraped 11132 items (at 22 items/min)
2015-11-04 12:56:32 [scrapy] INFO: Crawled 11465 pages (at 70 pages/min), scraped 11189 items (at 57 items/min)
2015-11-04 12:57:09 [scrapy] INFO: Crawled 11493 pages (at 28 pages/min), scraped 11226 items (at 37 items/min)
2015-11-04 12:58:02 [scrapy] INFO: Crawled 11542 pages (at 49 pages/min), scraped 11274 items (at 48 items/min)
2015-11-04 12:59:06 [scrapy] INFO: Crawled 11605 pages (at 63 pages/min), scraped 11335 items (at 61 items/min)
2015-11-04 13:00:03 [scrapy] INFO: Crawled 11690 pages (at 85 pages/min), scraped 11380 items (at 45 items/min)
2015-11-04 13:01:17 [scrapy] INFO: Crawled 11714 pages (at 24 pages/min), scraped 11453 items (at 73 items/min)
2015-11-04 13:02:09 [scrapy] INFO: Crawled 11786 pages (at 72 pages/min), scraped 11506 items (at 53 items/min)
2015-11-04 13:03:14 [scrapy] INFO: Crawled 11855 pages (at 69 pages/min), scraped 11568 items (at 62 items/min)
2015-11-04 13:04:38 [scrapy] INFO: Crawled 11892 pages (at 37 pages/min), scraped 11613 items (at 45 items/min)
2015-11-04 13:05:21 [scrapy] INFO: Crawled 11922 pages (at 30 pages/min), scraped 11635 items (at 22 items/min)
2015-11-04 13:06:23 [scrapy] INFO: Crawled 11944 pages (at 22 pages/min), scraped 11673 items (at 38 items/min)
2015-11-04 13:07:05 [scrapy] INFO: Crawled 11975 pages (at 31 pages/min), scraped 11703 items (at 30 items/min)
2015-11-04 13:08:06 [scrapy] INFO: Crawled 11994 pages (at 19 pages/min), scraped 11729 items (at 26 items/min)
2015-11-04 13:09:06 [scrapy] INFO: Crawled 12014 pages (at 20 pages/min), scraped 11755 items (at 26 items/min)
2015-11-04 13:10:19 [scrapy] INFO: Crawled 12041 pages (at 27 pages/min), scraped 11779 items (at 24 items/min)
2015-11-04 13:11:23 [scrapy] INFO: Crawled 12058 pages (at 17 pages/min), scraped 11797 items (at 18 items/min)
2015-11-04 13:12:25 [scrapy] INFO: Crawled 12073 pages (at 15 pages/min), scraped 11811 items (at 14 items/min)
2015-11-04 13:13:11 [scrapy] INFO: Crawled 12085 pages (at 12 pages/min), scraped 11824 items (at 13 items/min)
2015-11-04 13:14:35 [scrapy] INFO: Crawled 12102 pages (at 17 pages/min), scraped 11838 items (at 14 items/min)
2015-11-04 13:15:06 [scrapy] INFO: Crawled 12116 pages (at 14 pages/min), scraped 11847 items (at 9 items/min)
2015-11-04 13:16:05 [scrapy] INFO: Crawled 12136 pages (at 20 pages/min), scraped 11871 items (at 24 items/min)
2015-11-04 13:17:35 [scrapy] INFO: Crawled 12144 pages (at 8 pages/min), scraped 11879 items (at 8 items/min)
2015-11-04 13:18:03 [scrapy] INFO: Crawled 12163 pages (at 19 pages/min), scraped 11899 items (at 20 items/min)
2015-11-04 13:19:09 [scrapy] INFO: Crawled 12176 pages (at 13 pages/min), scraped 11914 items (at 15 items/min)
2015-11-04 13:20:46 [scrapy] INFO: Crawled 12195 pages (at 19 pages/min), scraped 11930 items (at 16 items/min)
2015-11-04 13:21:07 [scrapy] INFO: Crawled 12205 pages (at 10 pages/min), scraped 11946 items (at 16 items/min)
2015-11-04 13:22:19 [scrapy] INFO: Crawled 12219 pages (at 14 pages/min), scraped 11954 items (at 8 items/min)
2015-11-04 13:23:38 [scrapy] INFO: Crawled 12235 pages (at 16 pages/min), scraped 11968 items (at 14 items/min)
2015-11-04 13:25:15 [scrapy] INFO: Crawled 12242 pages (at 7 pages/min), scraped 11978 items (at 10 items/min)
2015-11-04 13:27:52 [scrapy] INFO: Crawled 12266 pages (at 24 pages/min), scraped 12001 items (at 23 items/min)
2015-11-04 13:28:11 [scrapy] INFO: Crawled 12272 pages (at 6 pages/min), scraped 12008 items (at 7 items/min)
2015-11-04 13:29:24 [scrapy] INFO: Crawled 12295 pages (at 23 pages/min), scraped 12030 items (at 22 items/min)
2015-11-04 13:32:41 [scrapy] INFO: Crawled 12307 pages (at 12 pages/min), scraped 12038 items (at 8 items/min)
2015-11-04 13:34:04 [scrapy] INFO: Crawled 12307 pages (at 0 pages/min), scraped 12050 items (at 12 items/min)
2015-11-04 13:35:18 [scrapy] INFO: Crawled 12333 pages (at 26 pages/min), scraped 12068 items (at 18 items/min)
2015-11-04 13:36:10 [scrapy] INFO: Crawled 12341 pages (at 8 pages/min), scraped 12082 items (at 14 items/min)
2015-11-04 13:39:29 [scrapy] INFO: Crawled 12358 pages (at 17 pages/min), scraped 12092 items (at 10 items/min)
2015-11-04 13:40:25 [scrapy] INFO: Crawled 12358 pages (at 0 pages/min), scraped 12099 items (at 7 items/min)
2015-11-04 13:41:27 [scrapy] INFO: Crawled 12360 pages (at 2 pages/min), scraped 12102 items (at 3 items/min)
2015-11-04 13:42:27 [scrapy] INFO: Crawled 12373 pages (at 13 pages/min), scraped 12105 items (at 3 items/min)
2015-11-04 13:44:25 [scrapy] INFO: Crawled 12379 pages (at 6 pages/min), scraped 12111 items (at 6 items/min)
2015-11-04 13:45:48 [scrapy] INFO: Crawled 12381 pages (at 2 pages/min), scraped 12119 items (at 8 items/min)
2015-11-04 13:46:20 [scrapy] INFO: Crawled 12383 pages (at 2 pages/min), scraped 12124 items (at 5 items/min)
2015-11-04 13:48:12 [scrapy] INFO: Crawled 12395 pages (at 12 pages/min), scraped 12134 items (at 10 items/min)
2015-11-04 13:49:29 [scrapy] INFO: Crawled 12416 pages (at 21 pages/min), scraped 12150 items (at 16 items/min)
2015-11-04 13:51:06 [scrapy] INFO: Crawled 12416 pages (at 0 pages/min), scraped 12158 items (at 8 items/min)
2015-11-04 13:52:09 [scrapy] INFO: Crawled 12423 pages (at 7 pages/min), scraped 12164 items (at 6 items/min)
2015-11-04 13:53:04 [scrapy] INFO: Crawled 12426 pages (at 3 pages/min), scraped 12166 items (at 2 items/min)
2015-11-04 13:54:48 [scrapy] INFO: Crawled 12442 pages (at 16 pages/min), scraped 12171 items (at 5 items/min)
2015-11-04 13:55:37 [scrapy] INFO: Crawled 12444 pages (at 2 pages/min), scraped 12179 items (at 8 items/min)
2015-11-04 13:56:05 [scrapy] INFO: Crawled 12451 pages (at 7 pages/min), scraped 12187 items (at 8 items/min)
2015-11-04 13:57:46 [scrapy] INFO: Crawled 12458 pages (at 7 pages/min), scraped 12194 items (at 7 items/min)
2015-11-04 13:58:42 [scrapy] INFO: Crawled 12467 pages (at 9 pages/min), scraped 12202 items (at 8 items/min)
2015-11-04 13:59:36 [scrapy] INFO: Crawled 12480 pages (at 13 pages/min), scraped 12215 items (at 13 items/min)
2015-11-04 14:01:11 [scrapy] INFO: Crawled 12481 pages (at 1 pages/min), scraped 12223 items (at 8 items/min)
2015-11-04 14:03:00 [scrapy] INFO: Crawled 12505 pages (at 24 pages/min), scraped 12240 items (at 17 items/min)
2015-11-04 14:03:24 [scrapy] INFO: Crawled 12511 pages (at 6 pages/min), scraped 12248 items (at 8 items/min)
2015-11-04 14:04:51 [scrapy] INFO: Crawled 12527 pages (at 16 pages/min), scraped 12262 items (at 14 items/min)
2015-11-04 14:05:22 [scrapy] INFO: Crawled 12532 pages (at 5 pages/min), scraped 12270 items (at 8 items/min)
2015-11-04 14:06:06 [scrapy] INFO: Crawled 12539 pages (at 7 pages/min), scraped 12275 items (at 5 items/min)
2015-11-04 14:07:29 [scrapy] INFO: Crawled 12552 pages (at 13 pages/min), scraped 12293 items (at 18 items/min)
2015-11-04 14:08:51 [scrapy] INFO: Crawled 12572 pages (at 20 pages/min), scraped 12303 items (at 10 items/min)
2015-11-04 14:09:27 [scrapy] INFO: Crawled 12576 pages (at 4 pages/min), scraped 12311 items (at 8 items/min)
2015-11-04 14:10:06 [scrapy] INFO: Crawled 12582 pages (at 6 pages/min), scraped 12319 items (at 8 items/min)
2015-11-04 14:12:03 [scrapy] INFO: Crawled 12598 pages (at 16 pages/min), scraped 12333 items (at 14 items/min)
2015-11-04 14:13:15 [scrapy] INFO: Crawled 12619 pages (at 21 pages/min), scraped 12354 items (at 21 items/min)
2015-11-04 14:14:46 [scrapy] INFO: Crawled 12645 pages (at 26 pages/min), scraped 12373 items (at 19 items/min)
2015-11-04 14:15:05 [scrapy] INFO: Crawled 12645 pages (at 0 pages/min), scraped 12380 items (at 7 items/min)
2015-11-04 14:16:08 [scrapy] INFO: Crawled 12661 pages (at 16 pages/min), scraped 12403 items (at 23 items/min)
2015-11-04 14:18:23 [scrapy] INFO: Crawled 12679 pages (at 18 pages/min), scraped 12411 items (at 8 items/min)
2015-11-04 14:19:16 [scrapy] INFO: Crawled 12693 pages (at 14 pages/min), scraped 12432 items (at 21 items/min)
2015-11-04 14:20:16 [scrapy] INFO: Crawled 12701 pages (at 8 pages/min), scraped 12436 items (at 4 items/min)
2015-11-04 14:21:45 [scrapy] INFO: Crawled 12707 pages (at 6 pages/min), scraped 12444 items (at 8 items/min)
2015-11-04 14:22:02 [scrapy] INFO: Crawled 12711 pages (at 4 pages/min), scraped 12450 items (at 6 items/min)
2015-11-04 14:23:32 [scrapy] INFO: Crawled 12723 pages (at 12 pages/min), scraped 12458 items (at 8 items/min)
2015-11-04 14:24:06 [scrapy] INFO: Crawled 12733 pages (at 10 pages/min), scraped 12473 items (at 15 items/min)
2015-11-04 14:25:19 [scrapy] INFO: Crawled 12749 pages (at 16 pages/min), scraped 12480 items (at 7 items/min)
2015-11-04 14:26:24 [scrapy] INFO: Crawled 12761 pages (at 12 pages/min), scraped 12496 items (at 16 items/min)
2015-11-04 14:27:23 [scrapy] INFO: Crawled 12768 pages (at 7 pages/min), scraped 12502 items (at 6 items/min)
2015-11-04 14:28:15 [scrapy] INFO: Crawled 12770 pages (at 2 pages/min), scraped 12504 items (at 2 items/min)
2015-11-04 14:30:55 [scrapy] INFO: Crawled 12778 pages (at 8 pages/min), scraped 12512 items (at 8 items/min)
2015-11-04 14:31:08 [scrapy] INFO: Crawled 12778 pages (at 0 pages/min), scraped 12518 items (at 6 items/min)
2015-11-04 14:32:10 [scrapy] INFO: Crawled 12801 pages (at 23 pages/min), scraped 12538 items (at 20 items/min)
2015-11-04 14:34:36 [scrapy] INFO: Crawled 12808 pages (at 7 pages/min), scraped 12544 items (at 6 items/min)
2015-11-04 14:36:07 [scrapy] INFO: Crawled 12811 pages (at 3 pages/min), scraped 12548 items (at 4 items/min)
2015-11-04 14:37:54 [scrapy] INFO: Crawled 12847 pages (at 36 pages/min), scraped 12574 items (at 26 items/min)
2015-11-04 14:38:18 [scrapy] INFO: Crawled 12849 pages (at 2 pages/min), scraped 12590 items (at 16 items/min)
2015-11-04 14:39:05 [scrapy] INFO: Crawled 12895 pages (at 46 pages/min), scraped 12616 items (at 26 items/min)
2015-11-04 14:40:30 [scrapy] INFO: Crawled 12955 pages (at 60 pages/min), scraped 12667 items (at 51 items/min)
2015-11-04 14:41:53 [scrapy] INFO: Crawled 12984 pages (at 29 pages/min), scraped 12704 items (at 37 items/min)
2015-11-04 14:42:31 [scrapy] INFO: Crawled 12993 pages (at 9 pages/min), scraped 12727 items (at 23 items/min)
2015-11-04 14:43:37 [scrapy] INFO: Crawled 13008 pages (at 15 pages/min), scraped 12744 items (at 17 items/min)
2015-11-04 14:44:04 [scrapy] INFO: Crawled 13009 pages (at 1 pages/min), scraped 12750 items (at 6 items/min)
2015-11-04 14:45:05 [scrapy] INFO: Crawled 13042 pages (at 33 pages/min), scraped 12775 items (at 25 items/min)
2015-11-04 14:46:30 [scrapy] INFO: Crawled 13085 pages (at 43 pages/min), scraped 12817 items (at 42 items/min)
2015-11-04 14:47:13 [scrapy] INFO: Crawled 13114 pages (at 29 pages/min), scraped 12849 items (at 32 items/min)
2015-11-04 14:49:23 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-xianggang-8101/> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:50:09 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-sanya-4602/> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:51:05 [scrapy] INFO: Crawled 13196 pages (at 82 pages/min), scraped 12897 items (at 48 items/min)
2015-11-04 14:52:18 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-maldives-2867/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:54:37 [scrapy] INFO: Crawled 13208 pages (at 12 pages/min), scraped 12930 items (at 33 items/min)
2015-11-04 14:55:16 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-america-2960/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:55:52 [scrapy] INFO: Crawled 13212 pages (at 4 pages/min), scraped 12941 items (at 11 items/min)
2015-11-04 14:55:55 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/o/oauth2/auth?client_id=616286913989.apps.googleusercontent.com&response_type=code&scope=https://www.googleapis.com/auth/plus.login%20https://www.googleapis.com/auth/userinfo.email%20https://www.googleapis.com/auth/plus.me&redirect_uri=http://auth.navegg.com/google-login/&state=state_code%3DpuUF3pPIN5oFTvsS83c2qn7TOdPnM3xs&request_visible_actions=http://schemas.google.com/AddActivity&authuser=0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:56:13 [scrapy] INFO: Crawled 13254 pages (at 42 pages/min), scraped 12958 items (at 17 items/min)
2015-11-04 14:57:26 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/city-phuket-3144/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:58:29 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/user/70c3d481c28cd0375f823e0edb1546a0> (referer: http://go.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:00:31 [scrapy] INFO: Crawled 13257 pages (at 3 pages/min), scraped 12972 items (at 14 items/min)
2015-11-04 15:00:31 [scrapy] ERROR: Error downloading <GET https://www.facebook.com/dialog/oauth?scope=email&redirect_uri=http%3A%2F%2Fauth.navegg.com%2Ffacebook-login%2F&client_id=1399230573629710>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:03:09 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/city-paris-4193/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:03:44 [scrapy] INFO: Crawled 13267 pages (at 10 pages/min), scraped 12987 items (at 15 items/min)
2015-11-04 15:05:11 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-czech-2797/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:06:14 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/abroad/country-turkey-2952/> (referer: http://go.ly.com/abroad/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:06:52 [scrapy] ERROR: Error downloading <GET http://passport.ly.com?pageurl=http%3a%2f%2fmember.ly.com%2forderlist.aspx>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://passport.ly.com?pageurl=http%3a%2f%2fmember.ly.com%2forderlist.aspx took longer than 180.0 seconds..
2015-11-04 15:06:52 [scrapy] INFO: Crawled 13267 pages (at 0 pages/min), scraped 12995 items (at 8 items/min)
2015-11-04 15:07:04 [scrapy] INFO: Crawled 13297 pages (at 30 pages/min), scraped 12996 items (at 1 items/min)
2015-11-04 15:11:07 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/jing-putuoshan-5809/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:11:54 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/jing-huanghelou-1275/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:12:00 [scrapy] INFO: Crawled 13316 pages (at 19 pages/min), scraped 13023 items (at 27 items/min)
2015-11-04 15:13:47 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-kunming-5301/> (referer: http://go.ly.com/gonglve/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:14:29 [scrapy] INFO: Crawled 13316 pages (at 0 pages/min), scraped 13041 items (at 18 items/min)
2015-11-04 15:14:56 [scrapy] ERROR: Spider error processing <GET http://gny.ly.com/py/> (referer: http://gny.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:16:36 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-lanzhou-6201> (referer: http://gny.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:17:16 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-dali-5329> (referer: http://gny.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:18:00 [scrapy] ERROR: Spider error processing <GET http://go.ly.com/gonglve/shi-lijiang-5307> (referer: http://gny.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:18:49 [scrapy] INFO: Crawled 13343 pages (at 27 pages/min), scraped 13056 items (at 15 items/min)
2015-11-04 15:19:50 [scrapy] INFO: Crawled 13351 pages (at 8 pages/min), scraped 13064 items (at 8 items/min)
2015-11-04 15:20:43 [scrapy] INFO: Crawled 13351 pages (at 0 pages/min), scraped 13072 items (at 8 items/min)
2015-11-04 15:21:14 [scrapy] INFO: Crawled 13376 pages (at 25 pages/min), scraped 13081 items (at 9 items/min)
2015-11-04 15:22:10 [scrapy] INFO: Crawled 13397 pages (at 21 pages/min), scraped 13104 items (at 23 items/min)
2015-11-04 15:22:59 [scrapy] ERROR: Spider error processing <GET http://gny.ly.com/pkg/2164/> (referer: http://gny.ly.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:23:09 [scrapy] INFO: Crawled 13419 pages (at 22 pages/min), scraped 13132 items (at 28 items/min)
2015-11-04 15:24:18 [scrapy] INFO: Crawled 13451 pages (at 32 pages/min), scraped 13163 items (at 31 items/min)
2015-11-04 15:25:09 [scrapy] INFO: Crawled 13469 pages (at 18 pages/min), scraped 13185 items (at 22 items/min)
2015-11-04 15:26:48 [scrapy] INFO: Crawled 13503 pages (at 34 pages/min), scraped 13213 items (at 28 items/min)
2015-11-04 15:27:26 [scrapy] INFO: Crawled 13503 pages (at 0 pages/min), scraped 13223 items (at 10 items/min)
2015-11-04 15:28:05 [scrapy] INFO: Crawled 13526 pages (at 23 pages/min), scraped 13232 items (at 9 items/min)
2015-11-04 15:29:04 [scrapy] INFO: Crawled 13526 pages (at 0 pages/min), scraped 13246 items (at 14 items/min)
2015-11-04 15:30:02 [scrapy] INFO: Crawled 13559 pages (at 33 pages/min), scraped 13263 items (at 17 items/min)
2015-11-04 15:31:04 [scrapy] INFO: Crawled 13572 pages (at 13 pages/min), scraped 13280 items (at 17 items/min)
2015-11-04 15:32:23 [scrapy] INFO: Crawled 13603 pages (at 31 pages/min), scraped 13308 items (at 28 items/min)
2015-11-04 15:33:22 [scrapy] INFO: Crawled 13603 pages (at 0 pages/min), scraped 13323 items (at 15 items/min)
2015-11-04 15:34:39 [scrapy] INFO: Crawled 13645 pages (at 42 pages/min), scraped 13348 items (at 25 items/min)
2015-11-04 15:35:43 [scrapy] INFO: Crawled 13645 pages (at 0 pages/min), scraped 13365 items (at 17 items/min)
2015-11-04 15:36:33 [scrapy] INFO: Crawled 13700 pages (at 55 pages/min), scraped 13386 items (at 21 items/min)
2015-11-04 15:37:50 [scrapy] INFO: Crawled 13700 pages (at 0 pages/min), scraped 13415 items (at 29 items/min)
2015-11-04 15:38:02 [scrapy] INFO: Crawled 13705 pages (at 5 pages/min), scraped 13421 items (at 6 items/min)
2015-11-04 15:39:40 [scrapy] INFO: Crawled 13748 pages (at 43 pages/min), scraped 13460 items (at 39 items/min)
2015-11-04 15:40:15 [scrapy] INFO: Crawled 13748 pages (at 0 pages/min), scraped 13468 items (at 8 items/min)
2015-11-04 15:41:35 [scrapy] INFO: Crawled 13785 pages (at 37 pages/min), scraped 13495 items (at 27 items/min)
2015-11-04 15:42:23 [scrapy] INFO: Crawled 13785 pages (at 0 pages/min), scraped 13505 items (at 10 items/min)
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:43:02 [scrapy] INFO: Closing spider (shutdown)
2015-11-04 15:43:03 [scrapy] INFO: Crawled 13804 pages (at 19 pages/min), scraped 13516 items (at 11 items/min)
ler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1303: Tag section invalid
2015-11-04 04:58:18 [scrapy] INFO: Crawled 1669 pages (at 38 pages/min), scraped 1517 items (at 31 items/min)
2015-11-04 04:59:17 [scrapy] INFO: Crawled 1685 pages (at 16 pages/min), scraped 1533 items (at 16 items/min)
2015-11-04 05:00:16 [scrapy] INFO: Crawled 1698 pages (at 13 pages/min), scraped 1545 items (at 12 items/min)
2015-11-04 05:01:20 [scrapy] INFO: Crawled 1715 pages (at 17 pages/min), scraped 1563 items (at 18 items/min)
2015-11-04 05:02:23 [scrapy] INFO: Crawled 1727 pages (at 12 pages/min), scraped 1579 items (at 16 items/min)
2015-11-04 05:03:18 [scrapy] INFO: Crawled 1739 pages (at 12 pages/min), scraped 1589 items (at 10 items/min)
2015-11-04 05:04:17 [scrapy] INFO: Crawled 1763 pages (at 24 pages/min), scraped 1609 items (at 20 items/min)
2015-11-04 05:05:17 [scrapy] INFO: Crawled 1780 pages (at 17 pages/min), scraped 1626 items (at 17 items/min)
2015-11-04 05:06:32 [scrapy] INFO: Crawled 1797 pages (at 17 pages/min), scraped 1650 items (at 24 items/min)
2015-11-04 05:07:17 [scrapy] INFO: Crawled 1818 pages (at 21 pages/min), scraped 1667 items (at 17 items/min)
2015-11-04 05:08:40 [scrapy] INFO: Crawled 1838 pages (at 20 pages/min), scraped 1691 items (at 24 items/min)
2015-11-04 05:09:28 [scrapy] INFO: Crawled 1854 pages (at 16 pages/min), scraped 1704 items (at 13 items/min)
2015-11-04 05:10:29 [scrapy] INFO: Crawled 1872 pages (at 18 pages/min), scraped 1725 items (at 21 items/min)
2015-11-04 05:11:17 [scrapy] INFO: Crawled 1891 pages (at 19 pages/min), scraped 1737 items (at 12 items/min)
2015-11-04 05:12:17 [scrapy] INFO: Crawled 1911 pages (at 20 pages/min), scraped 1760 items (at 23 items/min)
2015-11-04 05:13:37 [scrapy] INFO: Crawled 1933 pages (at 22 pages/min), scraped 1783 items (at 23 items/min)
2015-11-04 05:14:22 [scrapy] INFO: Crawled 1964 pages (at 31 pages/min), scraped 1812 items (at 29 items/min)
2015-11-04 05:14:46 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=sg&lang=en> (referer: https://www.credit-suisse.com/sg/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 273: htmlParseEntityRef: expecting ';'
2015-11-04 05:15:32 [scrapy] INFO: Crawled 2005 pages (at 41 pages/min), scraped 1852 items (at 40 items/min)
2015-11-04 05:16:41 [scrapy] INFO: Crawled 2023 pages (at 18 pages/min), scraped 1867 items (at 15 items/min)
2015-11-04 05:17:21 [scrapy] INFO: Crawled 2025 pages (at 2 pages/min), scraped 1877 items (at 10 items/min)
2015-11-04 05:18:25 [scrapy] INFO: Crawled 2043 pages (at 18 pages/min), scraped 1893 items (at 16 items/min)
2015-11-04 05:19:19 [scrapy] INFO: Crawled 2060 pages (at 17 pages/min), scraped 1904 items (at 11 items/min)
2015-11-04 05:20:33 [scrapy] INFO: Crawled 2071 pages (at 11 pages/min), scraped 1923 items (at 19 items/min)
2015-11-04 05:21:26 [scrapy] INFO: Crawled 2086 pages (at 15 pages/min), scraped 1936 items (at 13 items/min)
2015-11-04 05:22:42 [scrapy] INFO: Crawled 2104 pages (at 18 pages/min), scraped 1954 items (at 18 items/min)
2015-11-04 05:23:22 [scrapy] INFO: Crawled 2110 pages (at 6 pages/min), scraped 1962 items (at 8 items/min)
2015-11-04 05:24:19 [scrapy] INFO: Crawled 2128 pages (at 18 pages/min), scraped 1975 items (at 13 items/min)
2015-11-04 05:25:50 [scrapy] INFO: Crawled 2169 pages (at 41 pages/min), scraped 2011 items (at 36 items/min)
2015-11-04 05:26:56 [scrapy] INFO: Crawled 2188 pages (at 19 pages/min), scraped 2038 items (at 27 items/min)
2015-11-04 05:27:30 [scrapy] INFO: Crawled 2194 pages (at 6 pages/min), scraped 2046 items (at 8 items/min)
2015-11-04 05:28:20 [scrapy] INFO: Crawled 2211 pages (at 17 pages/min), scraped 2058 items (at 12 items/min)
2015-11-04 05:29:19 [scrapy] INFO: Crawled 2223 pages (at 12 pages/min), scraped 2072 items (at 14 items/min)
2015-11-04 05:30:38 [scrapy] INFO: Crawled 2241 pages (at 18 pages/min), scraped 2089 items (at 17 items/min)
2015-11-04 05:31:38 [scrapy] INFO: Crawled 2254 pages (at 13 pages/min), scraped 2105 items (at 16 items/min)
2015-11-04 05:32:25 [scrapy] INFO: Crawled 2262 pages (at 8 pages/min), scraped 2113 items (at 8 items/min)
2015-11-04 05:33:18 [scrapy] INFO: Crawled 2280 pages (at 18 pages/min), scraped 2128 items (at 15 items/min)
2015-11-04 05:34:26 [scrapy] INFO: Crawled 2298 pages (at 18 pages/min), scraped 2146 items (at 18 items/min)
2015-11-04 05:35:17 [scrapy] INFO: Crawled 2310 pages (at 12 pages/min), scraped 2160 items (at 14 items/min)
2015-11-04 05:36:17 [scrapy] INFO: Crawled 2327 pages (at 17 pages/min), scraped 2172 items (at 12 items/min)
2015-11-04 05:37:23 [scrapy] INFO: Crawled 2345 pages (at 18 pages/min), scraped 2191 items (at 19 items/min)
2015-11-04 05:38:17 [scrapy] INFO: Crawled 2359 pages (at 14 pages/min), scraped 2204 items (at 13 items/min)
2015-11-04 05:39:16 [scrapy] INFO: Crawled 2367 pages (at 8 pages/min), scraped 2218 items (at 14 items/min)
2015-11-04 05:40:26 [scrapy] INFO: Crawled 2399 pages (at 32 pages/min), scraped 2244 items (at 26 items/min)
2015-11-04 05:41:30 [scrapy] INFO: Crawled 2418 pages (at 19 pages/min), scraped 2264 items (at 20 items/min)
2015-11-04 05:42:37 [scrapy] INFO: Crawled 2430 pages (at 12 pages/min), scraped 2282 items (at 18 items/min)
2015-11-04 05:43:19 [scrapy] INFO: Crawled 2452 pages (at 22 pages/min), scraped 2298 items (at 16 items/min)
2015-11-04 05:44:17 [scrapy] INFO: Crawled 2473 pages (at 21 pages/min), scraped 2318 items (at 20 items/min)
2015-11-04 05:45:20 [scrapy] INFO: Crawled 2489 pages (at 16 pages/min), scraped 2341 items (at 23 items/min)
2015-11-04 05:46:33 [scrapy] INFO: Crawled 2521 pages (at 32 pages/min), scraped 2367 items (at 26 items/min)
2015-11-04 05:47:29 [scrapy] INFO: Crawled 2534 pages (at 13 pages/min), scraped 2384 items (at 17 items/min)
2015-11-04 05:48:35 [scrapy] INFO: Crawled 2551 pages (at 17 pages/min), scraped 2401 items (at 17 items/min)
2015-11-04 05:49:23 [scrapy] INFO: Crawled 2567 pages (at 16 pages/min), scraped 2413 items (at 12 items/min)
2015-11-04 05:50:35 [scrapy] INFO: Crawled 2575 pages (at 8 pages/min), scraped 2427 items (at 14 items/min)
2015-11-04 05:51:18 [scrapy] INFO: Crawled 2588 pages (at 13 pages/min), scraped 2439 items (at 12 items/min)
2015-11-04 05:52:26 [scrapy] INFO: Crawled 2605 pages (at 17 pages/min), scraped 2455 items (at 16 items/min)
2015-11-04 05:53:21 [scrapy] INFO: Crawled 2621 pages (at 16 pages/min), scraped 2467 items (at 12 items/min)
2015-11-04 05:54:18 [scrapy] INFO: Crawled 2622 pages (at 1 pages/min), scraped 2474 items (at 7 items/min)
2015-11-04 05:55:18 [scrapy] INFO: Crawled 2649 pages (at 27 pages/min), scraped 2494 items (at 20 items/min)
2015-11-04 05:56:18 [scrapy] INFO: Crawled 2667 pages (at 18 pages/min), scraped 2512 items (at 18 items/min)
2015-11-04 05:57:20 [scrapy] INFO: Crawled 2686 pages (at 19 pages/min), scraped 2530 items (at 18 items/min)
2015-11-04 05:58:50 [scrapy] INFO: Crawled 2706 pages (at 20 pages/min), scraped 2557 items (at 27 items/min)
2015-11-04 05:59:16 [scrapy] INFO: Crawled 2713 pages (at 7 pages/min), scraped 2565 items (at 8 items/min)
2015-11-04 06:00:29 [scrapy] INFO: Crawled 2744 pages (at 31 pages/min), scraped 2590 items (at 25 items/min)
2015-11-04 06:01:35 [scrapy] INFO: Crawled 2759 pages (at 15 pages/min), scraped 2611 items (at 21 items/min)
2015-11-04 06:02:32 [scrapy] INFO: Crawled 2777 pages (at 18 pages/min), scraped 2628 items (at 17 items/min)
2015-11-04 06:03:16 [scrapy] INFO: Crawled 2791 pages (at 14 pages/min), scraped 2637 items (at 9 items/min)
2015-11-04 06:04:32 [scrapy] INFO: Crawled 2813 pages (at 22 pages/min), scraped 2665 items (at 28 items/min)
2015-11-04 06:05:26 [scrapy] INFO: Crawled 2828 pages (at 15 pages/min), scraped 2676 items (at 11 items/min)
2015-11-04 06:06:20 [scrapy] INFO: Crawled 2851 pages (at 23 pages/min), scraped 2698 items (at 22 items/min)
2015-11-04 06:07:28 [scrapy] INFO: Crawled 2865 pages (at 14 pages/min), scraped 2717 items (at 19 items/min)
2015-11-04 06:08:25 [scrapy] INFO: Crawled 2882 pages (at 17 pages/min), scraped 2733 items (at 16 items/min)
2015-11-04 06:09:46 [scrapy] INFO: Crawled 2908 pages (at 26 pages/min), scraped 2757 items (at 24 items/min)
2015-11-04 06:10:24 [scrapy] INFO: Crawled 2919 pages (at 11 pages/min), scraped 2769 items (at 12 items/min)
2015-11-04 06:11:17 [scrapy] INFO: Crawled 2944 pages (at 25 pages/min), scraped 2788 items (at 19 items/min)
2015-11-04 06:12:20 [scrapy] INFO: Crawled 2960 pages (at 16 pages/min), scraped 2807 items (at 19 items/min)
2015-11-04 06:13:24 [scrapy] INFO: Crawled 2973 pages (at 13 pages/min), scraped 2824 items (at 17 items/min)
2015-11-04 06:14:18 [scrapy] INFO: Crawled 2992 pages (at 19 pages/min), scraped 2842 items (at 18 items/min)
2015-11-04 06:15:40 [scrapy] INFO: Crawled 3013 pages (at 21 pages/min), scraped 2860 items (at 18 items/min)
2015-11-04 06:16:21 [scrapy] INFO: Crawled 3017 pages (at 4 pages/min), scraped 2869 items (at 9 items/min)
2015-11-04 06:17:26 [scrapy] INFO: Crawled 3034 pages (at 17 pages/min), scraped 2884 items (at 15 items/min)
2015-11-04 06:18:22 [scrapy] INFO: Crawled 3048 pages (at 14 pages/min), scraped 2896 items (at 12 items/min)
2015-11-04 06:19:29 [scrapy] INFO: Crawled 3069 pages (at 21 pages/min), scraped 2912 items (at 16 items/min)
2015-11-04 06:20:40 [scrapy] INFO: Crawled 3091 pages (at 22 pages/min), scraped 2934 items (at 22 items/min)
2015-11-04 06:21:17 [scrapy] INFO: Crawled 3109 pages (at 18 pages/min), scraped 2957 items (at 23 items/min)
2015-11-04 06:22:02 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sg/en/products-and-services.html> (referer: https://www.credit-suisse.com/sg/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 4168: Tag section invalid
2015-11-04 06:22:30 [scrapy] INFO: Crawled 3138 pages (at 29 pages/min), scraped 2986 items (at 29 items/min)
2015-11-04 06:23:29 [scrapy] INFO: Crawled 3157 pages (at 19 pages/min), scraped 3006 items (at 20 items/min)
2015-11-04 06:24:19 [scrapy] INFO: Crawled 3173 pages (at 16 pages/min), scraped 3024 items (at 18 items/min)
2015-11-04 06:25:29 [scrapy] INFO: Crawled 3197 pages (at 24 pages/min), scraped 3045 items (at 21 items/min)
2015-11-04 06:26:22 [scrapy] INFO: Crawled 3242 pages (at 45 pages/min), scraped 3081 items (at 36 items/min)
2015-11-04 06:27:24 [scrapy] INFO: Crawled 3308 pages (at 66 pages/min), scraped 3151 items (at 70 items/min)
2015-11-04 06:28:19 [scrapy] INFO: Crawled 3361 pages (at 53 pages/min), scraped 3195 items (at 44 items/min)
2015-11-04 06:29:19 [scrapy] INFO: Crawled 3405 pages (at 44 pages/min), scraped 3245 items (at 50 items/min)
2015-11-04 06:30:22 [scrapy] INFO: Crawled 3463 pages (at 58 pages/min), scraped 3306 items (at 61 items/min)
2015-11-04 06:31:17 [scrapy] INFO: Crawled 3511 pages (at 48 pages/min), scraped 3362 items (at 56 items/min)
2015-11-04 06:32:33 [scrapy] INFO: Crawled 3538 pages (at 27 pages/min), scraped 3387 items (at 25 items/min)
2015-11-04 06:33:09 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=eg&lang=en> (referer: https://www.credit-suisse.com/eg/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 4805: Tag section invalid
2015-11-04 06:33:22 [scrapy] INFO: Crawled 3557 pages (at 19 pages/min), scraped 3407 items (at 20 items/min)
2015-11-04 06:34:13 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=li&lang=en> (referer: https://www.credit-suisse.com/li/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3986: Tag section invalid
2015-11-04 06:34:26 [scrapy] INFO: Crawled 3583 pages (at 26 pages/min), scraped 3431 items (at 24 items/min)
2015-11-04 06:34:36 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=il&lang=en> (referer: https://www.credit-suisse.com/il/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3674: Tag section invalid
2015-11-04 06:35:18 [scrapy] INFO: Crawled 3618 pages (at 35 pages/min), scraped 3457 items (at 26 items/min)
2015-11-04 06:35:32 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/at/de/careers-nav/experienced-professionals.html> (referer: https://www.credit-suisse.com/at/de.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3465: Tag section invalid
2015-11-04 06:35:41 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/at/de/careers-nav/campus-recruiting-emea.html> (referer: https://www.credit-suisse.com/at/de.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3845: Tag section invalid
2015-11-04 06:36:06 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=je&lang=en> (referer: https://www.credit-suisse.com/je/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3533: Tag section invalid
2015-11-04 06:36:19 [scrapy] INFO: Crawled 3646 pages (at 28 pages/min), scraped 3488 items (at 31 items/min)
2015-11-04 06:37:21 [scrapy] INFO: Crawled 3667 pages (at 21 pages/min), scraped 3507 items (at 19 items/min)
2015-11-04 06:38:28 [scrapy] INFO: Crawled 3684 pages (at 17 pages/min), scraped 3523 items (at 16 items/min)
2015-11-04 06:39:18 [scrapy] INFO: Crawled 3696 pages (at 12 pages/min), scraped 3536 items (at 13 items/min)
2015-11-04 06:40:51 [scrapy] INFO: Crawled 3713 pages (at 17 pages/min), scraped 3556 items (at 20 items/min)
2015-11-04 06:41:19 [scrapy] INFO: Crawled 3720 pages (at 7 pages/min), scraped 3565 items (at 9 items/min)
2015-11-04 06:42:16 [scrapy] INFO: Crawled 3740 pages (at 20 pages/min), scraped 3585 items (at 20 items/min)
2015-11-04 06:42:16 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ru/en.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:42:16 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gr/it.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:43:17 [scrapy] INFO: Crawled 3768 pages (at 28 pages/min), scraped 3606 items (at 21 items/min)
2015-11-04 06:44:18 [scrapy] INFO: Crawled 3776 pages (at 8 pages/min), scraped 3621 items (at 15 items/min)
2015-11-04 06:45:25 [scrapy] INFO: Crawled 3804 pages (at 28 pages/min), scraped 3647 items (at 26 items/min)
2015-11-04 06:45:28 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=lb&lang=en> (referer: https://www.credit-suisse.com/lb/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 308: htmlParseEntityRef: expecting ';'
2015-11-04 06:46:23 [scrapy] INFO: Crawled 3824 pages (at 20 pages/min), scraped 3665 items (at 18 items/min)
2015-11-04 06:47:34 [scrapy] INFO: Crawled 3841 pages (at 17 pages/min), scraped 3683 items (at 18 items/min)
2015-11-04 06:48:26 [scrapy] INFO: Crawled 3854 pages (at 13 pages/min), scraped 3696 items (at 13 items/min)
2015-11-04 06:49:18 [scrapy] INFO: Crawled 3865 pages (at 11 pages/min), scraped 3708 items (at 12 items/min)
2015-11-04 06:50:18 [scrapy] INFO: Crawled 3881 pages (at 16 pages/min), scraped 3721 items (at 13 items/min)
2015-11-04 06:51:17 [scrapy] INFO: Crawled 3898 pages (at 17 pages/min), scraped 3735 items (at 14 items/min)
2015-11-04 06:52:26 [scrapy] INFO: Crawled 3906 pages (at 8 pages/min), scraped 3750 items (at 15 items/min)
2015-11-04 06:53:16 [scrapy] INFO: Crawled 3923 pages (at 17 pages/min), scraped 3767 items (at 17 items/min)
2015-11-04 06:54:28 [scrapy] INFO: Crawled 3951 pages (at 28 pages/min), scraped 3792 items (at 25 items/min)
2015-11-04 06:55:16 [scrapy] INFO: Crawled 3956 pages (at 5 pages/min), scraped 3800 items (at 8 items/min)
2015-11-04 06:56:17 [scrapy] INFO: Crawled 3984 pages (at 28 pages/min), scraped 3820 items (at 20 items/min)
2015-11-04 06:57:25 [scrapy] INFO: Crawled 4009 pages (at 25 pages/min), scraped 3845 items (at 25 items/min)
2015-11-04 06:58:26 [scrapy] INFO: Crawled 4021 pages (at 12 pages/min), scraped 3861 items (at 16 items/min)
2015-11-04 06:59:19 [scrapy] INFO: Crawled 4037 pages (at 16 pages/min), scraped 3873 items (at 12 items/min)
2015-11-04 07:00:24 [scrapy] INFO: Crawled 4052 pages (at 15 pages/min), scraped 3895 items (at 22 items/min)
2015-11-04 07:01:24 [scrapy] INFO: Crawled 4070 pages (at 18 pages/min), scraped 3913 items (at 18 items/min)
2015-11-04 07:02:41 [scrapy] INFO: Crawled 4099 pages (at 29 pages/min), scraped 3938 items (at 25 items/min)
2015-11-04 07:02:42 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=it&lang=it> (referer: https://www.credit-suisse.com/it/it.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 5353: Tag section invalid
2015-11-04 07:03:22 [scrapy] INFO: Crawled 4126 pages (at 27 pages/min), scraped 3958 items (at 20 items/min)
2015-11-04 07:04:41 [scrapy] INFO: Crawled 4142 pages (at 16 pages/min), scraped 3984 items (at 26 items/min)
2015-11-04 07:05:17 [scrapy] INFO: Crawled 4155 pages (at 13 pages/min), scraped 3997 items (at 13 items/min)
2015-11-04 07:06:28 [scrapy] INFO: Crawled 4173 pages (at 18 pages/min), scraped 4015 items (at 18 items/min)
2015-11-04 07:07:47 [scrapy] INFO: Crawled 4192 pages (at 19 pages/min), scraped 4035 items (at 20 items/min)
2015-11-04 07:08:21 [scrapy] INFO: Crawled 4202 pages (at 10 pages/min), scraped 4044 items (at 9 items/min)
2015-11-04 07:09:40 [scrapy] INFO: Crawled 4225 pages (at 23 pages/min), scraped 4063 items (at 19 items/min)
2015-11-04 07:10:20 [scrapy] INFO: Crawled 4230 pages (at 5 pages/min), scraped 4072 items (at 9 items/min)
2015-11-04 07:11:19 [scrapy] INFO: Crawled 4245 pages (at 15 pages/min), scraped 4087 items (at 15 items/min)
2015-11-04 07:12:01 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/error/403.html> (referer: https://www.credit-suisse.com/it/it.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 672: Tag section invalid
2015-11-04 07:12:06 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/error/403.html> (referer: https://www.credit-suisse.com/it/it.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 427: htmlParseEntityRef: expecting ';'
2015-11-04 07:12:17 [scrapy] INFO: Crawled 4271 pages (at 26 pages/min), scraped 4112 items (at 25 items/min)
2015-11-04 07:13:22 [scrapy] INFO: Crawled 4290 pages (at 19 pages/min), scraped 4131 items (at 19 items/min)
2015-11-04 07:14:17 [scrapy] INFO: Crawled 4308 pages (at 18 pages/min), scraped 4149 items (at 18 items/min)
2015-11-04 07:14:48 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/error/403.html> (referer: https://www.credit-suisse.com/it/it.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3844: Tag section invalid
2015-11-04 07:15:28 [scrapy] INFO: Crawled 4340 pages (at 32 pages/min), scraped 4176 items (at 27 items/min)
2015-11-04 07:16:33 [scrapy] INFO: Crawled 4360 pages (at 20 pages/min), scraped 4192 items (at 16 items/min)
2015-11-04 07:17:16 [scrapy] INFO: Crawled 4363 pages (at 3 pages/min), scraped 4202 items (at 10 items/min)
2015-11-04 07:17:36 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/error/403.html> (referer: https://www.credit-suisse.com/it/it.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 672: Tag section invalid
2015-11-04 07:18:23 [scrapy] INFO: Crawled 4390 pages (at 27 pages/min), scraped 4227 items (at 25 items/min)
2015-11-04 07:19:20 [scrapy] INFO: Crawled 4416 pages (at 26 pages/min), scraped 4250 items (at 23 items/min)
2015-11-04 07:20:16 [scrapy] INFO: Crawled 4423 pages (at 7 pages/min), scraped 4261 items (at 11 items/min)
2015-11-04 07:21:23 [scrapy] INFO: Crawled 4440 pages (at 17 pages/min), scraped 4279 items (at 18 items/min)
2015-11-04 07:22:16 [scrapy] INFO: Crawled 4457 pages (at 17 pages/min), scraped 4296 items (at 17 items/min)
2015-11-04 07:23:20 [scrapy] INFO: Crawled 4489 pages (at 32 pages/min), scraped 4321 items (at 25 items/min)
2015-11-04 07:24:19 [scrapy] INFO: Crawled 4506 pages (at 17 pages/min), scraped 4344 items (at 23 items/min)
2015-11-04 07:25:20 [scrapy] INFO: Crawled 4520 pages (at 14 pages/min), scraped 4355 items (at 11 items/min)
2015-11-04 07:25:54 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/fr/fr/careers/campus-recruiting-emea.html> (referer: https://www.credit-suisse.com/fr/fr.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 672: Tag section invalid
2015-11-04 07:25:55 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/fr/%20/content/pwp/www-root/sites/office-locator.html?country=fr&lang=fr> (referer: https://www.credit-suisse.com/fr/fr.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 672: Tag section invalid
2015-11-04 07:26:23 [scrapy] INFO: Crawled 4539 pages (at 19 pages/min), scraped 4375 items (at 20 items/min)
2015-11-04 07:27:24 [scrapy] INFO: Crawled 4558 pages (at 19 pages/min), scraped 4392 items (at 17 items/min)
2015-11-04 07:28:16 [scrapy] INFO: Crawled 4568 pages (at 10 pages/min), scraped 4405 items (at 13 items/min)
2015-11-04 07:29:18 [scrapy] INFO: Crawled 4582 pages (at 14 pages/min), scraped 4419 items (at 14 items/min)
2015-11-04 07:30:18 [scrapy] INFO: Crawled 4603 pages (at 21 pages/min), scraped 4436 items (at 17 items/min)
2015-11-04 07:31:17 [scrapy] INFO: Crawled 4640 pages (at 37 pages/min), scraped 4467 items (at 31 items/min)
2015-11-04 07:32:18 [scrapy] INFO: Crawled 4654 pages (at 14 pages/min), scraped 4490 items (at 23 items/min)
2015-11-04 07:33:21 [scrapy] INFO: Crawled 4670 pages (at 16 pages/min), scraped 4504 items (at 14 items/min)
2015-11-04 07:34:28 [scrapy] INFO: Crawled 4684 pages (at 14 pages/min), scraped 4521 items (at 17 items/min)
2015-11-04 07:35:18 [scrapy] INFO: Crawled 4700 pages (at 16 pages/min), scraped 4533 items (at 12 items/min)
2015-11-04 07:36:27 [scrapy] INFO: Crawled 4718 pages (at 18 pages/min), scraped 4549 items (at 16 items/min)
2015-11-04 07:37:16 [scrapy] INFO: Crawled 4730 pages (at 12 pages/min), scraped 4560 items (at 11 items/min)
2015-11-04 07:38:16 [scrapy] INFO: Crawled 4738 pages (at 8 pages/min), scraped 4575 items (at 15 items/min)
2015-11-04 07:39:17 [scrapy] INFO: Crawled 4755 pages (at 17 pages/min), scraped 4590 items (at 15 items/min)
2015-11-04 07:40:22 [scrapy] INFO: Crawled 4769 pages (at 14 pages/min), scraped 4604 items (at 14 items/min)
2015-11-04 07:41:18 [scrapy] INFO: Crawled 4789 pages (at 20 pages/min), scraped 4623 items (at 19 items/min)
2015-11-04 07:42:18 [scrapy] INFO: Crawled 4815 pages (at 26 pages/min), scraped 4644 items (at 21 items/min)
2015-11-04 07:43:22 [scrapy] INFO: Crawled 4837 pages (at 22 pages/min), scraped 4666 items (at 22 items/min)
2015-11-04 07:44:30 [scrapy] INFO: Crawled 4857 pages (at 20 pages/min), scraped 4693 items (at 27 items/min)
2015-11-04 07:45:21 [scrapy] INFO: Crawled 4874 pages (at 17 pages/min), scraped 4709 items (at 16 items/min)
2015-11-04 07:46:22 [scrapy] INFO: Crawled 4880 pages (at 6 pages/min), scraped 4717 items (at 8 items/min)
2015-11-04 07:47:17 [scrapy] INFO: Crawled 4899 pages (at 19 pages/min), scraped 4731 items (at 14 items/min)
2015-11-04 07:48:31 [scrapy] INFO: Crawled 4917 pages (at 18 pages/min), scraped 4754 items (at 23 items/min)
2015-11-04 07:49:23 [scrapy] INFO: Crawled 4943 pages (at 26 pages/min), scraped 4774 items (at 20 items/min)
2015-11-04 07:50:19 [scrapy] INFO: Crawled 4965 pages (at 22 pages/min), scraped 4797 items (at 23 items/min)
2015-11-04 07:51:26 [scrapy] INFO: Crawled 4981 pages (at 16 pages/min), scraped 4814 items (at 17 items/min)
2015-11-04 07:52:16 [scrapy] INFO: Crawled 4995 pages (at 14 pages/min), scraped 4825 items (at 11 items/min)
2015-11-04 07:53:20 [scrapy] INFO: Crawled 5003 pages (at 8 pages/min), scraped 4840 items (at 15 items/min)
2015-11-04 07:54:36 [scrapy] INFO: Crawled 5018 pages (at 15 pages/min), scraped 4855 items (at 15 items/min)
2015-11-04 07:55:31 [scrapy] INFO: Crawled 5036 pages (at 18 pages/min), scraped 4873 items (at 18 items/min)
2015-11-04 07:56:18 [scrapy] INFO: Crawled 5053 pages (at 17 pages/min), scraped 4890 items (at 17 items/min)
2015-11-04 07:57:17 [scrapy] INFO: Crawled 5075 pages (at 22 pages/min), scraped 4912 items (at 22 items/min)
2015-11-04 07:58:49 [scrapy] INFO: Crawled 5089 pages (at 14 pages/min), scraped 4926 items (at 14 items/min)
2015-11-04 07:59:19 [scrapy] INFO: Crawled 5104 pages (at 15 pages/min), scraped 4937 items (at 11 items/min)
2015-11-04 08:00:31 [scrapy] INFO: Crawled 5117 pages (at 13 pages/min), scraped 4951 items (at 14 items/min)
2015-11-04 08:01:17 [scrapy] INFO: Crawled 5129 pages (at 12 pages/min), scraped 4962 items (at 11 items/min)
2015-11-04 08:01:45 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=pt&lang=en> (referer: https://www.credit-suisse.com/pt/pt.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 498: Tag section invalid
2015-11-04 08:01:46 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=pt&lang=pt> (referer: https://www.credit-suisse.com/pt/pt.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 498: Tag section invalid
2015-11-04 08:02:24 [scrapy] INFO: Crawled 5153 pages (at 24 pages/min), scraped 4985 items (at 23 items/min)
2015-11-04 08:03:29 [scrapy] INFO: Crawled 5172 pages (at 19 pages/min), scraped 5000 items (at 15 items/min)
2015-11-04 08:04:17 [scrapy] INFO: Crawled 5176 pages (at 4 pages/min), scraped 5011 items (at 11 items/min)
2015-11-04 08:05:20 [scrapy] INFO: Crawled 5218 pages (at 42 pages/min), scraped 5048 items (at 37 items/min)
2015-11-04 08:06:18 [scrapy] INFO: Crawled 5228 pages (at 10 pages/min), scraped 5059 items (at 11 items/min)
2015-11-04 08:07:18 [scrapy] INFO: Crawled 5243 pages (at 15 pages/min), scraped 5073 items (at 14 items/min)
2015-11-04 08:08:22 [scrapy] INFO: Crawled 5268 pages (at 25 pages/min), scraped 5095 items (at 22 items/min)
2015-11-04 08:09:55 [scrapy] INFO: Crawled 5286 pages (at 18 pages/min), scraped 5121 items (at 26 items/min)
2015-11-04 08:10:25 [scrapy] INFO: Crawled 5303 pages (at 17 pages/min), scraped 5137 items (at 16 items/min)
2015-11-04 08:11:18 [scrapy] INFO: Crawled 5323 pages (at 20 pages/min), scraped 5155 items (at 18 items/min)
2015-11-04 08:11:58 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=bh&lang=en> (referer: https://www.credit-suisse.com/bh/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3494: Tag section invalid
2015-11-04 08:12:18 [scrapy] INFO: Crawled 5353 pages (at 30 pages/min), scraped 5182 items (at 27 items/min)
2015-11-04 08:12:47 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=gr&lang=en> (referer: https://www.credit-suisse.com/gr/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 3597: Tag section invalid
2015-11-04 08:13:38 [scrapy] INFO: Crawled 5387 pages (at 34 pages/min), scraped 5211 items (at 29 items/min)
2015-11-04 08:14:37 [scrapy] INFO: Crawled 5403 pages (at 16 pages/min), scraped 5235 items (at 24 items/min)
2015-11-04 08:15:23 [scrapy] INFO: Crawled 5412 pages (at 9 pages/min), scraped 5245 items (at 10 items/min)
2015-11-04 08:16:21 [scrapy] INFO: Crawled 5428 pages (at 16 pages/min), scraped 5257 items (at 12 items/min)
2015-11-04 08:17:06 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=gg&lang=en> (referer: https://www.credit-suisse.com/gg/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 464: htmlParseEntityRef: expecting ';'
2015-11-04 08:17:17 [scrapy] INFO: Crawled 5455 pages (at 27 pages/min), scraped 5285 items (at 28 items/min)
2015-11-04 08:18:43 [scrapy] INFO: Crawled 5471 pages (at 16 pages/min), scraped 5303 items (at 18 items/min)
2015-11-04 08:19:21 [scrapy] INFO: Crawled 5480 pages (at 9 pages/min), scraped 5312 items (at 9 items/min)
2015-11-04 08:20:23 [scrapy] INFO: Crawled 5499 pages (at 19 pages/min), scraped 5324 items (at 12 items/min)
2015-11-04 08:21:44 [scrapy] INFO: Crawled 5517 pages (at 18 pages/min), scraped 5344 items (at 20 items/min)
2015-11-04 08:22:16 [scrapy] INFO: Crawled 5523 pages (at 6 pages/min), scraped 5353 items (at 9 items/min)
2015-11-04 08:23:20 [scrapy] INFO: Crawled 5549 pages (at 26 pages/min), scraped 5381 items (at 28 items/min)
2015-11-04 08:24:17 [scrapy] INFO: Crawled 5570 pages (at 21 pages/min), scraped 5402 items (at 21 items/min)
2015-11-04 08:25:34 [scrapy] INFO: Crawled 5588 pages (at 18 pages/min), scraped 5420 items (at 18 items/min)
2015-11-04 08:26:29 [scrapy] INFO: Crawled 5602 pages (at 14 pages/min), scraped 5431 items (at 11 items/min)
2015-11-04 08:27:16 [scrapy] INFO: Crawled 5617 pages (at 15 pages/min), scraped 5442 items (at 11 items/min)
2015-11-04 08:28:17 [scrapy] INFO: Crawled 5625 pages (at 8 pages/min), scraped 5457 items (at 15 items/min)
2015-11-04 08:29:25 [scrapy] INFO: Crawled 5641 pages (at 16 pages/min), scraped 5473 items (at 16 items/min)
2015-11-04 08:30:16 [scrapy] INFO: Crawled 5658 pages (at 17 pages/min), scraped 5483 items (at 10 items/min)
2015-11-04 08:31:21 [scrapy] INFO: Crawled 5671 pages (at 13 pages/min), scraped 5499 items (at 16 items/min)
2015-11-04 08:32:26 [scrapy] INFO: Crawled 5685 pages (at 14 pages/min), scraped 5514 items (at 15 items/min)
2015-11-04 08:33:30 [scrapy] INFO: Crawled 5693 pages (at 8 pages/min), scraped 5525 items (at 11 items/min)
2015-11-04 08:34:23 [scrapy] INFO: Crawled 5709 pages (at 16 pages/min), scraped 5538 items (at 13 items/min)
2015-11-04 08:35:35 [scrapy] INFO: Crawled 5726 pages (at 17 pages/min), scraped 5556 items (at 18 items/min)
2015-11-04 08:36:17 [scrapy] INFO: Crawled 5736 pages (at 10 pages/min), scraped 5565 items (at 9 items/min)
2015-11-04 08:37:24 [scrapy] INFO: Crawled 5750 pages (at 14 pages/min), scraped 5579 items (at 14 items/min)
2015-11-04 08:38:20 [scrapy] INFO: Crawled 5766 pages (at 16 pages/min), scraped 5592 items (at 13 items/min)
2015-11-04 08:39:16 [scrapy] INFO: Crawled 5770 pages (at 4 pages/min), scraped 5602 items (at 10 items/min)
2015-11-04 08:40:16 [scrapy] INFO: Crawled 5790 pages (at 20 pages/min), scraped 5619 items (at 17 items/min)
2015-11-04 08:41:39 [scrapy] INFO: Crawled 5813 pages (at 23 pages/min), scraped 5642 items (at 23 items/min)
2015-11-04 08:42:17 [scrapy] INFO: Crawled 5820 pages (at 7 pages/min), scraped 5651 items (at 9 items/min)
2015-11-04 08:43:18 [scrapy] INFO: Crawled 5841 pages (at 21 pages/min), scraped 5667 items (at 16 items/min)
2015-11-04 08:44:28 [scrapy] INFO: Crawled 5857 pages (at 16 pages/min), scraped 5687 items (at 20 items/min)
2015-11-04 08:45:18 [scrapy] INFO: Crawled 5874 pages (at 17 pages/min), scraped 5705 items (at 18 items/min)
2015-11-04 08:46:21 [scrapy] INFO: Crawled 5901 pages (at 27 pages/min), scraped 5726 items (at 21 items/min)
2015-11-04 08:47:24 [scrapy] INFO: Crawled 5925 pages (at 24 pages/min), scraped 5749 items (at 23 items/min)
2015-11-04 08:48:19 [scrapy] INFO: Crawled 5942 pages (at 17 pages/min), scraped 5767 items (at 18 items/min)
2015-11-04 08:49:24 [scrapy] INFO: Crawled 5960 pages (at 18 pages/min), scraped 5791 items (at 24 items/min)
2015-11-04 08:50:25 [scrapy] INFO: Crawled 5978 pages (at 18 pages/min), scraped 5809 items (at 18 items/min)
2015-11-04 08:51:17 [scrapy] INFO: Crawled 6003 pages (at 25 pages/min), scraped 5828 items (at 19 items/min)
2015-11-04 08:52:18 [scrapy] INFO: Crawled 6011 pages (at 8 pages/min), scraped 5843 items (at 15 items/min)
2015-11-04 08:53:16 [scrapy] INFO: Crawled 6025 pages (at 14 pages/min), scraped 5857 items (at 14 items/min)
2015-11-04 08:54:26 [scrapy] INFO: Crawled 6063 pages (at 38 pages/min), scraped 5887 items (at 30 items/min)
2015-11-04 08:55:16 [scrapy] INFO: Crawled 6090 pages (at 27 pages/min), scraped 5916 items (at 29 items/min)
2015-11-04 08:56:24 [scrapy] INFO: Crawled 6111 pages (at 21 pages/min), scraped 5940 items (at 24 items/min)
2015-11-04 08:57:19 [scrapy] INFO: Crawled 6128 pages (at 17 pages/min), scraped 5959 items (at 19 items/min)
2015-11-04 08:58:16 [scrapy] INFO: Crawled 6144 pages (at 16 pages/min), scraped 5968 items (at 9 items/min)
2015-11-04 08:59:16 [scrapy] INFO: Crawled 6152 pages (at 8 pages/min), scraped 5984 items (at 16 items/min)
2015-11-04 09:00:19 [scrapy] INFO: Crawled 6176 pages (at 24 pages/min), scraped 6005 items (at 21 items/min)
2015-11-04 09:01:24 [scrapy] INFO: Crawled 6187 pages (at 11 pages/min), scraped 6018 items (at 13 items/min)
2015-11-04 09:02:19 [scrapy] INFO: Crawled 6205 pages (at 18 pages/min), scraped 6037 items (at 19 items/min)
2015-11-04 09:03:25 [scrapy] INFO: Crawled 6222 pages (at 17 pages/min), scraped 6054 items (at 17 items/min)
2015-11-04 09:04:17 [scrapy] INFO: Crawled 6243 pages (at 21 pages/min), scraped 6074 items (at 20 items/min)
2015-11-04 09:05:45 [scrapy] INFO: Crawled 6265 pages (at 22 pages/min), scraped 6096 items (at 22 items/min)
2015-11-04 09:06:24 [scrapy] INFO: Crawled 6289 pages (at 24 pages/min), scraped 6114 items (at 18 items/min)
2015-11-04 09:07:19 [scrapy] INFO: Crawled 6300 pages (at 11 pages/min), scraped 6131 items (at 17 items/min)
2015-11-04 09:08:47 [scrapy] INFO: Crawled 6326 pages (at 26 pages/min), scraped 6158 items (at 27 items/min)
2015-11-04 09:09:27 [scrapy] INFO: Crawled 6350 pages (at 24 pages/min), scraped 6178 items (at 20 items/min)
2015-11-04 09:10:18 [scrapy] INFO: Crawled 6358 pages (at 8 pages/min), scraped 6189 items (at 11 items/min)
2015-11-04 09:11:17 [scrapy] INFO: Crawled 6383 pages (at 25 pages/min), scraped 6213 items (at 24 items/min)
2015-11-04 09:12:47 [scrapy] INFO: Crawled 6411 pages (at 28 pages/min), scraped 6242 items (at 29 items/min)
2015-11-04 09:13:28 [scrapy] INFO: Crawled 6428 pages (at 17 pages/min), scraped 6251 items (at 9 items/min)
2015-11-04 09:14:18 [scrapy] INFO: Crawled 6440 pages (at 12 pages/min), scraped 6267 items (at 16 items/min)
2015-11-04 09:15:20 [scrapy] INFO: Crawled 6459 pages (at 19 pages/min), scraped 6286 items (at 19 items/min)
2015-11-04 09:16:27 [scrapy] INFO: Crawled 6474 pages (at 15 pages/min), scraped 6305 items (at 19 items/min)
2015-11-04 09:17:36 [scrapy] INFO: Crawled 6491 pages (at 17 pages/min), scraped 6320 items (at 15 items/min)
2015-11-04 09:18:17 [scrapy] INFO: Crawled 6504 pages (at 13 pages/min), scraped 6331 items (at 11 items/min)
2015-11-04 09:19:17 [scrapy] INFO: Crawled 6523 pages (at 19 pages/min), scraped 6348 items (at 17 items/min)
2015-11-04 09:20:20 [scrapy] INFO: Crawled 6537 pages (at 14 pages/min), scraped 6367 items (at 19 items/min)
2015-11-04 09:21:40 [scrapy] INFO: Crawled 6556 pages (at 19 pages/min), scraped 6384 items (at 17 items/min)
2015-11-04 09:22:17 [scrapy] INFO: Crawled 6560 pages (at 4 pages/min), scraped 6392 items (at 8 items/min)
2015-11-04 09:23:16 [scrapy] INFO: Crawled 6577 pages (at 17 pages/min), scraped 6409 items (at 17 items/min)
2015-11-04 09:24:24 [scrapy] INFO: Crawled 6608 pages (at 31 pages/min), scraped 6433 items (at 24 items/min)
2015-11-04 09:25:16 [scrapy] INFO: Crawled 6619 pages (at 11 pages/min), scraped 6450 items (at 17 items/min)
2015-11-04 09:26:16 [scrapy] INFO: Crawled 6650 pages (at 31 pages/min), scraped 6475 items (at 25 items/min)
2015-11-04 09:27:21 [scrapy] INFO: Crawled 6669 pages (at 19 pages/min), scraped 6496 items (at 21 items/min)
2015-11-04 09:28:18 [scrapy] INFO: Crawled 6680 pages (at 11 pages/min), scraped 6512 items (at 16 items/min)
2015-11-04 09:29:20 [scrapy] INFO: Crawled 6696 pages (at 16 pages/min), scraped 6527 items (at 15 items/min)
2015-11-04 09:30:34 [scrapy] INFO: Crawled 6715 pages (at 19 pages/min), scraped 6547 items (at 20 items/min)
2015-11-04 09:30:39 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=nl&lang=en> (referer: https://www.credit-suisse.com/nl/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 263: htmlParseEntityRef: expecting ';'
2015-11-04 09:30:59 [scrapy] ERROR: Spider error processing <GET https://www.credit-suisse.com/sites/office-locator.html?country=pl&lang=en> (referer: https://www.credit-suisse.com/pl/en.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 418: htmlParseEntityRef: expecting ';'
2015-11-04 09:31:36 [scrapy] INFO: Crawled 6742 pages (at 27 pages/min), scraped 6572 items (at 25 items/min)
2015-11-04 09:32:23 [scrapy] INFO: Crawled 6759 pages (at 17 pages/min), scraped 6587 items (at 15 items/min)
2015-11-04 09:33:19 [scrapy] INFO: Crawled 6778 pages (at 19 pages/min), scraped 6608 items (at 21 items/min)
2015-11-04 09:34:17 [scrapy] INFO: Crawled 6805 pages (at 27 pages/min), scraped 6627 items (at 19 items/min)
2015-11-04 09:35:21 [scrapy] INFO: Crawled 6828 pages (at 23 pages/min), scraped 6652 items (at 25 items/min)
2015-11-04 09:36:17 [scrapy] INFO: Crawled 6834 pages (at 6 pages/min), scraped 6663 items (at 11 items/min)
2015-11-04 09:37:43 [scrapy] INFO: Crawled 6855 pages (at 21 pages/min), scraped 6685 items (at 22 items/min)
2015-11-04 09:38:18 [scrapy] INFO: Crawled 6862 pages (at 7 pages/min), scraped 6692 items (at 7 items/min)
2015-11-04 09:39:21 [scrapy] INFO: Crawled 6885 pages (at 23 pages/min), scraped 6707 items (at 15 items/min)
2015-11-04 09:40:30 [scrapy] INFO: Crawled 6893 pages (at 8 pages/min), scraped 6723 items (at 16 items/min)
2015-11-04 09:41:18 [scrapy] INFO: Crawled 6911 pages (at 18 pages/min), scraped 6736 items (at 13 items/min)
2015-11-04 09:42:34 [scrapy] INFO: Crawled 6926 pages (at 15 pages/min), scraped 6751 items (at 15 items/min)
2015-11-04 09:43:40 [scrapy] INFO: Crawled 6940 pages (at 14 pages/min), scraped 6767 items (at 16 items/min)
2015-11-04 09:44:16 [scrapy] INFO: Crawled 6949 pages (at 9 pages/min), scraped 6776 items (at 9 items/min)
2015-11-04 09:45:20 [scrapy] INFO: Crawled 6975 pages (at 26 pages/min), scraped 6801 items (at 25 items/min)
2015-11-04 09:46:39 [scrapy] INFO: Crawled 6993 pages (at 18 pages/min), scraped 6823 items (at 22 items/min)
2015-11-04 09:47:27 [scrapy] INFO: Crawled 7001 pages (at 8 pages/min), scraped 6831 items (at 8 items/min)
2015-11-04 09:48:19 [scrapy] INFO: Crawled 7018 pages (at 17 pages/min), scraped 6844 items (at 13 items/min)
2015-11-04 09:49:35 [scrapy] INFO: Crawled 7034 pages (at 16 pages/min), scraped 6861 items (at 17 items/min)
2015-11-04 09:50:23 [scrapy] INFO: Crawled 7049 pages (at 15 pages/min), scraped 6876 items (at 15 items/min)
2015-11-04 09:51:25 [scrapy] INFO: Crawled 7065 pages (at 16 pages/min), scraped 6892 items (at 16 items/min)
2015-11-04 09:52:37 [scrapy] INFO: Crawled 7080 pages (at 15 pages/min), scraped 6909 items (at 17 items/min)
2015-11-04 09:53:20 [scrapy] INFO: Crawled 7100 pages (at 20 pages/min), scraped 6924 items (at 15 items/min)
2015-11-04 09:54:36 [scrapy] INFO: Crawled 7121 pages (at 21 pages/min), scraped 6951 items (at 27 items/min)
2015-11-04 09:55:31 [scrapy] INFO: Crawled 7138 pages (at 17 pages/min), scraped 6968 items (at 17 items/min)
2015-11-04 09:56:16 [scrapy] INFO: Crawled 7160 pages (at 22 pages/min), scraped 6984 items (at 16 items/min)
2015-11-04 09:57:25 [scrapy] INFO: Crawled 7183 pages (at 23 pages/min), scraped 7009 items (at 25 items/min)
2015-11-04 09:58:19 [scrapy] INFO: Crawled 7212 pages (at 29 pages/min), scraped 7026 items (at 17 items/min)
2015-11-04 09:59:56 [scrapy] INFO: Crawled 7235 pages (at 23 pages/min), scraped 7057 items (at 31 items/min)
2015-11-04 10:00:22 [scrapy] INFO: Crawled 7252 pages (at 17 pages/min), scraped 7073 items (at 16 items/min)
2015-11-04 10:01:34 [scrapy] INFO: Crawled 7308 pages (at 56 pages/min), scraped 7129 items (at 56 items/min)
2015-11-04 10:02:19 [scrapy] INFO: Crawled 7361 pages (at 53 pages/min), scraped 7166 items (at 37 items/min)
2015-11-04 10:03:28 [scrapy] INFO: Crawled 7396 pages (at 35 pages/min), scraped 7204 items (at 38 items/min)
2015-11-04 10:04:22 [scrapy] INFO: Crawled 7436 pages (at 40 pages/min), scraped 7255 items (at 51 items/min)
2015-11-04 10:05:47 [scrapy] INFO: Crawled 7506 pages (at 70 pages/min), scraped 7323 items (at 68 items/min)
2015-11-04 10:06:18 [scrapy] INFO: Crawled 7523 pages (at 17 pages/min), scraped 7342 items (at 19 items/min)
2015-11-04 10:07:17 [scrapy] INFO: Crawled 7581 pages (at 58 pages/min), scraped 7400 items (at 58 items/min)
2015-11-04 10:08:16 [scrapy] INFO: Crawled 7645 pages (at 64 pages/min), scraped 7464 items (at 64 items/min)
2015-11-04 10:09:17 [scrapy] INFO: Crawled 7709 pages (at 64 pages/min), scraped 7528 items (at 64 items/min)
2015-11-04 10:10:19 [scrapy] INFO: Crawled 7775 pages (at 66 pages/min), scraped 7594 items (at 66 items/min)
2015-11-04 10:11:20 [scrapy] INFO: Crawled 7841 pages (at 66 pages/min), scraped 7659 items (at 65 items/min)
2015-11-04 10:12:19 [scrapy] INFO: Crawled 7901 pages (at 60 pages/min), scraped 7722 items (at 63 items/min)
2015-11-04 10:13:19 [scrapy] INFO: Crawled 7968 pages (at 67 pages/min), scraped 7787 items (at 65 items/min)
2015-11-04 10:14:18 [scrapy] INFO: Crawled 8032 pages (at 64 pages/min), scraped 7851 items (at 64 items/min)
2015-11-04 10:15:17 [scrapy] INFO: Crawled 8101 pages (at 69 pages/min), scraped 7914 items (at 63 items/min)
2015-11-04 10:16:21 [scrapy] INFO: Crawled 8163 pages (at 62 pages/min), scraped 7984 items (at 70 items/min)
2015-11-04 10:17:23 [scrapy] INFO: Crawled 8230 pages (at 67 pages/min), scraped 8051 items (at 67 items/min)
2015-11-04 10:18:23 [scrapy] INFO: Crawled 8294 pages (at 64 pages/min), scraped 8115 items (at 64 items/min)
2015-11-04 10:19:22 [scrapy] INFO: Crawled 8360 pages (at 66 pages/min), scraped 8179 items (at 64 items/min)
2015-11-04 10:20:20 [scrapy] INFO: Crawled 8422 pages (at 62 pages/min), scraped 8240 items (at 61 items/min)
2015-11-04 10:21:20 [scrapy] INFO: Crawled 8495 pages (at 73 pages/min), scraped 8301 items (at 61 items/min)
2015-11-04 10:22:21 [scrapy] INFO: Crawled 8571 pages (at 76 pages/min), scraped 8368 items (at 67 items/min)
2015-11-04 10:23:23 [scrapy] INFO: Crawled 8654 pages (at 83 pages/min), scraped 8448 items (at 80 items/min)
2015-11-04 10:24:18 [scrapy] INFO: Crawled 8751 pages (at 97 pages/min), scraped 8548 items (at 100 items/min)
2015-11-04 10:25:18 [scrapy] INFO: Crawled 8887 pages (at 136 pages/min), scraped 8683 items (at 135 items/min)
2015-11-04 10:26:37 [scrapy] INFO: Crawled 9030 pages (at 143 pages/min), scraped 8819 items (at 136 items/min)
2015-11-04 10:27:23 [scrapy] INFO: Crawled 9063 pages (at 33 pages/min), scraped 8872 items (at 53 items/min)
2015-11-04 10:28:28 [scrapy] INFO: Crawled 9157 pages (at 94 pages/min), scraped 8958 items (at 86 items/min)
2015-11-04 10:29:20 [scrapy] INFO: Crawled 9201 pages (at 44 pages/min), scraped 9016 items (at 58 items/min)
2015-11-04 10:30:19 [scrapy] INFO: Crawled 9252 pages (at 51 pages/min), scraped 9074 items (at 58 items/min)
2015-11-04 10:31:18 [scrapy] INFO: Crawled 9352 pages (at 100 pages/min), scraped 9150 items (at 76 items/min)
2015-11-04 10:32:27 [scrapy] INFO: Crawled 9417 pages (at 65 pages/min), scraped 9234 items (at 84 items/min)
2015-11-04 10:33:28 [scrapy] INFO: Crawled 9502 pages (at 85 pages/min), scraped 9305 items (at 71 items/min)
2015-11-04 10:34:18 [scrapy] INFO: Crawled 9606 pages (at 104 pages/min), scraped 9392 items (at 87 items/min)
2015-11-04 10:35:20 [scrapy] INFO: Crawled 9695 pages (at 89 pages/min), scraped 9500 items (at 108 items/min)
2015-11-04 10:36:24 [scrapy] INFO: Crawled 9838 pages (at 143 pages/min), scraped 9638 items (at 138 items/min)
2015-11-04 10:37:17 [scrapy] INFO: Crawled 9901 pages (at 63 pages/min), scraped 9707 items (at 69 items/min)
2015-11-04 10:38:40 [scrapy] INFO: Crawled 9976 pages (at 75 pages/min), scraped 9789 items (at 82 items/min)
2015-11-04 10:39:19 [scrapy] INFO: Crawled 10040 pages (at 64 pages/min), scraped 9839 items (at 50 items/min)
2015-11-04 10:40:23 [scrapy] INFO: Crawled 10095 pages (at 55 pages/min), scraped 9911 items (at 72 items/min)
2015-11-04 10:41:23 [scrapy] INFO: Crawled 10111 pages (at 16 pages/min), scraped 9930 items (at 19 items/min)
2015-11-04 10:42:16 [scrapy] INFO: Crawled 10126 pages (at 15 pages/min), scraped 9945 items (at 15 items/min)
2015-11-04 10:43:30 [scrapy] INFO: Crawled 10147 pages (at 21 pages/min), scraped 9965 items (at 20 items/min)
2015-11-04 10:44:31 [scrapy] INFO: Crawled 10164 pages (at 17 pages/min), scraped 9982 items (at 17 items/min)
2015-11-04 10:45:39 [scrapy] INFO: Crawled 10181 pages (at 17 pages/min), scraped 9999 items (at 17 items/min)
2015-11-04 10:46:17 [scrapy] INFO: Crawled 10201 pages (at 20 pages/min), scraped 10012 items (at 13 items/min)
2015-11-04 10:47:13 [scrapy] ERROR: Error downloading <GET https://creditsuisse.taleo.net/careersection/external/mysubmissions.ftl?lang=en>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 10:47:26 [scrapy] INFO: Crawled 10215 pages (at 14 pages/min), scraped 10032 items (at 20 items/min)
2015-11-04 10:47:50 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pl/en/investment-banking.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:48:17 [scrapy] INFO: Crawled 10226 pages (at 11 pages/min), scraped 10040 items (at 8 items/min)
2015-11-04 10:49:16 [scrapy] INFO: Crawled 10231 pages (at 5 pages/min), scraped 10049 items (at 9 items/min)
2015-11-04 10:50:20 [scrapy] INFO: Crawled 10250 pages (at 19 pages/min), scraped 10069 items (at 20 items/min)
2015-11-04 10:51:19 [scrapy] INFO: Crawled 10267 pages (at 17 pages/min), scraped 10083 items (at 14 items/min)
2015-11-04 10:52:31 [scrapy] INFO: Crawled 10279 pages (at 12 pages/min), scraped 10096 items (at 13 items/min)
2015-11-04 10:53:27 [scrapy] INFO: Crawled 10295 pages (at 16 pages/min), scraped 10111 items (at 15 items/min)
2015-11-04 10:54:24 [scrapy] INFO: Crawled 10309 pages (at 14 pages/min), scraped 10126 items (at 15 items/min)
2015-11-04 10:54:49 [scrapy] ERROR: Error downloading <GET https://creditsuisse.taleo.net/careersection/external/mysubmissions.ftl?lang=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:54:49 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/uk/en/careers/campus-recruiting.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:55:20 [scrapy] INFO: Crawled 10330 pages (at 21 pages/min), scraped 10143 items (at 17 items/min)
2015-11-04 10:56:35 [scrapy] INFO: Crawled 10347 pages (at 17 pages/min), scraped 10160 items (at 17 items/min)
2015-11-04 10:57:22 [scrapy] INFO: Crawled 10366 pages (at 19 pages/min), scraped 10177 items (at 17 items/min)
2015-11-04 10:58:20 [scrapy] INFO: Crawled 10382 pages (at 16 pages/min), scraped 10196 items (at 19 items/min)
2015-11-04 10:59:16 [scrapy] INFO: Crawled 10391 pages (at 9 pages/min), scraped 10209 items (at 13 items/min)
2015-11-04 11:00:27 [scrapy] INFO: Crawled 10407 pages (at 16 pages/min), scraped 10225 items (at 16 items/min)
2015-11-04 11:01:20 [scrapy] INFO: Crawled 10419 pages (at 12 pages/min), scraped 10235 items (at 10 items/min)
2015-11-04 11:02:36 [scrapy] INFO: Crawled 10435 pages (at 16 pages/min), scraped 10252 items (at 17 items/min)
2015-11-04 11:03:31 [scrapy] INFO: Crawled 10451 pages (at 16 pages/min), scraped 10265 items (at 13 items/min)
2015-11-04 11:04:18 [scrapy] INFO: Crawled 10464 pages (at 13 pages/min), scraped 10277 items (at 12 items/min)
2015-11-04 11:05:16 [scrapy] INFO: Crawled 10473 pages (at 9 pages/min), scraped 10292 items (at 15 items/min)
2015-11-04 11:06:22 [scrapy] INFO: Crawled 10490 pages (at 17 pages/min), scraped 10309 items (at 17 items/min)
2015-11-04 11:06:22 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/content/pwp/legalgates/cc/disclaimer-tier-2-buffer-capital-notes-2.html/gr/en/about-us/investor-relations/information-for-debt-investors/high-trigger-capital-instruments.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:07:18 [scrapy] INFO: Crawled 10508 pages (at 18 pages/min), scraped 10327 items (at 18 items/min)
2015-11-04 11:08:18 [scrapy] INFO: Crawled 10530 pages (at 22 pages/min), scraped 10348 items (at 21 items/min)
2015-11-04 11:09:33 [scrapy] INFO: Crawled 10546 pages (at 16 pages/min), scraped 10364 items (at 16 items/min)
2015-11-04 11:10:18 [scrapy] INFO: Crawled 10565 pages (at 19 pages/min), scraped 10380 items (at 16 items/min)
2015-11-04 11:11:27 [scrapy] INFO: Crawled 10582 pages (at 17 pages/min), scraped 10400 items (at 20 items/min)
2015-11-04 11:12:25 [scrapy] INFO: Crawled 10594 pages (at 12 pages/min), scraped 10413 items (at 13 items/min)
2015-11-04 11:13:16 [scrapy] INFO: Crawled 10615 pages (at 21 pages/min), scraped 10429 items (at 16 items/min)
2015-11-04 11:14:18 [scrapy] INFO: Crawled 10633 pages (at 18 pages/min), scraped 10448 items (at 19 items/min)
2015-11-04 11:15:20 [scrapy] INFO: Crawled 10649 pages (at 16 pages/min), scraped 10461 items (at 13 items/min)
2015-11-04 11:16:17 [scrapy] INFO: Crawled 10663 pages (at 14 pages/min), scraped 10477 items (at 16 items/min)
2015-11-04 11:17:31 [scrapy] INFO: Crawled 10677 pages (at 14 pages/min), scraped 10494 items (at 17 items/min)
2015-11-04 11:18:25 [scrapy] INFO: Crawled 10690 pages (at 13 pages/min), scraped 10508 items (at 14 items/min)
2015-11-04 11:19:00 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/content/pwp/legalgates/cc/disclaimer-tier-2-buffer-capital-notes-2.html/mc/en/about-us/investor-relations/information-for-debt-investors/high-trigger-capital-instruments.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:19:26 [scrapy] INFO: Crawled 10715 pages (at 25 pages/min), scraped 10523 items (at 15 items/min)
2015-11-04 11:20:35 [scrapy] INFO: Crawled 10726 pages (at 11 pages/min), scraped 10544 items (at 21 items/min)
2015-11-04 11:20:36 [scrapy] ERROR: Error downloading <GET https://creditsuisse.taleo.net/careersection/external/mysubmissions.ftl?lang=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:21:25 [scrapy] INFO: Crawled 10745 pages (at 19 pages/min), scraped 10559 items (at 15 items/min)
2015-11-04 11:22:32 [scrapy] INFO: Crawled 10765 pages (at 20 pages/min), scraped 10583 items (at 24 items/min)
2015-11-04 11:23:22 [scrapy] INFO: Crawled 10783 pages (at 18 pages/min), scraped 10598 items (at 15 items/min)
2015-11-04 11:24:20 [scrapy] INFO: Crawled 10801 pages (at 18 pages/min), scraped 10614 items (at 16 items/min)
2015-11-04 11:24:55 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/fr/fr.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:24:55 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/fr/fr/asset-management.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:25:18 [scrapy] INFO: Crawled 10815 pages (at 14 pages/min), scraped 10628 items (at 14 items/min)
2015-11-04 11:26:19 [scrapy] INFO: Crawled 10821 pages (at 6 pages/min), scraped 10640 items (at 12 items/min)
2015-11-04 11:26:20 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/de/en/investment-banking.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:27:16 [scrapy] INFO: Crawled 10838 pages (at 17 pages/min), scraped 10657 items (at 17 items/min)
2015-11-04 11:28:30 [scrapy] INFO: Crawled 10856 pages (at 18 pages/min), scraped 10674 items (at 17 items/min)
2015-11-04 11:28:32 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/error/403.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:29:15 [scrapy] ERROR: Error downloading <GET https://creditsuisse.taleo.net/careersection/external/mysubmissions.ftl?lang=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:29:38 [scrapy] INFO: Crawled 10876 pages (at 20 pages/min), scraped 10693 items (at 19 items/min)
2015-11-04 11:29:45 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/uk/en/careers/campus-recruiting.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:30:49 [scrapy] INFO: Crawled 10892 pages (at 16 pages/min), scraped 10704 items (at 11 items/min)
2015-11-04 11:31:22 [scrapy] INFO: Crawled 10905 pages (at 13 pages/min), scraped 10717 items (at 13 items/min)
2015-11-04 11:32:24 [scrapy] INFO: Crawled 10914 pages (at 9 pages/min), scraped 10732 items (at 15 items/min)
2015-11-04 11:33:23 [scrapy] INFO: Crawled 10933 pages (at 19 pages/min), scraped 10751 items (at 19 items/min)
2015-11-04 11:34:18 [scrapy] INFO: Crawled 10951 pages (at 18 pages/min), scraped 10769 items (at 18 items/min)
2015-11-04 11:35:32 [scrapy] INFO: Crawled 10971 pages (at 20 pages/min), scraped 10790 items (at 21 items/min)
2015-11-04 11:36:18 [scrapy] INFO: Crawled 10986 pages (at 15 pages/min), scraped 10805 items (at 15 items/min)
2015-11-04 11:37:22 [scrapy] INFO: Crawled 11006 pages (at 20 pages/min), scraped 10823 items (at 18 items/min)
2015-11-04 11:38:34 [scrapy] INFO: Crawled 11024 pages (at 18 pages/min), scraped 10842 items (at 19 items/min)
2015-11-04 11:39:23 [scrapy] INFO: Crawled 11042 pages (at 18 pages/min), scraped 10853 items (at 11 items/min)
2015-11-04 11:40:31 [scrapy] INFO: Crawled 11056 pages (at 14 pages/min), scraped 10870 items (at 17 items/min)
2015-11-04 11:41:24 [scrapy] INFO: Crawled 11071 pages (at 15 pages/min), scraped 10884 items (at 14 items/min)
2015-11-04 11:42:24 [scrapy] INFO: Crawled 11088 pages (at 17 pages/min), scraped 10902 items (at 18 items/min)
2015-11-04 11:43:18 [scrapy] INFO: Crawled 11100 pages (at 12 pages/min), scraped 10912 items (at 10 items/min)
2015-11-04 11:44:19 [scrapy] INFO: Crawled 11108 pages (at 8 pages/min), scraped 10926 items (at 14 items/min)
2015-11-04 11:45:17 [scrapy] INFO: Crawled 11125 pages (at 17 pages/min), scraped 10940 items (at 14 items/min)
2015-11-04 11:46:18 [scrapy] INFO: Crawled 11140 pages (at 15 pages/min), scraped 10953 items (at 13 items/min)
2015-11-04 11:47:26 [scrapy] INFO: Crawled 11150 pages (at 10 pages/min), scraped 10966 items (at 13 items/min)
2015-11-04 11:48:19 [scrapy] INFO: Crawled 11165 pages (at 15 pages/min), scraped 10979 items (at 13 items/min)
2015-11-04 11:49:16 [scrapy] INFO: Crawled 11188 pages (at 23 pages/min), scraped 11001 items (at 22 items/min)
2015-11-04 11:50:23 [scrapy] INFO: Crawled 11203 pages (at 15 pages/min), scraped 11020 items (at 19 items/min)
2015-11-04 11:51:16 [scrapy] INFO: Crawled 11214 pages (at 11 pages/min), scraped 11030 items (at 10 items/min)
2015-11-04 11:52:57 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/english/INVESTMENTPRODUCTS/QDIIFund/GlobalSelectiveFund/GlobalSelectiveFund/index.shtml> (referer: http://www.chinaamc.com/english/home/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:52:57 [scrapy] INFO: Crawled 11225 pages (at 11 pages/min), scraped 11041 items (at 11 items/min)
2015-11-04 11:53:44 [scrapy] INFO: Crawled 11230 pages (at 5 pages/min), scraped 11044 items (at 3 items/min)
2015-11-04 11:55:51 [scrapy] INFO: Crawled 11240 pages (at 10 pages/min), scraped 11053 items (at 9 items/min)
2015-11-04 11:58:35 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/english/INVESTMENTPRODUCTS/QDIIFund/GlobalSelectiveFund/hybridfund/LargeCapSelectFunds/index.shtml> (referer: http://www.chinaamc.com/english/home/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 11:58:35 [scrapy] INFO: Crawled 11240 pages (at 0 pages/min), scraped 11057 items (at 4 items/min)
2015-11-04 11:59:24 [scrapy] INFO: Crawled 11257 pages (at 17 pages/min), scraped 11070 items (at 13 items/min)
2015-11-04 12:00:23 [scrapy] INFO: Crawled 11271 pages (at 14 pages/min), scraped 11084 items (at 14 items/min)
2015-11-04 12:01:29 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/yanglaojijin/nianjin/zengzhi/index.shtml> (referer: http://www.chinaamc.com/yanglaojijin/nianjin/nianjinyeji/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:01:29 [scrapy] INFO: Crawled 11275 pages (at 4 pages/min), scraped 11090 items (at 6 items/min)
2015-11-04 12:02:25 [scrapy] INFO: Crawled 11292 pages (at 17 pages/min), scraped 11106 items (at 16 items/min)
2015-11-04 12:03:03 [scrapy] ERROR: Error downloading <GET https://www.chinaamc.com/capital/fund/006145/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.chinaamc.com/capital/fund/006145/ took longer than 180.0 seconds..
2015-11-04 12:03:23 [scrapy] INFO: Crawled 11308 pages (at 16 pages/min), scraped 11124 items (at 18 items/min)
2015-11-04 12:04:31 [scrapy] INFO: Crawled 11325 pages (at 17 pages/min), scraped 11141 items (at 17 items/min)
2015-11-04 12:05:18 [scrapy] INFO: Crawled 11338 pages (at 13 pages/min), scraped 11151 items (at 10 items/min)
2015-11-04 12:06:20 [scrapy] INFO: Crawled 11362 pages (at 24 pages/min), scraped 11170 items (at 19 items/min)
2015-11-04 12:07:24 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/guanyu/licaifuwu/6287314.shtml> (referer: http://www.chinaamc.com/guanyu/licaifuwu/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:07:26 [scrapy] INFO: Crawled 11368 pages (at 6 pages/min), scraped 11182 items (at 12 items/min)
2015-11-04 12:10:19 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/guanyu/licaifuwu/6455790.shtml> (referer: http://www.chinaamc.com/guanyu/licaifuwu/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:10:19 [scrapy] INFO: Crawled 11370 pages (at 2 pages/min), scraped 11182 items (at 0 items/min)
2015-11-04 12:11:43 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/guanyu/licaifuwu/6279990.shtml> (referer: http://www.chinaamc.com/guanyu/licaifuwu/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:11:43 [scrapy] INFO: Crawled 11370 pages (at 0 pages/min), scraped 11183 items (at 1 items/min)
2015-11-04 12:12:26 [scrapy] INFO: Crawled 11396 pages (at 26 pages/min), scraped 11208 items (at 25 items/min)
2015-11-04 12:13:30 [scrapy] INFO: Crawled 11404 pages (at 8 pages/min), scraped 11217 items (at 9 items/min)
2015-11-04 12:15:46 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/yanglaojijin/shebao/index.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:15:46 [scrapy] INFO: Crawled 11422 pages (at 18 pages/min), scraped 11230 items (at 13 items/min)
2015-11-04 12:16:47 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/portal/cn/hxxc/2012/huobijijin/index.html?WT.ac=hqt_newamc_syzb> (referer: https://fundtrade.chinaamc.com/etrading/etrading.jsp?fundcode=000051&jysrc=fdRight&transtype=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:18:16 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/wodejijin/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:18:16 [scrapy] INFO: Crawled 11422 pages (at 0 pages/min), scraped 11232 items (at 2 items/min)
2015-11-04 12:19:20 [scrapy] INFO: Crawled 11451 pages (at 29 pages/min), scraped 11256 items (at 24 items/min)
2015-11-04 12:20:26 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/portal/cn/hxxc/2012/huobijijin/index.html> (referer: http://www.chinaamc.com/zixun/caijing/5373519.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:20:27 [scrapy] INFO: Crawled 11466 pages (at 15 pages/min), scraped 11271 items (at 15 items/min)
2015-11-04 12:21:45 [scrapy] INFO: Crawled 11478 pages (at 12 pages/min), scraped 11285 items (at 14 items/min)
2015-11-04 12:22:23 [scrapy] INFO: Crawled 11485 pages (at 7 pages/min), scraped 11293 items (at 8 items/min)
2015-11-04 12:23:48 [scrapy] ERROR: Spider error processing <GET https://www.chinaamc.com/portal/cn/register/resetpassword01.jsp> (referer: http://www.chinaamc.com/zixun/caijing/5373519.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 630, in _read_chunked
    line = self.fp.readline(_MAXLINE + 1)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 480, in readline
    data = self._sock.recv(self._rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 12:23:48 [scrapy] INFO: Crawled 11485 pages (at 0 pages/min), scraped 11293 items (at 0 items/min)
2015-11-04 12:24:20 [scrapy] INFO: Crawled 11498 pages (at 13 pages/min), scraped 11299 items (at 6 items/min)
2015-11-04 12:25:33 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/portal/cn/customerManager/reset_pwd.jsp> (referer: http://www.chinaamc.com/portal/cn/customerManager/index.jsp)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 12:25:43 [scrapy] INFO: Crawled 11501 pages (at 3 pages/min), scraped 11307 items (at 8 items/min)
2015-11-04 12:26:34 [scrapy] INFO: Crawled 11515 pages (at 14 pages/min), scraped 11319 items (at 12 items/min)
2015-11-04 12:27:44 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/jiaoyi/xiugaimima/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:27:44 [scrapy] INFO: Crawled 11516 pages (at 1 pages/min), scraped 11322 items (at 3 items/min)
2015-11-04 12:28:53 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/jiaoyi/yuyueshuhui/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:28:53 [scrapy] INFO: Crawled 11519 pages (at 3 pages/min), scraped 11322 items (at 0 items/min)
2015-11-04 12:30:56 [scrapy] INFO: Crawled 11519 pages (at 0 pages/min), scraped 11323 items (at 1 items/min)
2015-11-04 12:31:44 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/jiaoyi/guangfazhuanhuan/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:31:44 [scrapy] INFO: Crawled 11519 pages (at 0 pages/min), scraped 11323 items (at 0 items/min)
2015-11-04 12:32:35 [scrapy] INFO: Crawled 11533 pages (at 14 pages/min), scraped 11335 items (at 12 items/min)
2015-11-04 12:33:35 [scrapy] INFO: Crawled 11549 pages (at 16 pages/min), scraped 11347 items (at 12 items/min)
2015-11-04 12:35:19 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/wangshang/zhongxinditu/1271889.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:35:19 [scrapy] INFO: Crawled 11552 pages (at 3 pages/min), scraped 11354 items (at 7 items/min)
2015-11-04 12:36:23 [scrapy] INFO: Crawled 11563 pages (at 11 pages/min), scraped 11361 items (at 7 items/min)
2015-11-04 12:37:17 [scrapy] INFO: Crawled 11569 pages (at 6 pages/min), scraped 11370 items (at 9 items/min)
2015-11-04 12:38:25 [scrapy] INFO: Crawled 11586 pages (at 17 pages/min), scraped 11381 items (at 11 items/min)
2015-11-04 12:39:22 [scrapy] INFO: Crawled 11598 pages (at 12 pages/min), scraped 11395 items (at 14 items/min)
2015-11-04 12:40:37 [scrapy] INFO: Crawled 11615 pages (at 17 pages/min), scraped 11409 items (at 14 items/min)
2015-11-04 12:41:51 [scrapy] INFO: Crawled 11621 pages (at 6 pages/min), scraped 11424 items (at 15 items/min)
2015-11-04 12:42:24 [scrapy] INFO: Crawled 11636 pages (at 15 pages/min), scraped 11435 items (at 11 items/min)
2015-11-04 12:43:16 [scrapy] INFO: Crawled 11641 pages (at 5 pages/min), scraped 11444 items (at 9 items/min)
2015-11-04 12:44:22 [scrapy] INFO: Crawled 11661 pages (at 20 pages/min), scraped 11460 items (at 16 items/min)
2015-11-04 12:46:03 [scrapy] ERROR: Spider error processing <GET https://www.chinaamc.com/qyzh/> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 12:46:03 [scrapy] INFO: Crawled 11661 pages (at 0 pages/min), scraped 11463 items (at 3 items/min)
2015-11-04 12:46:29 [scrapy] INFO: Crawled 11673 pages (at 12 pages/min), scraped 11470 items (at 7 items/min)
2015-11-04 12:47:24 [scrapy] INFO: Crawled 11692 pages (at 19 pages/min), scraped 11489 items (at 19 items/min)
2015-11-04 12:48:28 [scrapy] INFO: Crawled 11709 pages (at 17 pages/min), scraped 11509 items (at 20 items/min)
2015-11-04 12:49:28 [scrapy] INFO: Crawled 11726 pages (at 17 pages/min), scraped 11522 items (at 13 items/min)
2015-11-04 12:50:30 [scrapy] INFO: Crawled 11741 pages (at 15 pages/min), scraped 11541 items (at 19 items/min)
2015-11-04 12:51:28 [scrapy] INFO: Crawled 11761 pages (at 20 pages/min), scraped 11561 items (at 20 items/min)
2015-11-04 12:52:35 [scrapy] INFO: Crawled 11778 pages (at 17 pages/min), scraped 11578 items (at 17 items/min)
2015-11-04 12:53:30 [scrapy] INFO: Crawled 11792 pages (at 14 pages/min), scraped 11592 items (at 14 items/min)
2015-11-04 12:54:18 [scrapy] INFO: Crawled 11812 pages (at 20 pages/min), scraped 11607 items (at 15 items/min)
2015-11-04 12:55:24 [scrapy] INFO: Crawled 11827 pages (at 15 pages/min), scraped 11625 items (at 18 items/min)
2015-11-04 12:56:28 [scrapy] INFO: Crawled 11842 pages (at 15 pages/min), scraped 11638 items (at 13 items/min)
2015-11-04 12:57:17 [scrapy] INFO: Crawled 11854 pages (at 12 pages/min), scraped 11650 items (at 12 items/min)
2015-11-04 12:58:34 [scrapy] INFO: Crawled 11866 pages (at 12 pages/min), scraped 11666 items (at 16 items/min)
2015-11-04 12:59:22 [scrapy] INFO: Crawled 11879 pages (at 13 pages/min), scraped 11678 items (at 12 items/min)
2015-11-04 13:00:36 [scrapy] INFO: Crawled 11897 pages (at 18 pages/min), scraped 11693 items (at 15 items/min)
2015-11-04 13:01:20 [scrapy] INFO: Crawled 11902 pages (at 5 pages/min), scraped 11703 items (at 10 items/min)
2015-11-04 13:02:19 [scrapy] INFO: Crawled 11924 pages (at 22 pages/min), scraped 11721 items (at 18 items/min)
2015-11-04 13:03:21 [scrapy] INFO: Crawled 11932 pages (at 8 pages/min), scraped 11734 items (at 13 items/min)
2015-11-04 13:04:24 [scrapy] INFO: Crawled 11957 pages (at 25 pages/min), scraped 11753 items (at 19 items/min)
2015-11-04 13:05:29 [scrapy] INFO: Crawled 11974 pages (at 17 pages/min), scraped 11774 items (at 21 items/min)
2015-11-04 13:06:26 [scrapy] INFO: Crawled 12001 pages (at 27 pages/min), scraped 11789 items (at 15 items/min)
2015-11-04 13:07:30 [scrapy] INFO: Crawled 12014 pages (at 13 pages/min), scraped 11813 items (at 24 items/min)
2015-11-04 13:08:18 [scrapy] INFO: Crawled 12031 pages (at 17 pages/min), scraped 11826 items (at 13 items/min)
2015-11-04 13:09:21 [scrapy] INFO: Crawled 12039 pages (at 8 pages/min), scraped 11841 items (at 15 items/min)
2015-11-04 13:10:47 [scrapy] INFO: Crawled 12077 pages (at 38 pages/min), scraped 11871 items (at 30 items/min)
2015-11-04 13:11:17 [scrapy] INFO: Crawled 12078 pages (at 1 pages/min), scraped 11879 items (at 8 items/min)
2015-11-04 13:12:16 [scrapy] INFO: Crawled 12092 pages (at 14 pages/min), scraped 11893 items (at 14 items/min)
2015-11-04 13:13:20 [scrapy] INFO: Crawled 12118 pages (at 26 pages/min), scraped 11917 items (at 24 items/min)
2015-11-04 13:14:16 [scrapy] INFO: Crawled 12126 pages (at 8 pages/min), scraped 11928 items (at 11 items/min)
2015-11-04 13:15:18 [scrapy] INFO: Crawled 12154 pages (at 28 pages/min), scraped 11949 items (at 21 items/min)
2015-11-04 13:16:32 [scrapy] INFO: Crawled 12175 pages (at 21 pages/min), scraped 11972 items (at 23 items/min)
2015-11-04 13:17:18 [scrapy] INFO: Crawled 12179 pages (at 4 pages/min), scraped 11980 items (at 8 items/min)
2015-11-04 13:18:24 [scrapy] INFO: Crawled 12207 pages (at 28 pages/min), scraped 12001 items (at 21 items/min)
2015-11-04 13:19:26 [scrapy] INFO: Crawled 12228 pages (at 21 pages/min), scraped 12024 items (at 23 items/min)
2015-11-04 13:20:20 [scrapy] INFO: Crawled 12248 pages (at 20 pages/min), scraped 12044 items (at 20 items/min)
2015-11-04 13:21:25 [scrapy] INFO: Crawled 12271 pages (at 23 pages/min), scraped 12066 items (at 22 items/min)
2015-11-04 13:22:23 [scrapy] INFO: Crawled 12287 pages (at 16 pages/min), scraped 12086 items (at 20 items/min)
2015-11-04 13:23:19 [scrapy] INFO: Crawled 12306 pages (at 19 pages/min), scraped 12103 items (at 17 items/min)
2015-11-04 13:24:26 [scrapy] INFO: Crawled 12331 pages (at 25 pages/min), scraped 12119 items (at 16 items/min)
2015-11-04 13:25:22 [scrapy] INFO: Crawled 12341 pages (at 10 pages/min), scraped 12142 items (at 23 items/min)
2015-11-04 13:26:20 [scrapy] INFO: Crawled 12361 pages (at 20 pages/min), scraped 12155 items (at 13 items/min)
2015-11-04 13:27:16 [scrapy] INFO: Crawled 12365 pages (at 4 pages/min), scraped 12163 items (at 8 items/min)
2015-11-04 13:28:41 [scrapy] INFO: Crawled 12379 pages (at 14 pages/min), scraped 12181 items (at 18 items/min)
2015-11-04 13:29:42 [scrapy] INFO: Crawled 12404 pages (at 25 pages/min), scraped 12192 items (at 11 items/min)
2015-11-04 13:30:23 [scrapy] INFO: Crawled 12427 pages (at 23 pages/min), scraped 12222 items (at 30 items/min)
2015-11-04 13:31:19 [scrapy] INFO: Crawled 12449 pages (at 22 pages/min), scraped 12248 items (at 26 items/min)
2015-11-04 13:32:25 [scrapy] INFO: Crawled 12465 pages (at 16 pages/min), scraped 12262 items (at 14 items/min)
2015-11-04 13:33:18 [scrapy] INFO: Crawled 12479 pages (at 14 pages/min), scraped 12274 items (at 12 items/min)
2015-11-04 13:34:22 [scrapy] INFO: Crawled 12487 pages (at 8 pages/min), scraped 12289 items (at 15 items/min)
2015-11-04 13:35:22 [scrapy] INFO: Crawled 12503 pages (at 16 pages/min), scraped 12304 items (at 15 items/min)
2015-11-04 13:36:19 [scrapy] INFO: Crawled 12517 pages (at 14 pages/min), scraped 12317 items (at 13 items/min)
2015-11-04 13:37:21 [scrapy] INFO: Crawled 12537 pages (at 20 pages/min), scraped 12333 items (at 16 items/min)
2015-11-04 13:38:17 [scrapy] INFO: Crawled 12551 pages (at 14 pages/min), scraped 12346 items (at 13 items/min)
2015-11-04 13:39:28 [scrapy] INFO: Crawled 12567 pages (at 16 pages/min), scraped 12364 items (at 18 items/min)
2015-11-04 13:40:23 [scrapy] INFO: Crawled 12579 pages (at 12 pages/min), scraped 12375 items (at 11 items/min)
2015-11-04 13:41:33 [scrapy] INFO: Crawled 12587 pages (at 8 pages/min), scraped 12389 items (at 14 items/min)
2015-11-04 13:42:18 [scrapy] INFO: Crawled 12603 pages (at 16 pages/min), scraped 12399 items (at 10 items/min)
2015-11-04 13:43:20 [scrapy] INFO: Crawled 12617 pages (at 14 pages/min), scraped 12418 items (at 19 items/min)
2015-11-04 13:44:18 [scrapy] INFO: Crawled 12635 pages (at 18 pages/min), scraped 12437 items (at 19 items/min)
2015-11-04 13:45:36 [scrapy] INFO: Crawled 12649 pages (at 14 pages/min), scraped 12451 items (at 14 items/min)
2015-11-04 13:46:29 [scrapy] INFO: Crawled 12666 pages (at 17 pages/min), scraped 12466 items (at 15 items/min)
2015-11-04 13:47:20 [scrapy] INFO: Crawled 12680 pages (at 14 pages/min), scraped 12475 items (at 9 items/min)
2015-11-04 13:48:37 [scrapy] INFO: Crawled 12700 pages (at 20 pages/min), scraped 12502 items (at 27 items/min)
2015-11-04 13:49:18 [scrapy] INFO: Crawled 12715 pages (at 15 pages/min), scraped 12513 items (at 11 items/min)
2015-11-04 13:50:23 [scrapy] INFO: Crawled 12731 pages (at 16 pages/min), scraped 12526 items (at 13 items/min)
2015-11-04 13:51:27 [scrapy] INFO: Crawled 12748 pages (at 17 pages/min), scraped 12548 items (at 22 items/min)
2015-11-04 13:52:17 [scrapy] INFO: Crawled 12784 pages (at 36 pages/min), scraped 12573 items (at 25 items/min)
2015-11-04 13:53:17 [scrapy] INFO: Crawled 12814 pages (at 30 pages/min), scraped 12610 items (at 37 items/min)
2015-11-04 13:54:17 [scrapy] INFO: Crawled 12815 pages (at 1 pages/min), scraped 12617 items (at 7 items/min)
2015-11-04 13:55:25 [scrapy] INFO: Crawled 12847 pages (at 32 pages/min), scraped 12638 items (at 21 items/min)
2015-11-04 13:56:17 [scrapy] INFO: Crawled 12855 pages (at 8 pages/min), scraped 12657 items (at 19 items/min)
2015-11-04 13:57:32 [scrapy] INFO: Crawled 12870 pages (at 15 pages/min), scraped 12672 items (at 15 items/min)
2015-11-04 13:58:21 [scrapy] INFO: Crawled 12883 pages (at 13 pages/min), scraped 12685 items (at 13 items/min)
2015-11-04 13:59:26 [scrapy] INFO: Crawled 12905 pages (at 22 pages/min), scraped 12707 items (at 22 items/min)
2015-11-04 14:00:20 [scrapy] INFO: Crawled 12918 pages (at 13 pages/min), scraped 12719 items (at 12 items/min)
2015-11-04 14:01:17 [scrapy] INFO: Crawled 12939 pages (at 21 pages/min), scraped 12738 items (at 19 items/min)
2015-11-04 14:02:16 [scrapy] INFO: Crawled 12956 pages (at 17 pages/min), scraped 12757 items (at 19 items/min)
2015-11-04 14:03:17 [scrapy] INFO: Crawled 12972 pages (at 16 pages/min), scraped 12774 items (at 17 items/min)
2015-11-04 14:04:29 [scrapy] INFO: Crawled 12991 pages (at 19 pages/min), scraped 12791 items (at 17 items/min)
2015-11-04 14:05:27 [scrapy] INFO: Crawled 13005 pages (at 14 pages/min), scraped 12805 items (at 14 items/min)
2015-11-04 14:06:19 [scrapy] INFO: Crawled 13016 pages (at 11 pages/min), scraped 12817 items (at 12 items/min)
2015-11-04 14:07:26 [scrapy] INFO: Crawled 13031 pages (at 15 pages/min), scraped 12831 items (at 14 items/min)
2015-11-04 14:08:44 [scrapy] INFO: Crawled 13049 pages (at 18 pages/min), scraped 12846 items (at 15 items/min)
2015-11-04 14:09:19 [scrapy] INFO: Crawled 13053 pages (at 4 pages/min), scraped 12853 items (at 7 items/min)
2015-11-04 14:10:33 [scrapy] INFO: Crawled 13071 pages (at 18 pages/min), scraped 12873 items (at 20 items/min)
2015-11-04 14:11:27 [scrapy] INFO: Crawled 13090 pages (at 19 pages/min), scraped 12885 items (at 12 items/min)
2015-11-04 14:12:35 [scrapy] INFO: Crawled 13105 pages (at 15 pages/min), scraped 12903 items (at 18 items/min)
2015-11-04 14:13:18 [scrapy] INFO: Crawled 13120 pages (at 15 pages/min), scraped 12914 items (at 11 items/min)
2015-11-04 14:14:22 [scrapy] INFO: Crawled 13128 pages (at 8 pages/min), scraped 12930 items (at 16 items/min)
2015-11-04 14:15:23 [scrapy] INFO: Crawled 13148 pages (at 20 pages/min), scraped 12948 items (at 18 items/min)
2015-11-04 14:16:33 [scrapy] INFO: Crawled 13162 pages (at 14 pages/min), scraped 12962 items (at 14 items/min)
2015-11-04 14:17:20 [scrapy] INFO: Crawled 13183 pages (at 21 pages/min), scraped 12978 items (at 16 items/min)
2015-11-04 14:18:16 [scrapy] INFO: Crawled 13189 pages (at 6 pages/min), scraped 12991 items (at 13 items/min)
2015-11-04 14:19:18 [scrapy] INFO: Crawled 13204 pages (at 15 pages/min), scraped 13004 items (at 13 items/min)
2015-11-04 14:21:39 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/guanyu/gonggao/6550009.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:21:39 [scrapy] INFO: Crawled 13221 pages (at 17 pages/min), scraped 13019 items (at 15 items/min)
2015-11-04 14:22:58 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/zixun/caijing/6166161.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:22:58 [scrapy] INFO: Crawled 13223 pages (at 2 pages/min), scraped 13021 items (at 2 items/min)
2015-11-04 14:23:13 [scrapy] ERROR: Error downloading <GET http://www.hig>: DNS lookup failed: address 'www.hig' not found: [Errno -2] Name or service not known.
2015-11-04 14:23:13 [scrapy] ERROR: Error downloading <GET http://www.rid>: DNS lookup failed: address 'www.rid' not found: [Errno -2] Name or service not known.
2015-11-04 14:23:13 [scrapy] ERROR: Error downloading <GET http://www.fun>: DNS lookup failed: address 'www.fun' not found: [Errno -2] Name or service not known.
2015-11-04 14:23:13 [scrapy] ERROR: Error downloading <GET http://www.enhancedcapct.com>: DNS lookup failed: address 'www.enhancedcapct.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:23:16 [scrapy] INFO: Crawled 13232 pages (at 9 pages/min), scraped 13026 items (at 5 items/min)
2015-11-04 14:23:59 [scrapy] ERROR: Error downloading <GET http://www.csvpartners.com>: DNS lookup failed: address 'www.csvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:25:47 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3588917.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:25:47 [scrapy] INFO: Crawled 13254 pages (at 22 pages/min), scraped 13039 items (at 13 items/min)
2015-11-04 14:26:19 [scrapy] ERROR: Error downloading <GET https://creditsuisse.taleo.net/careersection/external/mysubmissions.ftl?lang=en>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 14:26:20 [scrapy] INFO: Crawled 13254 pages (at 0 pages/min), scraped 13047 items (at 8 items/min)
2015-11-04 14:26:23 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/us/en/careers/campus-recruiting.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:27:26 [scrapy] INFO: Crawled 13274 pages (at 20 pages/min), scraped 13065 items (at 18 items/min)
2015-11-04 14:28:23 [scrapy] INFO: Crawled 13294 pages (at 20 pages/min), scraped 13083 items (at 18 items/min)
2015-11-04 14:28:49 [scrapy] ERROR: Error downloading <GET http://www.adelphi-europe.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:28:49 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/co/en/about-us/responsibility/economy-society/global-commitments/icrc-humanitarian-aid.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:28:54 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/uy/en/about-us/research/research-institute.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:28:54 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/uy/en/about-us/research/research-institute/publications.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:28:54 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/co/en/about-us/responsibility/economy-society/focus-themes/employee-engagement.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:28:54 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/uy/en/about-us/research/research-institute/thought-leaders.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:28:54 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/uy/en/about-us/research/research-institute/news-and-videos.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:29:20 [scrapy] INFO: Crawled 13305 pages (at 11 pages/min), scraped 13096 items (at 13 items/min)
2015-11-04 14:30:26 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/co/en/about-us/responsibility/dialogue.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:30:26 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/cl/en/about-us/responsibility/news-stories.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:30:26 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/co/en/about-us/governance/shareholders/significant-shareholders.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:30:26 [scrapy] INFO: Crawled 13310 pages (at 5 pages/min), scraped 13105 items (at 9 items/min)
2015-11-04 14:30:26 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/co/en/about-us/governance/shareholders/executive-holdings.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:30:26 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/co/en/about-us/governance/executive-board.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:30:26 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/caijing/6158845.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:30:26 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/co/en/about-us/governance/shareholders.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:31:28 [scrapy] INFO: Crawled 13326 pages (at 16 pages/min), scraped 13121 items (at 16 items/min)
2015-11-04 14:31:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/cl/en/about-us/responsibility/dialogue/youth-barometer/download-center.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:31:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sites/office-locator.html?country=ca&lang=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:31:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/legal/privacy-and-cookie-policy.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:31:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/cl/en/about-us/responsibility/dialogue/youth-barometer/article-archive.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:31:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ca/en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:31:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/legal/global-patriot-act.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:32:21 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/gongju/lccpdqrcx/index.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:32:21 [scrapy] INFO: Crawled 13343 pages (at 17 pages/min), scraped 13131 items (at 10 items/min)
2015-11-04 14:33:17 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/portal/cn/hxxc/fxq/gnfl/page08.html> (referer: http://www.chinaamc.com/portal/cn/hxxc/fxq/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/4516247.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/4516247.shtml took longer than 180.0 seconds..
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/caijing/6158756.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zixun/caijing/6158756.shtml took longer than 180.0 seconds..
2015-11-04 14:33:21 [scrapy] INFO: Crawled 13345 pages (at 2 pages/min), scraped 13138 items (at 7 items/min)
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pa/en/about-us/our-company/global-reach/switzerland-and-credit-suisse.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pa/en/about-us/our-company/global-reach.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ca/en/about-us/governance.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ca/en/about-us/governance/standards-and-policies.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ca/en/about-us/governance/board-of-directors/board-of-directors-independence.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:21 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/wangshang/index.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ca/en/about-us/governance/standards-and-policies/code-of-conduct.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/about-us/governance/board-of-directors/meeting-attendance.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/gonggao/6549642.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/gonggao/6549642.shtml took longer than 180.0 seconds..
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/product/fundsDingtoutiyanjisuanqitwo.do>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/product/fundsDingtoutiyanjisuanqitwo.do took longer than 180.0 seconds..
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/about-us/governance/board-of-directors/board-of-directors-independence.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ca/en/about-us/media/news.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/about-us/governance/board-of-directors/committees.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/about-us/investor-relations/financial-disclosures/financial-reports/finacial-reports-2008.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/about-us/investor-relations/financial-disclosures/financial-reports/finacial-reports-2007.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pe/en/about-us/investor-relations/events-presentations.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:37 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ca/en/about-us/media.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:33:38 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3941732.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:34:21 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/gonggao/6550005.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/gonggao/6550005.shtml took longer than 180.0 seconds..
2015-11-04 14:34:21 [scrapy] INFO: Crawled 13365 pages (at 20 pages/min), scraped 13154 items (at 16 items/min)
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mx/en/about-us/responsibility/employer/talent-development.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/index.shtml took longer than 180.0 seconds..
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mx/en/about-us/responsibility/employer/global-diversity-inclusion.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mx/en/careers.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mx/en/news-and-expertise/switzerland.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/us/en/about-us/investor-relations/financial-disclosures/financial-reports/finacial-reports-2008.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mx/en/news-and-expertise/publications.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mx/en/news-and-expertise/follow-us.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:34:56 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mx/en/careers/experienced-professionals.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:35:19 [scrapy] INFO: Crawled 13380 pages (at 15 pages/min), scraped 13169 items (at 15 items/min)
2015-11-04 14:35:37 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/jiaoyi/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/jiaoyi/index.shtml took longer than 180.0 seconds..
2015-11-04 14:35:49 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3941683.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3941683.shtml took longer than 180.0 seconds..
2015-11-04 14:35:49 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3941688.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3941688.shtml took longer than 180.0 seconds..
2015-11-04 14:35:55 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/content/pwp/www-root/za/en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:35:55 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/content/pwp/www-root/ru/en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:35:55 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/content/pwp/www-root/sa/en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:35:55 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/cn/sc/careers/campus-recruiting-redirect-apac.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:36:20 [scrapy] INFO: Crawled 13398 pages (at 18 pages/min), scraped 13191 items (at 22 items/min)
2015-11-04 14:36:38 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3941733.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/redianwenti/jiaoyi/kaihu/3941733.shtml took longer than 180.0 seconds..
2015-11-04 14:36:38 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/wendang/feilv/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/wendang/feilv/index.shtml took longer than 180.0 seconds..
2015-11-04 14:36:38 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/fuwujieshao/fuwujieshao/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/fuwujieshao/fuwujieshao/index.shtml took longer than 180.0 seconds..
2015-11-04 14:37:49 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/wendang/zhixiaozhanghu/index.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:37:49 [scrapy] INFO: Crawled 13407 pages (at 9 pages/min), scraped 13200 items (at 9 items/min)
2015-11-04 14:37:50 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/index.shtml took longer than 180.0 seconds..
2015-11-04 14:38:16 [scrapy] INFO: Crawled 13407 pages (at 0 pages/min), scraped 13200 items (at 0 items/min)
2015-11-04 14:38:35 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/wendang/daixiao/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/wendang/daixiao/index.shtml took longer than 180.0 seconds..
2015-11-04 14:38:37 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/redianwenti/chanpin/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/redianwenti/chanpin/index.shtml took longer than 180.0 seconds..
2015-11-04 14:38:37 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/caozuo/kaihu/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/caozuo/kaihu/index.shtml took longer than 180.0 seconds..
2015-11-04 14:39:31 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/gongju/fengxianceshi/index.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:39:31 [scrapy] INFO: Crawled 13410 pages (at 3 pages/min), scraped 13200 items (at 0 items/min)
2015-11-04 14:41:01 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/portal/cn/hxxc/fxq/gnfl/page14.html> (referer: http://www.chinaamc.com/portal/cn/hxxc/fxq/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:41:01 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/index.shtml took longer than 180.0 seconds..
2015-11-04 14:41:01 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/dibu/wzdt/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/dibu/wzdt/index.shtml took longer than 180.0 seconds..
2015-11-04 14:41:01 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/research.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:41:01 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/sponsorship/art/goya-portraits.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:41:01 [scrapy] INFO: Crawled 13412 pages (at 2 pages/min), scraped 13201 items (at 1 items/min)
2015-11-04 14:41:55 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/portal/cn/hxxc/fxq/index4.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]
2015-11-04 14:42:08 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/falv/1715626.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zixun/falv/1715626.shtml took longer than 180.0 seconds..
2015-11-04 14:42:08 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/product/fundsShengoufeiyongtwo.do?type=shengou>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/product/fundsShengoufeiyongtwo.do?type=shengou took longer than 180.0 seconds..
2015-11-04 14:42:08 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/portal/cn/hxxc/fxq/gnfl/page13.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>, <twisted.python.failure.Failure twisted.web.http._DataLoss: >]
2015-11-04 14:42:08 [scrapy] INFO: Crawled 13413 pages (at 1 pages/min), scraped 13203 items (at 2 items/min)
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/sponsorship/swiss-football.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/sponsorship/further-commitments.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/sponsorship/classic-cars.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/research/research-institute/news-and-videos.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:27 [scrapy] INFO: Crawled 13414 pages (at 1 pages/min), scraped 13204 items (at 1 items/min)
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/sponsorship/classical-music.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ae/en/about-us/investor-relations/information-for-debt-investors/low-trigger-capital-instruments.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/sponsorship/art.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:27 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/portal/cn/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/dongtai/6549638.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/jjdc/5907873.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zixun/jjdc/5907873.shtml took longer than 180.0 seconds..
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/jjdc/5694847.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zixun/jjdc/5694847.shtml took longer than 180.0 seconds..
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/responsibility/approach-reporting.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/responsibility/approach-reporting/message-from-the-ceo-and-the-chairman.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/responsibility.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/responsibility/news-stories.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ae/en/about-us/our-company.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:43:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tr/en/about-us/responsibility/news-stories/newsletter.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:44:03 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/wendang/fengxiantishi/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/wendang/fengxiantishi/index.shtml took longer than 180.0 seconds..
2015-11-04 14:44:03 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/product/fundsTouzishouyijisuanqitwo.do>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/product/fundsTouzishouyijisuanqitwo.do took longer than 180.0 seconds..
2015-11-04 14:44:03 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/jjdc/6486359.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zixun/jjdc/6486359.shtml took longer than 180.0 seconds..
2015-11-04 14:44:03 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/jjdc/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zixun/jjdc/index.shtml took longer than 180.0 seconds..
2015-11-04 14:44:48 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/portal/cn/hxxc/fxq/index2.html> (referer: http://www.chinaamc.com/portal/cn/hxxc/fxq/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:45:07 [scrapy] INFO: Crawled 13426 pages (at 12 pages/min), scraped 13213 items (at 9 items/min)
2015-11-04 14:46:14 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/se/en/about-us/responsibility/economy-society/commitments-in-emea.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:46:14 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/se/en/about-us/responsibility/economy-society/commitments-in-americas.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:46:14 [scrapy] INFO: Crawled 13434 pages (at 8 pages/min), scraped 13216 items (at 3 items/min)
2015-11-04 14:46:37 [scrapy] INFO: Crawled 13434 pages (at 0 pages/min), scraped 13221 items (at 5 items/min)
2015-11-04 14:48:11 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/zixun/dongtai/index.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zixun/dongtai/6324919.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zixun/dongtai/6324919.shtml took longer than 180.0 seconds..
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/1267026.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/1267026.shtml took longer than 180.0 seconds..
2015-11-04 14:49:24 [scrapy] INFO: Crawled 13435 pages (at 1 pages/min), scraped 13223 items (at 2 items/min)
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/se/en/careers/experienced-professionals/search-apply.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/se/en/careers/experienced-professionals/meet-our-people.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/jp/ja/about-us/profiles/profile-tb/private-banking/services/products-and-services.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/se/en/careers/experienced-professionals/search-apply/how-to-apply.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/se/en/careers/experienced-professionals/search-apply/check-application.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:24 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/jp/ja/about-us/profiles/profile-tb/private-banking/client.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/wangshang/zhongxinditu/1271869.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/wangshang/zhongxinditu/1271869.shtml took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/wangshang/zhongxinditu/1271886.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/wangshang/zhongxinditu/1271886.shtml took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/fuwujieshao/duanxin/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/fuwujieshao/duanxin/index.shtml took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/fuwujieshao/dianhualiucheng/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/fuwujieshao/dianhualiucheng/index.shtml took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/fuwujieshao/tousujianyi/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/fuwujieshao/tousujianyi/index.shtml took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/jp/ja/about-us/profiles/profile-tb/private-banking/contact.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/jp/ja/about-us/profiles/profile-tb/private-banking/contact.html took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/jp/ja/about-us/profiles/profile-tb/private-banking/client/find-us.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/jp/ja/about-us/profiles/profile-tb/private-banking/client/find-us.html took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/wendang/mztk/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/wendang/mztk/index.shtml took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/fuwujieshao/youjian/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/fuwujieshao/youjian/index.shtml took longer than 180.0 seconds..
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us/sponsorship.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us/responsibility/dialogue/youth-barometer/article-archive.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us/responsibility/dialogue/youth-barometer/download-center.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hk/en/about-us/responsibility/dialogue.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hk/en/about-us/sponsorship/further-commitments.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us/responsibility/dialogue/youth-barometer/expertise-and-dialogue.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us/responsibility/dialogue/swiss-worry-barometer.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:49:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hk/en/about-us/research/research-institute.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:50:18 [scrapy] INFO: Crawled 13451 pages (at 16 pages/min), scraped 13231 items (at 8 items/min)
2015-11-04 14:50:24 [scrapy] ERROR: Error downloading <GET https://au-services.credit-suisse.com/cs/ibip/frontend/c/cls/auth>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:53:04 [scrapy] INFO: Crawled 13463 pages (at 12 pages/min), scraped 13243 items (at 12 items/min)
2015-11-04 14:53:04 [scrapy] ERROR: Error downloading <GET https://bs-services.credit-suisse.com/cs/ibip/frontend/bahamas/c/cls/auth?language=en>: DNS lookup failed: address 'bs-services.credit-suisse.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:55:39 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/jiaoyi/zhuanhuan/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:56:33 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/caozuo/chedan/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/jiaoyi/fenhongfangshi/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:57:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/careers/campus-recruiting/apply-now/Campus%20Events.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:57:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us/our-company/businesses.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:57:47 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/wendang/quanyixuzhi/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/wendang/quanyixuzhi/index.shtml took longer than 180.0 seconds..
2015-11-04 14:57:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hk/en/careers/campus-recruiting.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:57:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:57:47 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hk/en/about-us/investor-relations/financial-news.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:57:47 [scrapy] INFO: Crawled 13465 pages (at 2 pages/min), scraped 13249 items (at 6 items/min)
2015-11-04 14:57:48 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/au/en/about-us/our-company.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:58:45 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/kaihu/shanghai/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:59:30 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/kaihu/zhaoshang/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hk/en/about-us/investor-relations/corporate-information.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/hk/en/about-us/investor-relations/corporate-information.html took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/zhifubao/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/zhifubao/index.shtml took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hk/en/about-us/media/publications.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/hk/en/about-us/media/publications.html took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/portal/cn/dzdzddz/eBill.jsp>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/portal/cn/dzdzddz/eBill.jsp took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/portal/cn/hxxc/ydkhd/mobile/index.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/portal/cn/hxxc/ydkhd/mobile/index.html took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/hxyx/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/hxyx/index.shtml took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/caifu/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/caifu/index.shtml took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/sponsorship/classical-music.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/id/en/about-us/sponsorship/classical-music.html took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/sponsorship/roger-federer.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/id/en/about-us/sponsorship/roger-federer.html took longer than 180.0 seconds..
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.chinaamc.com/portal/cn/dhys/flash/wscxzc/wscxzc_demo.htm>: User timeout caused connection failure.
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/sponsorship.html>: User timeout caused connection failure.
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/sponsorship/news-and-stories.html>: User timeout caused connection failure.
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/responsibility/economy-society/commitments-in-switzerland/volunteering.html>: User timeout caused connection failure.
2015-11-04 14:59:30 [scrapy] INFO: Crawled 13465 pages (at 0 pages/min), scraped 13249 items (at 0 items/min)
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/about-us/responsibility/economy-society.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/tw/en/about-us/governance/executive-board.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/about-us/responsibility/banking/accessibility.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/responsibility/economy-society/commitments-in-emea.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/about-us/responsibility/banking/suppliers.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/about-us/responsibility/economy-society/focus-themes.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/about-us/responsibility/banking/automatic-exchange-of-information.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/bs/en/about-us/responsibility/environment.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:31 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/wenzhouyinhang/index.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:59:44 [scrapy] ERROR: Error downloading <GET https://www.chinaamc.com/portal/cn/register/reg.jsp>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:59:57 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/guangda/index.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 15:00:48 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/bangzhu/kaihu/huifu/index.shtml> (referer: http://www.chinaamc.com/bangzhu/fuwujieshao/wangdian/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:00:51 [scrapy] INFO: Crawled 13485 pages (at 20 pages/min), scraped 13263 items (at 14 items/min)
2015-11-04 15:01:34 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/pingan/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/pingan/index.shtml took longer than 180.0 seconds..
2015-11-04 15:01:34 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/zheshang/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/zheshang/index.shtml took longer than 180.0 seconds..
2015-11-04 15:01:34 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/nanjingyinhang/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/nanjingyinhang/index.shtml took longer than 180.0 seconds..
2015-11-04 15:01:34 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/zhongxin/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/zhongxin/index.shtml took longer than 180.0 seconds..
2015-11-04 15:01:34 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/about-us/investor-relations/events-presentations/annual-general-meeting/agm2008.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:01:34 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/investor-relations/events-presentations/annual-general-meeting/agm2014.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:01:34 [scrapy] INFO: Crawled 13486 pages (at 1 pages/min), scraped 13268 items (at 5 items/min)
2015-11-04 15:01:38 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/investor-relations/events-presentations/annual-general-meeting/agm2015.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:01:38 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/id/en/about-us/investor-relations/events-presentations/annual-general-meeting.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:01:38 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sg/en/about-us/responsibility/banking/suppliers.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:01:38 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sg/en/about-us/responsibility/banking/sustainability-commitments/sustainable-development-goals.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:01:38 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sg/en/about-us/responsibility/banking/sustainable-products-services.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:01:38 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ph/en/about-us/responsibility/approach-reporting/gri-content-index.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:04:06 [scrapy] INFO: Crawled 13506 pages (at 20 pages/min), scraped 13280 items (at 12 items/min)
2015-11-04 15:04:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/pufa/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/pufa/index.shtml took longer than 180.0 seconds..
2015-11-04 15:04:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/guangfa/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/guangfa/index.shtml took longer than 180.0 seconds..
2015-11-04 15:04:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/jianhang/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/kaihu/jianhang/index.shtml took longer than 180.0 seconds..
2015-11-04 15:04:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/wendang/jiaoyixieyi/1774259.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/bangzhu/wendang/jiaoyixieyi/1774259.shtml took longer than 180.0 seconds..
2015-11-04 15:04:30 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/portal/cn/include/newproducthome.jsp>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/portal/cn/include/newproducthome.jsp took longer than 180.0 seconds..
2015-11-04 15:04:30 [scrapy] ERROR: Error downloading <GET https://www.chinaamc.com/qyzh/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.chinaamc.com/qyzh/ took longer than 180.0 seconds..
2015-11-04 15:04:30 [scrapy] INFO: Crawled 13506 pages (at 0 pages/min), scraped 13288 items (at 8 items/min)
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/careers/campus-recruiting-redirect-apac.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sg/en/about-us/investor-relations/corporate-information/legal-structure.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sg/en/about-us/investor-relations/financial-disclosures/quarterly-results.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sg/en/about-us/investor-relations/corporate-information/acquisitions-and-divestitures.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/sg/en/about-us/investor-relations/financial-disclosures.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/in/en/careers/experienced-professionals/job-areas-in-india/mumbai.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/ took longer than 180.0 seconds..
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ph/en/about-us/investor-relations/financial-disclosures/quarterly-results/quarterly-results-2011.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ph/en/about-us/investor-relations/financial-disclosures/quarterly-results/quarterly-results-2012.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:05:05 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/bangzhu/kaihu/gonghang/index.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 15:05:22 [scrapy] INFO: Crawled 13527 pages (at 21 pages/min), scraped 13296 items (at 8 items/min)
2015-11-04 15:06:59 [scrapy] INFO: Crawled 13547 pages (at 20 pages/min), scraped 13323 items (at 27 items/min)
2015-11-04 15:06:59 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/wodefuwu/index.shtml>: TCP connection timed out: 110: Connection timed out.
2015-11-04 15:08:20 [scrapy] ERROR: Spider error processing <GET https://www.chinaamc.com/wodejijin/index.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 15:09:13 [scrapy] ERROR: Spider error processing <GET https://www.chinaamc.com/wodejijin/zhcx/index.shtml> (referer: http://www.chinaamc.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 15:10:28 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/guanyu/licaifuwu/6279368.shtml> (referer: http://www.chinaamc.com/guanyu/licaifuwu/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/th/en/about-us/investor-relations.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/th/en/about-us/media/pictures.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/th/en/careers.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/bin/mvc.do/country/select?target=%2Fcontent%2Fpwp%2Fwww-root%2Fie%2Fen>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:10:28 [scrapy] INFO: Crawled 13547 pages (at 0 pages/min), scraped 13327 items (at 4 items/min)
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET https://creditsuisse.taleo.net/careersection/external/mysubmissions.ftl?lang=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zhuanhu/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zhuanhu/index.shtml took longer than 180.0 seconds..
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/xuanjizhongxin/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/xuanjizhongxin/index.shtml took longer than 180.0 seconds..
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/index.shtml took longer than 180.0 seconds..
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6287404.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/6287404.shtml took longer than 180.0 seconds..
2015-11-04 15:10:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/th/en/news-and-expertise/switzerland.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/guanyu/licaifuwu/6401321.shtml> (referer: http://www.chinaamc.com/guanyu/licaifuwu/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us/governance/board-of-directors/board-of-directors-independence.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/no/en/about-us/governance/board-of-directors/board-of-directors-independence.html took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us/governance/board-of-directors/committees.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/no/en/about-us/governance/board-of-directors/committees.html took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6317297.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/6317297.shtml took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6325538.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/6325538.shtml took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6486091.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/6486091.shtml took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us/governance/board-of-directors.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/no/en/about-us/governance/board-of-directors.html took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us/governance/standards-and-policies/code-of-conduct.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/no/en/about-us/governance/standards-and-policies/code-of-conduct.html took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us/investor-relations/share-information.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/no/en/about-us/investor-relations/share-information.html took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us/investor-relations/events-presentations/annual-general-meeting/agm2007.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/no/en/about-us/investor-relations/events-presentations/annual-general-meeting/agm2007.html took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] INFO: Crawled 13548 pages (at 1 pages/min), scraped 13327 items (at 0 items/min)
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/no/en/about-us.html took longer than 180.0 seconds..
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/news-and-expertise/economy.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/about-us/our-company.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/products-and-services/investment-banking.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/je/en/about-us/responsibility/environment/climate-protection.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/news-and-expertise.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/products-and-services.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/no/en/products-and-services/asset-management.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:12:08 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6501824.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 15:12:29 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/wenhua/index.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 15:12:29 [scrapy] INFO: Crawled 13562 pages (at 14 pages/min), scraped 13333 items (at 6 items/min)
2015-11-04 15:12:44 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6486547.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 15:13:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/de/de/about-us/responsibility/banking/agreements-and-memberships.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:13:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/de/de/about-us/responsibility/banking/sustainable-products-services.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:13:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/at/de/about-us/responsibility/economy-society/commitments-in-switzerland/jubilee-fund.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:13:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/at/de/about-us/responsibility/economy-society/commitments-in-emea.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:13:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/de/de/about-us/investor-relations/corporate-information/own-credit-gains-and-losses.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:13:08 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/at/de/about-us/responsibility/economy-society/commitments-in-switzerland/volunteering.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:13:32 [scrapy] INFO: Crawled 13579 pages (at 17 pages/min), scraped 13353 items (at 20 items/min)
2015-11-04 15:13:39 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6502057.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/6502057.shtml took longer than 180.0 seconds..
2015-11-04 15:13:39 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6541435.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/6541435.shtml took longer than 180.0 seconds..
2015-11-04 15:13:39 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/6448202.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/6448202.shtml took longer than 180.0 seconds..
2015-11-04 15:13:39 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/licaifuwu/1711180.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/licaifuwu/1711180.shtml took longer than 180.0 seconds..
2015-11-04 15:13:39 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/gongyi/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/gongyi/index.shtml took longer than 180.0 seconds..
2015-11-04 15:14:09 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/il/en/about-us/responsibility/environment/climate-protection.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:14:09 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/il/en/about-us/responsibility/economy-society/focus-themes/education.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:14:09 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/il/en/about-us/responsibility/economy-society/focus-themes/employee-engagement.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:14:09 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/li/en/about-us/investor-relations/investor-relations-team.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:14:09 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/il/en/about-us/responsibility/economy-society/focus-themes/microfinance.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:14:09 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/il/en/about-us/responsibility/approach-reporting/sustainability-ratings-indexes.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:14:25 [scrapy] INFO: Crawled 13595 pages (at 16 pages/min), scraped 13370 items (at 17 items/min)
2015-11-04 15:14:31 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=4041903>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 15:15:03 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mc/en/about-us/responsibility/news-stories/newsletter_msm_moved.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:15:03 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mc/en/about-us/responsibility/news-stories/youth-barometer.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:15:03 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mc/en/about-us/investor-relations/financial-disclosures/financial-reports/finacial-reports-2010.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:15:03 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/mc/en/about-us/investor-relations/financial-disclosures/financial-reports/finacial-reports-2011.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:15:12 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2025982>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2025982 took longer than 180.0 seconds..
2015-11-04 15:15:34 [scrapy] INFO: Crawled 13620 pages (at 25 pages/min), scraped 13391 items (at 21 items/min)
2015-11-04 15:15:35 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2128312>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2128312 took longer than 180.0 seconds..
2015-11-04 15:15:53 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2570548>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2570548 took longer than 180.0 seconds..
2015-11-04 15:16:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/eg/en/about-us/investor-relations/financial-disclosures/quarterly-results.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ch/en/about-us/sponsorship/art/goya-portraits.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ch/en/about-us/sponsorship/roger-federer.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/content/pwp/legalgates/cc/disclaimer-covered-bonds.html/il/en/about-us/investor-relations/information-for-debt-investors/covered-bonds.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ch/en/about-us/research.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/eg/en/about-us/investor-relations/financial-disclosures/quarterly-results/quarterly-results-2010.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:05 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pt/en/investment-banking/awards-and-rankings.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:06 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ch/en/about-us/sponsorship/swiss-football.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:16:44 [scrapy] INFO: Crawled 13637 pages (at 17 pages/min), scraped 13409 items (at 18 items/min)
2015-11-04 15:16:48 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2897586>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=2897586 took longer than 180.0 seconds..
2015-11-04 15:16:48 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=3671629>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=3671629 took longer than 180.0 seconds..
2015-11-04 15:16:48 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/guanyu/jibenqingkuang/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/guanyu/jibenqingkuang/index.shtml took longer than 180.0 seconds..
2015-11-04 15:16:48 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=3318251>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=3318251 took longer than 180.0 seconds..
2015-11-04 15:17:19 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/bh/en/about-us/responsibility/banking/trust-and-expertise.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:17:19 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/research.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:17:19 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/bh/en/about-us/responsibility/banking/agreements-and-memberships.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:17:19 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/sponsorship/further-commitments.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:17:19 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/responsibility/approach-reporting/challenges-and-responses.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:17:19 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/bh/en/about-us/responsibility/banking.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:17:19 [scrapy] INFO: Crawled 13639 pages (at 2 pages/min), scraped 13418 items (at 9 items/min)
2015-11-04 15:17:43 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=4547641>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=4547641 took longer than 180.0 seconds..
2015-11-04 15:18:16 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=4977562>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=4977562 took longer than 180.0 seconds..
2015-11-04 15:18:16 [scrapy] INFO: Crawled 13657 pages (at 18 pages/min), scraped 13432 items (at 14 items/min)
2015-11-04 15:19:21 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/english/INVESTMENTPRODUCTS/QDIIFund/GlobalSelectiveFund/bondfund/BondFunds/index.shtml> (referer: http://www.chinaamc.com/english/home/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:19:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ch/en/about-us/media/publications.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:19:28 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ch/en/about-us/investor-relations/financial-disclosures/quarterly-results/quarterly-results-2009.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:19:28 [scrapy] INFO: Crawled 13661 pages (at 4 pages/min), scraped 13435 items (at 3 items/min)
2015-11-04 15:19:31 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=5525866>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=5525866 took longer than 180.0 seconds..
2015-11-04 15:19:31 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/yanglaojijin/lianxiwomen/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/yanglaojijin/lianxiwomen/index.shtml took longer than 180.0 seconds..
2015-11-04 15:19:31 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/governance/shareholders/shareholder-structure.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:19:31 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/ch/en/about-us/media/events.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:20:12 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/yanglaojijin/nianjin/yewu/index.shtml> (referer: http://www.chinaamc.com/yanglaojijin/nianjin/nianjinyeji/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:21:13 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/english/INVESTMENTPRODUCTS/ClosedEndFunds/XingHuaFund/index.shtml> (referer: http://www.chinaamc.com/english/home/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:22:15 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/english/International/fundbrochures/qfiibusinessmodels/index.shtml> (referer: http://www.chinaamc.com/english/home/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/governance/shareholders.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/governance/executive-board.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/governance/board-of-directors/meeting-attendance.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/governance/board-of-directors/committees.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/yanglaojijin/nianjin/youshi/index.shtml>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 15:22:15 [scrapy] INFO: Crawled 13661 pages (at 0 pages/min), scraped 13436 items (at 1 items/min)
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=6167055>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=6167055 took longer than 180.0 seconds..
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/english/International/Outreach/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/english/International/Outreach/index.shtml took longer than 180.0 seconds..
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/governance/board-of-directors/whistleblower-process.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.credit-suisse.com/hu/en/about-us/governance/board-of-directors/whistleblower-process.html took longer than 180.0 seconds..
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/hu/en/about-us/investor-relations/events-presentations/annual-general-meeting.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/fi/en/about-us/governance/shareholders/management-transactions.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gr/en/about-us/investor-relations/events-presentations/annual-general-meeting/agm2014.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:15 [scrapy] ERROR: Error downloading <GET https://www.chinaamc.com/capital/wdzc/zhsy/index.shtml>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:16 [scrapy] INFO: Crawled 13661 pages (at 0 pages/min), scraped 13436 items (at 0 items/min)
2015-11-04 15:22:16 [scrapy] ERROR: Error downloading <GET https://www.chinaamc.com/capital/zbdl/index.shtml>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:32 [scrapy] ERROR: Error downloading <GET https://www.chinaamc.com/capital/index.shtml>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:38 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=5581503>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=5581503 took longer than 180.0 seconds..
2015-11-04 15:22:38 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/english/INVESTMENTPRODUCTS/QDIIFund/GlobalSelectiveFund/indexfund/ChinaSMEETF/index.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/english/INVESTMENTPRODUCTS/QDIIFund/GlobalSelectiveFund/indexfund/ChinaSMEETF/index.shtml took longer than 180.0 seconds..
2015-11-04 15:22:38 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com/capital/cjwt/4654729.shtml>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com/capital/cjwt/4654729.shtml took longer than 180.0 seconds..
2015-11-04 15:23:51 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/english/AboutChinaAmc/CorporateCulture/index.shtml> (referer: http://www.chinaamc.com/english/home/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:23:51 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/cz/en/about-us/our-company/global-reach.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:23:51 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/cz/en/about-us/our-company/global-reach/switzerland-and-credit-suisse.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:23:51 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/cz/en/about-us/our-company.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:23:51 [scrapy] INFO: Crawled 13678 pages (at 17 pages/min), scraped 13451 items (at 15 items/min)
2015-11-04 15:24:40 [scrapy] INFO: Crawled 13678 pages (at 0 pages/min), scraped 13452 items (at 1 items/min)
2015-11-04 15:24:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gg/en/about-us/investor-relations/financial-disclosures/financial-reports/finacial-reports-2014.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gg/en/about-us/investor-relations/share-information/analyst-coverage.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gg/en/about-us/investor-relations/financial-disclosures/quarterly-results/quarterly-results-2008.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gg/en/about-us/investor-relations/financial-disclosures/financial-reports.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gi/en/about-us/sponsorship/news-and-stories.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gi/en/about-us/responsibility/dialogue/swiss-worry-barometer.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:40 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/gg/en/about-us/investor-relations/financial-disclosures/quarterly-results/quarterly-results-2007.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:20 [scrapy] ERROR: Error downloading <GET http://www.phoenixrg.com/fund-management/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:25:20 [scrapy] INFO: Crawled 13701 pages (at 23 pages/min), scraped 13469 items (at 17 items/min)
2015-11-04 15:25:21 [scrapy] ERROR: Error downloading <GET http://www.phoenixrg.com/operations/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:25:21 [scrapy] ERROR: Error downloading <GET http://www.phoenixrg.com/multi-housing-news-the-big-apple/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:25:30 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/lu/en/about-us/responsibility/economy-society/commitments-in-switzerland/youth-unemployment-initiative.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:45 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/lu/en/about-us/investor-relations/share-information/share-capital-and-statistics.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:45 [scrapy] ERROR: Error downloading <GET http://www.weathergagecapital.com/portfolio>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:25:48 [scrapy] ERROR: Error downloading <GET http://www.weathergagecapital.com/press/tim_s_blog_posts/how_much_early_stage_exposure_do_you_really_have>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:25:48 [scrapy] ERROR: Error downloading <GET http://www.weathergagecapital.com/blog>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:25:48 [scrapy] ERROR: Error downloading <GET http://www.weathergagecapital.com/press>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:25:48 [scrapy] ERROR: Error downloading <GET http://www.weathergagecapital.com/blog/micro-vc/how-much-early-stage-exposure-do-you-really-have/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 15:26:22 [scrapy] INFO: Crawled 13739 pages (at 38 pages/min), scraped 13501 items (at 32 items/min)
2015-11-04 15:27:06 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=5900587> (referer: http://www.chinaamc.com/bangzhu/wendang/xiazai/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 15:27:14 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pl/en/about-us/responsibility/employer/talent-development.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:27:14 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pl/en/about-us/responsibility/economy-society/commitments-in-apac.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:27:14 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/nl/en/about-us/responsibility/economy-society/focus-themes/microfinance.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:27:14 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pl/en/about-us/responsibility/economy-society/commitments-in-switzerland/volunteering.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:27:14 [scrapy] ERROR: Error downloading <GET https://tphnet.tphco.com/wp-login.php?redirect_to=https%3A%2F%2Ftphnet.tphco.com%2F&reauth=1>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:27:18 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/pl/en/about-us/responsibility/economy-society/commitments-in-americas.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:27:18 [scrapy] INFO: Crawled 13741 pages (at 2 pages/min), scraped 13510 items (at 9 items/min)
2015-11-04 15:28:18 [scrapy] INFO: Crawled 13762 pages (at 21 pages/min), scraped 13529 items (at 19 items/min)
2015-11-04 15:28:43 [scrapy] ERROR: Error downloading <GET https://creditsuisse.taleo.net/careersection/external/mysubmissions.ftl?lang=en>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 15:28:43 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/qa/en/careers/experienced-professionals/job-areas-in-india/pune.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:28:43 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/qa/en/about-us/our-company/campaign.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:28:43 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/qa/en/about-us/media.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:28:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/qa/en/careers/experienced-professionals/job-areas-in-india.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:28:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/qa/en/careers/experienced-professionals/search-apply/check-application.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:28:46 [scrapy] ERROR: Error downloading <GET https://www.credit-suisse.com/qa/en/careers/experienced-professionals/search-apply.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:29:16 [scrapy] INFO: Crawled 13774 pages (at 12 pages/min), scraped 13544 items (at 15 items/min)
2015-11-04 15:30:16 [scrapy] INFO: Crawled 13774 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:31:16 [scrapy] INFO: Crawled 13774 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:32:16 [scrapy] INFO: Crawled 13774 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:33:16 [scrapy] INFO: Crawled 13774 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:35:33 [scrapy] ERROR: Spider error processing <GET http://www.chinaamc.com/zcms/Services/AttachDownLoad.jsp?id=5525950> (referer: http://www.chinaamc.com/bangzhu/wendang/xiazai/index.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:35:33 [scrapy] INFO: Crawled 13775 pages (at 1 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:36:16 [scrapy] INFO: Crawled 13775 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:37:16 [scrapy] INFO: Crawled 13775 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:38:16 [scrapy] INFO: Crawled 13775 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:39:16 [scrapy] INFO: Crawled 13775 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:40:16 [scrapy] INFO: Crawled 13775 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:41:16 [scrapy] INFO: Crawled 13775 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:42:13 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:13 [scrapy] INFO: Closing spider (shutdown)
2015-11-04 15:42:16 [scrapy] INFO: Crawled 13775 pages (at 0 pages/min), scraped 13544 items (at 0 items/min)
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM twice, forcing unclean shutdown
