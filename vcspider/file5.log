usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
2015-11-04 00:30:59 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 00:30:59 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 00:30:59 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 00:30:59 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 00:30:59 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 00:30:59 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 00:31:00 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 00:31:00 [scrapy] INFO: Spider opened
2015-11-04 00:31:00 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 00:31:00 [scrapy] ERROR: Error downloading <GET http://www.shop.fayettechill.com>: DNS lookup failed: address 'www.shop.fayettechill.com' not found: [E2015-11-04 00:31:32 [scrapy] ERROR: Spider error processing <GET http://www.orbimed.com/en/news> (referer: http://www.orbimed.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packa2015-11-04 00:32:12 [scrapy] INFO: Crawled 183 pages (at 183 pages/min), scraped 71 items (at 71 items/min)
2015-11-04 00:33:26 [scrapy] INFO: Crawled 215 pages (at 32 pages/min), scraped 107 items (at 36 items/min)
2015-11-04 00:34:31 [scrapy] INFO: Crawled 223 pages (at 8 pages/min), scraped 131 items (at 24 items/min)
2015-11-04 00:34:46 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.fooducate.nutritionapp&referrer=utm_source%3Dfdct-redirect%26utm_campaign%3DFdct-Web-home-button-top%26utm_medium%3Dna>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:35:10 [scrapy] INFO: Crawled 249 pages (at 26 pages/min), scraped 152 items (at 21 items/min)
2015-11-04 00:35:10 [scrapy] ERROR: Error downloading <GET https://lockitron.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:36:01 [scrapy] INFO: Crawled 268 pages (at 19 pages/min), scraped 165 items (at 13 items/min)
2015-11-04 00:36:17 [scrapy] ERROR: Error downloading <GET http://tribunecontentsolutions.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 00:37:38 [scrapy] INFO: Crawled 299 pages (at 31 pages/min), scraped 211 items (at 46 items/min)
2015-11-04 00:37:43 [scrapy] ERROR: Error downloading <GET https://foundd.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:38:02 [scrapy] INFO: Crawled 332 pages (at 33 pages/min), scraped 225 items (at 14 items/min)
2015-11-04 00:39:10 [scrapy] INFO: Crawled 359 pages (at 27 pages/min), scraped 265 items (at 40 items/min)
2015-11-04 00:40:14 [scrapy] INFO: Crawled 400 pages (at 41 pages/min), scraped 301 items (at 36 items/min)
2015-11-04 00:41:14 [scrapy] INFO: Crawled 429 pages (at 29 pages/min), scraped 335 items (at 34 items/min)
2015-11-04 00:42:03 [scrapy] INFO: Crawled 471 pages (at 42 pages/min), scraped 364 items (at 29 items/min)
2015-11-04 00:43:08 [scrapy] INFO: Crawled 498 pages (at 27 pages/min), scraped 404 items (at 40 items/min)
2015-11-04 00:44:14 [scrapy] INFO: Crawled 537 pages (at 39 pages/min), scraped 443 items (at 39 items/min)
2015-11-04 00:45:10 [scrapy] INFO: Crawled 577 pages (at 40 pages/min), scraped 475 items (at 32 items/min)
2015-11-04 00:46:16 [scrapy] INFO: Crawled 609 pages (at 32 pages/min), scraped 515 items (at 40 items/min)
2015-11-04 00:47:01 [scrapy] INFO: Crawled 647 pages (at 38 pages/min), scraped 539 items (at 24 items/min)
2015-11-04 00:48:19 [scrapy] INFO: Crawled 681 pages (at 34 pages/min), scraped 587 items (at 48 items/min)
2015-11-04 00:49:15 [scrapy] INFO: Crawled 721 pages (at 40 pages/min), scraped 619 items (at 32 items/min)
2015-11-04 00:50:14 [scrapy] INFO: Crawled 750 pages (at 29 pages/min), scraped 656 items (at 37 items/min)
2015-11-04 00:51:05 [scrapy] INFO: Crawled 787 pages (at 37 pages/min), scraped 686 items (at 30 items/min)
2015-11-04 00:52:14 [scrapy] INFO: Crawled 826 pages (at 39 pages/min), scraped 724 items (at 38 items/min)
2015-11-04 00:53:15 [scrapy] INFO: Crawled 851 pages (at 25 pages/min), scraped 757 items (at 33 items/min)
2015-11-04 00:54:06 [scrapy] INFO: Crawled 891 pages (at 40 pages/min), scraped 789 items (at 32 items/min)
2015-11-04 00:55:09 [scrapy] INFO: Crawled 923 pages (at 32 pages/min), scraped 829 items (at 40 items/min)
2015-11-04 00:56:03 [scrapy] INFO: Crawled 967 pages (at 44 pages/min), scraped 860 items (at 31 items/min)
2015-11-04 00:57:09 [scrapy] INFO: Crawled 1004 pages (at 37 pages/min), scraped 900 items (at 40 items/min)
2015-11-04 00:58:05 [scrapy] INFO: Crawled 1038 pages (at 34 pages/min), scraped 932 items (at 32 items/min)
2015-11-04 00:59:05 [scrapy] INFO: Crawled 1075 pages (at 37 pages/min), scraped 970 items (at 38 items/min)
2015-11-04 01:00:28 [scrapy] INFO: Crawled 1112 pages (at 37 pages/min), scraped 1018 items (at 48 items/min)
2015-11-04 01:01:08 [scrapy] INFO: Crawled 1135 pages (at 23 pages/min), scraped 1041 items (at 23 items/min)
2015-11-04 01:02:06 [scrapy] INFO: Crawled 1164 pages (at 29 pages/min), scraped 1077 items (at 36 items/min)
2015-11-04 01:03:00 [scrapy] INFO: Crawled 1199 pages (at 35 pages/min), scraped 1106 items (at 29 items/min)
2015-11-04 01:04:01 [scrapy] INFO: Crawled 1246 pages (at 47 pages/min), scraped 1146 items (at 40 items/min)
2015-11-04 01:05:14 [scrapy] INFO: Crawled 1291 pages (at 45 pages/min), scraped 1196 items (at 50 items/min)
2015-11-04 01:06:04 [scrapy] INFO: Crawled 1321 pages (at 30 pages/min), scraped 1228 items (at 32 items/min)
2015-11-04 01:07:07 [scrapy] INFO: Crawled 1367 pages (at 46 pages/min), scraped 1275 items (at 47 items/min)
2015-11-04 01:08:01 [scrapy] INFO: Crawled 1410 pages (at 43 pages/min), scraped 1313 items (at 38 items/min)
2015-11-04 01:09:14 [scrapy] INFO: Crawled 1460 pages (at 50 pages/min), scraped 1361 items (at 48 items/min)
2015-11-04 01:10:18 [scrapy] INFO: Crawled 1496 pages (at 36 pages/min), scraped 1403 items (at 42 items/min)
2015-11-04 01:11:14 [scrapy] INFO: Crawled 1543 pages (at 47 pages/min), scraped 1439 items (at 36 items/min)
2015-11-04 01:12:23 [scrapy] INFO: Crawled 1562 pages (at 19 pages/min), scraped 1468 items (at 29 items/min)
2015-11-04 01:13:08 [scrapy] INFO: Crawled 1591 pages (at 29 pages/min), scraped 1491 items (at 23 items/min)
2015-11-04 01:14:24 [scrapy] INFO: Crawled 1640 pages (at 49 pages/min), scraped 1534 items (at 43 items/min)
2015-11-04 01:15:01 [scrapy] INFO: Crawled 1673 pages (at 33 pages/min), scraped 1554 items (at 20 items/min)
2015-11-04 01:16:11 [scrapy] INFO: Crawled 1685 pages (at 12 pages/min), scraped 1573 items (at 19 items/min)
2015-11-04 01:17:24 [scrapy] INFO: Crawled 1709 pages (at 24 pages/min), scraped 1605 items (at 32 items/min)
2015-11-04 01:18:03 [scrapy] INFO: Crawled 1729 pages (at 20 pages/min), scraped 1623 items (at 18 items/min)
2015-11-04 01:19:23 [scrapy] INFO: Crawled 1745 pages (at 16 pages/min), scraped 1648 items (at 25 items/min)
2015-11-04 01:20:10 [scrapy] INFO: Crawled 1766 pages (at 21 pages/min), scraped 1668 items (at 20 items/min)
2015-11-04 01:21:04 [scrapy] INFO: Crawled 1800 pages (at 34 pages/min), scraped 1687 items (at 19 items/min)
2015-11-04 01:22:44 [scrapy] INFO: Crawled 1819 pages (at 19 pages/min), scraped 1721 items (at 34 items/min)
2015-11-04 01:23:23 [scrapy] INFO: Crawled 1831 pages (at 12 pages/min), scraped 1733 items (at 12 items/min)
2015-11-04 01:24:20 [scrapy] INFO: Crawled 1863 pages (at 32 pages/min), scraped 1759 items (at 26 items/min)
2015-11-04 01:25:04 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=1131366>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:25:04 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=1218686>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:25:04 [scrapy] INFO: Crawled 1889 pages (at 26 pages/min), scraped 1777 items (at 18 items/min)
2015-11-04 01:25:53 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=1208831>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:26:47 [scrapy] INFO: Crawled 1915 pages (at 26 pages/min), scraped 1813 items (at 36 items/min)
2015-11-04 01:27:30 [scrapy] INFO: Crawled 1929 pages (at 14 pages/min), scraped 1828 items (at 15 items/min)
2015-11-04 01:28:05 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=1043776>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:28:06 [scrapy] INFO: Crawled 1945 pages (at 16 pages/min), scraped 1842 items (at 14 items/min)
2015-11-04 01:28:43 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=963940>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:28:43 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=1376936>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:28:43 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=936672>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:29:39 [scrapy] INFO: Crawled 1966 pages (at 21 pages/min), scraped 1864 items (at 22 items/min)
2015-11-04 01:30:11 [scrapy] INFO: Crawled 1975 pages (at 9 pages/min), scraped 1874 items (at 10 items/min)
2015-11-04 01:31:14 [scrapy] INFO: Crawled 1998 pages (at 23 pages/min), scraped 1883 items (at 9 items/min)
2015-11-04 01:32:11 [scrapy] INFO: Crawled 2006 pages (at 8 pages/min), scraped 1910 items (at 27 items/min)
2015-11-04 01:33:15 [scrapy] INFO: Crawled 2034 pages (at 28 pages/min), scraped 1934 items (at 24 items/min)
2015-11-04 01:34:22 [scrapy] INFO: Crawled 2070 pages (at 36 pages/min), scraped 1966 items (at 32 items/min)
2015-11-04 01:35:09 [scrapy] INFO: Crawled 2091 pages (at 21 pages/min), scraped 1982 items (at 16 items/min)
2015-11-04 01:36:28 [scrapy] INFO: Crawled 2113 pages (at 22 pages/min), scraped 2003 items (at 21 items/min)
2015-11-04 01:37:18 [scrapy] INFO: Crawled 2121 pages (at 8 pages/min), scraped 2028 items (at 25 items/min)
2015-11-04 01:38:43 [scrapy] INFO: Crawled 2180 pages (at 59 pages/min), scraped 2065 items (at 37 items/min)
2015-11-04 01:39:38 [scrapy] INFO: Crawled 2196 pages (at 16 pages/min), scraped 2088 items (at 23 items/min)
2015-11-04 01:40:10 [scrapy] INFO: Crawled 2208 pages (at 12 pages/min), scraped 2106 items (at 18 items/min)
2015-11-04 01:41:25 [scrapy] INFO: Crawled 2234 pages (at 26 pages/min), scraped 2134 items (at 28 items/min)
2015-11-04 01:42:35 [scrapy] INFO: Crawled 2257 pages (at 23 pages/min), scraped 2153 items (at 19 items/min)
2015-11-04 01:43:21 [scrapy] INFO: Crawled 2267 pages (at 10 pages/min), scraped 2167 items (at 14 items/min)
2015-11-04 01:44:26 [scrapy] INFO: Crawled 2284 pages (at 17 pages/min), scraped 2186 items (at 19 items/min)
2015-11-04 01:46:20 [scrapy] INFO: Crawled 2310 pages (at 26 pages/min), scraped 2210 items (at 24 items/min)
2015-11-04 01:47:10 [scrapy] INFO: Crawled 2351 pages (at 41 pages/min), scraped 2233 items (at 23 items/min)
2015-11-04 01:48:27 [scrapy] INFO: Crawled 2362 pages (at 11 pages/min), scraped 2260 items (at 27 items/min)
2015-11-04 01:49:19 [scrapy] INFO: Crawled 2371 pages (at 9 pages/min), scraped 2268 items (at 8 items/min)
2015-11-04 01:50:13 [scrapy] INFO: Crawled 2385 pages (at 14 pages/min), scraped 2291 items (at 23 items/min)
2015-11-04 01:51:01 [scrapy] INFO: Crawled 2430 pages (at 45 pages/min), scraped 2320 items (at 29 items/min)
2015-11-04 01:52:11 [scrapy] INFO: Crawled 2475 pages (at 45 pages/min), scraped 2370 items (at 50 items/min)
2015-11-04 01:53:09 [scrapy] INFO: Crawled 2543 pages (at 68 pages/min), scraped 2440 items (at 70 items/min)
2015-11-04 01:54:04 [scrapy] INFO: Crawled 2637 pages (at 94 pages/min), scraped 2525 items (at 85 items/min)
2015-11-04 01:54:49 [scrapy] ERROR: Error downloading <GET https://beta.modustri.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:54:55 [scrapy] ERROR: Error downloading <GET https://impulcity.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:55:06 [scrapy] INFO: Crawled 2689 pages (at 52 pages/min), scraped 2590 items (at 65 items/min)
2015-11-04 01:56:13 [scrapy] INFO: Crawled 2746 pages (at 57 pages/min), scraped 2640 items (at 50 items/min)
2015-11-04 01:57:20 [scrapy] INFO: Crawled 2791 pages (at 45 pages/min), scraped 2677 items (at 37 items/min)
2015-11-04 01:57:25 [scrapy] ERROR: Error downloading <GET https://www.inventalator.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:57:25 [scrapy] ERROR: Error downloading <GET https://itunes.apple.com/us/app/fooducate/id398436747?mt=8&uo=4&at=10l9MY&ct=Fdct-Web-home-button-top>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:57:25 [scrapy] ERROR: Error downloading <GET http://modustri.com/terms-conditions/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:58:24 [scrapy] INFO: Crawled 2816 pages (at 25 pages/min), scraped 2710 items (at 33 items/min)
2015-11-04 01:59:04 [scrapy] INFO: Crawled 2834 pages (at 18 pages/min), scraped 2735 items (at 25 items/min)
2015-11-04 02:00:00 [scrapy] INFO: Crawled 2898 pages (at 64 pages/min), scraped 2786 items (at 51 items/min)
2015-11-04 02:01:02 [scrapy] INFO: Crawled 2961 pages (at 63 pages/min), scraped 2840 items (at 54 items/min)
2015-11-04 02:02:02 [scrapy] INFO: Crawled 3028 pages (at 67 pages/min), scraped 2895 items (at 55 items/min)
2015-11-04 02:03:01 [scrapy] INFO: Crawled 3086 pages (at 58 pages/min), scraped 2949 items (at 54 items/min)
2015-11-04 02:04:02 [scrapy] INFO: Crawled 3172 pages (at 86 pages/min), scraped 3005 items (at 56 items/min)
2015-11-04 02:05:05 [scrapy] INFO: Crawled 3228 pages (at 56 pages/min), scraped 3061 items (at 56 items/min)
2015-11-04 02:06:04 [scrapy] INFO: Crawled 3284 pages (at 56 pages/min), scraped 3117 items (at 56 items/min)
2015-11-04 02:07:07 [scrapy] INFO: Crawled 3340 pages (at 56 pages/min), scraped 3173 items (at 56 items/min)
2015-11-04 02:08:01 [scrapy] INFO: Crawled 3386 pages (at 46 pages/min), scraped 3221 items (at 48 items/min)
2015-11-04 02:09:05 [scrapy] INFO: Crawled 3444 pages (at 58 pages/min), scraped 3282 items (at 61 items/min)
2015-11-04 02:10:06 [scrapy] INFO: Crawled 3498 pages (at 54 pages/min), scraped 3333 items (at 51 items/min)
2015-11-04 02:11:02 [scrapy] INFO: Crawled 3546 pages (at 48 pages/min), scraped 3379 items (at 46 items/min)
2015-11-04 02:12:06 [scrapy] INFO: Crawled 3594 pages (at 48 pages/min), scraped 3427 items (at 48 items/min)
2015-11-04 02:13:02 [scrapy] INFO: Crawled 3642 pages (at 48 pages/min), scraped 3477 items (at 50 items/min)
2015-11-04 02:14:09 [scrapy] INFO: Crawled 3697 pages (at 55 pages/min), scraped 3529 items (at 52 items/min)
2015-11-04 02:15:05 [scrapy] INFO: Crawled 3741 pages (at 44 pages/min), scraped 3571 items (at 42 items/min)
2015-11-04 02:15:23 [scrapy] ERROR: Spider error processing <GET http://blog.logograb.com/> (referer: https://www.logograb.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 02:16:04 [scrapy] INFO: Crawled 3815 pages (at 74 pages/min), scraped 3640 items (at 69 items/min)
2015-11-04 02:17:08 [scrapy] INFO: Crawled 3877 pages (at 62 pages/min), scraped 3713 items (at 73 items/min)
2015-11-04 02:18:01 [scrapy] INFO: Crawled 3918 pages (at 41 pages/min), scraped 3750 items (at 37 items/min)
2015-11-04 02:19:04 [scrapy] INFO: Crawled 3988 pages (at 70 pages/min), scraped 3826 items (at 76 items/min)
2015-11-04 02:20:02 [scrapy] INFO: Crawled 4060 pages (at 72 pages/min), scraped 3896 items (at 70 items/min)
2015-11-04 02:21:01 [scrapy] INFO: Crawled 4131 pages (at 71 pages/min), scraped 3963 items (at 67 items/min)
2015-11-04 02:22:01 [scrapy] INFO: Crawled 4211 pages (at 80 pages/min), scraped 4043 items (at 80 items/min)
2015-11-04 02:23:01 [scrapy] INFO: Crawled 4291 pages (at 80 pages/min), scraped 4124 items (at 81 items/min)
2015-11-04 02:24:01 [scrapy] INFO: Crawled 4371 pages (at 80 pages/min), scraped 4203 items (at 79 items/min)
2015-11-04 02:25:00 [scrapy] INFO: Crawled 4451 pages (at 80 pages/min), scraped 4283 items (at 80 items/min)
2015-11-04 02:26:03 [scrapy] INFO: Crawled 4539 pages (at 88 pages/min), scraped 4372 items (at 89 items/min)
2015-11-04 02:27:01 [scrapy] INFO: Crawled 4619 pages (at 80 pages/min), scraped 4452 items (at 80 items/min)
2015-11-04 02:28:03 [scrapy] INFO: Crawled 4693 pages (at 74 pages/min), scraped 4531 items (at 79 items/min)
2015-11-04 02:29:02 [scrapy] INFO: Crawled 4765 pages (at 72 pages/min), scraped 4599 items (at 68 items/min)
2015-11-04 02:30:03 [scrapy] INFO: Crawled 4839 pages (at 74 pages/min), scraped 4675 items (at 76 items/min)
2015-11-04 02:31:01 [scrapy] INFO: Crawled 4915 pages (at 76 pages/min), scraped 4749 items (at 74 items/min)
2015-11-04 02:32:02 [scrapy] INFO: Crawled 4989 pages (at 74 pages/min), scraped 4823 items (at 74 items/min)
2015-11-04 02:33:03 [scrapy] INFO: Crawled 5059 pages (at 70 pages/min), scraped 4893 items (at 70 items/min)
2015-11-04 02:34:13 [scrapy] INFO: Crawled 5140 pages (at 81 pages/min), scraped 4970 items (at 77 items/min)
2015-11-04 02:35:01 [scrapy] INFO: Crawled 5187 pages (at 47 pages/min), scraped 5027 items (at 57 items/min)
2015-11-04 02:36:03 [scrapy] INFO: Crawled 5269 pages (at 82 pages/min), scraped 5093 items (at 66 items/min)
2015-11-04 02:37:03 [scrapy] ERROR: Spider error processing <GET http://investors.apigee.com/Tearsheet.ashx?c=253958> (referer: http://investors.apigee.com/phoenix.zhtml?c=253958&p=irol-irhome)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:37:14 [scrapy] INFO: Crawled 5329 pages (at 60 pages/min), scraped 5158 items (at 65 items/min)
2015-11-04 02:38:04 [scrapy] INFO: Crawled 5382 pages (at 53 pages/min), scraped 5193 items (at 35 items/min)
2015-11-04 02:39:22 [scrapy] INFO: Crawled 5405 pages (at 23 pages/min), scraped 5236 items (at 43 items/min)
2015-11-04 02:39:22 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/institute.html>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 02:40:01 [scrapy] INFO: Crawled 5426 pages (at 21 pages/min), scraped 5254 items (at 18 items/min)
2015-11-04 02:41:22 [scrapy] INFO: Crawled 5484 pages (at 58 pages/min), scraped 5314 items (at 60 items/min)
2015-11-04 02:42:15 [scrapy] INFO: Crawled 5509 pages (at 25 pages/min), scraped 5345 items (at 31 items/min)
2015-11-04 02:43:05 [scrapy] INFO: Crawled 5554 pages (at 45 pages/min), scraped 5383 items (at 38 items/min)
2015-11-04 02:44:06 [scrapy] INFO: Crawled 5610 pages (at 56 pages/min), scraped 5439 items (at 56 items/min)
2015-11-04 02:45:04 [scrapy] INFO: Crawled 5654 pages (at 44 pages/min), scraped 5485 items (at 46 items/min)
2015-11-04 02:46:01 [scrapy] INFO: Crawled 5710 pages (at 56 pages/min), scraped 5539 items (at 54 items/min)
2015-11-04 02:47:01 [scrapy] INFO: Crawled 5761 pages (at 51 pages/min), scraped 5590 items (at 51 items/min)
2015-11-04 02:48:07 [scrapy] INFO: Crawled 5814 pages (at 53 pages/min), scraped 5645 items (at 55 items/min)
2015-11-04 02:49:04 [scrapy] INFO: Crawled 5870 pages (at 56 pages/min), scraped 5699 items (at 54 items/min)
2015-11-04 02:50:01 [scrapy] INFO: Crawled 5920 pages (at 50 pages/min), scraped 5750 items (at 51 items/min)
2015-11-04 02:51:03 [scrapy] INFO: Crawled 5982 pages (at 62 pages/min), scraped 5811 items (at 61 items/min)
2015-11-04 02:52:05 [scrapy] INFO: Crawled 6036 pages (at 54 pages/min), scraped 5869 items (at 58 items/min)
2015-11-04 02:53:02 [scrapy] INFO: Crawled 6096 pages (at 60 pages/min), scraped 5926 items (at 57 items/min)
2015-11-04 02:54:04 [scrapy] INFO: Crawled 6153 pages (at 57 pages/min), scraped 5986 items (at 60 items/min)
2015-11-04 02:55:22 [scrapy] INFO: Crawled 6188 pages (at 35 pages/min), scraped 6016 items (at 30 items/min)
2015-11-04 02:56:01 [scrapy] INFO: Crawled 6219 pages (at 31 pages/min), scraped 6053 items (at 37 items/min)
2015-11-04 02:57:00 [scrapy] INFO: Crawled 6257 pages (at 38 pages/min), scraped 6086 items (at 33 items/min)
2015-11-04 02:58:00 [scrapy] INFO: Crawled 6288 pages (at 31 pages/min), scraped 6118 items (at 32 items/min)
2015-11-04 02:59:08 [scrapy] INFO: Crawled 6341 pages (at 53 pages/min), scraped 6170 items (at 52 items/min)
2015-11-04 03:00:07 [scrapy] INFO: Crawled 6403 pages (at 62 pages/min), scraped 6224 items (at 54 items/min)
2015-11-04 03:01:01 [scrapy] INFO: Crawled 6470 pages (at 67 pages/min), scraped 6277 items (at 53 items/min)
2015-11-04 03:02:03 [scrapy] INFO: Crawled 6539 pages (at 69 pages/min), scraped 6339 items (at 62 items/min)
2015-11-04 03:03:06 [scrapy] INFO: Crawled 6566 pages (at 27 pages/min), scraped 6383 items (at 44 items/min)
2015-11-04 03:04:15 [scrapy] INFO: Crawled 6582 pages (at 16 pages/min), scraped 6409 items (at 26 items/min)
2015-11-04 03:05:22 [scrapy] INFO: Crawled 6615 pages (at 33 pages/min), scraped 6437 items (at 28 items/min)
2015-11-04 03:06:18 [scrapy] INFO: Crawled 6654 pages (at 39 pages/min), scraped 6468 items (at 31 items/min)
2015-11-04 03:07:02 [scrapy] INFO: Crawled 6693 pages (at 39 pages/min), scraped 6506 items (at 38 items/min)
2015-11-04 03:08:16 [scrapy] INFO: Crawled 6734 pages (at 41 pages/min), scraped 6562 items (at 56 items/min)
2015-11-04 03:09:03 [scrapy] INFO: Crawled 6785 pages (at 51 pages/min), scraped 6601 items (at 39 items/min)
2015-11-04 03:10:14 [scrapy] INFO: Crawled 6819 pages (at 34 pages/min), scraped 6640 items (at 39 items/min)
2015-11-04 03:11:16 [scrapy] INFO: Crawled 6859 pages (at 40 pages/min), scraped 6681 items (at 41 items/min)
2015-11-04 03:12:06 [scrapy] INFO: Crawled 6912 pages (at 53 pages/min), scraped 6721 items (at 40 items/min)
2015-11-04 03:13:10 [scrapy] INFO: Crawled 6960 pages (at 48 pages/min), scraped 6775 items (at 54 items/min)
2015-11-04 03:14:25 [scrapy] INFO: Crawled 7006 pages (at 46 pages/min), scraped 6821 items (at 46 items/min)
2015-11-04 03:15:45 [scrapy] INFO: Crawled 7035 pages (at 29 pages/min), scraped 6843 items (at 22 items/min)
2015-11-04 03:16:43 [scrapy] INFO: Crawled 7039 pages (at 4 pages/min), scraped 6862 items (at 19 items/min)
2015-11-04 03:17:11 [scrapy] INFO: Crawled 7046 pages (at 7 pages/min), scraped 6876 items (at 14 items/min)
2015-11-04 03:18:16 [scrapy] INFO: Crawled 7093 pages (at 47 pages/min), scraped 6906 items (at 30 items/min)
2015-11-04 03:19:28 [scrapy] INFO: Crawled 7099 pages (at 6 pages/min), scraped 6921 items (at 15 items/min)
2015-11-04 03:20:06 [scrapy] INFO: Crawled 7132 pages (at 33 pages/min), scraped 6940 items (at 19 items/min)
2015-11-04 03:21:10 [scrapy] INFO: Crawled 7148 pages (at 16 pages/min), scraped 6968 items (at 28 items/min)
2015-11-04 03:22:38 [scrapy] INFO: Crawled 7185 pages (at 37 pages/min), scraped 6999 items (at 31 items/min)
2015-11-04 03:23:17 [scrapy] INFO: Crawled 7201 pages (at 16 pages/min), scraped 7019 items (at 20 items/min)
2015-11-04 03:24:07 [scrapy] INFO: Crawled 7229 pages (at 28 pages/min), scraped 7054 items (at 35 items/min)
2015-11-04 03:25:13 [scrapy] INFO: Crawled 7284 pages (at 55 pages/min), scraped 7100 items (at 46 items/min)
2015-11-04 03:26:01 [scrapy] INFO: Crawled 7315 pages (at 31 pages/min), scraped 7131 items (at 31 items/min)
2015-11-04 03:27:23 [scrapy] INFO: Crawled 7372 pages (at 57 pages/min), scraped 7187 items (at 56 items/min)
2015-11-04 03:28:21 [scrapy] INFO: Crawled 7413 pages (at 41 pages/min), scraped 7217 items (at 30 items/min)
2015-11-04 03:29:21 [scrapy] INFO: Crawled 7429 pages (at 16 pages/min), scraped 7242 items (at 25 items/min)
2015-11-04 03:30:36 [scrapy] INFO: Crawled 7461 pages (at 32 pages/min), scraped 7270 items (at 28 items/min)
2015-11-04 03:31:22 [scrapy] INFO: Crawled 7486 pages (at 25 pages/min), scraped 7285 items (at 15 items/min)
2015-11-04 03:32:19 [scrapy] INFO: Crawled 7502 pages (at 16 pages/min), scraped 7310 items (at 25 items/min)
2015-11-04 03:33:45 [scrapy] INFO: Crawled 7520 pages (at 18 pages/min), scraped 7328 items (at 18 items/min)
2015-11-04 03:34:36 [scrapy] INFO: Crawled 7521 pages (at 1 pages/min), scraped 7344 items (at 16 items/min)
2015-11-04 03:36:50 [scrapy] INFO: Crawled 7571 pages (at 50 pages/min), scraped 7383 items (at 39 items/min)
2015-11-04 03:37:11 [scrapy] INFO: Crawled 7577 pages (at 6 pages/min), scraped 7389 items (at 6 items/min)
2015-11-04 03:38:10 [scrapy] INFO: Crawled 7614 pages (at 37 pages/min), scraped 7416 items (at 27 items/min)
2015-11-04 03:39:21 [scrapy] INFO: Crawled 7625 pages (at 11 pages/min), scraped 7420 items (at 4 items/min)
2015-11-04 03:41:10 [scrapy] INFO: Crawled 7625 pages (at 0 pages/min), scraped 7438 items (at 18 items/min)
2015-11-04 03:41:36 [scrapy] ERROR: Spider error processing <GET http://knowledgebase.cradlepoint.com/articles/Support/Resolve-Strict-NAT-Issues> (referer: https://cradlepoint.com/products/aer-2100)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 642, in _read_chunked
    raise IncompleteRead(''.join(value))
IncompleteRead: IncompleteRead(7633 bytes read)
2015-11-04 03:41:50 [scrapy] ERROR: Spider error processing <GET http://knowledgebase.cradlepoint.com/articles/Support/fw6-Activate-your-Cradlepoint-International-Modem> (referer: https://cradlepoint.com/products/arc-cba850)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 642, in _read_chunked
    raise IncompleteRead(''.join(value))
IncompleteRead: IncompleteRead(464 bytes read)
2015-11-04 03:42:02 [scrapy] INFO: Crawled 7671 pages (at 46 pages/min), scraped 7463 items (at 25 items/min)
2015-11-04 03:43:17 [scrapy] INFO: Crawled 7687 pages (at 16 pages/min), scraped 7494 items (at 31 items/min)
2015-11-04 03:44:15 [scrapy] INFO: Crawled 7719 pages (at 32 pages/min), scraped 7525 items (at 31 items/min)
2015-11-04 03:45:05 [scrapy] INFO: Crawled 7751 pages (at 32 pages/min), scraped 7557 items (at 32 items/min)
2015-11-04 03:46:16 [scrapy] INFO: Crawled 7773 pages (at 22 pages/min), scraped 7582 items (at 25 items/min)
2015-11-04 03:47:01 [scrapy] INFO: Crawled 7789 pages (at 16 pages/min), scraped 7597 items (at 15 items/min)
2015-11-04 03:48:16 [scrapy] INFO: Crawled 7829 pages (at 40 pages/min), scraped 7635 items (at 38 items/min)
2015-11-04 03:49:04 [scrapy] INFO: Crawled 7829 pages (at 0 pages/min), scraped 7651 items (at 16 items/min)
2015-11-04 03:50:33 [scrapy] INFO: Crawled 7861 pages (at 32 pages/min), scraped 7680 items (at 29 items/min)
2015-11-04 03:51:09 [scrapy] INFO: Crawled 7876 pages (at 15 pages/min), scraped 7693 items (at 13 items/min)
2015-11-04 03:52:00 [scrapy] INFO: Crawled 7907 pages (at 31 pages/min), scraped 7716 items (at 23 items/min)
2015-11-04 03:53:26 [scrapy] INFO: Crawled 7950 pages (at 43 pages/min), scraped 7764 items (at 48 items/min)
2015-11-04 03:54:29 [scrapy] INFO: Crawled 7966 pages (at 16 pages/min), scraped 7780 items (at 16 items/min)
2015-11-04 03:55:00 [scrapy] INFO: Crawled 7981 pages (at 15 pages/min), scraped 7788 items (at 8 items/min)
2015-11-04 03:56:14 [scrapy] INFO: Crawled 8005 pages (at 24 pages/min), scraped 7812 items (at 24 items/min)
2015-11-04 03:57:28 [scrapy] INFO: Crawled 8033 pages (at 28 pages/min), scraped 7844 items (at 32 items/min)
2015-11-04 03:58:20 [scrapy] INFO: Crawled 8059 pages (at 26 pages/min), scraped 7864 items (at 20 items/min)
2015-11-04 03:58:42 [scrapy] ERROR: Spider error processing <GET http://www.atinternet.com/pt-br/empresa/empresa/> (referer: http://www.atinternet.com/corporate/corporate/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 03:59:01 [scrapy] INFO: Crawled 8068 pages (at 9 pages/min), scraped 7880 items (at 16 items/min)
2015-11-04 04:00:01 [scrapy] INFO: Crawled 8079 pages (at 11 pages/min), scraped 7889 items (at 9 items/min)
2015-11-04 04:01:26 [scrapy] INFO: Crawled 8089 pages (at 10 pages/min), scraped 7901 items (at 12 items/min)
2015-11-04 04:02:00 [scrapy] INFO: Crawled 8098 pages (at 9 pages/min), scraped 7912 items (at 11 items/min)
2015-11-04 04:03:48 [scrapy] INFO: Crawled 8129 pages (at 31 pages/min), scraped 7942 items (at 30 items/min)
2015-11-04 04:04:16 [scrapy] INFO: Crawled 8134 pages (at 5 pages/min), scraped 7950 items (at 8 items/min)
2015-11-04 04:06:22 [scrapy] INFO: Crawled 8177 pages (at 43 pages/min), scraped 7990 items (at 40 items/min)
2015-11-04 04:07:15 [scrapy] INFO: Crawled 8203 pages (at 26 pages/min), scraped 8008 items (at 18 items/min)
2015-11-04 04:08:45 [scrapy] INFO: Crawled 8242 pages (at 39 pages/min), scraped 8046 items (at 38 items/min)
2015-11-04 04:09:26 [scrapy] INFO: Crawled 8258 pages (at 16 pages/min), scraped 8063 items (at 17 items/min)
2015-11-04 04:10:03 [scrapy] INFO: Crawled 8264 pages (at 6 pages/min), scraped 8079 items (at 16 items/min)
2015-11-04 04:11:16 [scrapy] INFO: Crawled 8289 pages (at 25 pages/min), scraped 8098 items (at 19 items/min)
2015-11-04 04:12:57 [scrapy] INFO: Crawled 8317 pages (at 28 pages/min), scraped 8125 items (at 27 items/min)
2015-11-04 04:13:53 [scrapy] INFO: Crawled 8332 pages (at 15 pages/min), scraped 8138 items (at 13 items/min)
2015-11-04 04:14:37 [scrapy] INFO: Crawled 8347 pages (at 15 pages/min), scraped 8153 items (at 15 items/min)
2015-11-04 04:15:57 [scrapy] INFO: Crawled 8362 pages (at 15 pages/min), scraped 8170 items (at 17 items/min)
2015-11-04 04:16:30 [scrapy] INFO: Crawled 8371 pages (at 9 pages/min), scraped 8183 items (at 13 items/min)
2015-11-04 04:16:34 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mpw3gd0aEb4mMAjhf5xgl32bjPGx0y5N2kgh5G0qpXd9TxfsgyXa3NtXbQ4azPY9pfw%3D%3D&t=2> (referer: http://ir.opgen.com/phoenix.zhtml?c=253977&nyo=0&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:17:51 [scrapy] INFO: Crawled 8442 pages (at 71 pages/min), scraped 8241 items (at 58 items/min)
2015-11-04 04:17:54 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp8zAIlCZSpPzRRCh+YNoR5DF0Yd99RgspZ8P5YHbvH4g6pQ3rqFR4mRENh9Rj9KOgw%3D%3D&t=2> (referer: http://ir.opgen.com/phoenix.zhtml?c=253977&nyo=0&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:18:35 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp5q9w0O5tixgzWqJa96sKXjFm11OC6HFHjkvXd4GCkkdJZCUukf4hCYV9f441X4OVA%3D%3D&t=2> (referer: http://ir.opgen.com/phoenix.zhtml?c=253977&nyo=0&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:18:36 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp9pV+mLyZi3INYRNUxL4G2M6q3kaQ8QCMCjFhdWZommXytmux2EPzyXOKFr8FRnyfg%3D%3D&t=2> (referer: http://ir.opgen.com/phoenix.zhtml?c=253977&nyo=0&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:18:37 [scrapy] INFO: Crawled 8456 pages (at 14 pages/min), scraped 8259 items (at 18 items/min)
2015-11-04 04:18:37 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp32v3mng1eGDpCmuMZBTCzdBO4EA8D79jUqFsxxLNjZpJenQ4zN+cUU9+OM+bKmUBQ%3D%3D&t=2> (referer: http://ir.opgen.com/phoenix.zhtml?c=253977&nyo=0&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:18:42 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mpwr2yRHCANnFADPLGI+HFILsd2Kixiz42PO3Rgwkf%2FSLevqfi9XYigPtldFUKSHTWA%3D%3D&t=2> (referer: http://ir.opgen.com/phoenix.zhtml?c=253977&nyo=0&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:18:43 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp%2FkfOIMCUj1TfCR78az6hQSKvUntTuQIevgJw2tIt7UmfTjIzYrkH+WVzNRNrlZc7A%3D%3D&t=2> (referer: http://ir.opgen.com/phoenix.zhtml?c=253977&nyo=0&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:19:06 [scrapy] INFO: Crawled 8511 pages (at 55 pages/min), scraped 8299 items (at 40 items/min)
2015-11-04 04:20:01 [scrapy] INFO: Crawled 8576 pages (at 65 pages/min), scraped 8363 items (at 64 items/min)
2015-11-04 04:21:10 [scrapy] INFO: Crawled 8615 pages (at 39 pages/min), scraped 8423 items (at 60 items/min)
2015-11-04 04:22:01 [scrapy] INFO: Crawled 8654 pages (at 39 pages/min), scraped 8456 items (at 33 items/min)
2015-11-04 04:23:09 [scrapy] INFO: Crawled 8698 pages (at 44 pages/min), scraped 8501 items (at 45 items/min)
2015-11-04 04:24:05 [scrapy] INFO: Crawled 8738 pages (at 40 pages/min), scraped 8539 items (at 38 items/min)
2015-11-04 04:25:01 [scrapy] INFO: Crawled 8772 pages (at 34 pages/min), scraped 8576 items (at 37 items/min)
2015-11-04 04:26:01 [scrapy] INFO: Crawled 8818 pages (at 46 pages/min), scraped 8614 items (at 38 items/min)
2015-11-04 04:27:00 [scrapy] INFO: Crawled 8854 pages (at 36 pages/min), scraped 8660 items (at 46 items/min)
2015-11-04 04:28:01 [scrapy] INFO: Crawled 8900 pages (at 46 pages/min), scraped 8702 items (at 42 items/min)
2015-11-04 04:29:00 [scrapy] INFO: Crawled 8940 pages (at 40 pages/min), scraped 8742 items (at 40 items/min)
2015-11-04 04:30:01 [scrapy] INFO: Crawled 8988 pages (at 48 pages/min), scraped 8789 items (at 47 items/min)
2015-11-04 04:31:01 [scrapy] INFO: Crawled 9041 pages (at 53 pages/min), scraped 8837 items (at 48 items/min)
2015-11-04 04:32:26 [scrapy] INFO: Crawled 9089 pages (at 48 pages/min), scraped 8889 items (at 52 items/min)
2015-11-04 04:33:08 [scrapy] INFO: Crawled 9123 pages (at 34 pages/min), scraped 8921 items (at 32 items/min)
2015-11-04 04:34:06 [scrapy] INFO: Crawled 9197 pages (at 74 pages/min), scraped 8974 items (at 53 items/min)
2015-11-04 04:35:05 [scrapy] INFO: Crawled 9257 pages (at 60 pages/min), scraped 9040 items (at 66 items/min)
2015-11-04 04:36:11 [scrapy] INFO: Crawled 9284 pages (at 27 pages/min), scraped 9089 items (at 49 items/min)
2015-11-04 04:37:14 [scrapy] INFO: Crawled 9324 pages (at 40 pages/min), scraped 9123 items (at 34 items/min)
2015-11-04 04:38:18 [scrapy] INFO: Crawled 9380 pages (at 56 pages/min), scraped 9172 items (at 49 items/min)
2015-11-04 04:39:02 [scrapy] INFO: Crawled 9403 pages (at 23 pages/min), scraped 9193 items (at 21 items/min)
2015-11-04 04:40:17 [scrapy] INFO: Crawled 9446 pages (at 43 pages/min), scraped 9234 items (at 41 items/min)
2015-11-04 04:41:14 [scrapy] INFO: Crawled 9494 pages (at 48 pages/min), scraped 9283 items (at 49 items/min)
2015-11-04 04:42:09 [scrapy] INFO: Crawled 9558 pages (at 64 pages/min), scraped 9340 items (at 57 items/min)
2015-11-04 04:43:00 [scrapy] INFO: Crawled 9597 pages (at 39 pages/min), scraped 9388 items (at 48 items/min)
2015-11-04 04:44:02 [scrapy] INFO: Crawled 9676 pages (at 79 pages/min), scraped 9458 items (at 70 items/min)
2015-11-04 04:45:06 [scrapy] INFO: Crawled 9731 pages (at 55 pages/min), scraped 9509 items (at 51 items/min)
2015-11-04 04:46:19 [scrapy] INFO: Crawled 9794 pages (at 63 pages/min), scraped 9568 items (at 59 items/min)
2015-11-04 04:47:12 [scrapy] INFO: Crawled 9862 pages (at 68 pages/min), scraped 9631 items (at 63 items/min)
2015-11-04 04:48:19 [scrapy] INFO: Crawled 9932 pages (at 70 pages/min), scraped 9702 items (at 71 items/min)
2015-11-04 04:50:10 [scrapy] INFO: Crawled 9997 pages (at 65 pages/min), scraped 9761 items (at 59 items/min)
2015-11-04 04:50:11 [scrapy] ERROR: Error downloading <GET https://www.informatica.com/us/company/about-informatica>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:50:11 [scrapy] ERROR: Error downloading <GET https://www.informatica.com/us/company/news-and-events-calendar/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:50:11 [scrapy] ERROR: Error downloading <GET https://www.informatica.com/us/company/investor-relations/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:50:11 [scrapy] ERROR: Error downloading <GET https://www.informatica.com/us/company/about-informatica/leadership/default.aspx>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:51:04 [scrapy] INFO: Crawled 10007 pages (at 10 pages/min), scraped 9785 items (at 24 items/min)
2015-11-04 04:52:37 [scrapy] INFO: Crawled 10037 pages (at 30 pages/min), scraped 9802 items (at 17 items/min)
2015-11-04 04:53:42 [scrapy] ERROR: Spider error processing <GET http://www.adwo.com/shtml/case8.shtml> (referer: http://www.adwo.com/shtml/case.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 04:53:55 [scrapy] INFO: Crawled 10056 pages (at 19 pages/min), scraped 9808 items (at 6 items/min)
2015-11-04 04:55:37 [scrapy] INFO: Crawled 10062 pages (at 6 pages/min), scraped 9835 items (at 27 items/min)
2015-11-04 04:56:11 [scrapy] ERROR: Error downloading <GET https://liquor.com/user-profile/?wpid=1375339>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:56:11 [scrapy] INFO: Crawled 10062 pages (at 0 pages/min), scraped 9849 items (at 14 items/min)
2015-11-04 04:57:08 [scrapy] INFO: Crawled 10132 pages (at 70 pages/min), scraped 9902 items (at 53 items/min)
2015-11-04 04:58:12 [scrapy] INFO: Crawled 10184 pages (at 52 pages/min), scraped 9962 items (at 60 items/min)
2015-11-04 04:59:01 [scrapy] INFO: Crawled 10252 pages (at 68 pages/min), scraped 10019 items (at 57 items/min)
2015-11-04 05:00:16 [scrapy] INFO: Crawled 10326 pages (at 74 pages/min), scraped 10099 items (at 80 items/min)
2015-11-04 05:01:04 [scrapy] INFO: Crawled 10384 pages (at 58 pages/min), scraped 10149 items (at 50 items/min)
2015-11-04 05:01:48 [scrapy] ERROR: Spider error processing <GET http://es.unb.ca/apps/policy-repository/_resources/php/download-policy.php?id=Ypym> (referer: http://www.unb.ca/admissions/request-information.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:02:03 [scrapy] INFO: Crawled 10443 pages (at 59 pages/min), scraped 10215 items (at 66 items/min)
2015-11-04 05:03:02 [scrapy] INFO: Crawled 10534 pages (at 91 pages/min), scraped 10290 items (at 75 items/min)
2015-11-04 05:04:04 [scrapy] INFO: Crawled 10583 pages (at 49 pages/min), scraped 10338 items (at 48 items/min)
2015-11-04 05:05:05 [scrapy] INFO: Crawled 10609 pages (at 26 pages/min), scraped 10376 items (at 38 items/min)
2015-11-04 05:06:10 [scrapy] INFO: Crawled 10640 pages (at 31 pages/min), scraped 10408 items (at 32 items/min)
2015-11-04 05:06:21 [scrapy] ERROR: Spider error processing <GET http://support.belladati.com/exportword?pageId=13041962> (referer: http://support.belladati.com/techdoc/BellaDati+Developers+Network)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:07:07 [scrapy] INFO: Crawled 10722 pages (at 82 pages/min), scraped 10476 items (at 68 items/min)
2015-11-04 05:08:31 [scrapy] INFO: Crawled 10785 pages (at 63 pages/min), scraped 10536 items (at 60 items/min)
2015-11-04 05:09:10 [scrapy] INFO: Crawled 10814 pages (at 29 pages/min), scraped 10556 items (at 20 items/min)
2015-11-04 05:10:12 [scrapy] INFO: Crawled 10833 pages (at 19 pages/min), scraped 10592 items (at 36 items/min)
2015-11-04 05:10:12 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/techdoc/Installing+BellaDati+on+Mac+OS+X>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:10:12 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/download/temp/pdfexport-20151103-031115-2106-16463/BDTECHDOC-BellaDatiDevelopersNetwork-031115-2106-16464.pdf?contentType=application/pdf>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:10:12 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/techdoc/Installation+and+Update+Guide>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:11:35 [scrapy] INFO: Crawled 10890 pages (at 57 pages/min), scraped 10626 items (at 34 items/min)
2015-11-04 05:12:02 [scrapy] ERROR: Spider error processing <GET http://support.belladati.com/exportword?pageId=4620293> (referer: http://support.belladati.com/index/Home)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:12:03 [scrapy] INFO: Crawled 10892 pages (at 2 pages/min), scraped 10641 items (at 15 items/min)
2015-11-04 05:12:25 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/techdoc/Installing+BellaDati+Standalone>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:13:03 [scrapy] INFO: Crawled 10949 pages (at 57 pages/min), scraped 10680 items (at 39 items/min)
2015-11-04 05:14:03 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/pages/recentlyupdated.action?key=INDEX>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:14:09 [scrapy] INFO: Crawled 10971 pages (at 22 pages/min), scraped 10719 items (at 39 items/min)
2015-11-04 05:15:00 [scrapy] INFO: Crawled 11007 pages (at 36 pages/min), scraped 10751 items (at 32 items/min)
2015-11-04 05:16:07 [scrapy] INFO: Crawled 11060 pages (at 53 pages/min), scraped 10794 items (at 43 items/min)
2015-11-04 05:17:09 [scrapy] INFO: Crawled 11094 pages (at 34 pages/min), scraped 10841 items (at 47 items/min)
2015-11-04 05:18:09 [scrapy] INFO: Crawled 11145 pages (at 51 pages/min), scraped 10885 items (at 44 items/min)
2015-11-04 05:18:59 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/11-reasons-louisvillians-should-cross-the-ohio-for-indiana>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:18:59 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/ctrl-alt-dance-movie-premiere-will-make-the-perfect-valentines-day-date>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:18:59 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/dining-around-the-world-in-cincinnati>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:07 [scrapy] INFO: Crawled 11191 pages (at 46 pages/min), scraped 10920 items (at 35 items/min)
2015-11-04 05:19:34 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/17-american-craft-breweries-you-need-to-check-out>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:34 [scrapy] ERROR: Error downloading <GET https://impulcity.com/profile/forgot/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:34 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/the-9-best-st-patricks-day-celebrations-in-america>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:34 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/the-11-most-incredible-island-towns-in-america>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:39 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/7-of-the-most-amazing-snow-hikes-in-colorado>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:39 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/12-epic-music-festivals-you-dont-have-to-wait-til-summer-for>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:39 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/8-amazing-places-you-can-visit-without-a-passport>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:19:39 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/the-13-most-beautiful-off-the-beaten-path-beaches-on-earth>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:08 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/the-12-best-restaurants-in-ohio>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:20:08 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/grab-a-drink>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:08 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/food>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:08 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/live-music>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:08 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/scenic-spots>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:08 [scrapy] INFO: Crawled 11240 pages (at 49 pages/min), scraped 10975 items (at 55 items/min)
2015-11-04 05:20:18 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/date-ideas>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:20:18 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/the-11-strangest-abandoned-places-in-new-york>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:20:18 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/coffee-spots>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:20:36 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/the-11-strangest-abandoned-places-in-alabama>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:36 [scrapy] ERROR: Error downloading <GET https://impulcity.com/Cincinnati>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:36 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/crafted-cocktails>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:36 [scrapy] ERROR: Error downloading <GET https://impulcity.com/articles/13-of-the-most-breathtaking-views-of-canada>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:36 [scrapy] ERROR: Error downloading <GET https://impulcity.com/Louisville>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:40 [scrapy] ERROR: Error downloading <GET https://impulcity.com/travel>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:40 [scrapy] ERROR: Error downloading <GET https://impulcity.com/food-drink>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:43 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/outdoors>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:20:43 [scrapy] ERROR: Error downloading <GET https://impulcity.com/national>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:43 [scrapy] ERROR: Error downloading <GET https://impulcity.com/Chicago>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:43 [scrapy] ERROR: Error downloading <GET https://impulcity.com/Columbus>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:43 [scrapy] ERROR: Error downloading <GET https://impulcity.com/Nashville>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:43 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/all-activities>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:21:03 [scrapy] INFO: Crawled 11289 pages (at 49 pages/min), scraped 11026 items (at 51 items/min)
2015-11-04 05:21:07 [scrapy] ERROR: Error downloading <GET https://impulcity.com//explore/wine-bars>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:22:08 [scrapy] INFO: Crawled 11342 pages (at 53 pages/min), scraped 11083 items (at 57 items/min)
2015-11-04 05:23:03 [scrapy] INFO: Crawled 11401 pages (at 59 pages/min), scraped 11132 items (at 49 items/min)
2015-11-04 05:24:03 [scrapy] INFO: Crawled 11458 pages (at 57 pages/min), scraped 11177 items (at 45 items/min)
2015-11-04 05:25:06 [scrapy] INFO: Crawled 11492 pages (at 34 pages/min), scraped 11221 items (at 44 items/min)
2015-11-04 05:26:03 [scrapy] INFO: Crawled 11537 pages (at 45 pages/min), scraped 11259 items (at 38 items/min)
2015-11-04 05:27:05 [scrapy] INFO: Crawled 11590 pages (at 53 pages/min), scraped 11311 items (at 52 items/min)
2015-11-04 05:28:06 [scrapy] INFO: Crawled 11668 pages (at 78 pages/min), scraped 11376 items (at 65 items/min)
2015-11-04 05:29:04 [scrapy] INFO: Crawled 11725 pages (at 57 pages/min), scraped 11432 items (at 56 items/min)
2015-11-04 05:30:03 [scrapy] INFO: Crawled 11837 pages (at 112 pages/min), scraped 11488 items (at 56 items/min)
2015-11-04 05:31:02 [scrapy] INFO: Crawled 11894 pages (at 57 pages/min), scraped 11563 items (at 75 items/min)
2015-11-04 05:32:12 [scrapy] INFO: Crawled 11943 pages (at 49 pages/min), scraped 11621 items (at 58 items/min)
2015-11-04 05:33:08 [scrapy] INFO: Crawled 11994 pages (at 51 pages/min), scraped 11669 items (at 48 items/min)
2015-11-04 05:33:40 [scrapy] ERROR: Spider error processing <GET http://www.openmhealth.org/documentation/> (referer: http://www.openmhealth.org)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 05:34:07 [scrapy] INFO: Crawled 12107 pages (at 113 pages/min), scraped 11744 items (at 75 items/min)
2015-11-04 05:35:02 [scrapy] INFO: Crawled 12188 pages (at 81 pages/min), scraped 11840 items (at 96 items/min)
2015-11-04 05:36:01 [scrapy] INFO: Crawled 12277 pages (at 89 pages/min), scraped 11930 items (at 90 items/min)
2015-11-04 05:37:02 [scrapy] INFO: Crawled 12388 pages (at 111 pages/min), scraped 12052 items (at 122 items/min)
2015-11-04 05:38:02 [scrapy] INFO: Crawled 12539 pages (at 151 pages/min), scraped 12178 items (at 126 items/min)
2015-11-04 05:39:08 [scrapy] INFO: Crawled 12625 pages (at 86 pages/min), scraped 12277 items (at 99 items/min)
2015-11-04 05:39:36 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.fooducate.nutritionapp&referrer=utm_source%3Dfdct-redirect%26utm_campaign%3DFdct-Web-footer-button%26utm_medium%3Dna>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:39:42 [scrapy] WARNING: Expected response size (43633470) larger than download warn size (33554432).
2015-11-04 05:40:01 [scrapy] INFO: Crawled 12717 pages (at 92 pages/min), scraped 12364 items (at 87 items/min)
2015-11-04 05:40:20 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.fooducate.nutritionapp&referrer=utm_source%3Dfdct-redirect%26utm_campaign%3DFdct-Blog-PostContent%26utm_medium%3Dna>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:41:03 [scrapy] INFO: Crawled 12799 pages (at 82 pages/min), scraped 12457 items (at 93 items/min)
2015-11-04 05:42:14 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.fooducate.nutritionapp&referrer=utm_source%3Dfdct-redirect%26utm_campaign%3Dblog-NewHereText%26utm_medium%3Dna>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:42:14 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.fooducate.nutritionapp&referrer=utm_source%3Dfdct-redirect%26utm_campaign%3Dblog-NewHereIcon%26utm_medium%3Dna>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:42:14 [scrapy] INFO: Crawled 12862 pages (at 63 pages/min), scraped 12503 items (at 46 items/min)
2015-11-04 05:42:39 [scrapy] ERROR: Error downloading <GET https://my.labguru.com/signup?plan=industry>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:43:51 [scrapy] ERROR: Spider error processing <GET http://www.adwo.com/shtml/20140908.shtml> (referer: http://www.adwo.com/shtml/news.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:45:21 [scrapy] ERROR: Spider error processing <GET http://www.adwo.com/shtml/20150316-02.shtml> (referer: http://www.adwo.com/shtml/news.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:46:06 [scrapy] INFO: Crawled 12923 pages (at 61 pages/min), scraped 12564 items (at 61 items/min)
2015-11-04 05:47:38 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/phoenix.zhtml?c=253977&cat=news&id=2096107&p=RssLanding> (referer: http://opgen.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 99, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 05:47:38 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/phoenix.zhtml?c=253977&cat=news&id=2089458&p=RssLanding> (referer: http://opgen.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 99, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 05:47:38 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/phoenix.zhtml?c=253977&cat=news&id=2079545&p=RssLanding> (referer: http://opgen.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 99, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 05:47:38 [scrapy] INFO: Crawled 12951 pages (at 28 pages/min), scraped 12591 items (at 27 items/min)
2015-11-04 05:48:34 [scrapy] ERROR: Spider error processing <GET http://www.adwo.com/shtml/20151026.shtml> (referer: http://www.adwo.com/shtml/news.shtml)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 05:48:43 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/techdoc/Whitelabeling+BellaDati>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:48:43 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/display/INDEX/Home>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:48:43 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/doc/BellaDati+2.7.14.1>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:48:43 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/doc/BellaDati+User's+Documentation>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:48:43 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/download/temp/pdfexport-20151103-031115-2141-16465/INDEX-Home-031115-2141-16466.pdf?contentType=application/pdf>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:48:43 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/pages/tinyurl.action?urlIdentifier=BYBG>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:48:59 [scrapy] INFO: Crawled 12957 pages (at 6 pages/min), scraped 12608 items (at 17 items/min)
2015-11-04 05:49:05 [scrapy] INFO: Crawled 12979 pages (at 22 pages/min), scraped 12614 items (at 6 items/min)
2015-11-04 05:54:30 [scrapy] INFO: Crawled 13014 pages (at 35 pages/min), scraped 12669 items (at 55 items/min)
2015-11-04 05:54:34 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20MpxwpoFTE6N6ANAjK7YWWTLeT5B0%2FnB2v6ysU+IsPF+b7BCN9PY2TZITWJh1NCh0vIA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:54:48 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp7DD7cPrB%2FbhbiffqfaM0ybf71QaBTM3WSNMjZNNMi0meoR1DBAU1ueWuHw1J8CxNg%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:54:48 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp9M9wkJg7U021tLMm0jiwBs1mM6YzuxCvSq4Z%2FJaVTCV6hDNuZz12Kbr%2FmbQgt1WkA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:54:48 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mpz77MwDrgTz8juceHL5wJrP2jwz26nUpXxMizLjDBr1gp2a+GOaK7seZker8L6hN2A%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:54:50 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp13KVZBoHKET%2FbmnD9PLfQVlgpUJH92gX12cZKxfW06MdEhFDiZPHxIyVQfrkp0ZQA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:55:08 [scrapy] INFO: Crawled 13089 pages (at 75 pages/min), scraped 12705 items (at 36 items/min)
2015-11-04 05:55:29 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp0N2EqGIdPBldds8bmMpwbiYvHhFLFZ36F1gTLnumkSWygIf8Lbqu6J7x24s%2FmFPww%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:55:30 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp6HZxcLvqlX%2FpNFjFXLQw8CRH7BUoV9xL9J9FSDyF0EprJonZ2Np0FV3ji2RIRPlWg%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:55:43 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp97%2FEs%2FWHSjTXqyIKZuBkK650LW+TY124mC2a+h7Th3vhzp11n8E+jLgY6vH+JYTtg%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:55:51 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp++Rc0uoXwR0eVnA4HvS%2F8sruIowNPkoXK+C8VQNP5zTwHA1+otmetMPlIQ6+Ly32g%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:56:00 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp8hseJnuIyUbK3n+gk1iy7ISR1ZRuZUPu8tTBxMmUkJdJER8BvqjP1g%2Fk4KxXoSkSA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:56:02 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp4VNlImq+ES+ckJcQPRQYSY3xNM3Cs%2FS5fFWTUbnyvb9L1Tp+eX1fJDNiD%2FeVKJ+RQ%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:56:24 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20MpwTui0bKux31CKu7cIjYAc4jrC7ChNfIc%2FRaLmdJf8dQKzgLH1Nv3GGETkngj1PsHA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:56:33 [scrapy] INFO: Crawled 13104 pages (at 15 pages/min), scraped 12739 items (at 34 items/min)
2015-11-04 05:56:34 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20MpxFzIvYPTYWQX4xgSxEdbAn3MiB8W7iAOaQgAHje87g8D6TDUIjYEChCRmGhkrWI0g%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:56:44 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp0O3MbXpVyq0zszth%2Fs8Sn4I7XWV2Ro28q+hrQSknhO5r9RmmoIrKEaJpPNfZ+CW4g%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:56:46 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mpxvu1kukVIeQzcAfuszrLZaEzBJVg4KsgbqAp0VNoXNgIh4IqtfwIZo25+mREcMTZA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:56:49 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp9L%2FWpVMJjlqhowJrEjj9jwBCUMCzaJn9eFPmWBNxxOK2I4qJU+OfF3vbE%2FwB3xaiQ%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:57:17 [scrapy] INFO: Crawled 13122 pages (at 18 pages/min), scraped 12759 items (at 20 items/min)
2015-11-04 05:57:27 [scrapy] ERROR: Error downloading <GET http://www.opentabs.de>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 05:58:00 [scrapy] INFO: Crawled 13158 pages (at 36 pages/min), scraped 12791 items (at 32 items/min)
2015-11-04 05:59:01 [scrapy] INFO: Crawled 13214 pages (at 56 pages/min), scraped 12847 items (at 56 items/min)
2015-11-04 06:00:05 [scrapy] INFO: Crawled 13278 pages (at 64 pages/min), scraped 12911 items (at 64 items/min)
2015-11-04 06:01:07 [scrapy] INFO: Crawled 13338 pages (at 60 pages/min), scraped 12979 items (at 68 items/min)
2015-11-04 06:02:05 [scrapy] INFO: Crawled 13402 pages (at 64 pages/min), scraped 13035 items (at 56 items/min)
2015-11-04 06:03:01 [scrapy] INFO: Crawled 13451 pages (at 49 pages/min), scraped 13085 items (at 50 items/min)
2015-11-04 06:04:02 [scrapy] INFO: Crawled 13502 pages (at 51 pages/min), scraped 13118 items (at 33 items/min)
2015-11-04 06:04:41 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp3%2F4VhhiwxsvKQjhCk22UB2idF5ed6gSXlSvqlR9Tzw%2Fp8uAK269YVWUoQFeH52iUg%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:04:52 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20Mp3IxSPwSm6yJc+Cp06pWZGSTLqq6RG7zOZ1DfFR2q9gWSVmugIsMkcWtt4CD0mGYzQ%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:05:13 [scrapy] INFO: Crawled 13537 pages (at 35 pages/min), scraped 13146 items (at 28 items/min)
2015-11-04 06:05:33 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp8RfJp2%2F5WuW7a8M85cYkoHyQbOp+irjdxwhMqqVklNaDGumtjg8ZB2TZrL1KxFdag%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:05:40 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20MpwHp3RDxvl7jRLBdi5AqpgaveyR17PtrI5iqIHFmfyS3kY2vHMaPZptZZdPB4YwuBA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:05:54 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp0eZpNS8hjuRh058CfcfJUojgMQsyarOperro8ncQYq6n2LZpysY4OT9mgTifay03w%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:06:05 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mpz9oKSrW+Xj+dLoFuO45BgCzQ3HKPVAHDvGOKDZl8PLSlnfdf4Q8oUjqDdDKEnX8Og%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:06:11 [scrapy] INFO: Crawled 13554 pages (at 17 pages/min), scraped 13175 items (at 29 items/min)
2015-11-04 06:06:12 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp9LcWOAcx0EAgiiBbBfCDf+UiGBqSJgC6F7X8lSJoC%2F8IKwg0VmQbrFB2751ZD3f%2Fw%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:06:18 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp+3JcSYldlgpeCQGqV4s%2Fe4TR6s4M8B0CuhAbLgeSoOsDSRWEDvlz6VPPBmedsDnLg%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:06:20 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mp8RRu1WOuAeLgr87svsTseFgbAw0RFVb8R4zIq0+M+Db8xgq%2FQtHw9ZSxB8EuWdttA%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:06:22 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500263915312&item=g7rqBLVLuv81UAmrh20Mpxu1pmhIbyNUioVtI7bBXBzT%2FOdFhi3HzCkRcBQiL06jkjMNkaeeNZga+sLPWlxj%2FQ%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:06:24 [scrapy] ERROR: Spider error processing <GET http://ir.opgen.com/External.File?cb=635802500264071335&item=g7rqBLVLuv81UAmrh20MpzCHGYsxQkL+ogQVfsuDqWpZs2SVBhdh6+PVpreRq9yzZIDQdxtFIrYK4zMI2sN+CQ%3D%3D&t=2> (referer: http://ir.opgen.com/Phoenix.zhtml?c=253977&p=irol-news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:07:02 [scrapy] INFO: Crawled 13572 pages (at 18 pages/min), scraped 13191 items (at 16 items/min)
2015-11-04 06:08:02 [scrapy] INFO: Crawled 13583 pages (at 11 pages/min), scraped 13207 items (at 16 items/min)
2015-11-04 06:09:06 [scrapy] INFO: Crawled 13653 pages (at 70 pages/min), scraped 13271 items (at 64 items/min)
2015-11-04 06:10:07 [scrapy] INFO: Crawled 13691 pages (at 38 pages/min), scraped 13311 items (at 40 items/min)
2015-11-04 06:11:07 [scrapy] INFO: Crawled 13727 pages (at 36 pages/min), scraped 13352 items (at 41 items/min)
2015-11-04 06:12:04 [scrapy] INFO: Crawled 13788 pages (at 61 pages/min), scraped 13412 items (at 60 items/min)
2015-11-04 06:13:04 [scrapy] INFO: Crawled 13860 pages (at 72 pages/min), scraped 13477 items (at 65 items/min)
2015-11-04 06:14:01 [scrapy] INFO: Crawled 13916 pages (at 56 pages/min), scraped 13541 items (at 64 items/min)
2015-11-04 06:15:04 [scrapy] INFO: Crawled 13988 pages (at 72 pages/min), scraped 13613 items (at 72 items/min)
2015-11-04 06:16:01 [scrapy] INFO: Crawled 14060 pages (at 72 pages/min), scraped 13676 items (at 63 items/min)
2015-11-04 06:17:05 [scrapy] INFO: Crawled 14124 pages (at 64 pages/min), scraped 13748 items (at 72 items/min)
2015-11-04 06:18:02 [scrapy] INFO: Crawled 14192 pages (at 68 pages/min), scraped 13812 items (at 64 items/min)
2015-11-04 06:19:01 [scrapy] INFO: Crawled 14260 pages (at 68 pages/min), scraped 13876 items (at 64 items/min)
2015-11-04 06:20:04 [scrapy] INFO: Crawled 14332 pages (at 72 pages/min), scraped 13956 items (at 80 items/min)
2015-11-04 06:21:01 [scrapy] INFO: Crawled 14404 pages (at 72 pages/min), scraped 14028 items (at 72 items/min)
2015-11-04 06:22:01 [scrapy] INFO: Crawled 14491 pages (at 87 pages/min), scraped 14108 items (at 80 items/min)
2015-11-04 06:23:00 [scrapy] INFO: Crawled 14563 pages (at 72 pages/min), scraped 14179 items (at 71 items/min)
2015-11-04 06:24:04 [scrapy] INFO: Crawled 14634 pages (at 71 pages/min), scraped 14251 items (at 72 items/min)
2015-11-04 06:25:04 [scrapy] INFO: Crawled 14700 pages (at 66 pages/min), scraped 14322 items (at 71 items/min)
2015-11-04 06:26:03 [scrapy] INFO: Crawled 14770 pages (at 70 pages/min), scraped 14386 items (at 64 items/min)
2015-11-04 06:27:05 [scrapy] INFO: Crawled 14831 pages (at 61 pages/min), scraped 14453 items (at 67 items/min)
2015-11-04 06:27:52 [scrapy] ERROR: Error downloading <GET http://www.vocaliq.com>: DNS lookup failed: address 'www.vocaliq.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:27:52 [scrapy] ERROR: Error downloading <GET http://www.healthdataminder.com>: DNS lookup failed: address 'www.healthdataminder.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:27:52 [scrapy] ERROR: Error downloading <GET http://www.opendesks.com>: DNS lookup failed: address 'www.opendesks.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:27:52 [scrapy] ERROR: Error downloading <GET http://www.nuospace.com>: DNS lookup failed: address 'www.nuospace.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:27:57 [scrapy] ERROR: Error downloading <GET http://www.floridasrealtynetwork.com>: DNS lookup failed: address 'www.floridasrealtynetwork.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:28:03 [scrapy] ERROR: Error downloading <GET http://www.tipjoy.com>: DNS lookup failed: address 'www.tipjoy.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:28:03 [scrapy] INFO: Crawled 14902 pages (at 71 pages/min), scraped 14516 items (at 63 items/min)
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/watch-demo-edge-reg.html?utm_campaign=watch-demo&utm_content=edge&utm_medium=website&utm_source=follow>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/contact-sales-reg.html?utm_campaign=contact-us&utm_content=contact-us&utm_medium=website&utm_source=follow>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/contact-sales-reg.html?int_campaign=contact-us&int_content=pricing&int_medium=website&int_source=pricing>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-delivering-customized-experiences-in-travel-and-hospitality-reg.html?int_campaign=ebook&int_content=delivering-customized-experiences-in-travel-and-hospitality&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-apis-are-different-than-integration-reg.html?int_campaign=ebook&int_content=apis-different-than-integration&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-two-speed-it-agility-and-stability-with-apis-reg.html?int_campaign=ebook&int_content=two-speed-it-agility-and-stability-with-apis&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/eBook-The-Role-of-APIs-in-Media-and-Entertainment.html?int_campaign=ebook&int_content=the-role-of-apis-in-media-and-entertainment&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:29 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-the-definitive-guide-to-api-management-reg.html?int_campaign=ebook&int_content=the-definitive-guide-to-api-management&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:55 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/digital-transformation-ebook-web-reg.html?int_campaign=ebook&int_content=digital-transformation&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:55 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/watch-demo-edge-reg.html?int_campaign=watch-demo&int_content=retail&int_medium=website&int_source=solution>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:55 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-predictive-analytics-build-vs-buy-reg.html?int_campaign=ebook&int_content=predictive-analytics-build-vs-buy&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:55 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-whats-your-problem-reg.html?int_campaign=ebook&int_medium=website>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:55 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-why-apis-reg.html?int_campaign=ebook&int_medium=website>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:07 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/watch-demo-edge-reg.html?int_campaign=watch-demo&int_content=telco&int_medium=website&int_source=solution>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:07 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-digital-ready-it-an-api-platform-story-reg.html?int_campaign=ebook&int_content=digital-ready-it-an-api-platform-story&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:07 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/watch-demo-edge-reg.html?int_campaign=watch-demo&int_content=finance&int_medium=website&int_source=solution>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:07 [scrapy] ERROR: Error downloading <GET https://healthapix.apigee.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:08 [scrapy] INFO: Crawled 14997 pages (at 95 pages/min), scraped 14589 items (at 73 items/min)
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET https://blog.documentcloud.org/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET http://www.concurrentinc.com/contact/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET http://www.concurrentinc.com/terms/www.concurrent.com/copyright>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET http://www.concurrentinc.com/privacy-policy/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/watch-demo-edge-reg.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-apis-for-dummies-reg.html?int_campaign=ebook&int_content=dummies&int_medium=website&int_source=resources-banner>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/digital-transformation-ebook-web-reg.html?int_campaign=ebook&int_content=digital-transformation&int_medium=website&int_source=resources-banner>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/2015-10-28-webcast-understand-your-developer-reg.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:09 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/ebook-apis-are-different-than-integration-reg.html?int_campaign=ebook&int_content=apis-different-than-integration&int_medium=website&int_source=resources-banner>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:20 [scrapy] WARNING: Expected response size (43633470) larger than download warn size (33554432).
2015-11-04 06:29:21 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/eBook-learn-and-adapt-engaging-customers-the-right-way-reg.html?int_campaign=ebook&int_content=learn-and-adapt-engaging-customers-the-right-way&int_medium=website&int_source=resources-main>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:21 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/gartner-report-reg.html?int_campaign=website&int_content=apigee-and-the-gartner-magic-quadrant&int_source=resources-banner>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:21 [scrapy] ERROR: Error downloading <GET https://pages.apigee.com/2015-11-12-webcast-product-demo-reg.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:26 [scrapy] INFO: Crawled 15043 pages (at 46 pages/min), scraped 14615 items (at 26 items/min)
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET https://www.documentcloud.org/reset_password>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET https://www.documentcloud.org/opensource>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET https://www.documentcloud.org/apply>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET https://www.documentcloud.org/help>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET https://www.documentcloud.org/about>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET https://www.documentcloud.org/public/search/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/index/Home?lang=cs>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/index/%20%20%20%20/display/~rmicka%0A>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/index/Home?lang=en>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/display/BellaDati27/BellaDati+Mobile>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/display/INDEX/Home?showChildren=true>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/pages/diffpagesbyversion.action?pageId=4620293&selectedPageVersions=495&selectedPageVersions=496>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://support.belladati.com/display/BellaDati27/BellaApps>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:52 [scrapy] ERROR: Error downloading <GET http://tnuck.com/collections/ladies-vests/products/mens-polarquilt-zip-in-liner>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/user/profile/berdschluechter>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/marketplace/product/878/Ex%20Libris>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/redbutton/static/800/Krall+dir+einen+CHF+200.-+Gutschein+von+Ex+Libris+Woche+4>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/user/profile/desiree>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:31:02 [scrapy] INFO: Crawled 15046 pages (at 3 pages/min), scraped 14640 items (at 25 items/min)
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET http://tnuck.com/products/natural-faux-fur-open-vest>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET http://tnuck.com/collections/ladies-vests/products/natural-faux-fur-open-vest>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET http://tnuck.com/collections/run/products/seafoam-stripe-tankini-w-2-pockets>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET http://tnuck.com/products/patriot-red-star-shorts>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:31:02 [scrapy] ERROR: Error downloading <GET http://tnuck.com/collections/run/products/seafoam-green-stripe-shorts>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:32:03 [scrapy] INFO: Crawled 15110 pages (at 64 pages/min), scraped 14698 items (at 58 items/min)
2015-11-04 06:32:05 [scrapy] ERROR: Error downloading <GET http://assets.labguru.com/plugins/UpFolder.dmg>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://assets.labguru.com/plugins/UpFolder.dmg took longer than 180.0 seconds..
2015-11-04 06:33:10 [scrapy] INFO: Crawled 15168 pages (at 58 pages/min), scraped 14756 items (at 58 items/min)
2015-11-04 06:33:10 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/marketplace/product/56/2%20Tickets%20f%C3%BCr%2077%20Bombay%20Street%20in%20Herisau>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:33:10 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/marketplace/product/11/2%20Tickets%20f%C3%BCr%2077%20Bombay%20Street%20in%20Solothurn%20am%2004.%20November>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:33:10 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/marketplace/product/8/1%20Avatar%203D%20Edition%20(Blu-ray%20&%20DVD)>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:33:10 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/marketplace/product/9/2%20Fink%20Konzert-Tickets>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:33:10 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/user/profile>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:33:10 [scrapy] ERROR: Error downloading <GET http://www.joiz.ch/home>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:34:06 [scrapy] INFO: Crawled 15200 pages (at 32 pages/min), scraped 14786 items (at 30 items/min)
2015-11-04 06:35:31 [scrapy] INFO: Crawled 15246 pages (at 46 pages/min), scraped 14833 items (at 47 items/min)
2015-11-04 06:36:11 [scrapy] INFO: Crawled 15288 pages (at 42 pages/min), scraped 14868 items (at 35 items/min)
2015-11-04 06:37:07 [scrapy] INFO: Crawled 15337 pages (at 49 pages/min), scraped 14924 items (at 56 items/min)
2015-11-04 06:38:34 [scrapy] INFO: Crawled 15377 pages (at 40 pages/min), scraped 14963 items (at 39 items/min)
2015-11-04 06:39:01 [scrapy] INFO: Crawled 15410 pages (at 33 pages/min), scraped 14995 items (at 32 items/min)
2015-11-04 06:40:08 [scrapy] INFO: Crawled 15433 pages (at 23 pages/min), scraped 15027 items (at 32 items/min)
2015-11-04 06:40:08 [scrapy] ERROR: Error downloading <GET https://thedrop.mobi/login/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:08 [scrapy] ERROR: Error downloading <GET https://thedrop.mobi/signup/?s=1>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:41:12 [scrapy] INFO: Crawled 15481 pages (at 48 pages/min), scraped 15075 items (at 48 items/min)
2015-11-04 06:42:12 [scrapy] INFO: Crawled 15519 pages (at 38 pages/min), scraped 15110 items (at 35 items/min)
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:42:30 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f023e0a6c08>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 06:42:31 [scrapy] ERROR: Error downloading <GET https://knowledgebase.cradlepoint.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:43:10 [scrapy] INFO: Crawled 15564 pages (at 45 pages/min), scraped 15151 items (at 41 items/min)
2015-11-04 06:43:46 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/redbutton/static/799/Live+at+joiz+mit+Anti+Flag+-+sei+dabei!>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:43:46 [scrapy] ERROR: Error downloading <GET http://www.joiz.ch/blog/youtube>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:43:46 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/marketplace/product/4/2%20Tickets%20f%C3%BCr%2077%20Bombay%20Street%20in%20Neuch%C3%A2tel>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:43:46 [scrapy] ERROR: Error downloading <GET https://www.joiz.ch/marketplace/product/67/Pel%C3%A9%20Sports%20T-Shirt%20(Gr%C3%B6sse%20M)>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:44:00 [scrapy] ERROR: Error downloading <GET http://www.zingfin.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:44:00 [scrapy] INFO: Crawled 15584 pages (at 20 pages/min), scraped 15178 items (at 27 items/min)
2015-11-04 06:44:01 [scrapy] ERROR: Error downloading <GET http://www.natrogen.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:45:26 [scrapy] INFO: Crawled 15631 pages (at 47 pages/min), scraped 15225 items (at 47 items/min)
2015-11-04 06:46:10 [scrapy] INFO: Crawled 15670 pages (at 39 pages/min), scraped 15256 items (at 31 items/min)
2015-11-04 06:47:00 [scrapy] INFO: Crawled 15705 pages (at 35 pages/min), scraped 15298 items (at 42 items/min)
2015-11-04 06:48:16 [scrapy] INFO: Crawled 15751 pages (at 46 pages/min), scraped 15336 items (at 38 items/min)
2015-11-04 06:48:17 [scrapy] ERROR: Error downloading <GET http://thedrop.mobi/discography/f4fdbb4c-e4b7-47a0-b83b-d91bbfcfa387/Ariana-Grande>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:42 [scrapy] ERROR: Error downloading <GET http://lighthouse.opgen.com/>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:48:49 [scrapy] ERROR: Error downloading <GET http://thedrop.mobi/discography/149f91ef-1287-46da-9a8e-87fee02f1471/Pharrell-Williams>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:49 [scrapy] ERROR: Error downloading <GET http://thedrop.mobi/single/4wcocwnq44bcDCe1M1dtVG/Fall-Out-Boy/profile.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:49 [scrapy] ERROR: Error downloading <GET http://blog.shoefitr.com/2014/05/21/introducing-arch-view-for-athletic-shoes/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:49 [scrapy] ERROR: Error downloading <GET https://my.belladati.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:48:49 [scrapy] ERROR: Error downloading <GET https://my.belladati.com/en/mytrgiman/product/downloadlist>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET https://www.tango.me/careers>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET https://www.hirevue.com/blog/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET http://thedrop.mobi/single/4wcocwnq44bcDCe1M1dtVG/Fall-Out-Boy/modal.lockme.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET http://thedrop.mobi/discography/3e1f2ee4-16be-4406-bf18-6173840cf2b1/Martin-Garrix>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET http://thedrop.mobi/single/6qPaWwSDkDEkswL71I0Zbm/Calvin-Harris/modal.lockme.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET http://thedrop.mobi/discography/e3e0abcd-7671-4482-a9d8-462f5acc9be5/Incubus>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET https://service.belladati.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET http://lib.unb.ca/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:48:51 [scrapy] ERROR: Error downloading <GET http://www.lib.unb.ca/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:49:08 [scrapy] INFO: Crawled 15810 pages (at 59 pages/min), scraped 15372 items (at 36 items/min)
2015-11-04 06:49:23 [scrapy] ERROR: Error downloading <GET https://my.labguru.com/system/attachments/images?type=image_bank>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:23 [scrapy] ERROR: Error downloading <GET https://my.labguru.com/system/attachments?type=attachments>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:23 [scrapy] ERROR: Error downloading <GET https://www.hirevue.com/demo/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:26 [scrapy] ERROR: Error downloading <GET https://my.labguru.com/sessions/forgot_password>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:26 [scrapy] ERROR: Error downloading <GET https://my.labguru.com/signup?utm_campaign=my&utm_medium=link&utm_source=my>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:26 [scrapy] ERROR: Error downloading <GET https://www.labguru.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:05 [scrapy] INFO: Crawled 15876 pages (at 66 pages/min), scraped 15436 items (at 64 items/min)
2015-11-04 06:51:34 [scrapy] INFO: Crawled 15924 pages (at 48 pages/min), scraped 15485 items (at 49 items/min)
2015-11-04 06:52:13 [scrapy] INFO: Crawled 15935 pages (at 11 pages/min), scraped 15493 items (at 8 items/min)
2015-11-04 06:53:18 [scrapy] INFO: Crawled 15968 pages (at 33 pages/min), scraped 15525 items (at 32 items/min)
2015-11-04 06:54:18 [scrapy] INFO: Crawled 15976 pages (at 8 pages/min), scraped 15533 items (at 8 items/min)
2015-11-04 06:55:04 [scrapy] INFO: Crawled 15978 pages (at 2 pages/min), scraped 15542 items (at 9 items/min)
2015-11-04 06:56:04 [scrapy] INFO: Crawled 15998 pages (at 20 pages/min), scraped 15560 items (at 18 items/min)
2015-11-04 06:57:00 [scrapy] INFO: Crawled 16012 pages (at 14 pages/min), scraped 15569 items (at 9 items/min)
2015-11-04 06:58:05 [scrapy] INFO: Crawled 16019 pages (at 7 pages/min), scraped 15582 items (at 13 items/min)
2015-11-04 06:59:34 [scrapy] INFO: Crawled 16036 pages (at 17 pages/min), scraped 15593 items (at 11 items/min)
2015-11-04 07:00:48 [scrapy] INFO: Crawled 16041 pages (at 5 pages/min), scraped 15601 items (at 8 items/min)
2015-11-04 07:01:30 [scrapy] INFO: Crawled 16041 pages (at 0 pages/min), scraped 15606 items (at 5 items/min)
2015-11-04 07:02:02 [scrapy] INFO: Crawled 16049 pages (at 8 pages/min), scraped 15611 items (at 5 items/min)
2015-11-04 07:03:04 [scrapy] INFO: Crawled 16063 pages (at 14 pages/min), scraped 15621 items (at 10 items/min)
2015-11-04 07:04:27 [scrapy] INFO: Crawled 16082 pages (at 19 pages/min), scraped 15639 items (at 18 items/min)
2015-11-04 07:05:12 [scrapy] INFO: Crawled 16091 pages (at 9 pages/min), scraped 15651 items (at 12 items/min)
2015-11-04 07:06:10 [scrapy] INFO: Crawled 16110 pages (at 19 pages/min), scraped 15664 items (at 13 items/min)
2015-11-04 07:07:10 [scrapy] INFO: Crawled 16122 pages (at 12 pages/min), scraped 15679 items (at 15 items/min)
2015-11-04 07:08:16 [scrapy] INFO: Crawled 16138 pages (at 16 pages/min), scraped 15695 items (at 16 items/min)
2015-11-04 07:09:36 [scrapy] INFO: Crawled 16154 pages (at 16 pages/min), scraped 15711 items (at 16 items/min)
2015-11-04 07:10:15 [scrapy] INFO: Crawled 16159 pages (at 5 pages/min), scraped 15719 items (at 8 items/min)
2015-11-04 07:11:06 [scrapy] INFO: Crawled 16173 pages (at 14 pages/min), scraped 15730 items (at 11 items/min)
2015-11-04 07:12:51 [scrapy] INFO: Crawled 16186 pages (at 13 pages/min), scraped 15746 items (at 16 items/min)
2015-11-04 07:13:41 [scrapy] INFO: Crawled 16191 pages (at 5 pages/min), scraped 15751 items (at 5 items/min)
2015-11-04 07:14:07 [scrapy] INFO: Crawled 16196 pages (at 5 pages/min), scraped 15756 items (at 5 items/min)
2015-11-04 07:15:45 [scrapy] INFO: Crawled 16211 pages (at 15 pages/min), scraped 15769 items (at 13 items/min)
2015-11-04 07:16:22 [scrapy] INFO: Crawled 16215 pages (at 4 pages/min), scraped 15776 items (at 7 items/min)
2015-11-04 07:17:01 [scrapy] INFO: Crawled 16226 pages (at 11 pages/min), scraped 15782 items (at 6 items/min)
2015-11-04 07:18:17 [scrapy] INFO: Crawled 16242 pages (at 16 pages/min), scraped 15796 items (at 14 items/min)
2015-11-04 07:19:35 [scrapy] INFO: Crawled 16255 pages (at 13 pages/min), scraped 15810 items (at 14 items/min)
2015-11-04 07:20:22 [scrapy] INFO: Crawled 16263 pages (at 8 pages/min), scraped 15817 items (at 7 items/min)
2015-11-04 07:21:02 [scrapy] INFO: Crawled 16269 pages (at 6 pages/min), scraped 15824 items (at 7 items/min)
2015-11-04 07:22:16 [scrapy] INFO: Crawled 16285 pages (at 16 pages/min), scraped 15838 items (at 14 items/min)
2015-11-04 07:23:50 [scrapy] INFO: Crawled 16298 pages (at 13 pages/min), scraped 15853 items (at 15 items/min)
2015-11-04 07:24:22 [scrapy] INFO: Crawled 16303 pages (at 5 pages/min), scraped 15859 items (at 6 items/min)
2015-11-04 07:25:26 [scrapy] INFO: Crawled 16314 pages (at 11 pages/min), scraped 15871 items (at 12 items/min)
2015-11-04 07:26:05 [scrapy] INFO: Crawled 16322 pages (at 8 pages/min), scraped 15881 items (at 10 items/min)
2015-11-04 07:27:01 [scrapy] INFO: Crawled 16338 pages (at 16 pages/min), scraped 15894 items (at 13 items/min)
2015-11-04 07:28:01 [scrapy] INFO: Crawled 16352 pages (at 14 pages/min), scraped 15907 items (at 13 items/min)
2015-11-04 07:29:00 [scrapy] INFO: Crawled 16364 pages (at 12 pages/min), scraped 15918 items (at 11 items/min)
2015-11-04 07:30:06 [scrapy] INFO: Crawled 16378 pages (at 14 pages/min), scraped 15929 items (at 11 items/min)
2015-11-04 07:31:39 [scrapy] INFO: Crawled 16393 pages (at 15 pages/min), scraped 15944 items (at 15 items/min)
2015-11-04 07:32:21 [scrapy] INFO: Crawled 16395 pages (at 2 pages/min), scraped 15951 items (at 7 items/min)
2015-11-04 07:33:01 [scrapy] INFO: Crawled 16404 pages (at 9 pages/min), scraped 15962 items (at 11 items/min)
2015-11-04 07:34:03 [scrapy] INFO: Crawled 16425 pages (at 21 pages/min), scraped 15980 items (at 18 items/min)
2015-11-04 07:35:03 [scrapy] INFO: Crawled 16436 pages (at 11 pages/min), scraped 15993 items (at 13 items/min)
2015-11-04 07:36:02 [scrapy] INFO: Crawled 16450 pages (at 14 pages/min), scraped 16006 items (at 13 items/min)
2015-11-04 07:37:03 [scrapy] INFO: Crawled 16460 pages (at 10 pages/min), scraped 16013 items (at 7 items/min)
2015-11-04 07:38:30 [scrapy] INFO: Crawled 16476 pages (at 16 pages/min), scraped 16026 items (at 13 items/min)
2015-11-04 07:39:06 [scrapy] INFO: Crawled 16479 pages (at 3 pages/min), scraped 16030 items (at 4 items/min)
2015-11-04 07:40:00 [scrapy] INFO: Crawled 16480 pages (at 1 pages/min), scraped 16037 items (at 7 items/min)
2015-11-04 07:41:13 [scrapy] INFO: Crawled 16490 pages (at 10 pages/min), scraped 16047 items (at 10 items/min)
2015-11-04 07:42:01 [scrapy] INFO: Crawled 16504 pages (at 14 pages/min), scraped 16057 items (at 10 items/min)
2015-11-04 07:43:07 [scrapy] INFO: Crawled 16514 pages (at 10 pages/min), scraped 16071 items (at 14 items/min)
2015-11-04 07:44:14 [scrapy] INFO: Crawled 16529 pages (at 15 pages/min), scraped 16084 items (at 13 items/min)
2015-11-04 07:45:14 [scrapy] INFO: Crawled 16540 pages (at 11 pages/min), scraped 16093 items (at 9 items/min)
2015-11-04 07:46:44 [scrapy] INFO: Crawled 16554 pages (at 14 pages/min), scraped 16105 items (at 12 items/min)
2015-11-04 07:47:23 [scrapy] INFO: Crawled 16559 pages (at 5 pages/min), scraped 16112 items (at 7 items/min)
2015-11-04 07:48:36 [scrapy] INFO: Crawled 16573 pages (at 14 pages/min), scraped 16122 items (at 10 items/min)
2015-11-04 07:49:22 [scrapy] INFO: Crawled 16578 pages (at 5 pages/min), scraped 16130 items (at 8 items/min)
2015-11-04 07:50:18 [scrapy] INFO: Crawled 16588 pages (at 10 pages/min), scraped 16139 items (at 9 items/min)
2015-11-04 07:51:05 [scrapy] INFO: Crawled 16595 pages (at 7 pages/min), scraped 16145 items (at 6 items/min)
2015-11-04 07:52:23 [scrapy] INFO: Crawled 16603 pages (at 8 pages/min), scraped 16157 items (at 12 items/min)
2015-11-04 07:53:22 [scrapy] INFO: Crawled 16615 pages (at 12 pages/min), scraped 16165 items (at 8 items/min)
2015-11-04 07:54:14 [scrapy] INFO: Crawled 16623 pages (at 8 pages/min), scraped 16172 items (at 7 items/min)
2015-11-04 07:55:07 [scrapy] INFO: Crawled 16627 pages (at 4 pages/min), scraped 16180 items (at 8 items/min)
2015-11-04 07:56:03 [scrapy] INFO: Crawled 16647 pages (at 20 pages/min), scraped 16195 items (at 15 items/min)
2015-11-04 07:57:15 [scrapy] INFO: Crawled 16663 pages (at 16 pages/min), scraped 16212 items (at 17 items/min)
2015-11-04 07:58:24 [scrapy] INFO: Crawled 16679 pages (at 16 pages/min), scraped 16228 items (at 16 items/min)
2015-11-04 07:59:05 [scrapy] INFO: Crawled 16681 pages (at 2 pages/min), scraped 16236 items (at 8 items/min)
2015-11-04 08:00:23 [scrapy] INFO: Crawled 16697 pages (at 16 pages/min), scraped 16251 items (at 15 items/min)
2015-11-04 08:01:05 [scrapy] INFO: Crawled 16707 pages (at 10 pages/min), scraped 16260 items (at 9 items/min)
2015-11-04 08:02:03 [scrapy] INFO: Crawled 16719 pages (at 12 pages/min), scraped 16273 items (at 13 items/min)
2015-11-04 08:03:03 [scrapy] INFO: Crawled 16732 pages (at 13 pages/min), scraped 16283 items (at 10 items/min)
2015-11-04 08:04:11 [scrapy] INFO: Crawled 16747 pages (at 15 pages/min), scraped 16293 items (at 10 items/min)
2015-11-04 08:05:02 [scrapy] INFO: Crawled 16757 pages (at 10 pages/min), scraped 16302 items (at 9 items/min)
2015-11-04 08:06:34 [scrapy] INFO: Crawled 16772 pages (at 15 pages/min), scraped 16318 items (at 16 items/min)
2015-11-04 08:07:33 [scrapy] INFO: Crawled 16779 pages (at 7 pages/min), scraped 16327 items (at 9 items/min)
2015-11-04 08:08:09 [scrapy] INFO: Crawled 16781 pages (at 2 pages/min), scraped 16334 items (at 7 items/min)
2015-11-04 08:09:06 [scrapy] INFO: Crawled 16800 pages (at 19 pages/min), scraped 16347 items (at 13 items/min)
2015-11-04 08:10:05 [scrapy] INFO: Crawled 16816 pages (at 16 pages/min), scraped 16368 items (at 21 items/min)
2015-11-04 08:11:00 [scrapy] INFO: Crawled 16830 pages (at 14 pages/min), scraped 16383 items (at 15 items/min)
2015-11-04 08:12:02 [scrapy] INFO: Crawled 16845 pages (at 15 pages/min), scraped 16398 items (at 15 items/min)
2015-11-04 08:13:04 [scrapy] INFO: Crawled 16861 pages (at 16 pages/min), scraped 16412 items (at 14 items/min)
2015-11-04 08:14:01 [scrapy] INFO: Crawled 16873 pages (at 12 pages/min), scraped 16427 items (at 15 items/min)
2015-11-04 08:15:01 [scrapy] INFO: Crawled 16887 pages (at 14 pages/min), scraped 16441 items (at 14 items/min)
2015-11-04 08:16:00 [scrapy] INFO: Crawled 16906 pages (at 19 pages/min), scraped 16456 items (at 15 items/min)
2015-11-04 08:17:00 [scrapy] INFO: Crawled 16913 pages (at 7 pages/min), scraped 16467 items (at 11 items/min)
2015-11-04 08:18:04 [scrapy] INFO: Crawled 16929 pages (at 16 pages/min), scraped 16482 items (at 15 items/min)
2015-11-04 08:19:01 [scrapy] INFO: Crawled 16945 pages (at 16 pages/min), scraped 16497 items (at 15 items/min)
2015-11-04 08:20:21 [scrapy] INFO: Crawled 16958 pages (at 13 pages/min), scraped 16508 items (at 11 items/min)
2015-11-04 08:21:52 [scrapy] INFO: Crawled 16973 pages (at 15 pages/min), scraped 16520 items (at 12 items/min)
2015-11-04 08:22:41 [scrapy] INFO: Crawled 16978 pages (at 5 pages/min), scraped 16528 items (at 8 items/min)
2015-11-04 08:23:06 [scrapy] INFO: Crawled 16982 pages (at 4 pages/min), scraped 16533 items (at 5 items/min)
2015-11-04 08:24:04 [scrapy] INFO: Crawled 16996 pages (at 14 pages/min), scraped 16543 items (at 10 items/min)
2015-11-04 08:25:27 [scrapy] INFO: Crawled 17006 pages (at 10 pages/min), scraped 16559 items (at 16 items/min)
2015-11-04 08:26:25 [scrapy] INFO: Crawled 17022 pages (at 16 pages/min), scraped 16569 items (at 10 items/min)
2015-11-04 08:27:21 [scrapy] INFO: Crawled 17030 pages (at 8 pages/min), scraped 16577 items (at 8 items/min)
2015-11-04 08:28:06 [scrapy] INFO: Crawled 17033 pages (at 3 pages/min), scraped 16585 items (at 8 items/min)
2015-11-04 08:29:03 [scrapy] INFO: Crawled 17043 pages (at 10 pages/min), scraped 16591 items (at 6 items/min)
2015-11-04 08:30:29 [scrapy] INFO: Crawled 17055 pages (at 12 pages/min), scraped 16606 items (at 15 items/min)
2015-11-04 08:31:14 [scrapy] INFO: Crawled 17061 pages (at 6 pages/min), scraped 16610 items (at 4 items/min)
2015-11-04 08:32:22 [scrapy] INFO: Crawled 17068 pages (at 7 pages/min), scraped 16616 items (at 6 items/min)
2015-11-04 08:33:21 [scrapy] INFO: Crawled 17071 pages (at 3 pages/min), scraped 16623 items (at 7 items/min)
2015-11-04 08:34:11 [scrapy] INFO: Crawled 17078 pages (at 7 pages/min), scraped 16629 items (at 6 items/min)
2015-11-04 08:35:41 [scrapy] INFO: Crawled 17091 pages (at 13 pages/min), scraped 16639 items (at 10 items/min)
2015-11-04 08:36:22 [scrapy] INFO: Crawled 17096 pages (at 5 pages/min), scraped 16646 items (at 7 items/min)
2015-11-04 08:37:28 [scrapy] INFO: Crawled 17109 pages (at 13 pages/min), scraped 16657 items (at 11 items/min)
2015-11-04 08:38:24 [scrapy] INFO: Crawled 17118 pages (at 9 pages/min), scraped 16665 items (at 8 items/min)
2015-11-04 08:39:55 [scrapy] INFO: Crawled 17123 pages (at 5 pages/min), scraped 16673 items (at 8 items/min)
2015-11-04 08:40:54 [scrapy] INFO: Crawled 17123 pages (at 0 pages/min), scraped 16678 items (at 5 items/min)
2015-11-04 08:41:00 [scrapy] INFO: Crawled 17123 pages (at 0 pages/min), scraped 16678 items (at 0 items/min)
2015-11-04 08:42:22 [scrapy] INFO: Crawled 17136 pages (at 13 pages/min), scraped 16687 items (at 9 items/min)
2015-11-04 08:43:02 [scrapy] INFO: Crawled 17142 pages (at 6 pages/min), scraped 16691 items (at 4 items/min)
2015-11-04 08:44:01 [scrapy] INFO: Crawled 17150 pages (at 8 pages/min), scraped 16697 items (at 6 items/min)
2015-11-04 08:45:28 [scrapy] INFO: Crawled 17155 pages (at 5 pages/min), scraped 16705 items (at 8 items/min)
2015-11-04 08:46:09 [scrapy] INFO: Crawled 17155 pages (at 0 pages/min), scraped 16710 items (at 5 items/min)
2015-11-04 08:47:14 [scrapy] INFO: Crawled 17165 pages (at 10 pages/min), scraped 16717 items (at 7 items/min)
2015-11-04 08:48:27 [scrapy] INFO: Crawled 17177 pages (at 12 pages/min), scraped 16724 items (at 7 items/min)
2015-11-04 08:49:31 [scrapy] INFO: Crawled 17185 pages (at 8 pages/min), scraped 16731 items (at 7 items/min)
2015-11-04 08:50:29 [scrapy] INFO: Crawled 17190 pages (at 5 pages/min), scraped 16739 items (at 8 items/min)
2015-11-04 08:51:06 [scrapy] INFO: Crawled 17194 pages (at 4 pages/min), scraped 16744 items (at 5 items/min)
2015-11-04 08:52:27 [scrapy] INFO: Crawled 17205 pages (at 11 pages/min), scraped 16753 items (at 9 items/min)
2015-11-04 08:53:21 [scrapy] INFO: Crawled 17211 pages (at 6 pages/min), scraped 16759 items (at 6 items/min)
2015-11-04 08:54:03 [scrapy] INFO: Crawled 17215 pages (at 4 pages/min), scraped 16765 items (at 6 items/min)
2015-11-04 08:55:28 [scrapy] INFO: Crawled 17227 pages (at 12 pages/min), scraped 16774 items (at 9 items/min)
2015-11-04 08:56:31 [scrapy] INFO: Crawled 17234 pages (at 7 pages/min), scraped 16781 items (at 7 items/min)
2015-11-04 08:57:06 [scrapy] INFO: Crawled 17236 pages (at 2 pages/min), scraped 16788 items (at 7 items/min)
2015-11-04 08:58:24 [scrapy] INFO: Crawled 17252 pages (at 16 pages/min), scraped 16799 items (at 11 items/min)
2015-11-04 08:59:14 [scrapy] INFO: Crawled 17260 pages (at 8 pages/min), scraped 16806 items (at 7 items/min)
2015-11-04 09:00:02 [scrapy] INFO: Crawled 17264 pages (at 4 pages/min), scraped 16814 items (at 8 items/min)
2015-11-04 09:01:06 [scrapy] INFO: Crawled 17275 pages (at 11 pages/min), scraped 16823 items (at 9 items/min)
2015-11-04 09:02:36 [scrapy] INFO: Crawled 17289 pages (at 14 pages/min), scraped 16836 items (at 13 items/min)
2015-11-04 09:03:27 [scrapy] INFO: Crawled 17295 pages (at 6 pages/min), scraped 16843 items (at 7 items/min)
2015-11-04 09:04:15 [scrapy] INFO: Crawled 17301 pages (at 6 pages/min), scraped 16852 items (at 9 items/min)
2015-11-04 09:05:08 [scrapy] INFO: Crawled 17314 pages (at 13 pages/min), scraped 16860 items (at 8 items/min)
2015-11-04 09:06:16 [scrapy] INFO: Crawled 17326 pages (at 12 pages/min), scraped 16871 items (at 11 items/min)
2015-11-04 09:07:15 [scrapy] INFO: Crawled 17332 pages (at 6 pages/min), scraped 16880 items (at 9 items/min)
2015-11-04 09:08:53 [scrapy] INFO: Crawled 17348 pages (at 16 pages/min), scraped 16894 items (at 14 items/min)
2015-11-04 09:09:45 [scrapy] INFO: Crawled 17352 pages (at 4 pages/min), scraped 16902 items (at 8 items/min)
2015-11-04 09:10:10 [scrapy] INFO: Crawled 17356 pages (at 4 pages/min), scraped 16906 items (at 4 items/min)
2015-11-04 09:11:25 [scrapy] INFO: Crawled 17370 pages (at 14 pages/min), scraped 16916 items (at 10 items/min)
2015-11-04 09:12:12 [scrapy] INFO: Crawled 17375 pages (at 5 pages/min), scraped 16923 items (at 7 items/min)
2015-11-04 09:13:18 [scrapy] INFO: Crawled 17389 pages (at 14 pages/min), scraped 16934 items (at 11 items/min)
2015-11-04 09:14:01 [scrapy] INFO: Crawled 17396 pages (at 7 pages/min), scraped 16942 items (at 8 items/min)
2015-11-04 09:15:17 [scrapy] INFO: Crawled 17407 pages (at 11 pages/min), scraped 16954 items (at 12 items/min)
2015-11-04 09:16:00 [scrapy] INFO: Crawled 17415 pages (at 8 pages/min), scraped 16960 items (at 6 items/min)
2015-11-04 09:17:22 [scrapy] INFO: Crawled 17427 pages (at 12 pages/min), scraped 16975 items (at 15 items/min)
2015-11-04 09:18:12 [scrapy] INFO: Crawled 17435 pages (at 8 pages/min), scraped 16980 items (at 5 items/min)
2015-11-04 09:19:07 [scrapy] INFO: Crawled 17442 pages (at 7 pages/min), scraped 16991 items (at 11 items/min)
2015-11-04 09:20:22 [scrapy] INFO: Crawled 17454 pages (at 12 pages/min), scraped 17001 items (at 10 items/min)
2015-11-04 09:21:10 [scrapy] INFO: Crawled 17471 pages (at 17 pages/min), scraped 17018 items (at 17 items/min)
2015-11-04 09:22:19 [scrapy] INFO: Crawled 17485 pages (at 14 pages/min), scraped 17028 items (at 10 items/min)
2015-11-04 09:23:14 [scrapy] INFO: Crawled 17489 pages (at 4 pages/min), scraped 17037 items (at 9 items/min)
2015-11-04 09:24:26 [scrapy] INFO: Crawled 17503 pages (at 14 pages/min), scraped 17047 items (at 10 items/min)
2015-11-04 09:25:20 [scrapy] INFO: Crawled 17511 pages (at 8 pages/min), scraped 17055 items (at 8 items/min)
2015-11-04 09:26:24 [scrapy] INFO: Crawled 17515 pages (at 4 pages/min), scraped 17063 items (at 8 items/min)
2015-11-04 09:27:18 [scrapy] INFO: Crawled 17524 pages (at 9 pages/min), scraped 17070 items (at 7 items/min)
2015-11-04 09:28:47 [scrapy] INFO: Crawled 17537 pages (at 13 pages/min), scraped 17083 items (at 13 items/min)
2015-11-04 09:29:48 [scrapy] INFO: Crawled 17542 pages (at 5 pages/min), scraped 17089 items (at 6 items/min)
2015-11-04 09:30:19 [scrapy] INFO: Crawled 17542 pages (at 0 pages/min), scraped 17094 items (at 5 items/min)
2015-11-04 09:31:08 [scrapy] INFO: Crawled 17549 pages (at 7 pages/min), scraped 17099 items (at 5 items/min)
2015-11-04 09:32:09 [scrapy] INFO: Crawled 17559 pages (at 10 pages/min), scraped 17105 items (at 6 items/min)
2015-11-04 09:33:01 [scrapy] INFO: Crawled 17568 pages (at 9 pages/min), scraped 17115 items (at 10 items/min)
2015-11-04 09:34:48 [scrapy] INFO: Crawled 17577 pages (at 9 pages/min), scraped 17125 items (at 10 items/min)
2015-11-04 09:35:04 [scrapy] INFO: Crawled 17577 pages (at 0 pages/min), scraped 17128 items (at 3 items/min)
2015-11-04 09:37:23 [scrapy] INFO: Crawled 17594 pages (at 17 pages/min), scraped 17139 items (at 11 items/min)
2015-11-04 09:38:00 [scrapy] INFO: Crawled 17598 pages (at 4 pages/min), scraped 17148 items (at 9 items/min)
2015-11-04 09:39:40 [scrapy] INFO: Crawled 17621 pages (at 23 pages/min), scraped 17164 items (at 16 items/min)
2015-11-04 09:40:25 [scrapy] INFO: Crawled 17622 pages (at 1 pages/min), scraped 17172 items (at 8 items/min)
2015-11-04 09:41:22 [scrapy] INFO: Crawled 17636 pages (at 14 pages/min), scraped 17177 items (at 5 items/min)
2015-11-04 09:42:26 [scrapy] INFO: Crawled 17641 pages (at 5 pages/min), scraped 17187 items (at 10 items/min)
2015-11-04 09:43:22 [scrapy] INFO: Crawled 17644 pages (at 3 pages/min), scraped 17192 items (at 5 items/min)
2015-11-04 09:44:35 [scrapy] INFO: Crawled 17655 pages (at 11 pages/min), scraped 17199 items (at 7 items/min)
2015-11-04 09:45:36 [scrapy] INFO: Crawled 17659 pages (at 4 pages/min), scraped 17206 items (at 7 items/min)
2015-11-04 09:46:00 [scrapy] INFO: Crawled 17659 pages (at 0 pages/min), scraped 17210 items (at 4 items/min)
2015-11-04 09:47:05 [scrapy] INFO: Crawled 17678 pages (at 19 pages/min), scraped 17222 items (at 12 items/min)
2015-11-04 09:48:13 [scrapy] INFO: Crawled 17680 pages (at 2 pages/min), scraped 17229 items (at 7 items/min)
2015-11-04 09:49:03 [scrapy] INFO: Crawled 17692 pages (at 12 pages/min), scraped 17238 items (at 9 items/min)
2015-11-04 09:50:33 [scrapy] INFO: Crawled 17702 pages (at 10 pages/min), scraped 17249 items (at 11 items/min)
2015-11-04 09:51:15 [scrapy] INFO: Crawled 17713 pages (at 11 pages/min), scraped 17256 items (at 7 items/min)
2015-11-04 09:52:22 [scrapy] INFO: Crawled 17722 pages (at 9 pages/min), scraped 17264 items (at 8 items/min)
2015-11-04 09:53:38 [scrapy] INFO: Crawled 17722 pages (at 0 pages/min), scraped 17273 items (at 9 items/min)
2015-11-04 09:54:05 [scrapy] INFO: Crawled 17728 pages (at 6 pages/min), scraped 17276 items (at 3 items/min)
2015-11-04 09:55:00 [scrapy] INFO: Crawled 17741 pages (at 13 pages/min), scraped 17284 items (at 8 items/min)
2015-11-04 09:56:20 [scrapy] INFO: Crawled 17751 pages (at 10 pages/min), scraped 17298 items (at 14 items/min)
2015-11-04 09:57:30 [scrapy] INFO: Crawled 17768 pages (at 17 pages/min), scraped 17310 items (at 12 items/min)
2015-11-04 09:58:20 [scrapy] INFO: Crawled 17776 pages (at 8 pages/min), scraped 17318 items (at 8 items/min)
2015-11-04 09:59:04 [scrapy] INFO: Crawled 17781 pages (at 5 pages/min), scraped 17326 items (at 8 items/min)
2015-11-04 10:00:09 [scrapy] INFO: Crawled 17792 pages (at 11 pages/min), scraped 17336 items (at 10 items/min)
2015-11-04 10:01:06 [scrapy] INFO: Crawled 17807 pages (at 15 pages/min), scraped 17351 items (at 15 items/min)
2015-11-04 10:02:05 [scrapy] INFO: Crawled 17820 pages (at 13 pages/min), scraped 17368 items (at 17 items/min)
2015-11-04 10:03:16 [scrapy] INFO: Crawled 17835 pages (at 15 pages/min), scraped 17377 items (at 9 items/min)
2015-11-04 10:04:12 [scrapy] INFO: Crawled 17842 pages (at 7 pages/min), scraped 17384 items (at 7 items/min)
2015-11-04 10:05:22 [scrapy] INFO: Crawled 17846 pages (at 4 pages/min), scraped 17391 items (at 7 items/min)
2015-11-04 10:06:04 [scrapy] INFO: Crawled 17846 pages (at 0 pages/min), scraped 17395 items (at 4 items/min)
2015-11-04 10:07:17 [scrapy] INFO: Crawled 17858 pages (at 12 pages/min), scraped 17403 items (at 8 items/min)
2015-11-04 10:08:44 [scrapy] INFO: Crawled 17873 pages (at 15 pages/min), scraped 17414 items (at 11 items/min)
2015-11-04 10:09:50 [scrapy] INFO: Crawled 17877 pages (at 4 pages/min), scraped 17422 items (at 8 items/min)
2015-11-04 10:10:20 [scrapy] INFO: Crawled 17880 pages (at 3 pages/min), scraped 17426 items (at 4 items/min)
2015-11-04 10:11:06 [scrapy] INFO: Crawled 17889 pages (at 9 pages/min), scraped 17432 items (at 6 items/min)
2015-11-04 10:12:13 [scrapy] INFO: Crawled 17901 pages (at 12 pages/min), scraped 17441 items (at 9 items/min)
2015-11-04 10:13:08 [scrapy] INFO: Crawled 17908 pages (at 7 pages/min), scraped 17450 items (at 9 items/min)
2015-11-04 10:14:01 [scrapy] INFO: Crawled 17914 pages (at 6 pages/min), scraped 17457 items (at 7 items/min)
2015-11-04 10:15:29 [scrapy] INFO: Crawled 17930 pages (at 16 pages/min), scraped 17471 items (at 14 items/min)
2015-11-04 10:16:25 [scrapy] INFO: Crawled 17935 pages (at 5 pages/min), scraped 17479 items (at 8 items/min)
2015-11-04 10:17:04 [scrapy] INFO: Crawled 17938 pages (at 3 pages/min), scraped 17484 items (at 5 items/min)
2015-11-04 10:18:35 [scrapy] INFO: Crawled 17954 pages (at 16 pages/min), scraped 17495 items (at 11 items/min)
2015-11-04 10:19:28 [scrapy] INFO: Crawled 17961 pages (at 7 pages/min), scraped 17503 items (at 8 items/min)
2015-11-04 10:20:16 [scrapy] INFO: Crawled 17965 pages (at 4 pages/min), scraped 17510 items (at 7 items/min)
2015-11-04 10:21:35 [scrapy] INFO: Crawled 17978 pages (at 13 pages/min), scraped 17519 items (at 9 items/min)
2015-11-04 10:22:26 [scrapy] INFO: Crawled 17983 pages (at 5 pages/min), scraped 17527 items (at 8 items/min)
2015-11-04 10:23:04 [scrapy] INFO: Crawled 17992 pages (at 9 pages/min), scraped 17535 items (at 8 items/min)
2015-11-04 10:24:00 [scrapy] INFO: Crawled 18004 pages (at 12 pages/min), scraped 17547 items (at 12 items/min)
2015-11-04 10:25:01 [scrapy] INFO: Crawled 18018 pages (at 14 pages/min), scraped 17558 items (at 11 items/min)
2015-11-04 10:26:13 [scrapy] INFO: Crawled 18030 pages (at 12 pages/min), scraped 17568 items (at 10 items/min)
2015-11-04 10:27:03 [scrapy] INFO: Crawled 18034 pages (at 4 pages/min), scraped 17576 items (at 8 items/min)
2015-11-04 10:28:22 [scrapy] INFO: Crawled 18045 pages (at 11 pages/min), scraped 17585 items (at 9 items/min)
2015-11-04 10:29:22 [scrapy] INFO: Crawled 18050 pages (at 5 pages/min), scraped 17591 items (at 6 items/min)
2015-11-04 10:30:18 [scrapy] INFO: Crawled 18054 pages (at 4 pages/min), scraped 17598 items (at 7 items/min)
2015-11-04 10:31:19 [scrapy] INFO: Crawled 18065 pages (at 11 pages/min), scraped 17607 items (at 9 items/min)
2015-11-04 10:32:41 [scrapy] INFO: Crawled 18079 pages (at 14 pages/min), scraped 17617 items (at 10 items/min)
2015-11-04 10:33:34 [scrapy] INFO: Crawled 18084 pages (at 5 pages/min), scraped 17625 items (at 8 items/min)
2015-11-04 10:34:03 [scrapy] INFO: Crawled 18086 pages (at 2 pages/min), scraped 17630 items (at 5 items/min)
2015-11-04 10:35:02 [scrapy] INFO: Crawled 18097 pages (at 11 pages/min), scraped 17642 items (at 12 items/min)
2015-11-04 10:36:07 [scrapy] INFO: Crawled 18119 pages (at 22 pages/min), scraped 17662 items (at 20 items/min)
2015-11-04 10:37:09 [scrapy] INFO: Crawled 18138 pages (at 19 pages/min), scraped 17677 items (at 15 items/min)
2015-11-04 10:38:26 [scrapy] INFO: Crawled 18152 pages (at 14 pages/min), scraped 17688 items (at 11 items/min)
2015-11-04 10:39:38 [scrapy] INFO: Crawled 18158 pages (at 6 pages/min), scraped 17696 items (at 8 items/min)
2015-11-04 10:40:20 [scrapy] INFO: Crawled 18161 pages (at 3 pages/min), scraped 17702 items (at 6 items/min)
2015-11-04 10:41:11 [scrapy] INFO: Crawled 18170 pages (at 9 pages/min), scraped 17709 items (at 7 items/min)
2015-11-04 10:42:37 [scrapy] INFO: Crawled 18184 pages (at 14 pages/min), scraped 17720 items (at 11 items/min)
2015-11-04 10:43:29 [scrapy] INFO: Crawled 18188 pages (at 4 pages/min), scraped 17728 items (at 8 items/min)
2015-11-04 10:44:08 [scrapy] INFO: Crawled 18196 pages (at 8 pages/min), scraped 17734 items (at 6 items/min)
2015-11-04 10:45:35 [scrapy] INFO: Crawled 18210 pages (at 14 pages/min), scraped 17745 items (at 11 items/min)
2015-11-04 10:46:27 [scrapy] INFO: Crawled 18210 pages (at 0 pages/min), scraped 17753 items (at 8 items/min)
2015-11-04 10:47:01 [scrapy] INFO: Crawled 18220 pages (at 10 pages/min), scraped 17762 items (at 9 items/min)
2015-11-04 10:48:05 [scrapy] INFO: Crawled 18234 pages (at 14 pages/min), scraped 17775 items (at 13 items/min)
2015-11-04 10:49:13 [scrapy] INFO: Crawled 18245 pages (at 11 pages/min), scraped 17786 items (at 11 items/min)
2015-11-04 10:50:11 [scrapy] INFO: Crawled 18260 pages (at 15 pages/min), scraped 17800 items (at 14 items/min)
2015-11-04 10:51:15 [scrapy] INFO: Crawled 18269 pages (at 9 pages/min), scraped 17806 items (at 6 items/min)
2015-11-04 10:52:07 [scrapy] INFO: Crawled 18275 pages (at 6 pages/min), scraped 17812 items (at 6 items/min)
2015-11-04 10:53:08 [scrapy] INFO: Crawled 18281 pages (at 6 pages/min), scraped 17818 items (at 6 items/min)
2015-11-04 10:54:01 [scrapy] INFO: Crawled 18288 pages (at 7 pages/min), scraped 17827 items (at 9 items/min)
2015-11-04 10:55:00 [scrapy] INFO: Crawled 18295 pages (at 7 pages/min), scraped 17837 items (at 10 items/min)
2015-11-04 10:56:01 [scrapy] INFO: Crawled 18312 pages (at 17 pages/min), scraped 17854 items (at 17 items/min)
2015-11-04 10:57:00 [scrapy] INFO: Crawled 18327 pages (at 15 pages/min), scraped 17869 items (at 15 items/min)
2015-11-04 10:58:11 [scrapy] INFO: Crawled 18351 pages (at 24 pages/min), scraped 17886 items (at 17 items/min)
2015-11-04 10:59:32 [scrapy] INFO: Crawled 18363 pages (at 12 pages/min), scraped 17899 items (at 13 items/min)
2015-11-04 11:00:15 [scrapy] INFO: Crawled 18369 pages (at 6 pages/min), scraped 17906 items (at 7 items/min)
2015-11-04 11:01:17 [scrapy] INFO: Crawled 18372 pages (at 3 pages/min), scraped 17915 items (at 9 items/min)
2015-11-04 11:02:00 [scrapy] INFO: Crawled 18382 pages (at 10 pages/min), scraped 17924 items (at 9 items/min)
2015-11-04 11:03:05 [scrapy] INFO: Crawled 18397 pages (at 15 pages/min), scraped 17938 items (at 14 items/min)
2015-11-04 11:04:16 [scrapy] INFO: Crawled 18413 pages (at 16 pages/min), scraped 17952 items (at 14 items/min)
2015-11-04 11:05:08 [scrapy] INFO: Crawled 18420 pages (at 7 pages/min), scraped 17962 items (at 10 items/min)
2015-11-04 11:06:12 [scrapy] INFO: Crawled 18433 pages (at 13 pages/min), scraped 17974 items (at 12 items/min)
2015-11-04 11:07:10 [scrapy] INFO: Crawled 18441 pages (at 8 pages/min), scraped 17982 items (at 8 items/min)
2015-11-04 11:08:03 [scrapy] INFO: Crawled 18448 pages (at 7 pages/min), scraped 17989 items (at 7 items/min)
2015-11-04 11:09:02 [scrapy] INFO: Crawled 18457 pages (at 9 pages/min), scraped 17998 items (at 9 items/min)
2015-11-04 11:10:02 [scrapy] INFO: Crawled 18469 pages (at 12 pages/min), scraped 18011 items (at 13 items/min)
2015-11-04 11:11:06 [scrapy] INFO: Crawled 18488 pages (at 19 pages/min), scraped 18023 items (at 12 items/min)
2015-11-04 11:12:07 [scrapy] INFO: Crawled 18507 pages (at 19 pages/min), scraped 18042 items (at 19 items/min)
2015-11-04 11:13:07 [scrapy] INFO: Crawled 18517 pages (at 10 pages/min), scraped 18054 items (at 12 items/min)
2015-11-04 11:14:09 [scrapy] INFO: Crawled 18525 pages (at 8 pages/min), scraped 18065 items (at 11 items/min)
2015-11-04 11:15:05 [scrapy] INFO: Crawled 18528 pages (at 3 pages/min), scraped 18068 items (at 3 items/min)
2015-11-04 11:16:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 1 items/min)
2015-11-04 11:17:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:18:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:19:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:20:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:21:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:22:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:23:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:24:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:25:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:26:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:27:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:28:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:29:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:30:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:31:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:32:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:33:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:34:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:35:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:36:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:37:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:38:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:39:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:40:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:41:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:42:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:43:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:44:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:45:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:46:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:47:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:48:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:49:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:50:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:51:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:52:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:53:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:54:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:55:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:56:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:57:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:58:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 11:59:00 [scrapy] INFO: Crawled 18528 pages (at 0 pages/min), scraped 18069 items (at 0 items/min)
2015-11-04 12:00:03 [scrapy] INFO: Crawled 18548 pages (at 20 pages/min), scraped 18088 items (at 19 items/min)
2015-11-04 12:01:00 [scrapy] INFO: Crawled 18560 pages (at 12 pages/min), scraped 18101 items (at 13 items/min)
2015-11-04 12:02:00 [scrapy] INFO: Crawled 18560 pages (at 0 pages/min), scraped 18101 items (at 0 items/min)
2015-11-04 12:03:00 [scrapy] INFO: Crawled 18560 pages (at 0 pages/min), scraped 18101 items (at 0 items/min)
2015-11-04 12:04:00 [scrapy] INFO: Crawled 18560 pages (at 0 pages/min), scraped 18101 items (at 0 items/min)
2015-11-04 12:05:00 [scrapy] INFO: Crawled 18564 pages (at 4 pages/min), scraped 18103 items (at 2 items/min)
2015-11-04 12:06:06 [scrapy] INFO: Crawled 18581 pages (at 17 pages/min), scraped 18121 items (at 18 items/min)
2015-11-04 12:07:00 [scrapy] INFO: Crawled 18581 pages (at 0 pages/min), scraped 18122 items (at 1 items/min)
2015-11-04 12:08:00 [scrapy] INFO: Crawled 18581 pages (at 0 pages/min), scraped 18122 items (at 0 items/min)
2015-11-04 12:09:00 [scrapy] INFO: Crawled 18584 pages (at 3 pages/min), scraped 18125 items (at 3 items/min)
2015-11-04 12:10:04 [scrapy] INFO: Crawled 18603 pages (at 19 pages/min), scraped 18138 items (at 13 items/min)
2015-11-04 12:11:00 [scrapy] INFO: Crawled 18603 pages (at 0 pages/min), scraped 18144 items (at 6 items/min)
2015-11-04 12:12:00 [scrapy] INFO: Crawled 18603 pages (at 0 pages/min), scraped 18144 items (at 0 items/min)
2015-11-04 12:13:00 [scrapy] INFO: Crawled 18603 pages (at 0 pages/min), scraped 18144 items (at 0 items/min)
2015-11-04 12:14:00 [scrapy] INFO: Crawled 18603 pages (at 0 pages/min), scraped 18144 items (at 0 items/min)
2015-11-04 12:15:00 [scrapy] INFO: Crawled 18612 pages (at 9 pages/min), scraped 18152 items (at 8 items/min)
2015-11-04 12:16:00 [scrapy] INFO: Crawled 18632 pages (at 20 pages/min), scraped 18171 items (at 19 items/min)
2015-11-04 12:17:04 [scrapy] INFO: Crawled 18639 pages (at 7 pages/min), scraped 18180 items (at 9 items/min)
2015-11-04 12:18:00 [scrapy] INFO: Crawled 18646 pages (at 7 pages/min), scraped 18187 items (at 7 items/min)
2015-11-04 12:19:00 [scrapy] INFO: Crawled 18656 pages (at 10 pages/min), scraped 18197 items (at 10 items/min)
2015-11-04 12:20:00 [scrapy] INFO: Crawled 18656 pages (at 0 pages/min), scraped 18197 items (at 0 items/min)
2015-11-04 12:21:00 [scrapy] INFO: Crawled 18656 pages (at 0 pages/min), scraped 18197 items (at 0 items/min)
2015-11-04 12:22:00 [scrapy] INFO: Crawled 18663 pages (at 7 pages/min), scraped 18202 items (at 5 items/min)
2015-11-04 12:23:00 [scrapy] INFO: Crawled 18684 pages (at 21 pages/min), scraped 18223 items (at 21 items/min)
2015-11-04 12:24:01 [scrapy] INFO: Crawled 18705 pages (at 21 pages/min), scraped 18244 items (at 21 items/min)
2015-11-04 12:25:00 [scrapy] INFO: Crawled 18720 pages (at 15 pages/min), scraped 18261 items (at 17 items/min)
2015-11-04 12:26:00 [scrapy] INFO: Crawled 18720 pages (at 0 pages/min), scraped 18261 items (at 0 items/min)
2015-11-04 12:27:00 [scrapy] INFO: Crawled 18723 pages (at 3 pages/min), scraped 18262 items (at 1 items/min)
2015-11-04 12:28:01 [scrapy] INFO: Crawled 18743 pages (at 20 pages/min), scraped 18282 items (at 20 items/min)
2015-11-04 12:29:00 [scrapy] INFO: Crawled 18746 pages (at 3 pages/min), scraped 18287 items (at 5 items/min)
2015-11-04 12:30:02 [scrapy] INFO: Crawled 18755 pages (at 9 pages/min), scraped 18296 items (at 9 items/min)
2015-11-04 12:31:00 [scrapy] INFO: Crawled 18770 pages (at 15 pages/min), scraped 18311 items (at 15 items/min)
2015-11-04 12:32:00 [scrapy] INFO: Crawled 18784 pages (at 14 pages/min), scraped 18325 items (at 14 items/min)
2015-11-04 12:33:00 [scrapy] INFO: Crawled 18797 pages (at 13 pages/min), scraped 18338 items (at 13 items/min)
2015-11-04 12:34:00 [scrapy] INFO: Crawled 18807 pages (at 10 pages/min), scraped 18348 items (at 10 items/min)
2015-11-04 12:35:00 [scrapy] INFO: Crawled 18814 pages (at 7 pages/min), scraped 18355 items (at 7 items/min)
2015-11-04 12:36:00 [scrapy] INFO: Crawled 18814 pages (at 0 pages/min), scraped 18355 items (at 0 items/min)
2015-11-04 12:37:04 [scrapy] INFO: Crawled 18831 pages (at 17 pages/min), scraped 18370 items (at 15 items/min)
2015-11-04 12:38:01 [scrapy] INFO: Crawled 18842 pages (at 11 pages/min), scraped 18383 items (at 13 items/min)
2015-11-04 12:39:02 [scrapy] INFO: Crawled 18855 pages (at 13 pages/min), scraped 18394 items (at 11 items/min)
2015-11-04 12:40:02 [scrapy] INFO: Crawled 18868 pages (at 13 pages/min), scraped 18408 items (at 14 items/min)
2015-11-04 12:41:05 [scrapy] INFO: Crawled 18876 pages (at 8 pages/min), scraped 18416 items (at 8 items/min)
2015-11-04 12:42:00 [scrapy] INFO: Crawled 18885 pages (at 9 pages/min), scraped 18426 items (at 10 items/min)
2015-11-04 12:43:00 [scrapy] INFO: Crawled 18898 pages (at 13 pages/min), scraped 18439 items (at 13 items/min)
2015-11-04 12:44:00 [scrapy] INFO: Crawled 18906 pages (at 8 pages/min), scraped 18447 items (at 8 items/min)
2015-11-04 12:45:00 [scrapy] INFO: Crawled 18916 pages (at 10 pages/min), scraped 18456 items (at 9 items/min)
2015-11-04 12:46:00 [scrapy] INFO: Crawled 18934 pages (at 18 pages/min), scraped 18475 items (at 19 items/min)
2015-11-04 12:47:00 [scrapy] INFO: Crawled 18952 pages (at 18 pages/min), scraped 18493 items (at 18 items/min)
2015-11-04 12:48:02 [scrapy] INFO: Crawled 18968 pages (at 16 pages/min), scraped 18509 items (at 16 items/min)
2015-11-04 12:49:00 [scrapy] INFO: Crawled 18976 pages (at 8 pages/min), scraped 18517 items (at 8 items/min)
2015-11-04 12:50:02 [scrapy] INFO: Crawled 18981 pages (at 5 pages/min), scraped 18521 items (at 4 items/min)
2015-11-04 12:51:07 [scrapy] INFO: Crawled 18997 pages (at 16 pages/min), scraped 18536 items (at 15 items/min)
2015-11-04 12:52:02 [scrapy] INFO: Crawled 19013 pages (at 16 pages/min), scraped 18552 items (at 16 items/min)
2015-11-04 12:53:00 [scrapy] INFO: Crawled 19015 pages (at 2 pages/min), scraped 18555 items (at 3 items/min)
2015-11-04 12:54:22 [scrapy] INFO: Crawled 19033 pages (at 18 pages/min), scraped 18566 items (at 11 items/min)
2015-11-04 12:55:04 [scrapy] INFO: Crawled 19044 pages (at 11 pages/min), scraped 18573 items (at 7 items/min)
2015-11-04 12:56:04 [scrapy] INFO: Crawled 19049 pages (at 5 pages/min), scraped 18584 items (at 11 items/min)
2015-11-04 12:57:07 [scrapy] INFO: Crawled 19064 pages (at 15 pages/min), scraped 18595 items (at 11 items/min)
2015-11-04 12:58:02 [scrapy] INFO: Crawled 19072 pages (at 8 pages/min), scraped 18604 items (at 9 items/min)
2015-11-04 12:59:06 [scrapy] INFO: Crawled 19078 pages (at 6 pages/min), scraped 18615 items (at 11 items/min)
2015-11-04 13:00:16 [scrapy] INFO: Crawled 19089 pages (at 11 pages/min), scraped 18624 items (at 9 items/min)
2015-11-04 13:01:00 [scrapy] INFO: Crawled 19089 pages (at 0 pages/min), scraped 18629 items (at 5 items/min)
2015-11-04 13:02:00 [scrapy] INFO: Crawled 19101 pages (at 12 pages/min), scraped 18638 items (at 9 items/min)
2015-11-04 13:03:00 [scrapy] INFO: Crawled 19111 pages (at 10 pages/min), scraped 18651 items (at 13 items/min)
2015-11-04 13:04:01 [scrapy] INFO: Crawled 19115 pages (at 4 pages/min), scraped 18653 items (at 2 items/min)
2015-11-04 13:05:09 [scrapy] INFO: Crawled 19130 pages (at 15 pages/min), scraped 18663 items (at 10 items/min)
2015-11-04 13:06:49 [scrapy] INFO: Crawled 19143 pages (at 13 pages/min), scraped 18678 items (at 15 items/min)
2015-11-04 13:07:24 [scrapy] INFO: Crawled 19147 pages (at 4 pages/min), scraped 18683 items (at 5 items/min)
2015-11-04 13:08:34 [scrapy] INFO: Crawled 19160 pages (at 13 pages/min), scraped 18692 items (at 9 items/min)
2015-11-04 13:09:27 [scrapy] INFO: Crawled 19167 pages (at 7 pages/min), scraped 18700 items (at 8 items/min)
2015-11-04 13:10:29 [scrapy] INFO: Crawled 19171 pages (at 4 pages/min), scraped 18707 items (at 7 items/min)
2015-11-04 13:11:00 [scrapy] INFO: Crawled 19171 pages (at 0 pages/min), scraped 18711 items (at 4 items/min)
2015-11-04 13:12:07 [scrapy] INFO: Crawled 19182 pages (at 11 pages/min), scraped 18721 items (at 10 items/min)
2015-11-04 13:13:04 [scrapy] INFO: Crawled 19196 pages (at 14 pages/min), scraped 18734 items (at 13 items/min)
2015-11-04 13:14:00 [scrapy] INFO: Crawled 19212 pages (at 16 pages/min), scraped 18752 items (at 18 items/min)
2015-11-04 13:15:30 [scrapy] INFO: Crawled 19232 pages (at 20 pages/min), scraped 18764 items (at 12 items/min)
2015-11-04 13:16:05 [scrapy] INFO: Crawled 19240 pages (at 8 pages/min), scraped 18772 items (at 8 items/min)
2015-11-04 13:17:11 [scrapy] INFO: Crawled 19250 pages (at 10 pages/min), scraped 18788 items (at 16 items/min)
2015-11-04 13:18:00 [scrapy] INFO: Crawled 19250 pages (at 0 pages/min), scraped 18790 items (at 2 items/min)
2015-11-04 13:19:03 [scrapy] INFO: Crawled 19254 pages (at 4 pages/min), scraped 18793 items (at 3 items/min)
2015-11-04 13:20:02 [scrapy] INFO: Crawled 19262 pages (at 8 pages/min), scraped 18799 items (at 6 items/min)
2015-11-04 13:21:34 [scrapy] INFO: Crawled 19277 pages (at 15 pages/min), scraped 18811 items (at 12 items/min)
2015-11-04 13:22:21 [scrapy] INFO: Crawled 19284 pages (at 7 pages/min), scraped 18817 items (at 6 items/min)
2015-11-04 13:23:06 [scrapy] INFO: Crawled 19292 pages (at 8 pages/min), scraped 18824 items (at 7 items/min)
2015-11-04 13:24:24 [scrapy] INFO: Crawled 19304 pages (at 12 pages/min), scraped 18838 items (at 14 items/min)
2015-11-04 13:25:00 [scrapy] INFO: Crawled 19312 pages (at 8 pages/min), scraped 18844 items (at 6 items/min)
2015-11-04 13:26:33 [scrapy] INFO: Crawled 19321 pages (at 9 pages/min), scraped 18860 items (at 16 items/min)
2015-11-04 13:27:07 [scrapy] INFO: Crawled 19325 pages (at 4 pages/min), scraped 18865 items (at 5 items/min)
2015-11-04 13:28:04 [scrapy] INFO: Crawled 19336 pages (at 11 pages/min), scraped 18875 items (at 10 items/min)
2015-11-04 13:29:28 [scrapy] INFO: Crawled 19357 pages (at 21 pages/min), scraped 18895 items (at 20 items/min)
2015-11-04 13:30:28 [scrapy] INFO: Crawled 19373 pages (at 16 pages/min), scraped 18905 items (at 10 items/min)
2015-11-04 13:31:12 [scrapy] INFO: Crawled 19376 pages (at 3 pages/min), scraped 18913 items (at 8 items/min)
2015-11-04 13:32:20 [scrapy] INFO: Crawled 19395 pages (at 19 pages/min), scraped 18927 items (at 14 items/min)
2015-11-04 13:33:01 [scrapy] INFO: Crawled 19403 pages (at 8 pages/min), scraped 18935 items (at 8 items/min)
2015-11-04 13:34:05 [scrapy] INFO: Crawled 19409 pages (at 6 pages/min), scraped 18946 items (at 11 items/min)
2015-11-04 13:35:00 [scrapy] INFO: Crawled 19415 pages (at 6 pages/min), scraped 18953 items (at 7 items/min)
2015-11-04 13:36:00 [scrapy] INFO: Crawled 19418 pages (at 3 pages/min), scraped 18956 items (at 3 items/min)
2015-11-04 13:37:22 [scrapy] INFO: Crawled 19437 pages (at 19 pages/min), scraped 18967 items (at 11 items/min)
2015-11-04 13:38:15 [scrapy] INFO: Crawled 19444 pages (at 7 pages/min), scraped 18975 items (at 8 items/min)
2015-11-04 13:39:17 [scrapy] INFO: Crawled 19452 pages (at 8 pages/min), scraped 18984 items (at 9 items/min)
2015-11-04 13:40:00 [scrapy] INFO: Crawled 19460 pages (at 8 pages/min), scraped 18990 items (at 6 items/min)
2015-11-04 13:41:26 [scrapy] INFO: Crawled 19470 pages (at 10 pages/min), scraped 19003 items (at 13 items/min)
2015-11-04 13:42:29 [scrapy] INFO: Crawled 19483 pages (at 13 pages/min), scraped 19013 items (at 10 items/min)
2015-11-04 13:44:02 [scrapy] INFO: Crawled 19492 pages (at 9 pages/min), scraped 19022 items (at 9 items/min)
2015-11-04 13:45:00 [scrapy] INFO: Crawled 19492 pages (at 0 pages/min), scraped 19030 items (at 8 items/min)
2015-11-04 13:46:01 [scrapy] INFO: Crawled 19512 pages (at 20 pages/min), scraped 19042 items (at 12 items/min)
2015-11-04 13:47:04 [scrapy] INFO: Crawled 19524 pages (at 12 pages/min), scraped 19058 items (at 16 items/min)
2015-11-04 13:48:16 [scrapy] INFO: Crawled 19542 pages (at 18 pages/min), scraped 19073 items (at 15 items/min)
2015-11-04 13:49:01 [scrapy] INFO: Crawled 19549 pages (at 7 pages/min), scraped 19084 items (at 11 items/min)
2015-11-04 13:50:21 [scrapy] INFO: Crawled 19572 pages (at 23 pages/min), scraped 19101 items (at 17 items/min)
2015-11-04 13:51:02 [scrapy] INFO: Crawled 19580 pages (at 8 pages/min), scraped 19109 items (at 8 items/min)
2015-11-04 13:52:19 [scrapy] INFO: Crawled 19589 pages (at 9 pages/min), scraped 19124 items (at 15 items/min)
2015-11-04 13:53:45 [scrapy] INFO: Crawled 19609 pages (at 20 pages/min), scraped 19138 items (at 14 items/min)
2015-11-04 13:54:23 [scrapy] INFO: Crawled 19613 pages (at 4 pages/min), scraped 19146 items (at 8 items/min)
2015-11-04 13:55:11 [scrapy] INFO: Crawled 19620 pages (at 7 pages/min), scraped 19155 items (at 9 items/min)
2015-11-04 13:56:00 [scrapy] INFO: Crawled 19632 pages (at 12 pages/min), scraped 19164 items (at 9 items/min)
2015-11-04 13:57:00 [scrapy] INFO: Crawled 19637 pages (at 5 pages/min), scraped 19173 items (at 9 items/min)
2015-11-04 13:58:10 [scrapy] INFO: Crawled 19652 pages (at 15 pages/min), scraped 19183 items (at 10 items/min)
2015-11-04 14:00:01 [scrapy] INFO: Crawled 19664 pages (at 12 pages/min), scraped 19195 items (at 12 items/min)
2015-11-04 14:01:04 [scrapy] INFO: Crawled 19667 pages (at 3 pages/min), scraped 19203 items (at 8 items/min)
2015-11-04 14:02:03 [scrapy] INFO: Crawled 19683 pages (at 16 pages/min), scraped 19212 items (at 9 items/min)
2015-11-04 14:03:27 [scrapy] INFO: Crawled 19695 pages (at 12 pages/min), scraped 19228 items (at 16 items/min)
2015-11-04 14:04:29 [scrapy] INFO: Crawled 19709 pages (at 14 pages/min), scraped 19238 items (at 10 items/min)
2015-11-04 14:05:15 [scrapy] INFO: Crawled 19717 pages (at 8 pages/min), scraped 19246 items (at 8 items/min)
2015-11-04 14:06:05 [scrapy] INFO: Crawled 19723 pages (at 6 pages/min), scraped 19254 items (at 8 items/min)
2015-11-04 14:07:09 [scrapy] INFO: Crawled 19738 pages (at 15 pages/min), scraped 19268 items (at 14 items/min)
2015-11-04 14:08:12 [scrapy] INFO: Crawled 19752 pages (at 14 pages/min), scraped 19283 items (at 15 items/min)
2015-11-04 14:09:00 [scrapy] INFO: Crawled 19753 pages (at 1 pages/min), scraped 19290 items (at 7 items/min)
2015-11-04 14:10:06 [scrapy] INFO: Crawled 19766 pages (at 13 pages/min), scraped 19297 items (at 7 items/min)
2015-11-04 14:11:37 [scrapy] INFO: Crawled 19780 pages (at 14 pages/min), scraped 19311 items (at 14 items/min)
2015-11-04 14:12:27 [scrapy] INFO: Crawled 19782 pages (at 2 pages/min), scraped 19317 items (at 6 items/min)
2015-11-04 14:13:06 [scrapy] INFO: Crawled 19785 pages (at 3 pages/min), scraped 19321 items (at 4 items/min)
2015-11-04 14:14:11 [scrapy] INFO: Crawled 19788 pages (at 3 pages/min), scraped 19325 items (at 4 items/min)
2015-11-04 14:15:00 [scrapy] INFO: Crawled 19789 pages (at 1 pages/min), scraped 19326 items (at 1 items/min)
2015-11-04 14:16:04 [scrapy] INFO: Crawled 19793 pages (at 4 pages/min), scraped 19329 items (at 3 items/min)
2015-11-04 14:17:03 [scrapy] INFO: Crawled 19801 pages (at 8 pages/min), scraped 19335 items (at 6 items/min)
2015-11-04 14:18:00 [scrapy] INFO: Crawled 19805 pages (at 4 pages/min), scraped 19341 items (at 6 items/min)
2015-11-04 14:19:19 [scrapy] INFO: Crawled 19818 pages (at 13 pages/min), scraped 19349 items (at 8 items/min)
2015-11-04 14:20:00 [scrapy] INFO: Crawled 19826 pages (at 8 pages/min), scraped 19355 items (at 6 items/min)
2015-11-04 14:21:17 [scrapy] INFO: Crawled 19835 pages (at 9 pages/min), scraped 19369 items (at 14 items/min)
2015-11-04 14:22:38 [scrapy] INFO: Crawled 19855 pages (at 20 pages/min), scraped 19384 items (at 15 items/min)
2015-11-04 14:23:22 [scrapy] INFO: Crawled 19858 pages (at 3 pages/min), scraped 19392 items (at 8 items/min)
2015-11-04 14:24:04 [scrapy] INFO: Crawled 19864 pages (at 6 pages/min), scraped 19398 items (at 6 items/min)
2015-11-04 14:25:00 [scrapy] INFO: Crawled 19869 pages (at 5 pages/min), scraped 19406 items (at 8 items/min)
2015-11-04 14:26:07 [scrapy] INFO: Crawled 19875 pages (at 6 pages/min), scraped 19411 items (at 5 items/min)
2015-11-04 14:27:07 [scrapy] INFO: Crawled 19885 pages (at 10 pages/min), scraped 19417 items (at 6 items/min)
2015-11-04 14:28:00 [scrapy] INFO: Crawled 19892 pages (at 7 pages/min), scraped 19427 items (at 10 items/min)
2015-11-04 14:29:00 [scrapy] INFO: Crawled 19901 pages (at 9 pages/min), scraped 19436 items (at 9 items/min)
2015-11-04 14:30:03 [scrapy] INFO: Crawled 19907 pages (at 6 pages/min), scraped 19443 items (at 7 items/min)
2015-11-04 14:31:04 [scrapy] INFO: Crawled 19926 pages (at 19 pages/min), scraped 19455 items (at 12 items/min)
2015-11-04 14:32:12 [scrapy] INFO: Crawled 19939 pages (at 13 pages/min), scraped 19470 items (at 15 items/min)
2015-11-04 14:33:00 [scrapy] INFO: Crawled 19939 pages (at 0 pages/min), scraped 19475 items (at 5 items/min)
2015-11-04 14:34:13 [scrapy] INFO: Crawled 19946 pages (at 7 pages/min), scraped 19480 items (at 5 items/min)
2015-11-04 14:35:36 [scrapy] INFO: Crawled 19965 pages (at 19 pages/min), scraped 19493 items (at 13 items/min)
2015-11-04 14:36:14 [scrapy] INFO: Crawled 19970 pages (at 5 pages/min), scraped 19500 items (at 7 items/min)
2015-11-04 14:37:22 [scrapy] INFO: Crawled 19985 pages (at 15 pages/min), scraped 19512 items (at 12 items/min)
2015-11-04 14:38:12 [scrapy] INFO: Crawled 19992 pages (at 7 pages/min), scraped 19520 items (at 8 items/min)
2015-11-04 14:39:16 [scrapy] INFO: Crawled 19999 pages (at 7 pages/min), scraped 19531 items (at 11 items/min)
2015-11-04 14:40:00 [scrapy] INFO: Crawled 19999 pages (at 0 pages/min), scraped 19534 items (at 3 items/min)
2015-11-04 14:41:26 [scrapy] INFO: Crawled 20020 pages (at 21 pages/min), scraped 19547 items (at 13 items/min)
2015-11-04 14:42:10 [scrapy] INFO: Crawled 20028 pages (at 8 pages/min), scraped 19555 items (at 8 items/min)
2015-11-04 14:43:25 [scrapy] INFO: Crawled 20040 pages (at 12 pages/min), scraped 19569 items (at 14 items/min)
2015-11-04 14:44:40 [scrapy] INFO: Crawled 20056 pages (at 16 pages/min), scraped 19583 items (at 14 items/min)
2015-11-04 14:45:28 [scrapy] INFO: Crawled 20062 pages (at 6 pages/min), scraped 19591 items (at 8 items/min)
2015-11-04 14:46:01 [scrapy] INFO: Crawled 20064 pages (at 2 pages/min), scraped 19597 items (at 6 items/min)
2015-11-04 14:47:07 [scrapy] INFO: Crawled 20081 pages (at 17 pages/min), scraped 19609 items (at 12 items/min)
2015-11-04 14:48:01 [scrapy] INFO: Crawled 20088 pages (at 7 pages/min), scraped 19623 items (at 14 items/min)
2015-11-04 14:49:10 [scrapy] INFO: Crawled 20111 pages (at 23 pages/min), scraped 19640 items (at 17 items/min)
2015-11-04 14:50:04 [scrapy] INFO: Crawled 20119 pages (at 8 pages/min), scraped 19651 items (at 11 items/min)
2015-11-04 14:51:32 [scrapy] INFO: Crawled 20141 pages (at 22 pages/min), scraped 19668 items (at 17 items/min)
2015-11-04 14:52:13 [scrapy] INFO: Crawled 20146 pages (at 5 pages/min), scraped 19676 items (at 8 items/min)
2015-11-04 14:53:25 [scrapy] INFO: Crawled 20161 pages (at 15 pages/min), scraped 19688 items (at 12 items/min)
2015-11-04 14:54:12 [scrapy] INFO: Crawled 20167 pages (at 6 pages/min), scraped 19696 items (at 8 items/min)
2015-11-04 14:55:17 [scrapy] INFO: Crawled 20177 pages (at 10 pages/min), scraped 19708 items (at 12 items/min)
2015-11-04 14:56:00 [scrapy] INFO: Crawled 20185 pages (at 8 pages/min), scraped 19719 items (at 11 items/min)
2015-11-04 14:57:02 [scrapy] INFO: Crawled 20202 pages (at 17 pages/min), scraped 19736 items (at 17 items/min)
2015-11-04 14:58:08 [scrapy] INFO: Crawled 20224 pages (at 22 pages/min), scraped 19755 items (at 19 items/min)
2015-11-04 14:59:03 [scrapy] INFO: Crawled 20231 pages (at 7 pages/min), scraped 19764 items (at 9 items/min)
2015-11-04 15:00:23 [scrapy] INFO: Crawled 20251 pages (at 20 pages/min), scraped 19778 items (at 14 items/min)
2015-11-04 15:01:11 [scrapy] INFO: Crawled 20259 pages (at 8 pages/min), scraped 19786 items (at 8 items/min)
2015-11-04 15:02:30 [scrapy] INFO: Crawled 20270 pages (at 11 pages/min), scraped 19799 items (at 13 items/min)
2015-11-04 15:03:06 [scrapy] INFO: Crawled 20278 pages (at 8 pages/min), scraped 19805 items (at 6 items/min)
2015-11-04 15:04:29 [scrapy] INFO: Crawled 20292 pages (at 14 pages/min), scraped 19821 items (at 16 items/min)
2015-11-04 15:05:03 [scrapy] INFO: Crawled 20294 pages (at 2 pages/min), scraped 19827 items (at 6 items/min)
2015-11-04 15:06:00 [scrapy] INFO: Crawled 20303 pages (at 9 pages/min), scraped 19838 items (at 11 items/min)
2015-11-04 15:07:08 [scrapy] INFO: Crawled 20323 pages (at 20 pages/min), scraped 19851 items (at 13 items/min)
2015-11-04 15:08:29 [scrapy] INFO: Crawled 20337 pages (at 14 pages/min), scraped 19866 items (at 15 items/min)
2015-11-04 15:09:01 [scrapy] INFO: Crawled 20344 pages (at 7 pages/min), scraped 19872 items (at 6 items/min)
2015-11-04 15:10:22 [scrapy] INFO: Crawled 20358 pages (at 14 pages/min), scraped 19887 items (at 15 items/min)
2015-11-04 15:11:30 [scrapy] INFO: Crawled 20367 pages (at 9 pages/min), scraped 19899 items (at 12 items/min)
2015-11-04 15:12:00 [scrapy] INFO: Crawled 20370 pages (at 3 pages/min), scraped 19904 items (at 5 items/min)
2015-11-04 15:13:05 [scrapy] INFO: Crawled 20389 pages (at 19 pages/min), scraped 19916 items (at 12 items/min)
2015-11-04 15:14:44 [scrapy] INFO: Crawled 20404 pages (at 15 pages/min), scraped 19932 items (at 16 items/min)
2015-11-04 15:15:24 [scrapy] INFO: Crawled 20406 pages (at 2 pages/min), scraped 19939 items (at 7 items/min)
2015-11-04 15:16:16 [scrapy] INFO: Crawled 20421 pages (at 15 pages/min), scraped 19949 items (at 10 items/min)
2015-11-04 15:17:36 [scrapy] INFO: Crawled 20437 pages (at 16 pages/min), scraped 19964 items (at 15 items/min)
2015-11-04 15:18:19 [scrapy] INFO: Crawled 20437 pages (at 0 pages/min), scraped 19972 items (at 8 items/min)
2015-11-04 15:19:10 [scrapy] INFO: Crawled 20448 pages (at 11 pages/min), scraped 19979 items (at 7 items/min)
2015-11-04 15:20:07 [scrapy] INFO: Crawled 20463 pages (at 15 pages/min), scraped 19990 items (at 11 items/min)
2015-11-04 15:21:06 [scrapy] INFO: Crawled 20476 pages (at 13 pages/min), scraped 20001 items (at 11 items/min)
2015-11-04 15:22:06 [scrapy] INFO: Crawled 20484 pages (at 8 pages/min), scraped 20013 items (at 12 items/min)
2015-11-04 15:23:26 [scrapy] INFO: Crawled 20498 pages (at 14 pages/min), scraped 20025 items (at 12 items/min)
2015-11-04 15:24:06 [scrapy] INFO: Crawled 20498 pages (at 0 pages/min), scraped 20033 items (at 8 items/min)
2015-11-04 15:25:09 [scrapy] INFO: Crawled 20517 pages (at 19 pages/min), scraped 20046 items (at 13 items/min)
2015-11-04 15:26:04 [scrapy] INFO: Crawled 20528 pages (at 11 pages/min), scraped 20059 items (at 13 items/min)
2015-11-04 15:27:17 [scrapy] INFO: Crawled 20545 pages (at 17 pages/min), scraped 20072 items (at 13 items/min)
2015-11-04 15:28:03 [scrapy] INFO: Crawled 20552 pages (at 7 pages/min), scraped 20080 items (at 8 items/min)
2015-11-04 15:29:24 [scrapy] INFO: Crawled 20565 pages (at 13 pages/min), scraped 20093 items (at 13 items/min)
2015-11-04 15:30:08 [scrapy] INFO: Crawled 20572 pages (at 7 pages/min), scraped 20100 items (at 7 items/min)
2015-11-04 15:31:11 [scrapy] INFO: Crawled 20579 pages (at 7 pages/min), scraped 20112 items (at 12 items/min)
2015-11-04 15:32:12 [scrapy] INFO: Crawled 20595 pages (at 16 pages/min), scraped 20123 items (at 11 items/min)
2015-11-04 15:33:31 [scrapy] INFO: Crawled 20610 pages (at 15 pages/min), scraped 20138 items (at 15 items/min)
2015-11-04 15:34:11 [scrapy] INFO: Crawled 20616 pages (at 6 pages/min), scraped 20145 items (at 7 items/min)
2015-11-04 15:35:38 [scrapy] INFO: Crawled 20629 pages (at 13 pages/min), scraped 20159 items (at 14 items/min)
2015-11-04 15:36:05 [scrapy] INFO: Crawled 20630 pages (at 1 pages/min), scraped 20163 items (at 4 items/min)
2015-11-04 15:37:06 [scrapy] INFO: Crawled 20647 pages (at 17 pages/min), scraped 20180 items (at 17 items/min)
2015-11-04 15:38:00 [scrapy] INFO: Crawled 20648 pages (at 1 pages/min), scraped 20182 items (at 2 items/min)
2015-11-04 15:39:00 [scrapy] INFO: Crawled 20648 pages (at 0 pages/min), scraped 20182 items (at 0 items/min)
2015-11-04 15:40:00 [scrapy] INFO: Crawled 20648 pages (at 0 pages/min), scraped 20182 items (at 0 items/min)
2015-11-04 15:41:00 [scrapy] INFO: Crawled 20648 pages (at 0 pages/min), scraped 20182 items (at 0 items/min)
2015-11-04 15:42:00 [scrapy] INFO: Crawled 20648 pages (at 0 pages/min), scraped 20182 items (at 0 items/min)
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:41 [scrapy] INFO: Closing spider (shutdown)
2015-11-04 15:43:00 [scrapy] INFO: Crawled 20648 pages (at 0 pages/min), scraped 20182 items (at 0 items/min)
2015-11-04 15:43:04 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1360,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 25,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 25,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 42,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 85,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1180,
 'downloader/request_bytes': 21344731,
 'downloader/request_count': 40866,
 'downloader/request_method_count/GET': 40866,
 'downloader/response_bytes': 712758717,
 'downloader/response_count': 39506,
 'downloader/response_status_count/200': 20319,
 'downloader/response_status_count/301': 1054,
 'downloader/response_status_count/302': 17577,
 'downloader/response_status_count/303': 3,
 'downloader/response_status_count/400': 236,
 'downloader/response_status_count/403': 17,
 'downloader/response_status_count/404': 230,
 'downloader/response_status_count/408': 58,
 'downloader/response_status_count/500': 12,
 'dupefilter/filtered': 207041,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2015, 11, 4, 15, 43, 4, 70972),
 'item_scraped_count': 20182,
 'log_count/CRITICAL': 1,
 'log_count/ERROR': 220,
 'log_count/INFO': 904,
 'log_count/WARNING': 2,
 'offsite/domains': 2047,
 'offsite/filtered': 24228,
 'request_depth_max': 2,
 'response_received_count': 20648,
 'scheduler/dequeued': 40866,
 'scheduler/dequeued/memory': 40866,
 'scheduler/enqueued': 51109,
 'scheduler/enqueued/memory': 51109,
 'spider_exceptions/AttributeError': 37,
 'spider_exceptions/IncompleteRead': 2,
 'spider_exceptions/IndexError': 6,
 'spider_exceptions/TypeError': 1,
 'spider_exceptions/timeout': 4,
 'start_time': datetime.datetime(2015, 11, 4, 0, 31, 0, 62001)}
2015-11-04 15:43:04 [scrapy] INFO: Spider closed (shutdown)
se/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 104, in feed
    self.goahead(0)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 138, in goahead
    k = self.parse_starttag(i)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 296, in parse_starttag
    self.finish_starttag(tag, attrs)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 345, in finish_starttag
    self.handle_starttag(tag, method, attrs)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 381, in handle_starttag
    method(attrs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1597, in start_meta
    if (self.declaredHTMLEncoding is not None or
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1204, in __getattr__
    return Tag.__getattr__(self, methodName)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 666, in __getattr__
    return self.find(tag)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 829, in find
    l = self.findAll(name, attrs, recursive, text, 1, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 849, in findAll
    return self._findAll(name, attrs, text, limit, generator, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 368, in _findAll
    strainer = SoupStrainer(name, attrs, text, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 895, in __init__
    if isinstance(attrs, basestring):
RuntimeError: maximum recursion depth exceeded in __instancecheck__
2015-11-04 03:19:09 [scrapy] INFO: Crawled 4442 pages (at 90 pages/min), scraped 4111 items (at 74 items/min)
2015-11-04 03:19:58 [scrapy] ERROR: Spider error processing <GET http://www.eurekahedge.com/attachments/09%20Nov%202006%20-%20Event-driven%20is%20one%20of%20the%20oldest%20and%20most%20successful%20hedge%20fund%20strategies-%20HFM%20Week.pdf> (referer: http://www.twincap.com/twin-capital-in-the-news.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:20:03 [scrapy] INFO: Crawled 4598 pages (at 156 pages/min), scraped 4249 items (at 138 items/min)
2015-11-04 03:20:59 [scrapy] INFO: Crawled 4708 pages (at 110 pages/min), scraped 4341 items (at 92 items/min)
2015-11-04 03:21:07 [PIL.ImageFile] ERROR: %s
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/ImageFile.py", line 100, in __init__
    self._open()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/IptcImagePlugin.py", line 95, in _open
    tag, size = self.field()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/IptcImagePlugin.py", line 71, in field
    tag = i8(s[1]), i8(s[2])
IndexError: string index out of range
2015-11-04 03:21:07 [PIL.ImageFile] ERROR: %s
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/ImageFile.py", line 100, in __init__
    self._open()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/MpegImagePlugin.py", line 71, in _open
    if s.read(32) != 0x1B3:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/MpegImagePlugin.py", line 53, in read
    v = self.peek(bits)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/MpegImagePlugin.py", line 38, in peek
    c = self.next()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/MpegImagePlugin.py", line 34, in next
    return i8(self.fp.read(1))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/_binary.py", line 18, in i8
    return ord(c)
TypeError: ord() expected a character, but string of length 0 found
2015-11-04 03:21:07 [PIL.ImageFile] ERROR: %s
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/ImageFile.py", line 100, in __init__
    self._open()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/TgaImagePlugin.py", line 59, in _open
    colormaptype = i8(s[1])
IndexError: string index out of range
2015-11-04 03:22:02 [scrapy] INFO: Crawled 4815 pages (at 107 pages/min), scraped 4459 items (at 118 items/min)
2015-11-04 03:22:56 [scrapy] INFO: Crawled 4879 pages (at 64 pages/min), scraped 4526 items (at 67 items/min)
2015-11-04 03:23:54 [scrapy] INFO: Crawled 4929 pages (at 50 pages/min), scraped 4582 items (at 56 items/min)
2015-11-04 03:24:54 [scrapy] INFO: Crawled 4992 pages (at 63 pages/min), scraped 4646 items (at 64 items/min)
2015-11-04 03:25:58 [scrapy] INFO: Crawled 5076 pages (at 84 pages/min), scraped 4721 items (at 75 items/min)
2015-11-04 03:27:00 [scrapy] INFO: Crawled 5154 pages (at 78 pages/min), scraped 4804 items (at 83 items/min)
2015-11-04 03:27:54 [scrapy] INFO: Crawled 5215 pages (at 61 pages/min), scraped 4863 items (at 59 items/min)
2015-11-04 03:28:57 [scrapy] INFO: Crawled 5298 pages (at 83 pages/min), scraped 4923 items (at 60 items/min)
2015-11-04 03:29:53 [scrapy] INFO: Crawled 5390 pages (at 92 pages/min), scraped 4977 items (at 54 items/min)
2015-11-04 03:30:53 [scrapy] INFO: Crawled 5407 pages (at 17 pages/min), scraped 4991 items (at 14 items/min)
2015-11-04 03:31:53 [scrapy] INFO: Crawled 5474 pages (at 67 pages/min), scraped 5057 items (at 66 items/min)
2015-11-04 03:32:59 [scrapy] INFO: Crawled 5618 pages (at 144 pages/min), scraped 5188 items (at 131 items/min)
2015-11-04 03:33:59 [scrapy] INFO: Crawled 5794 pages (at 176 pages/min), scraped 5336 items (at 148 items/min)
2015-11-04 03:34:49 [scrapy] ERROR: Spider error processing <GET http://www.equuspartners.com/media/b132962f-e527-4b0b-9288-3d558791c038/Newsletter/EquusNewsletter_2Q15_Final_pdf> (referer: http://www.equuspartners.com/News/newsletter)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:34:54 [scrapy] INFO: Crawled 5880 pages (at 86 pages/min), scraped 5412 items (at 76 items/min)
2015-11-04 03:35:54 [scrapy] INFO: Crawled 5916 pages (at 36 pages/min), scraped 5433 items (at 21 items/min)
2015-11-04 03:36:59 [scrapy] ERROR: Spider error processing <GET http://www.martincurrie.com/docs/corporate_brochure/> (referer: http://www.martincurrie.com/home/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:37:00 [scrapy] INFO: Crawled 5959 pages (at 43 pages/min), scraped 5466 items (at 33 items/min)
2015-11-04 03:38:40 [scrapy] INFO: Crawled 5991 pages (at 32 pages/min), scraped 5475 items (at 9 items/min)
2015-11-04 03:38:49 [scrapy] ERROR: Error downloading <GET http://www.citlon.com/../privacy.php>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 03:38:49 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050010.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 03:38:56 [scrapy] ERROR: Spider error processing <GET http://www.martincurrie.com/docs/annual-report-2013/> (referer: http://www.martincurrie.com/home/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/PCE.php>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.fed>: DNS lookup failed: address 'www.fed' not found: [Errno -2] Name or service not known.
2015-11-04 03:47:57 [scrapy] INFO: Crawled 5991 pages (at 0 pages/min), scraped 5492 items (at 17 items/min)
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET https://www.trianpartners.com/about-us/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.trianpartners.com/about-us/ took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET https://www.trianpartners.com/team-members/nelson-peltz/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.trianpartners.com/team-members/nelson-peltz/ took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/index.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.praesidian.com/index.php took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/news.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.praesidian.com/news.php took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/contact.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.praesidian.com/contact.php took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/about.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.praesidian.com/about.php took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/team.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.praesidian.com/team.php took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/sel_trans.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.praesidian.com/sel_trans.php took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/inv_crit.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.praesidian.com/inv_crit.php took longer than 180.0 seconds..
2015-11-04 03:47:57 [scrapy] ERROR: Error downloading <GET http://www.praesidian.com/inv_strat.php>: User timeout caused connection failure.
2015-11-04 03:49:05 [scrapy] INFO: Crawled 6023 pages (at 32 pages/min), scraped 5523 items (at 31 items/min)
2015-11-04 03:50:16 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=BOC> (referer: https://trade.bosera.com/acctMgr/openAcct/selectBankCard)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 03:50:44 [scrapy] INFO: Crawled 6049 pages (at 26 pages/min), scraped 5545 items (at 22 items/min)
2015-11-04 03:50:47 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/specialFund/mySpecialFundDetail>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:50:47 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/specialFund/specialFundIndex>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:50:47 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:50:47 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/myFundList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:51:15 [scrapy] INFO: Crawled 6058 pages (at 9 pages/min), scraped 5548 items (at 3 items/min)
2015-11-04 03:51:16 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=000936>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 03:51:20 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/userFeedback/feedbackForm>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:51:54 [scrapy] INFO: Crawled 6068 pages (at 10 pages/min), scraped 5566 items (at 18 items/min)
2015-11-04 03:51:54 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=SPDB>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:51:54 [scrapy] INFO: Closing spider (finished)
2015-11-04 03:51:54 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 212,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 36,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 4,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 28,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 140,
 'downloader/request_bytes': 2810324,
 'downloader/request_count': 6766,
 'downloader/request_method_count/GET': 6766,
 'downloader/response_bytes': 83848144,
 'downloader/response_count': 6554,
 'downloader/response_status_count/200': 5703,
 'downloader/response_status_count/301': 244,
 'downloader/response_status_count/302': 144,
 'downloader/response_status_count/303': 29,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 135,
 'downloader/response_status_count/404': 245,
 'downloader/response_status_count/500': 52,
 'dupefilter/filtered': 32937,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 3, 51, 54, 763859),
 'item_scraped_count': 5566,
 'log_count/ERROR': 56,
 'log_count/INFO': 109,
 'offsite/domains': 402,
 'offsite/filtered': 1718,
 'request_depth_max': 2,
 'response_received_count': 6068,
 'scheduler/dequeued': 6766,
 'scheduler/dequeued/memory': 6766,
 'scheduler/enqueued': 6766,
 'scheduler/enqueued/memory': 6766,
 'spider_exceptions/AttributeError': 5,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/RuntimeError': 1,
 'spider_exceptions/SSLError': 1,
 'spider_exceptions/TypeError': 9,
 'spider_exceptions/timeout': 3,
 'start_time': datetime.datetime(2015, 11, 4, 1, 58, 53, 933396)}
2015-11-04 03:51:54 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 03:52:56 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 03:52:56 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 03:52:56 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 03:52:56 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 03:52:56 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 03:52:56 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 03:52:56 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 03:52:57 [scrapy] INFO: Spider opened
2015-11-04 03:52:57 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 03:52:57 [scrapy] ERROR: Error downloading <GET http://www.investor.gppfunds.com>: DNS lookup failed: address 'www.investor.gppfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:02:30 [scrapy] INFO: Crawled 48 pages (at 48 pages/min), scraped 23 items (at 23 items/min)
2015-11-04 04:03:00 [scrapy] ERROR: Error downloading <GET http://www.lan>: DNS lookup failed: address 'www.lan' not found: [Errno -2] Name or service not known.
2015-11-04 04:03:00 [scrapy] INFO: Crawled 48 pages (at 0 pages/min), scraped 24 items (at 1 items/min)
2015-11-04 04:03:00 [scrapy] ERROR: Error downloading <GET http://www.investor.pccpllc.amiesdigital.com>: User timeout caused connection failure.
2015-11-04 04:03:00 [scrapy] ERROR: Error downloading <GET http://www.arb>: DNS lookup failed: address 'www.arb' not found: [Errno -2] Name or service not known.
2015-11-04 04:03:00 [scrapy] ERROR: Error downloading <GET http://www.atl>: DNS lookup failed: address 'www.atl' not found: [Errno -2] Name or service not known.
2015-11-04 04:03:00 [scrapy] ERROR: Error downloading <GET https://www.magnitudecapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]
2015-11-04 04:04:55 [scrapy] INFO: Crawled 227 pages (at 179 pages/min), scraped 131 items (at 107 items/min)
2015-11-04 04:05:29 [scrapy] INFO: Crawled 233 pages (at 6 pages/min), scraped 145 items (at 14 items/min)
2015-11-04 04:05:58 [scrapy] INFO: Crawled 259 pages (at 26 pages/min), scraped 150 items (at 5 items/min)
2015-11-04 04:07:52 [scrapy] INFO: Crawled 263 pages (at 4 pages/min), scraped 170 items (at 20 items/min)
2015-11-04 04:09:25 [scrapy] INFO: Crawled 275 pages (at 12 pages/min), scraped 182 items (at 12 items/min)
2015-11-04 04:09:58 [scrapy] INFO: Crawled 318 pages (at 43 pages/min), scraped 213 items (at 31 items/min)
2015-11-04 04:11:33 [scrapy] INFO: Crawled 326 pages (at 8 pages/min), scraped 229 items (at 16 items/min)
2015-11-04 04:11:57 [scrapy] INFO: Crawled 358 pages (at 32 pages/min), scraped 254 items (at 25 items/min)
2015-11-04 04:14:32 [scrapy] INFO: Crawled 409 pages (at 51 pages/min), scraped 308 items (at 54 items/min)
2015-11-04 04:16:37 [scrapy] INFO: Crawled 440 pages (at 31 pages/min), scraped 339 items (at 31 items/min)
2015-11-04 04:17:55 [scrapy] INFO: Crawled 481 pages (at 41 pages/min), scraped 376 items (at 37 items/min)
2015-11-04 04:18:12 [scrapy] INFO: Crawled 505 pages (at 24 pages/min), scraped 396 items (at 20 items/min)
2015-11-04 04:20:21 [scrapy] INFO: Crawled 534 pages (at 29 pages/min), scraped 419 items (at 23 items/min)
2015-11-04 04:21:40 [scrapy] INFO: Crawled 578 pages (at 44 pages/min), scraped 456 items (at 37 items/min)
2015-11-04 04:22:16 [scrapy] INFO: Crawled 602 pages (at 24 pages/min), scraped 492 items (at 36 items/min)
2015-11-04 04:23:07 [scrapy] INFO: Crawled 632 pages (at 30 pages/min), scraped 536 items (at 44 items/min)
2015-11-04 04:23:59 [scrapy] INFO: Crawled 704 pages (at 72 pages/min), scraped 597 items (at 61 items/min)
2015-11-04 04:25:13 [scrapy] INFO: Crawled 786 pages (at 82 pages/min), scraped 692 items (at 95 items/min)
2015-11-04 04:26:36 [scrapy] INFO: Crawled 806 pages (at 20 pages/min), scraped 713 items (at 21 items/min)
2015-11-04 04:27:00 [scrapy] INFO: Crawled 832 pages (at 26 pages/min), scraped 738 items (at 25 items/min)
2015-11-04 04:28:26 [scrapy] INFO: Crawled 929 pages (at 97 pages/min), scraped 813 items (at 75 items/min)
2015-11-04 04:29:21 [scrapy] INFO: Crawled 953 pages (at 24 pages/min), scraped 833 items (at 20 items/min)
2015-11-04 04:29:30 [scrapy] ERROR: Spider error processing <GET https://www.paamco.com/Publications/Pages/Viewpoints/The-World-is-Round---well,-at-least-not-completely-flat!.aspx> (referer: https://www.paamco.com/Publications/Pages/PAAMCO-Research.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 184, in crawl
    self.get_image()
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 206, in get_image
    self.article.top_image = self.image_extractor.get_best_image(doc, top_node)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 81, in get_best_image
    image = self.check_large_images(topNode, 0, 0)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 134, in check_large_images
    depth_obj.parent_depth, depth_obj.sibling_depth)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 115, in check_large_images
    good_images = self.get_image_candidates(node)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 272, in get_image_candidates
    images = self.get_node_images(node)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/images.py", line 238, in get_node_images
    images = self.parser.getElementsByTag(node, tag='img')
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 84, in getElementsByTag
    elems = node.xpath(selector, namespaces={"re": NS})
  File "lxml.etree.pyx", line 1495, in lxml.etree._Element.xpath (src/lxml/lxml.etree.c:52246)
  File "xpath.pxi", line 261, in lxml.etree.XPathElementEvaluator.__init__ (src/lxml/lxml.etree.c:151647)
  File "xpath.pxi", line 133, in lxml.etree._XPathEvaluatorBase.__init__ (src/lxml/lxml.etree.c:149994)
  File "xpath.pxi", line 57, in lxml.etree._XPathContext.__init__ (src/lxml/lxml.etree.c:148988)
  File "extensions.pxi", line 79, in lxml.etree._BaseContext.__init__ (src/lxml/lxml.etree.c:138854)
RuntimeError: maximum recursion depth exceeded while calling a Python object
2015-11-04 04:30:10 [scrapy] INFO: Crawled 1017 pages (at 64 pages/min), scraped 902 items (at 69 items/min)
2015-11-04 04:30:12 [scrapy] ERROR: Error downloading <GET https://cag.elliottadvisors.hk/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:31:00 [scrapy] INFO: Crawled 1102 pages (at 85 pages/min), scraped 987 items (at 85 items/min)
2015-11-04 04:32:00 [scrapy] INFO: Crawled 1181 pages (at 79 pages/min), scraped 1065 items (at 78 items/min)
2015-11-04 04:33:09 [scrapy] INFO: Crawled 1279 pages (at 98 pages/min), scraped 1164 items (at 99 items/min)
2015-11-04 04:33:31 [scrapy] ERROR: Spider error processing <GET http://madisonint.com/de/our-business/special-situations/> (referer: http://madisonint.com/our-business/special-situations/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:34:11 [scrapy] INFO: Crawled 1362 pages (at 83 pages/min), scraped 1236 items (at 72 items/min)
2015-11-04 04:34:57 [scrapy] INFO: Crawled 1410 pages (at 48 pages/min), scraped 1294 items (at 58 items/min)
2015-11-04 04:36:04 [scrapy] INFO: Crawled 1473 pages (at 63 pages/min), scraped 1360 items (at 66 items/min)
2015-11-04 04:37:07 [scrapy] INFO: Crawled 1544 pages (at 71 pages/min), scraped 1424 items (at 64 items/min)
2015-11-04 04:38:01 [scrapy] INFO: Crawled 1612 pages (at 68 pages/min), scraped 1507 items (at 83 items/min)
2015-11-04 04:38:26 [scrapy] ERROR: Error downloading <GET http://www.mcpartners.com/portfolio/view_by_sector/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 04:38:51 [scrapy] ERROR: Error downloading <GET http://www.santanderasset.com>: DNS lookup failed: address 'www.santanderasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:51 [scrapy] ERROR: Error downloading <GET http://www.enhancedcapct.com>: DNS lookup failed: address 'www.enhancedcapct.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:58 [scrapy] ERROR: Error downloading <GET http://www.freshfordcapital.com>: DNS lookup failed: address 'www.freshfordcapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:59 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:59 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:59 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:59 [scrapy] ERROR: Error downloading <GET http://www.vnc>: DNS lookup failed: address 'www.vnc' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:59 [scrapy] ERROR: Error downloading <GET http://www.dbhedgeworks.cib.db.com>: DNS lookup failed: address 'www.dbhedgeworks.cib.db.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:59 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 04:38:59 [scrapy] INFO: Crawled 1719 pages (at 107 pages/min), scraped 1601 items (at 94 items/min)
2015-11-04 04:39:57 [scrapy] INFO: Crawled 1792 pages (at 73 pages/min), scraped 1672 items (at 71 items/min)
2015-11-04 04:40:33 [scrapy] ERROR: Error downloading <GET http://www.seamarkcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 04:40:57 [scrapy] INFO: Crawled 1792 pages (at 0 pages/min), scraped 1672 items (at 0 items/min)
2015-11-04 04:41:57 [scrapy] INFO: Crawled 1792 pages (at 0 pages/min), scraped 1672 items (at 0 items/min)
2015-11-04 04:42:57 [scrapy] INFO: Crawled 1792 pages (at 0 pages/min), scraped 1672 items (at 0 items/min)
2015-11-04 04:43:57 [scrapy] INFO: Crawled 1792 pages (at 0 pages/min), scraped 1672 items (at 0 items/min)
2015-11-04 04:44:57 [scrapy] INFO: Crawled 1792 pages (at 0 pages/min), scraped 1672 items (at 0 items/min)
2015-11-04 04:45:37 [scrapy] ERROR: Error downloading <GET https://sso.huntcompanies.com/cla/auth/index.php?return_path=>: TCP connection timed out: 110: Connection timed out.
2015-11-04 04:45:37 [scrapy] INFO: Closing spider (finished)
2015-11-04 04:45:37 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 370,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 12,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 41,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 31,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 51,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 235,
 'downloader/request_bytes': 765847,
 'downloader/request_count': 2367,
 'downloader/request_method_count/GET': 2367,
 'downloader/response_bytes': 30338585,
 'downloader/response_count': 1997,
 'downloader/response_status_count/200': 1759,
 'downloader/response_status_count/301': 101,
 'downloader/response_status_count/302': 99,
 'downloader/response_status_count/303': 2,
 'downloader/response_status_count/404': 36,
 'dupefilter/filtered': 15928,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 4, 45, 37, 534532),
 'item_scraped_count': 1672,
 'log_count/ERROR': 21,
 'log_count/INFO': 47,
 'offsite/domains': 574,
 'offsite/filtered': 1876,
 'request_depth_max': 2,
 'response_received_count': 1792,
 'scheduler/dequeued': 2367,
 'scheduler/dequeued/memory': 2367,
 'scheduler/enqueued': 2367,
 'scheduler/enqueued/memory': 2367,
 'spider_exceptions/RuntimeError': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 3, 52, 57, 1496)}
2015-11-04 04:45:37 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 04:46:39 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 04:46:39 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 04:46:39 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 04:46:39 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 04:46:39 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 04:46:39 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 04:46:39 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 04:46:39 [scrapy] INFO: Spider opened
2015-11-04 04:46:39 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 04:46:40 [scrapy] ERROR: Error downloading <GET http://www.lar>: DNS lookup failed: address 'www.lar' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:40 [scrapy] ERROR: Error downloading <GET http://www.clairvuecapital.com>: DNS lookup failed: address 'www.clairvuecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:40 [scrapy] ERROR: Error downloading <GET http://www.enhancedcapct.com>: DNS lookup failed: address 'www.enhancedcapct.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:40 [scrapy] ERROR: Error downloading <GET http://www.riverside-pm.com>: DNS lookup failed: address 'www.riverside-pm.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:40 [scrapy] ERROR: Error downloading <GET http://www.woodbinecapital.com>: DNS lookup failed: address 'www.woodbinecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:40 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:41 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:41 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:41 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:46:43 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 04:49:27 [scrapy] INFO: Crawled 153 pages (at 153 pages/min), scraped 73 items (at 73 items/min)
2015-11-04 04:49:34 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-6-at-DownloadNews.pdf&ORG_FILENAME=Chuys.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:35 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-5-at-DownloadNews.pdf&ORG_FILENAME=Intermix.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:35 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-4-at-DownloadNews.pdf&ORG_FILENAME=Rosa_Mexicano.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:35 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-3-at-DownloadNews.pdf&ORG_FILENAME=Strike.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:36 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-2-at-DownloadNews.pdf&ORG_FILENAME=Skullcandy.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:36 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-8-at-DownloadNews.pdf&ORG_FILENAME=All_Saints.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:43 [scrapy] INFO: Crawled 197 pages (at 44 pages/min), scraped 104 items (at 31 items/min)
2015-11-04 04:49:44 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-7-at-DownloadNews.pdf&ORG_FILENAME=Entrepreneur_Magazine.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:50 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-9-at-DownloadNews.pdf&ORG_FILENAME=Skullcandy_IPO.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:49:58 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-1-at-DownloadNews.pdf&ORG_FILENAME=Fortune_Magazine.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:50:02 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-11-at-DownloadNews.pdf&ORG_FILENAME=Lacrosse_Unlimited.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:50:03 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-10-at-DownloadNews.pdf&ORG_FILENAME=Chuys_IPO.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:50:04 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-14-at-DownloadNews.pdf&ORG_FILENAME=Chuys_IPO_2.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:54:52 [scrapy] INFO: Crawled 247 pages (at 50 pages/min), scraped 141 items (at 37 items/min)
2015-11-04 04:54:53 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-13-at-DownloadNews.pdf&ORG_FILENAME=Press_Release.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:54:59 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-18-at-DownloadNews.pdf&ORG_FILENAME=Rosa_Mexicano_Tom_Dillon.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:00 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-16-at-DownloadNews.pdf&ORG_FILENAME=DKB_John_Tucker.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:04 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-10-at-DownloadPR.pdf&ORG_FILENAME=Intermix.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:06 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-8-at-DownloadPR.pdf&ORG_FILENAME=All_Saints.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:06 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-6-at-DownloadPR.pdf&ORG_FILENAME=Skullcandy.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:07 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-5-at-DownloadPR.pdf&ORG_FILENAME=Strike.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:09 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-9-at-DownloadPR.pdf&ORG_FILENAME=Lacrosse__Unlimited.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:16 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-4-at-DownloadPR.pdf&ORG_FILENAME=Rosa_Mexicano.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:16 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-1-at-DownloadPR.pdf&ORG_FILENAME=Chuys.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:17 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-11-at-DownloadPR.pdf&ORG_FILENAME=DKB.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:17 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-24-at-DownloadNews.pdf&ORG_FILENAME=DKB.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:18 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-12-at-DownloadPR.pdf&ORG_FILENAME=Sneaker_Villa.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:27 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=12-13-at-DownloadPR.pdf&ORG_FILENAME=La_Colombe_Press_Release.pdf> (referer: http://www.goodepartners.com/portfolio/portfolio.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:55:42 [scrapy] INFO: Crawled 314 pages (at 67 pages/min), scraped 195 items (at 54 items/min)
2015-11-04 04:56:40 [scrapy] INFO: Crawled 379 pages (at 65 pages/min), scraped 260 items (at 65 items/min)
2015-11-04 04:57:44 [scrapy] INFO: Crawled 439 pages (at 60 pages/min), scraped 324 items (at 64 items/min)
2015-11-04 04:58:43 [scrapy] INFO: Crawled 503 pages (at 64 pages/min), scraped 388 items (at 64 items/min)
2015-11-04 04:59:43 [scrapy] INFO: Crawled 575 pages (at 72 pages/min), scraped 456 items (at 68 items/min)
2015-11-04 05:00:43 [scrapy] INFO: Crawled 651 pages (at 76 pages/min), scraped 532 items (at 76 items/min)
2015-11-04 05:01:43 [scrapy] INFO: Crawled 718 pages (at 67 pages/min), scraped 602 items (at 70 items/min)
2015-11-04 05:19:45 [scrapy] INFO: Crawled 777 pages (at 59 pages/min), scraped 663 items (at 61 items/min)
2015-11-04 05:20:49 [scrapy] INFO: Crawled 868 pages (at 91 pages/min), scraped 732 items (at 69 items/min)
2015-11-04 05:21:42 [scrapy] INFO: Crawled 900 pages (at 32 pages/min), scraped 779 items (at 47 items/min)
2015-11-04 05:22:43 [scrapy] INFO: Crawled 959 pages (at 59 pages/min), scraped 829 items (at 50 items/min)
2015-11-04 05:23:50 [scrapy] INFO: Crawled 1015 pages (at 56 pages/min), scraped 881 items (at 52 items/min)
2015-11-04 05:24:40 [scrapy] INFO: Crawled 1053 pages (at 38 pages/min), scraped 927 items (at 46 items/min)
2015-11-04 05:25:44 [scrapy] INFO: Crawled 1117 pages (at 64 pages/min), scraped 987 items (at 60 items/min)
2015-11-04 05:26:43 [scrapy] INFO: Crawled 1184 pages (at 67 pages/min), scraped 1058 items (at 71 items/min)
2015-11-04 05:27:46 [scrapy] INFO: Crawled 1261 pages (at 77 pages/min), scraped 1138 items (at 80 items/min)
2015-11-04 05:28:42 [scrapy] INFO: Crawled 1336 pages (at 75 pages/min), scraped 1210 items (at 72 items/min)
2015-11-04 05:29:41 [scrapy] INFO: Crawled 1408 pages (at 72 pages/min), scraped 1282 items (at 72 items/min)
2015-11-04 05:30:44 [scrapy] INFO: Crawled 1485 pages (at 77 pages/min), scraped 1362 items (at 80 items/min)
2015-11-04 05:31:43 [scrapy] INFO: Crawled 1560 pages (at 75 pages/min), scraped 1434 items (at 72 items/min)
2015-11-04 05:32:41 [scrapy] INFO: Crawled 1630 pages (at 70 pages/min), scraped 1504 items (at 70 items/min)
2015-11-04 05:33:41 [scrapy] INFO: Crawled 1702 pages (at 72 pages/min), scraped 1576 items (at 72 items/min)
2015-11-04 05:34:43 [scrapy] INFO: Crawled 1778 pages (at 76 pages/min), scraped 1651 items (at 75 items/min)
2015-11-04 05:35:43 [scrapy] INFO: Crawled 1854 pages (at 76 pages/min), scraped 1722 items (at 71 items/min)
2015-11-04 05:36:49 [scrapy] INFO: Crawled 1929 pages (at 75 pages/min), scraped 1794 items (at 72 items/min)
2015-11-04 05:37:46 [scrapy] INFO: Crawled 2016 pages (at 87 pages/min), scraped 1849 items (at 55 items/min)
2015-11-04 05:40:50 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 05:40:50 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 05:40:50 [scrapy] ERROR: Error downloading <GET http://www.ecp.altareturn.com>: DNS lookup failed: address 'www.ecp.altareturn.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:40:50 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:40:50 [scrapy] INFO: Crawled 2065 pages (at 49 pages/min), scraped 1913 items (at 64 items/min)
2015-11-04 05:41:02 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-17-at-DownloadNews.pdf&ORG_FILENAME=Bowlmor-AMF_Merger.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:41:16 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-23-at-DownloadNews.pdf&ORG_FILENAME=La_Colombe.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:41:17 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-22-at-DownloadNews.pdf&ORG_FILENAME=Michael_Stanley_Promotion.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:41:19 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-21-at-DownloadNews.pdf&ORG_FILENAME=Villa-DK.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:41:53 [scrapy] INFO: Crawled 2117 pages (at 52 pages/min), scraped 1976 items (at 63 items/min)
2015-11-04 05:42:47 [scrapy] INFO: Crawled 2155 pages (at 38 pages/min), scraped 2025 items (at 49 items/min)
2015-11-04 05:43:42 [scrapy] INFO: Crawled 2211 pages (at 56 pages/min), scraped 2081 items (at 56 items/min)
2015-11-04 05:44:40 [scrapy] INFO: Crawled 2267 pages (at 56 pages/min), scraped 2137 items (at 56 items/min)
2015-11-04 05:45:41 [scrapy] INFO: Crawled 2333 pages (at 66 pages/min), scraped 2201 items (at 64 items/min)
2015-11-04 05:45:58 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-15-at-DownloadNews.pdf&ORG_FILENAME=DKB_-_Walt_Freese.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:45:58 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 05:45:58 [scrapy] ERROR: Error downloading <GET http://www.susafund.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 05:45:58 [scrapy] ERROR: Error downloading <GET http://www.esemplia.com>: DNS lookup failed: address 'www.esemplia.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:45:58 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:45:58 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-20-at-DownloadNews.pdf&ORG_FILENAME=LOH_-_Robert_Marc.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:46:16 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-19-at-DownloadNews.pdf&ORG_FILENAME=La_Colombe.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:46:47 [scrapy] INFO: Crawled 2404 pages (at 71 pages/min), scraped 2270 items (at 69 items/min)
2015-11-04 05:46:47 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 05:46:55 [scrapy] ERROR: Spider error processing <GET http://www.goodepartners.com/webyep-system/program/download.php?FILENAME=10-12-at-DownloadNews.pdf&ORG_FILENAME=Entrepreneur_Magazine.pdf> (referer: http://www.goodepartners.com/news/news.php)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://www.vscapitalpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://www.intrepidcap.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://www.adelphi-europe.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://www.quintanacapitalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=media>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=daniel-graf-von-der-schulenberg-%E2%80%93-senior-analyst>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=wilson-chang-%E2%80%93-senior-associate>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=david-tuohy-%E2%80%93-operating-partner>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=joseph-e-slamm-%E2%80%93-partner>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:34 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=paul-ho-%E2%80%93-managing-director-0>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:39 [scrapy] INFO: Crawled 2446 pages (at 42 pages/min), scraped 2316 items (at 46 items/min)
2015-11-04 05:47:48 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=john-cavalier-%E2%80%93-managing-partner>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:48 [scrapy] ERROR: Error downloading <GET http://hudsoncep.com/?q=neil-z-auerbach-%E2%80%93-managing-partner>: TCP connection timed out: 110: Connection timed out.
2015-11-04 05:47:48 [scrapy] INFO: Closing spider (finished)
2015-11-04 05:47:48 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 173,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 45,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 36,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 38,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 42,
 'downloader/request_bytes': 1251234,
 'downloader/request_count': 2694,
 'downloader/request_method_count/GET': 2694,
 'downloader/response_bytes': 26454375,
 'downloader/response_count': 2521,
 'downloader/response_status_count/200': 2444,
 'downloader/response_status_count/301': 23,
 'downloader/response_status_count/302': 18,
 'downloader/response_status_count/303': 7,
 'downloader/response_status_count/400': 2,
 'downloader/response_status_count/403': 11,
 'downloader/response_status_count/404': 11,
 'downloader/response_status_count/500': 2,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 6137,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 5, 47, 48, 862308),
 'item_scraped_count': 2316,
 'log_count/ERROR': 65,
 'log_count/INFO': 44,
 'offsite/domains': 282,
 'offsite/filtered': 973,
 'request_depth_max': 2,
 'response_received_count': 2446,
 'scheduler/dequeued': 2694,
 'scheduler/dequeued/memory': 2694,
 'scheduler/enqueued': 2694,
 'scheduler/enqueued/memory': 2694,
 'spider_exceptions/AttributeError': 34,
 'start_time': datetime.datetime(2015, 11, 4, 4, 46, 39, 868166)}
2015-11-04 05:47:48 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 05:48:51 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 05:48:51 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 05:48:51 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 05:48:51 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 05:48:51 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 05:48:51 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 05:48:51 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 05:48:51 [scrapy] INFO: Spider opened
2015-11-04 05:48:51 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.sco>: DNS lookup failed: address 'www.sco' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.ecp.altareturn.com>: DNS lookup failed: address 'www.ecp.altareturn.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.coo>: DNS lookup failed: address 'www.coo' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.investor.gppfunds.com>: DNS lookup failed: address 'www.investor.gppfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.torshencapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.portal.krfs.com>: DNS lookup failed: address 'www.portal.krfs.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.cshg.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.par>: DNS lookup failed: address 'www.par' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.key>: DNS lookup failed: address 'www.key' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:52 [scrapy] ERROR: Error downloading <GET http://www.zca>: DNS lookup failed: address 'www.zca' not found: [Errno -2] Name or service not known.
2015-11-04 05:48:53 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 05:50:08 [scrapy] INFO: Crawled 191 pages (at 191 pages/min), scraped 116 items (at 116 items/min)
2015-11-04 05:50:57 [scrapy] INFO: Crawled 253 pages (at 62 pages/min), scraped 179 items (at 63 items/min)
2015-11-04 05:52:00 [scrapy] INFO: Crawled 325 pages (at 72 pages/min), scraped 250 items (at 71 items/min)
2015-11-04 05:52:57 [scrapy] INFO: Crawled 388 pages (at 63 pages/min), scraped 315 items (at 65 items/min)
2015-11-04 05:53:52 [scrapy] INFO: Crawled 463 pages (at 75 pages/min), scraped 383 items (at 68 items/min)
2015-11-04 05:54:54 [scrapy] INFO: Crawled 528 pages (at 65 pages/min), scraped 453 items (at 70 items/min)
2015-11-04 05:55:54 [scrapy] INFO: Crawled 601 pages (at 73 pages/min), scraped 525 items (at 72 items/min)
2015-11-04 05:56:58 [scrapy] INFO: Crawled 674 pages (at 73 pages/min), scraped 599 items (at 74 items/min)
2015-11-04 05:57:54 [scrapy] INFO: Crawled 737 pages (at 63 pages/min), scraped 662 items (at 63 items/min)
2015-11-04 05:58:52 [scrapy] INFO: Crawled 805 pages (at 68 pages/min), scraped 724 items (at 62 items/min)
2015-11-04 05:59:53 [scrapy] INFO: Crawled 855 pages (at 50 pages/min), scraped 780 items (at 56 items/min)
2015-11-04 06:00:53 [scrapy] INFO: Crawled 925 pages (at 70 pages/min), scraped 844 items (at 64 items/min)
2015-11-04 06:01:55 [scrapy] INFO: Crawled 983 pages (at 58 pages/min), scraped 908 items (at 64 items/min)
2015-11-04 06:02:57 [scrapy] INFO: Crawled 1046 pages (at 63 pages/min), scraped 971 items (at 63 items/min)
2015-11-04 06:03:58 [scrapy] INFO: Crawled 1119 pages (at 73 pages/min), scraped 1038 items (at 67 items/min)
2015-11-04 06:05:01 [scrapy] INFO: Crawled 1184 pages (at 65 pages/min), scraped 1108 items (at 70 items/min)
2015-11-04 06:06:00 [scrapy] INFO: Crawled 1240 pages (at 56 pages/min), scraped 1163 items (at 55 items/min)
2015-11-04 06:06:54 [scrapy] INFO: Crawled 1314 pages (at 74 pages/min), scraped 1221 items (at 58 items/min)
2015-11-04 06:07:46 [scrapy] ERROR: Error downloading <GET http://www.cqs>: DNS lookup failed: address 'www.cqs' not found: [Errno -2] Name or service not known.
2015-11-04 06:07:46 [scrapy] ERROR: Error downloading <GET http://www.emergingmanagersgroup.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 06:07:56 [scrapy] INFO: Crawled 1398 pages (at 84 pages/min), scraped 1315 items (at 94 items/min)
2015-11-04 06:08:02 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 06:08:57 [scrapy] INFO: Crawled 1492 pages (at 94 pages/min), scraped 1384 items (at 69 items/min)
2015-11-04 06:09:56 [scrapy] INFO: Crawled 1537 pages (at 45 pages/min), scraped 1444 items (at 60 items/min)
2015-11-04 06:10:53 [scrapy] INFO: Crawled 1601 pages (at 64 pages/min), scraped 1508 items (at 64 items/min)
2015-11-04 06:11:57 [scrapy] INFO: Crawled 1673 pages (at 72 pages/min), scraped 1580 items (at 72 items/min)
2015-11-04 06:12:51 [scrapy] INFO: Crawled 1737 pages (at 64 pages/min), scraped 1644 items (at 64 items/min)
2015-11-04 06:13:57 [scrapy] INFO: Crawled 1817 pages (at 80 pages/min), scraped 1724 items (at 80 items/min)
2015-11-04 06:14:53 [scrapy] INFO: Crawled 1873 pages (at 56 pages/min), scraped 1780 items (at 56 items/min)
2015-11-04 06:15:58 [scrapy] INFO: Crawled 1937 pages (at 64 pages/min), scraped 1844 items (at 64 items/min)
2015-11-04 06:17:01 [scrapy] INFO: Crawled 2005 pages (at 68 pages/min), scraped 1910 items (at 66 items/min)
2015-11-04 06:17:55 [scrapy] INFO: Crawled 2064 pages (at 59 pages/min), scraped 1966 items (at 56 items/min)
2015-11-04 06:18:56 [scrapy] INFO: Crawled 2134 pages (at 70 pages/min), scraped 2036 items (at 70 items/min)
2015-11-04 06:19:25 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:19:34 [scrapy] ERROR: Error downloading <GET http://www.esemplia.com>: DNS lookup failed: address 'www.esemplia.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:19:59 [scrapy] INFO: Crawled 2211 pages (at 77 pages/min), scraped 2111 items (at 75 items/min)
2015-11-04 06:20:09 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:20:31 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:20:53 [scrapy] INFO: Crawled 2281 pages (at 70 pages/min), scraped 2191 items (at 80 items/min)
2015-11-04 06:21:24 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:21:24 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:21:24 [scrapy] ERROR: Error downloading <GET http://www.altacomm.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:21:24 [scrapy] INFO: Closing spider (finished)
2015-11-04 06:21:24 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 78,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 7,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 48,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 9,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 5,
 'downloader/request_bytes': 1164950,
 'downloader/request_count': 2479,
 'downloader/request_method_count/GET': 2479,
 'downloader/response_bytes': 18188717,
 'downloader/response_count': 2401,
 'downloader/response_status_count/200': 2306,
 'downloader/response_status_count/301': 36,
 'downloader/response_status_count/302': 36,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 14,
 'downloader/response_status_count/404': 2,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 6363,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 6, 21, 24, 990261),
 'item_scraped_count': 2214,
 'log_count/ERROR': 24,
 'log_count/INFO': 39,
 'offsite/domains': 116,
 'offsite/filtered': 554,
 'request_depth_max': 2,
 'response_received_count': 2304,
 'scheduler/dequeued': 2479,
 'scheduler/dequeued/memory': 2479,
 'scheduler/enqueued': 2479,
 'scheduler/enqueued/memory': 2479,
 'start_time': datetime.datetime(2015, 11, 4, 5, 48, 51, 896214)}
2015-11-04 06:21:24 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 06:22:27 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 06:22:27 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 06:22:27 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 06:22:27 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 06:22:27 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 06:22:27 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 06:22:27 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 06:22:27 [scrapy] INFO: Spider opened
2015-11-04 06:22:27 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 06:22:28 [scrapy] ERROR: Error downloading <GET http://www.cmsco.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 06:22:28 [scrapy] ERROR: Error downloading <GET http://www.torshencapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 06:22:29 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 06:22:29 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 06:23:36 [scrapy] INFO: Crawled 227 pages (at 227 pages/min), scraped 111 items (at 111 items/min)
2015-11-04 06:24:29 [scrapy] INFO: Crawled 300 pages (at 73 pages/min), scraped 175 items (at 64 items/min)
2015-11-04 06:25:10 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:25:10 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7fadb7535668>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 06:25:32 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/ssctechnologies>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:25:32 [scrapy] INFO: Crawled 354 pages (at 54 pages/min), scraped 238 items (at 63 items/min)
2015-11-04 06:25:47 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/q4websystems>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:25:50 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/google+maps+50+Rowes+Wharf/data=!4m2!2m1!4b1?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:26:44 [scrapy] INFO: Crawled 380 pages (at 26 pages/min), scraped 274 items (at 36 items/min)
2015-11-04 06:26:44 [scrapy] ERROR: Error downloading <GET https://www.lightspeed.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:26:51 [scrapy] ERROR: Error downloading <GET https://www.kcg.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:27:29 [scrapy] INFO: Crawled 453 pages (at 73 pages/min), scraped 334 items (at 60 items/min)
2015-11-04 06:28:01 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/14241+Dallas+Parkway,+Dallas,+TX/@33.053554,-96.82948,14559m/data=!3m2!1e3!4b1?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:50 [scrapy] INFO: Crawled 527 pages (at 74 pages/min), scraped 408 items (at 74 items/min)
2015-11-04 06:29:05 [scrapy] ERROR: Error downloading <GET https://helpx.adobe.com/support.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:07 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=%40NetBase&esheet=51206683&id=smartlink&index=3&lan=en-US&md5=67c9a53f93d98177a86b9e3d18b9dbd1&newsitemid=20151021006518&url=https%3A%2F%2Ftwitter.com%2FNetBase> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1005: Tag nav invalid
2015-11-04 06:29:07 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.netbase.com&esheet=51206683&id=smartlink&index=2&lan=en-US&md5=cadcbc9bd53a2cca5118452730a3db3f&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1005: Tag nav invalid
2015-11-04 06:29:08 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase+LIVE+Pulse&esheet=51206683&id=smartlink&index=4&lan=en-US&md5=b082c27a4dc5393f87d50dae85bc9e4a&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com%2Fproducts-overview%2Flive-pulse-product-suite%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1005: Tag nav invalid
2015-11-04 06:29:08 [scrapy] ERROR: Spider error processing <GET https://www.backstopsolutions.com/documents/1701153> (referer: https://www.backstopsolutions.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:29:10 [scrapy] ERROR: Spider error processing <GET https://www.whi.com/documents/1795075> (referer: https://www.whi.com/pages/3062)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:29:14 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/191+N.+Wacker+Drive,/@41.893205,-87.635279,14z/data=!4m2!2m1!4b1?source=embed&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:29:32 [scrapy] INFO: Crawled 567 pages (at 40 pages/min), scraped 452 items (at 44 items/min)
2015-11-04 06:30:29 [scrapy] INFO: Crawled 625 pages (at 58 pages/min), scraped 496 items (at 44 items/min)
2015-11-04 06:30:46 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase&esheet=51155453&id=smartlink&index=1&lan=en-US&md5=afb50cfa7109b276baff40e02e4eb89d&newsitemid=20150804005417&url=http%3A%2F%2Fwww.netbase.com%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1200: Element script embeds close tag
2015-11-04 06:30:56 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase&esheet=51168484&id=smartlink&index=1&lan=en-US&md5=9b593437d08385a91e0495900221720f&newsitemid=20150825005342&url=http%3A%2F%2Fwww.netbase.com%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1: htmlParseEntityRef: no name
2015-11-04 06:30:56 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase&esheet=51206683&id=smartlink&index=1&lan=en-US&md5=5b222d35082163093383787f2190a7f7&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com> (referer: http://springlakeequitypartners.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1: htmlParseEntityRef: no name
2015-11-04 06:31:29 [scrapy] INFO: Crawled 688 pages (at 63 pages/min), scraped 547 items (at 51 items/min)
2015-11-04 06:32:07 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=3&lan=en-US&md5=b2c03c2adec48fc1257e632f5893599f&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1: htmlParseEntityRef: no name
2015-11-04 06:32:08 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=2&lan=en-US&md5=5158266120384eecdd44b68c41040b1f&newsitemid=20151103007039&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1: htmlParseEntityRef: no name
2015-11-04 06:32:12 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=1&lan=en-US&md5=3b5f5d52f28fddf355051fd8768bf656&newsitemid=20151103007039&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com%2F> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 10958: Tag footer invalid
2015-11-04 06:32:13 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51201762&id=smartlink&index=2&lan=en-US&md5=7debb5b50ba2b3e82184d018d846e791&newsitemid=20151014006687&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Company-Update-for-September-2015/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 10958: Tag footer invalid
2015-11-04 06:32:13 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51201762&id=smartlink&index=1&lan=en-US&md5=72e6b29e66ffbd45c2cc962332ab5694&newsitemid=20151014006687&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Company-Update-for-September-2015/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 10958: Tag footer invalid
2015-11-04 06:32:13 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51193482&id=smartlink&index=2&lan=en-US&md5=1d97f15d0085b2ad9ff18003c6868369&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Fourth-Quarter-2015-Preferred-Distributions/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 10958: Tag footer invalid
2015-11-04 06:32:14 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51193482&id=smartlink&index=1&lan=en-US&md5=35501886b80f5ce4fea6c8e2127ad6f5&newsitemid=20151001006977&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Fourth-Quarter-2015-Preferred-Distributions/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 10958: Tag footer invalid
2015-11-04 06:32:47 [scrapy] INFO: Crawled 766 pages (at 78 pages/min), scraped 629 items (at 82 items/min)
2015-11-04 06:34:09 [scrapy] INFO: Crawled 858 pages (at 92 pages/min), scraped 695 items (at 66 items/min)
2015-11-04 06:34:18 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:34:57 [scrapy] INFO: Crawled 878 pages (at 20 pages/min), scraped 736 items (at 41 items/min)
2015-11-04 06:35:00 [scrapy] ERROR: Error downloading <GET https://www.oracle.com/hyperion/index.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:36:21 [scrapy] INFO: Crawled 980 pages (at 102 pages/min), scraped 818 items (at 82 items/min)
2015-11-04 06:36:34 [scrapy] INFO: Crawled 991 pages (at 11 pages/min), scraped 841 items (at 23 items/min)
2015-11-04 06:37:28 [scrapy] INFO: Crawled 1105 pages (at 114 pages/min), scraped 924 items (at 83 items/min)
2015-11-04 06:38:28 [scrapy] ERROR: Spider error processing <GET http://assets.legal.web.com/TermsOfUse.pdf> (referer: http://www.networksolutions.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:38:31 [scrapy] ERROR: Spider error processing <GET http://assets.legal.web.com/PrivacyPolicy.pdf> (referer: http://www.networksolutions.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:38:35 [scrapy] INFO: Crawled 1208 pages (at 103 pages/min), scraped 1039 items (at 115 items/min)
2015-11-04 06:39:29 [scrapy] INFO: Crawled 1276 pages (at 68 pages/min), scraped 1105 items (at 66 items/min)
2015-11-04 06:39:40 [scrapy] ERROR: Error downloading <GET https://www.rbc.com/newsroom/contact/index.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:10 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/8+Century+Boulevard+,+Shanghai,+China+200121/@31.240545,121.491365,15z?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:16 [scrapy] ERROR: Error downloading <GET http://emergingcapitalmarket.com>: DNS lookup failed: address 'emergingcapitalmarket.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:40:30 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/place/335+Bryant+St,+Palo+Alto,+CA+94301/data=!4m2!3m1!1s0x808fbb3788735f0d:0xde282a76de7c2507>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:30 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/1603+Orrington+Avenue,+Suite+815,+Evanston,+IL+60201/@42.047492,-87.680962,16z?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:30 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/47%2FF+Cheung+Kong+Center,+2+Queen's+Road+Central,+Hong+Kong/@22.27954,114.160548,3285m/data=!3m1!4b1?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:30 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/Maximilianstrasse+11+80539+Muenchen/@51.51126,-0.141943,1207m/data=!3m1!4b1?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:31 [scrapy] INFO: Crawled 1348 pages (at 72 pages/min), scraped 1179 items (at 74 items/min)
2015-11-04 06:40:37 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/place/John+Hancock+Tower/data=!4m2!3m1!1s0x89e37a0cbf6b58ab:0x340545ed96e2a771?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:37 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/2nd+Floor,+Free+Press+House,+Nariman+Point,+Mumbai+400+021,+India/@18.924068,72.82352,15z?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:37 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/590+Madison+Avenue+New+York,+NY+10022/@42.346453,-71.081543,1476m/data=!3m1!4b1?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:40:47 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/Bain+Capital+LTD,+Mayfair+Place,+London,+W1J+8AJ/@51.507006,-0.142887,16z?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:41:33 [scrapy] INFO: Crawled 1434 pages (at 86 pages/min), scraped 1256 items (at 77 items/min)
2015-11-04 06:42:29 [scrapy] INFO: Crawled 1461 pages (at 27 pages/min), scraped 1309 items (at 53 items/min)
2015-11-04 06:42:32 [scrapy] ERROR: Error downloading <GET https://www.mediaplatform.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:42:32 [scrapy] ERROR: Error downloading <GET https://www.fidelissecurity.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 06:43:31 [scrapy] INFO: Crawled 1516 pages (at 55 pages/min), scraped 1364 items (at 55 items/min)
2015-11-04 06:43:31 [scrapy] ERROR: Error downloading <GET http://www.pgamlp.com>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:43:31 [scrapy] ERROR: Error downloading <GET http://www.phi>: DNS lookup failed: address 'www.phi' not found: [Errno -2] Name or service not known.
2015-11-04 06:43:31 [scrapy] ERROR: Error downloading <GET http://www.mad>: DNS lookup failed: address 'www.mad' not found: [Errno -2] Name or service not known.
2015-11-04 06:43:31 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 06:43:32 [scrapy] ERROR: Error downloading <GET http://www.5tides.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:43:54 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 06:43:54 [scrapy] ERROR: Error downloading <GET http://www.esemplia.com>: DNS lookup failed: address 'www.esemplia.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET https://forums.adobe.com/welcome?promoid=KLXMY>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET https://forums.adobe.com/welcome?promoid=KLXNB>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/search/10+Mill+Pond+Lane+Simsbury,+CT+Landmark+Partners/@51.50702,-0.139668,1221m/data=!3m2!1e3!4b1?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/search/265+Franklin+Street,+18th+Floor+Boston,+MA/@42.357902,-71.057307,17z/data=!4m2!2m1!4b1>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/search/681+Fifth+Avenue+New+York,+NY+10022/@40.68489,-74.041469,94836m/data=!3m2!1e3!4b1?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/search/52+Jermyn+Street,+London,+SW1Y+6LX/@51.507634,-0.139496,16z/data=!3m1!1e3?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET http://www.neocera.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:44:18 [scrapy] ERROR: Error downloading <GET http://www.cdnow.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:44:34 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/investor-relations.html?promoid=KLXNF>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:34 [scrapy] ERROR: Error downloading <GET http://www.cart.hostricity.com/whoiscart>: DNS lookup failed: address 'www.cart.hostricity.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:34 [scrapy] ERROR: Error downloading <GET http://www.dis>: DNS lookup failed: address 'www.dis' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:34 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:34 [scrapy] ERROR: Error downloading <GET http://www.rid>: DNS lookup failed: address 'www.rid' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:34 [scrapy] ERROR: Error downloading <GET http://www.coo>: DNS lookup failed: address 'www.coo' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:34 [scrapy] INFO: Crawled 1564 pages (at 48 pages/min), scraped 1404 items (at 40 items/min)
2015-11-04 06:44:35 [scrapy] ERROR: Error downloading <GET https://www.paypal.com/us/cgi-bin/webscr?cmd=_refer-mrb&pal=PVTEFLZB2BY44>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:35 [scrapy] ERROR: Error downloading <GET http://investor.sankaty.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:44:35 [scrapy] ERROR: Error downloading <GET http://www.aboutyou.bwater.com>: DNS lookup failed: address 'www.aboutyou.bwater.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:35 [scrapy] ERROR: Error downloading <GET http://www.837washington.com/>: DNS lookup failed: address 'www.837washington.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:41 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/privacy.html?promoid=KLXNG>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:41 [scrapy] ERROR: Error downloading <GET http://www.aetherip.applicationexperts.com>: DNS lookup failed: address 'www.aetherip.applicationexperts.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:41 [scrapy] ERROR: Error downloading <GET http://www.jrc>: DNS lookup failed: address 'www.jrc' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:41 [scrapy] ERROR: Error downloading <GET http://www.enhancedcapct.com>: DNS lookup failed: address 'www.enhancedcapct.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.rbc.com/community-sustainability/contact-us.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.sankaty.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/buy/students.html?promoid=KLXMM>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/digital-publishing-solution.html?promoid=KLXMT>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/elements-family.html?promoid=KQQSD>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/buy/education.html?promoid=KLXMP>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud.html?promoid=KLXMQ>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:42 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/business/teams.html?promoid=KLXMN>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:44 [scrapy] ERROR: Error downloading <GET http://www.invictafund.com/Home_Page.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 06:44:44 [scrapy] ERROR: Error downloading <GET http://www.invictafund.com/About_Us.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 06:44:44 [scrapy] ERROR: Error downloading <GET http://www.invictafund.com/Resources.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/photography.html?promoid=KLXML>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/online-advertising-management.html?promoid=KOUES>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/enterprise-content-management.html?promoid=KOUER>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/campaign-management.html?promoid=KOUEQ>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/online-marketing-solutions.html?promoid=KRVUY>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/catalog/software._sl_id-contentfilter_sl_catalog_sl_software_sl_mostpopular.html?promoid=KLXMI>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/air/?promoid=KLXMG>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer/?promoid=KLXMF>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/shockwave/?promoid=KLXMH>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:12 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader/?promoid=KLXME>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:16 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/primetime-tv-platform.html?promoid=KOUEV>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:16 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/web-analytics.html?promoid=KOUEP>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:18 [scrapy] ERROR: Spider error processing <GET https://www.woodcreek.com/uploads/view/538e10d0-0138-48f5-96a4-4433ffdd4aa0/?filename=WCCM_Real_Assets_White_Paper.pdf> (referer: https://www.woodcreek.com/article/real-assets-take-on-the-inflation-bogey/1001)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:45:31 [scrapy] INFO: Crawled 1618 pages (at 54 pages/min), scraped 1455 items (at 51 items/min)
2015-11-04 06:45:39 [scrapy] ERROR: Error downloading <GET http://www.cw.com/new/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:45:39 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud.html?promoid=KLXLR>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:45:44 [scrapy] ERROR: Spider error processing <GET https://www.whi.com/documents/1795077> (referer: https://www.whi.com/pages/3062)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:46:27 [scrapy] INFO: Crawled 1619 pages (at 1 pages/min), scraped 1463 items (at 8 items/min)
2015-11-04 06:46:50 [scrapy] ERROR: Error downloading <GET http://www.permalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:46:50 [scrapy] ERROR: Error downloading <GET http://www.seamarkcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:47:27 [scrapy] INFO: Crawled 1619 pages (at 0 pages/min), scraped 1463 items (at 0 items/min)
2015-11-04 06:48:27 [scrapy] INFO: Crawled 1619 pages (at 0 pages/min), scraped 1463 items (at 0 items/min)
2015-11-04 06:48:41 [scrapy] ERROR: Error downloading <GET http://www.precisionderm.com/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:48:41 [scrapy] INFO: Closing spider (finished)
2015-11-04 06:48:41 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 421,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 17,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 4,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 51,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 13,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 325,
 'downloader/request_bytes': 910836,
 'downloader/request_count': 2438,
 'downloader/request_method_count/GET': 2438,
 'downloader/response_bytes': 57827512,
 'downloader/response_count': 2017,
 'downloader/response_status_count/200': 1567,
 'downloader/response_status_count/301': 225,
 'downloader/response_status_count/302': 139,
 'downloader/response_status_count/307': 1,
 'downloader/response_status_count/400': 12,
 'downloader/response_status_count/401': 24,
 'downloader/response_status_count/403': 5,
 'downloader/response_status_count/404': 30,
 'downloader/response_status_count/408': 9,
 'downloader/response_status_count/999': 5,
 'dupefilter/filtered': 6580,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 6, 48, 41, 598017),
 'item_scraped_count': 1463,
 'log_count/CRITICAL': 1,
 'log_count/ERROR': 105,
 'log_count/INFO': 33,
 'offsite/domains': 18,
 'offsite/filtered': 42,
 'request_depth_max': 2,
 'response_received_count': 1619,
 'scheduler/dequeued': 2438,
 'scheduler/dequeued/memory': 2438,
 'scheduler/enqueued': 2438,
 'scheduler/enqueued/memory': 2438,
 'spider_exceptions/AttributeError': 6,
 'spider_exceptions/XMLSyntaxError': 13,
 'start_time': datetime.datetime(2015, 11, 4, 6, 22, 27, 860958)}
2015-11-04 06:48:41 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 06:49:44 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 06:49:44 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 06:49:44 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 06:49:44 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 06:49:44 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 06:49:44 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 06:49:44 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 06:49:44 [scrapy] INFO: Spider opened
2015-11-04 06:49:44 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 06:49:44 [scrapy] ERROR: Error downloading <GET http://www.dai>: DNS lookup failed: address 'www.dai' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:44 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:44 [scrapy] ERROR: Error downloading <GET http://www.sco>: DNS lookup failed: address 'www.sco' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:44 [scrapy] ERROR: Error downloading <GET http://www.citicapitaladvisors.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:49:45 [scrapy] ERROR: Error downloading <GET http://www.coo>: DNS lookup failed: address 'www.coo' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:45 [scrapy] ERROR: Error downloading <GET https://www.magnitudecapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]
2015-11-04 06:49:45 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:45 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:46 [scrapy] ERROR: Error downloading <GET http://www.uni>: DNS lookup failed: address 'www.uni' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:54 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:54 [scrapy] ERROR: Error downloading <GET http://www.meridianfunds.com>: DNS lookup failed: address 'www.meridianfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:54 [scrapy] ERROR: Error downloading <GET http://www.cqs>: DNS lookup failed: address 'www.cqs' not found: [Errno -2] Name or service not known.
2015-11-04 06:49:54 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 06:50:02 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 06:50:16 [scrapy] ERROR: Error downloading <GET http://www.aim13.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 06:50:38 [scrapy] ERROR: Error downloading <GET http://www.lineagecapital.com>: DNS lookup failed: address 'www.lineagecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET http://www.aboutyou.bwater.com>: DNS lookup failed: address 'www.aboutyou.bwater.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:39 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:44 [scrapy] ERROR: Error downloading <GET http://www.wellfieldpartners.com>: DNS lookup failed: address 'www.wellfieldpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:50:57 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:57 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:57 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:57 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:57 [scrapy] INFO: Crawled 217 pages (at 217 pages/min), scraped 107 items (at 107 items/min)
2015-11-04 06:51:38 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:52:28 [scrapy] INFO: Crawled 247 pages (at 30 pages/min), scraped 144 items (at 37 items/min)
2015-11-04 06:52:28 [scrapy] ERROR: Error downloading <GET http://www.car>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:52:28 [scrapy] ERROR: Error downloading <GET http://www.zad>: DNS lookup failed: address 'www.zad' not found: [Errno -2] Name or service not known.
2015-11-04 06:53:10 [scrapy] INFO: Crawled 260 pages (at 13 pages/min), scraped 152 items (at 8 items/min)
2015-11-04 06:53:56 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 06:53:56 [scrapy] INFO: Crawled 275 pages (at 15 pages/min), scraped 163 items (at 11 items/min)
2015-11-04 06:55:02 [scrapy] INFO: Crawled 309 pages (at 34 pages/min), scraped 204 items (at 41 items/min)
2015-11-04 06:55:14 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/922828.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:55:14 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/926211.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:56:49 [scrapy] ERROR: Spider error processing <GET http://www.fosuncapital.com/index.php/team/view/id/13> (referer: http://www.fosuncapital.com/index.php/team)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 06:56:53 [scrapy] INFO: Crawled 314 pages (at 5 pages/min), scraped 206 items (at 2 items/min)
2015-11-04 06:57:31 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/926698.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:57:45 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:57:45 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:57:45 [scrapy] ERROR: Error downloading <GET http://www.permalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:57:45 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:57:45 [scrapy] ERROR: Error downloading <GET http://www.ironsidespartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:57:45 [scrapy] INFO: Crawled 329 pages (at 15 pages/min), scraped 213 items (at 7 items/min)
2015-11-04 06:58:44 [scrapy] INFO: Crawled 329 pages (at 0 pages/min), scraped 220 items (at 7 items/min)
2015-11-04 06:59:04 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/922676.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:59:49 [scrapy] INFO: Crawled 343 pages (at 14 pages/min), scraped 233 items (at 13 items/min)
2015-11-04 07:00:50 [scrapy] INFO: Crawled 352 pages (at 9 pages/min), scraped 241 items (at 8 items/min)
2015-11-04 07:01:43 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:01:43 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 116,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 48,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 15,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 39,
 'downloader/request_bytes': 163831,
 'downloader/request_count': 541,
 'downloader/request_method_count/GET': 541,
 'downloader/response_bytes': 26848977,
 'downloader/response_count': 425,
 'downloader/response_status_count/200': 330,
 'downloader/response_status_count/301': 18,
 'downloader/response_status_count/302': 44,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 4,
 'downloader/response_status_count/403': 4,
 'downloader/response_status_count/404': 22,
 'dupefilter/filtered': 924,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 1, 43, 698151),
 'item_scraped_count': 248,
 'log_count/ERROR': 44,
 'log_count/INFO': 17,
 'offsite/domains': 99,
 'offsite/filtered': 425,
 'request_depth_max': 2,
 'response_received_count': 358,
 'scheduler/dequeued': 541,
 'scheduler/dequeued/memory': 541,
 'scheduler/enqueued': 541,
 'scheduler/enqueued/memory': 541,
 'spider_exceptions/AttributeError': 5,
 'spider_exceptions/timeout': 1,
 'start_time': datetime.datetime(2015, 11, 4, 6, 49, 44, 447540)}
2015-11-04 07:01:43 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:02:46 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:02:46 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:02:46 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:02:46 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:02:46 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:02:46 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:02:46 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:02:46 [scrapy] INFO: Spider opened
2015-11-04 07:02:46 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.enhancedcapct.com>: DNS lookup failed: address 'www.enhancedcapct.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.mountainpacificadvisors.com>: DNS lookup failed: address 'www.mountainpacificadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.woodbinecapital.com>: DNS lookup failed: address 'www.woodbinecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.lan>: DNS lookup failed: address 'www.lan' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:46 [scrapy] ERROR: Error downloading <GET http://www.arg>: DNS lookup failed: address 'www.arg' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.nom>: DNS lookup failed: address 'www.nom' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.dbhedgeworks.cib.db.com>: DNS lookup failed: address 'www.dbhedgeworks.cib.db.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.imc>: DNS lookup failed: address 'www.imc' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.inc>: DNS lookup failed: address 'www.inc' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:47 [scrapy] ERROR: Error downloading <GET http://www.glaxisllc.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:02:48 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:49 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:52 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:54 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:54 [scrapy] ERROR: Error downloading <GET http://www.zca>: DNS lookup failed: address 'www.zca' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:54 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:03:49 [scrapy] INFO: Crawled 159 pages (at 159 pages/min), scraped 83 items (at 83 items/min)
2015-11-04 07:05:02 [scrapy] INFO: Crawled 169 pages (at 10 pages/min), scraped 92 items (at 9 items/min)
2015-11-04 07:06:12 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/926698.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:06:12 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/922828.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:06:12 [scrapy] INFO: Crawled 183 pages (at 14 pages/min), scraped 105 items (at 13 items/min)
2015-11-04 07:07:03 [scrapy] INFO: Crawled 190 pages (at 7 pages/min), scraped 113 items (at 8 items/min)
2015-11-04 07:07:11 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/926211.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:07:30 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/922676.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:07:46 [scrapy] INFO: Crawled 195 pages (at 5 pages/min), scraped 116 items (at 3 items/min)
2015-11-04 07:08:46 [scrapy] INFO: Crawled 195 pages (at 0 pages/min), scraped 116 items (at 0 items/min)
2015-11-04 07:09:38 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:09:38 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:09:38 [scrapy] ERROR: Error downloading <GET http://www.ironsidespartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:09:38 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:09:38 [scrapy] ERROR: Error downloading <GET http://www.seamarkcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:09:38 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:09:38 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:09:38 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 89,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 60,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 18,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 90050,
 'downloader/request_count': 343,
 'downloader/request_method_count/GET': 343,
 'downloader/response_bytes': 24542644,
 'downloader/response_count': 254,
 'downloader/response_status_count/200': 194,
 'downloader/response_status_count/301': 29,
 'downloader/response_status_count/302': 26,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 2,
 'dupefilter/filtered': 224,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 9, 38, 179046),
 'item_scraped_count': 116,
 'log_count/ERROR': 33,
 'log_count/INFO': 13,
 'offsite/domains': 69,
 'offsite/filtered': 490,
 'request_depth_max': 2,
 'response_received_count': 195,
 'scheduler/dequeued': 343,
 'scheduler/dequeued/memory': 343,
 'scheduler/enqueued': 343,
 'scheduler/enqueued/memory': 343,
 'spider_exceptions/AttributeError': 4,
 'start_time': datetime.datetime(2015, 11, 4, 7, 2, 46, 366350)}
2015-11-04 07:09:38 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:10:40 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:10:40 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:10:40 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:10:40 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:10:40 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:10:40 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:10:40 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:10:40 [scrapy] INFO: Spider opened
2015-11-04 07:10:40 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.uni>: DNS lookup failed: address 'www.uni' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.omn>: DNS lookup failed: address 'www.omn' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.arg>: DNS lookup failed: address 'www.arg' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.imc>: DNS lookup failed: address 'www.imc' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.sco>: DNS lookup failed: address 'www.sco' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.phi>: DNS lookup failed: address 'www.phi' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.tia>: DNS lookup failed: address 'www.tia' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:40 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:42 [scrapy] ERROR: Error downloading <GET http://www.lan>: DNS lookup failed: address 'www.lan' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:43 [scrapy] ERROR: Error downloading <GET http://www.par>: DNS lookup failed: address 'www.par' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:44 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:45 [scrapy] ERROR: Error downloading <GET http://www.aetherip.applicationexperts.com>: DNS lookup failed: address 'www.aetherip.applicationexperts.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:46 [scrapy] ERROR: Error downloading <GET http://www.inglesideadvisors.com>: DNS lookup failed: address 'www.inglesideadvisors.com' not found: [Errno -2] Name or service not known.
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 07:10:47 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7fbd242b6aa0>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 07:10:47 [scrapy] ERROR: Error downloading <GET http://www.meridianfunds.com>: DNS lookup failed: address 'www.meridianfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:48 [scrapy] ERROR: Error downloading <GET http://www.adamshillpartners.com>: DNS lookup failed: address 'www.adamshillpartners.com' not found: [Errno -2] Name or service not known.
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 07:11:10 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7fbd243e9758>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 07:11:10 [scrapy] ERROR: Error downloading <GET http://www.mainlineco.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:11:10 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 07:11:11 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7fbd24420e60>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 07:11:11 [scrapy] ERROR: Error downloading <GET http://www.emergingmanagersgroup.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:12:29 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/specialFund/specialFundIndex> (referer: http://www.bosera.com/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 07:12:33 [scrapy] INFO: Crawled 141 pages (at 141 pages/min), scraped 46 items (at 46 items/min)
2015-11-04 07:13:22 [scrapy] INFO: Crawled 142 pages (at 1 pages/min), scraped 58 items (at 12 items/min)
2015-11-04 07:13:58 [scrapy] INFO: Crawled 156 pages (at 14 pages/min), scraped 67 items (at 9 items/min)
2015-11-04 07:14:43 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/common/infoDetail.jsp?classid=0002000200080004&infoid=1415742> (referer: http://www.bosera.com/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 648, in _read_chunked
    value.append(self._safe_read(amt))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 07:14:56 [scrapy] INFO: Crawled 157 pages (at 1 pages/min), scraped 72 items (at 5 items/min)
2015-11-04 07:15:32 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020001&infoid=1607268> (referer: http://www.bosera.com/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 07:15:42 [scrapy] INFO: Crawled 168 pages (at 11 pages/min), scraped 74 items (at 2 items/min)
2015-11-04 07:16:49 [scrapy] INFO: Crawled 187 pages (at 19 pages/min), scraped 96 items (at 22 items/min)
2015-11-04 07:17:49 [scrapy] INFO: Crawled 206 pages (at 19 pages/min), scraped 113 items (at 17 items/min)
2015-11-04 07:18:50 [scrapy] INFO: Crawled 228 pages (at 22 pages/min), scraped 133 items (at 20 items/min)
2015-11-04 07:19:42 [scrapy] INFO: Crawled 246 pages (at 18 pages/min), scraped 148 items (at 15 items/min)
2015-11-04 07:21:20 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/acctAsset/specialFund/mySpecialFundDetail> (referer: http://www.bosera.com/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 07:22:02 [scrapy] INFO: Crawled 246 pages (at 0 pages/min), scraped 159 items (at 11 items/min)
2015-11-04 07:22:35 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/V3/pension/index.jsp> (referer: http://www.bosera.com/column/index.jsp?classid=00020002000600090001)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 07:22:40 [scrapy] INFO: Crawled 257 pages (at 11 pages/min), scraped 161 items (at 2 items/min)
2015-11-04 07:23:07 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228800> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:23:40 [scrapy] INFO: Crawled 265 pages (at 8 pages/min), scraped 177 items (at 16 items/min)
2015-11-04 07:25:01 [scrapy] INFO: Crawled 277 pages (at 12 pages/min), scraped 189 items (at 12 items/min)
2015-11-04 07:25:47 [scrapy] INFO: Crawled 290 pages (at 13 pages/min), scraped 202 items (at 13 items/min)
2015-11-04 07:27:21 [scrapy] INFO: Crawled 309 pages (at 19 pages/min), scraped 217 items (at 15 items/min)
2015-11-04 07:27:41 [scrapy] INFO: Crawled 313 pages (at 4 pages/min), scraped 221 items (at 4 items/min)
2015-11-04 07:29:13 [scrapy] INFO: Crawled 327 pages (at 14 pages/min), scraped 237 items (at 16 items/min)
2015-11-04 07:29:43 [scrapy] INFO: Crawled 342 pages (at 15 pages/min), scraped 246 items (at 9 items/min)
2015-11-04 07:30:38 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/service/jijingoumaiqudao.html> (referer: http://www.bosera.com/service/xiazaizhongxinbiaogexiazai.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 07:30:47 [scrapy] INFO: Crawled 365 pages (at 23 pages/min), scraped 268 items (at 22 items/min)
2015-11-04 07:32:00 [scrapy] INFO: Crawled 396 pages (at 31 pages/min), scraped 295 items (at 27 items/min)
2015-11-04 07:33:36 [scrapy] INFO: Crawled 397 pages (at 1 pages/min), scraped 306 items (at 11 items/min)
2015-11-04 07:33:36 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:33:36 [scrapy] ERROR: Error downloading <GET http://www.madisonint.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:33:42 [scrapy] INFO: Crawled 397 pages (at 0 pages/min), scraped 307 items (at 1 items/min)
2015-11-04 07:35:06 [scrapy] INFO: Crawled 420 pages (at 23 pages/min), scraped 325 items (at 18 items/min)
2015-11-04 07:35:20 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=ICBC>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 07:35:20 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=BOC>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:36:18 [scrapy] INFO: Crawled 431 pages (at 11 pages/min), scraped 335 items (at 10 items/min)
2015-11-04 07:36:52 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=tty>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 07:36:52 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/userFeedback/feedbackForm>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 07:36:52 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050015>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:36:52 [scrapy] INFO: Crawled 432 pages (at 1 pages/min), scraped 337 items (at 2 items/min)
2015-11-04 07:36:58 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=000734>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:37:13 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020003&infoid=1558671> (referer: http://www.bosera.com/aboutus/chuanmeibaodao.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 07:37:17 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228801> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:37:19 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/myFundList>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctAsset/myFund/myFundList took longer than 180.0 seconds..
2015-11-04 07:37:19 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=001055>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 07:37:43 [scrapy] INFO: Crawled 450 pages (at 18 pages/min), scraped 350 items (at 13 items/min)
2015-11-04 07:38:34 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=SPDB>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=SPDB took longer than 180.0 seconds..
2015-11-04 07:38:37 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228798> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:38:49 [scrapy] INFO: Crawled 479 pages (at 29 pages/min), scraped 367 items (at 17 items/min)
2015-11-04 07:39:10 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228799> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:39:49 [scrapy] INFO: Crawled 499 pages (at 20 pages/min), scraped 387 items (at 20 items/min)
2015-11-04 07:40:44 [scrapy] INFO: Crawled 520 pages (at 21 pages/min), scraped 406 items (at 19 items/min)
2015-11-04 07:41:40 [scrapy] INFO: Crawled 531 pages (at 11 pages/min), scraped 422 items (at 16 items/min)
2015-11-04 07:42:40 [scrapy] INFO: Crawled 531 pages (at 0 pages/min), scraped 422 items (at 0 items/min)
2015-11-04 07:43:26 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:43:26 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 163,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 7,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 63,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 6,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 19,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 68,
 'downloader/request_bytes': 259270,
 'downloader/request_count': 788,
 'downloader/request_method_count/GET': 788,
 'downloader/response_bytes': 11409260,
 'downloader/response_count': 625,
 'downloader/response_status_count/200': 506,
 'downloader/response_status_count/301': 22,
 'downloader/response_status_count/302': 46,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 3,
 'downloader/response_status_count/404': 12,
 'downloader/response_status_count/500': 33,
 'dupefilter/filtered': 1692,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 43, 26, 309522),
 'item_scraped_count': 423,
 'log_count/CRITICAL': 3,
 'log_count/ERROR': 45,
 'log_count/INFO': 38,
 'offsite/domains': 74,
 'offsite/filtered': 398,
 'request_depth_max': 2,
 'response_received_count': 532,
 'scheduler/dequeued': 788,
 'scheduler/dequeued/memory': 788,
 'scheduler/enqueued': 788,
 'scheduler/enqueued/memory': 788,
 'spider_exceptions/AttributeError': 4,
 'spider_exceptions/SSLError': 3,
 'spider_exceptions/error': 2,
 'spider_exceptions/timeout': 2,
 'start_time': datetime.datetime(2015, 11, 4, 7, 10, 40, 516502)}
2015-11-04 07:43:26 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:44:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:44:28 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:44:28 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:44:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:44:28 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:44:28 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:44:28 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:44:28 [scrapy] INFO: Spider opened
2015-11-04 07:44:28 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:44:28 [scrapy] ERROR: Error downloading <GET http://www.gim>: DNS lookup failed: address 'www.gim' not found: [Errno -2] Name or service not known.
2015-11-04 07:44:29 [scrapy] ERROR: Error downloading <GET http://www.citicapitaladvisors.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:44:29 [scrapy] ERROR: Error downloading <GET http://www.lan>: DNS lookup failed: address 'www.lan' not found: [Errno -2] Name or service not known.
2015-11-04 07:44:29 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:44:29 [scrapy] ERROR: Error downloading <GET http://www.tit>: DNS lookup failed: address 'www.tit' not found: [Errno -2] Name or service not known.
2015-11-04 07:45:45 [scrapy] INFO: Crawled 177 pages (at 177 pages/min), scraped 107 items (at 107 items/min)
2015-11-04 07:46:40 [scrapy] INFO: Crawled 245 pages (at 68 pages/min), scraped 181 items (at 74 items/min)
2015-11-04 07:47:30 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/BMOcommunity>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:47:30 [scrapy] ERROR: Error downloading <GET https://www.bmo.com/home/about/banking/privacy-security/subscription-centre>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:47:30 [scrapy] INFO: Crawled 302 pages (at 57 pages/min), scraped 228 items (at 47 items/min)
2015-11-04 07:48:47 [scrapy] INFO: Crawled 373 pages (at 71 pages/min), scraped 300 items (at 72 items/min)
2015-11-04 07:48:55 [scrapy] ERROR: Spider error processing <GET http://www.atlantictrust.com/documents/10184/10605/2012YouYourWealthAndYourManager.pdf/9376c65f-4e6e-4c2d-8dc7-61037cf60194> (referer: http://www.atlantictrust.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 07:49:53 [scrapy] INFO: Crawled 446 pages (at 73 pages/min), scraped 364 items (at 64 items/min)
2015-11-04 07:49:54 [scrapy] ERROR: Spider error processing <GET http://www.atlantictrust.com/documents/10184/0/Luxury+Institute+ranking/92097af9-400e-4b33-a533-4e499eb37e35> (referer: http://www.atlantictrust.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 07:50:34 [scrapy] INFO: Crawled 498 pages (at 52 pages/min), scraped 399 items (at 35 items/min)
2015-11-04 07:50:52 [scrapy] ERROR: Spider error processing <GET http://www.invescomortgagecapital.com/Cache/1001201618.PDF?Y=&O=PDF&D=&fid=1001201618&T=&iid=4209142> (referer: http://www.invescomortgagecapital.com/govlanding.aspx?iid=4209142)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 07:51:00 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/PowerSharesETFs>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:51:00 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/InvescoCanada>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:51:00 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/InvescoUS>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:51:33 [scrapy] INFO: Crawled 547 pages (at 49 pages/min), scraped 461 items (at 62 items/min)
2015-11-04 07:52:01 [scrapy] ERROR: Error downloading <GET https://twitter.com/InvescoInsights>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:52:14 [scrapy] ERROR: Spider error processing <GET http://www.hsequity.com/viewdocument.aspx?DocumentKey=50> (referer: http://hsrealtyco.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 07:52:34 [scrapy] INFO: Crawled 591 pages (at 44 pages/min), scraped 508 items (at 47 items/min)
2015-11-04 07:52:58 [scrapy] ERROR: Error downloading <GET https://www.caesars.com/corporate/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:53:52 [scrapy] INFO: Crawled 660 pages (at 69 pages/min), scraped 573 items (at 65 items/min)
2015-11-04 07:54:33 [scrapy] INFO: Crawled 685 pages (at 25 pages/min), scraped 603 items (at 30 items/min)
2015-11-04 07:55:32 [scrapy] INFO: Crawled 782 pages (at 97 pages/min), scraped 673 items (at 70 items/min)
2015-11-04 07:56:31 [scrapy] INFO: Crawled 825 pages (at 43 pages/min), scraped 730 items (at 57 items/min)
2015-11-04 07:57:32 [scrapy] INFO: Crawled 896 pages (at 71 pages/min), scraped 801 items (at 71 items/min)
2015-11-04 07:58:24 [scrapy] ERROR: Spider error processing <GET http://apps2.shareholder.com/tearsheet/generate.aspx?CompanyID=AMDA-UYH8V&id=271639&s=963712180> (referer: http://ir.carlyle.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 07:58:41 [scrapy] INFO: Crawled 980 pages (at 84 pages/min), scraped 871 items (at 70 items/min)
2015-11-04 08:00:14 [scrapy] INFO: Crawled 1022 pages (at 42 pages/min), scraped 922 items (at 51 items/min)
2015-11-04 08:00:58 [scrapy] INFO: Crawled 1037 pages (at 15 pages/min), scraped 940 items (at 18 items/min)
2015-11-04 08:01:29 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:01:40 [scrapy] INFO: Crawled 1059 pages (at 22 pages/min), scraped 954 items (at 14 items/min)
2015-11-04 08:02:35 [scrapy] ERROR: Error downloading <GET https://support.microsoft.com/kb/2536204>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:02:35 [scrapy] ERROR: Error downloading <GET https://twitter.com/home?status=Currently+reading+http%3A%2F%2Fir.carlyle.com%2Findex.cfm%3F>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:02:35 [scrapy] INFO: Crawled 1093 pages (at 34 pages/min), scraped 975 items (at 21 items/min)
2015-11-04 08:03:30 [scrapy] INFO: Crawled 1138 pages (at 45 pages/min), scraped 1023 items (at 48 items/min)
2015-11-04 08:04:21 [scrapy] ERROR: Error downloading <GET https://twitter.com/share?text=Browse+Happy%3A+Online.+Worry-free.+Upgrade+your+browser+today%21&url=http%3A%2F%2Fbrowsehappy.com%2F>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:04:43 [scrapy] INFO: Crawled 1223 pages (at 85 pages/min), scraped 1110 items (at 87 items/min)
2015-11-04 08:05:39 [scrapy] INFO: Crawled 1268 pages (at 45 pages/min), scraped 1160 items (at 50 items/min)
2015-11-04 08:05:55 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/onecarlyle>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:06:44 [scrapy] INFO: Crawled 1309 pages (at 41 pages/min), scraped 1200 items (at 40 items/min)
2015-11-04 08:07:05 [scrapy] ERROR: Error downloading <GET https://subscribe.ft.com/barrier/logic?location=http%3A%2F%2Fwww.ft.com%2Fcms%2Fs%2F0%2F054b820c-fe31-11e0-bac4-00144feabdc0.html%3Fsiteedition%3Duk&referer=http%3A%2F%2Fwww.carlyle.com%2Fabout-carlyle%2Filluminate-commentary%2F52&classification=conditional_standard>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:05 [scrapy] ERROR: Error downloading <GET https://workforcenow.adp.com/jobs/apply/posting.html?ccId=19000101_000001&client=optimas&lang=en_US&type=MP>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/contact-us/general-industries-education/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/contact-us/consumer-group/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/contact-us/business-services/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/?page_id=1436>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/contact-us/investor-relations/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/contact-us/senior-analysts/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/contact-us/associates/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.gryphon-inv.com/contact-us/senior-associates/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:07:11 [scrapy] ERROR: Error downloading <GET http://www.polunin.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 15:42:13 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM twice, forcing unclean shutdown
