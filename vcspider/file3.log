usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
2015-11-04 00:30:55 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 00:30:55 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 00:30:55 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 00:30:55 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 00:30:55 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 00:30:55 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 00:30:56 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 00:30:56 [scrapy] INFO: Spider opened
2015-11-04 00:30:56 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 00:31:00 [scrapy] ERROR: Error downloading <GET http://www.bresadvisors.com>: DNS lookup failed: address 'www.bresadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 00:31:45 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/about.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/l2015-11-04 00:31:48 [scrapy] INFO: Crawled 235 pages (at 235 pages/min), scraped 126 items (at 126 items/min)
2015-11-04 00:32:41 [scrapy] INFO: Crawled 242 pages (at 7 pages/min), scraped 156 items (at 30 items/min)
2015-11-04 00:33:55 [scrapy] INFO: Crawled 304 pages (at 62 pages/min), scraped 209 items (at 53 items/min)
2015-11-04 00:34:00 [scrapy] ERROR: Error downloading <GET https://cag.elliottadvisors.hk/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:35:08 [scrapy] INFO: Crawled 305 pages (at 1 pages/min), scraped 229 items (at 20 items/min)
2015-11-04 00:35:44 [scrapy] INFO: Crawled 322 pages (at 17 pages/min), scraped 237 items (at 8 items/min)
2015-11-04 00:36:56 [scrapy] INFO: Crawled 326 pages (at 4 pages/min), scraped 250 items (at 13 items/min)
2015-11-04 00:37:29 [scrapy] INFO: Crawled 343 pages (at 17 pages/min), scraped 254 items (at 4 items/min)
2015-11-04 00:38:58 [scrapy] INFO: Crawled 343 pages (at 0 pages/min), scraped 267 items (at 13 items/min)
2015-11-04 00:40:28 [scrapy] INFO: Crawled 358 pages (at 15 pages/min), scraped 281 items (at 14 items/min)
2015-11-04 00:41:37 [scrapy] INFO: Crawled 381 pages (at 23 pages/min), scraped 294 items (at 13 items/min)
2015-11-04 00:42:36 [scrapy] INFO: Crawled 381 pages (at 0 pages/min), scraped 305 items (at 11 items/min)
2015-11-04 00:44:19 [scrapy] INFO: Crawled 400 pages (at 19 pages/min), scraped 323 items (at 18 items/min)
2015-11-04 00:44:28 [scrapy] INFO: Crawled 425 pages (at 25 pages/min), scraped 331 items (at 8 items/min)
2015-11-04 00:45:42 [scrapy] INFO: Crawled 434 pages (at 9 pages/min), scraped 353 items (at 22 items/min)
2015-11-04 00:47:02 [scrapy] INFO: Crawled 472 pages (at 38 pages/min), scraped 374 items (at 21 items/min)
2015-11-04 00:47:35 [scrapy] INFO: Crawled 473 pages (at 1 pages/min), scraped 386 items (at 12 items/min)
2015-11-04 00:48:32 [scrapy] INFO: Crawled 519 pages (at 46 pages/min), scraped 405 items (at 19 items/min)
2015-11-04 00:49:39 [scrapy] INFO: Crawled 543 pages (at 24 pages/min), scraped 429 items (at 24 items/min)
2015-11-04 00:51:09 [scrapy] INFO: Crawled 559 pages (at 16 pages/min), scraped 472 items (at 43 items/min)
2015-11-04 00:52:28 [scrapy] INFO: Crawled 604 pages (at 45 pages/min), scraped 503 items (at 31 items/min)
2015-11-04 00:53:37 [scrapy] INFO: Crawled 609 pages (at 5 pages/min), scraped 531 items (at 28 items/min)
2015-11-04 00:54:55 [scrapy] INFO: Crawled 681 pages (at 72 pages/min), scraped 585 items (at 54 items/min)
2015-11-04 00:55:42 [scrapy] INFO: Crawled 693 pages (at 12 pages/min), scraped 603 items (at 18 items/min)
2015-11-04 00:56:01 [scrapy] ERROR: Spider error processing <GET http://www.serengeti-am.com/1039353.pdf> (referer: http://www.serengeti-am.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 00:56:55 [scrapy] INFO: Crawled 775 pages (at 82 pages/min), scraped 677 items (at 74 items/min)
2015-11-04 00:57:05 [scrapy] ERROR: Spider error processing <GET http://www.lonestarfunds.com/> (referer: http://www.lonestarfunds.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 00:57:46 [scrapy] INFO: Crawled 880 pages (at 105 pages/min), scraped 767 items (at 90 items/min)
2015-11-04 00:58:32 [scrapy] INFO: Crawled 973 pages (at 93 pages/min), scraped 847 items (at 80 items/min)
2015-11-04 00:59:42 [scrapy] INFO: Crawled 1037 pages (at 64 pages/min), scraped 931 items (at 84 items/min)
2015-11-04 01:00:26 [scrapy] INFO: Crawled 1113 pages (at 76 pages/min), scraped 1005 items (at 74 items/min)
2015-11-04 01:01:44 [scrapy] INFO: Crawled 1194 pages (at 81 pages/min), scraped 1096 items (at 91 items/min)
2015-11-04 01:02:37 [scrapy] INFO: Crawled 1269 pages (at 75 pages/min), scraped 1174 items (at 78 items/min)
2015-11-04 01:03:31 [scrapy] INFO: Crawled 1365 pages (at 96 pages/min), scraped 1268 items (at 94 items/min)
2015-11-04 01:05:05 [scrapy] INFO: Crawled 1507 pages (at 142 pages/min), scraped 1402 items (at 134 items/min)
2015-11-04 01:05:26 [scrapy] INFO: Crawled 1546 pages (at 39 pages/min), scraped 1437 items (at 35 items/min)
2015-11-04 01:06:29 [scrapy] INFO: Crawled 1606 pages (at 60 pages/min), scraped 1496 items (at 59 items/min)
2015-11-04 01:07:51 [scrapy] INFO: Crawled 1673 pages (at 67 pages/min), scraped 1568 items (at 72 items/min)
2015-11-04 01:08:29 [scrapy] INFO: Crawled 1693 pages (at 20 pages/min), scraped 1593 items (at 25 items/min)
2015-11-04 01:09:40 [scrapy] INFO: Crawled 1727 pages (at 34 pages/min), scraped 1623 items (at 30 items/min)
2015-11-04 01:11:11 [scrapy] INFO: Crawled 1776 pages (at 49 pages/min), scraped 1661 items (at 38 items/min)
2015-11-04 01:12:14 [scrapy] INFO: Crawled 1776 pages (at 0 pages/min), scraped 1676 items (at 15 items/min)
2015-11-04 01:12:24 [scrapy] ERROR: Error downloading <GET http://www.fun>: DNS lookup failed: address 'www.fun' not found: [Errno -2] Name or service not known.
2015-11-04 01:12:24 [scrapy] ERROR: Error downloading <GET http://www.5tides.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 01:12:25 [scrapy] INFO: Crawled 1784 pages (at 8 pages/min), scraped 1687 items (at 11 items/min)
2015-11-04 01:13:53 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/contact/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:13:53 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/terms-and-conditions/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:13:53 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/news/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:13:53 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/team/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:13:53 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/investments/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:13:53 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/about-us/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:13:53 [scrapy] INFO: Crawled 1840 pages (at 56 pages/min), scraped 1728 items (at 41 items/min)
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.esemplia.com>: DNS lookup failed: address 'www.esemplia.com' not found: [Errno -2] Name or service not known.
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.turnbridgecapital.com/our-team/matthew-reckling/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.turnbridgecapital.com/our-portfolio/recent-investments/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.turnbridgecapital.com/our-team/todd-m-tomlin/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.turnbridgecapital.com/our-team/j-kent-sweezey/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.saronafund.com/about-us/careers/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.saronafund.com/contact/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.saronafund.com/data-rooms/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.saronafund.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.siocapital.com/investment_style>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.siocapital.com/management>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.siocapital.com/competitive_advantage>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.siocapital.com/fund_login>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.siocapital.com/healthcare_sector>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.siocapital.com/about>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.heronbridge.com/investment.htm>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.heronbridge.com/disclaimer.htm>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.heronbridge.com/about_us.htm>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.heronbridge.com/contact.htm>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:20 [scrapy] ERROR: Error downloading <GET http://www.rid>: DNS lookup failed: address 'www.rid' not found: [Errno -2] Name or service not known.
2015-11-04 01:14:21 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/peter-o-wilde>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:21 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/jamie-r-smith>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:21 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/michael-song>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:21 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/jerry-wong>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:14:25 [scrapy] INFO: Crawled 1887 pages (at 47 pages/min), scraped 1760 items (at 32 items/min)
2015-11-04 01:15:23 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 01:16:24 [scrapy] INFO: Crawled 1935 pages (at 48 pages/min), scraped 1804 items (at 44 items/min)
2015-11-04 01:16:43 [scrapy] INFO: Crawled 1938 pages (at 3 pages/min), scraped 1817 items (at 13 items/min)
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/aaron-w-fine>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/henrik-c-a-fridlund>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/brent-s-buckley>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/jason-s-brupbacher>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/team/stephen-r-byers>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/portfolio/survey-sampling-ssi>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/portfolio/tele1-europe-song-networks>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:16:43 [scrapy] ERROR: Error downloading <GET http://www.provequity.com/portfolio/tdc>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:18:09 [scrapy] INFO: Crawled 2037 pages (at 99 pages/min), scraped 1889 items (at 72 items/min)
2015-11-04 01:18:39 [scrapy] INFO: Crawled 2062 pages (at 25 pages/min), scraped 1915 items (at 26 items/min)
2015-11-04 01:19:26 [scrapy] ERROR: Spider error processing <GET http://josephgunnar.com/wp-content/uploads/2010/09/MEGA_KH5.rtf> (referer: http://josephgunnar.com/legal-regulatory-information/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:19:26 [scrapy] INFO: Crawled 2124 pages (at 62 pages/min), scraped 1976 items (at 61 items/min)
2015-11-04 01:19:26 [scrapy] ERROR: Spider error processing <GET http://josephgunnar.com/wp-content/uploads/2010/09/SEC-Rule-606-Quarterly-Report-for-the-Quarter-Ending-September-30-2010.rtf> (referer: http://josephgunnar.com/legal-regulatory-information/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:20:54 [scrapy] INFO: Crawled 2172 pages (at 48 pages/min), scraped 2018 items (at 42 items/min)
2015-11-04 01:21:39 [scrapy] INFO: Crawled 2199 pages (at 27 pages/min), scraped 2036 items (at 18 items/min)
2015-11-04 01:22:25 [scrapy] INFO: Crawled 2203 pages (at 4 pages/min), scraped 2055 items (at 19 items/min)
2015-11-04 01:22:55 [scrapy] ERROR: Spider error processing <GET http://josephgunnar.com/wp-content/uploads/2010/09/MEGA_KH5-Sept-2011.rtf> (referer: http://josephgunnar.com/legal-regulatory-information/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:22:56 [scrapy] ERROR: Spider error processing <GET http://josephgunnar.com/wp-content/uploads/2013/10/MEGA_KH5.rtf> (referer: http://josephgunnar.com/legal-regulatory-information/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:23:01 [scrapy] ERROR: Spider error processing <GET http://josephgunnar.com/wp-content/uploads/2010/09/SEC-Rule-606-Quarterly-Report-for-the-Quarter-Ending-June-30-2012.rtf> (referer: http://josephgunnar.com/legal-regulatory-information/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:23:02 [scrapy] ERROR: Spider error processing <GET http://josephgunnar.com/wp-content/uploads/2012/10/SEC-Rule-606-Quarterly-Report-for-the-Quarter-Ending-September-30-2012.rtf> (referer: http://josephgunnar.com/legal-regulatory-information/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:23:31 [scrapy] INFO: Crawled 2289 pages (at 86 pages/min), scraped 2126 items (at 71 items/min)
2015-11-04 01:25:01 [scrapy] INFO: Crawled 2387 pages (at 98 pages/min), scraped 2217 items (at 91 items/min)
2015-11-04 01:25:28 [scrapy] INFO: Crawled 2428 pages (at 41 pages/min), scraped 2252 items (at 35 items/min)
2015-11-04 01:26:27 [scrapy] INFO: Crawled 2501 pages (at 73 pages/min), scraped 2342 items (at 90 items/min)
2015-11-04 01:27:30 [scrapy] INFO: Crawled 2572 pages (at 71 pages/min), scraped 2422 items (at 80 items/min)
2015-11-04 01:27:58 [scrapy] INFO: Closing spider (finished)
2015-11-04 01:27:58 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 425,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 18,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 401,
 'downloader/request_bytes': 1214095,
 'downloader/request_count': 3224,
 'downloader/request_method_count/GET': 3224,
 'downloader/response_bytes': 40695841,
 'downloader/response_count': 2799,
 'downloader/response_status_count/200': 2548,
 'downloader/response_status_count/301': 157,
 'downloader/response_status_count/302': 25,
 'downloader/response_status_count/400': 24,
 'downloader/response_status_count/401': 6,
 'downloader/response_status_count/404': 36,
 'downloader/response_status_count/408': 1,
 'downloader/response_status_count/504': 2,
 'dupefilter/filtered': 19402,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 1, 27, 58, 248671),
 'item_scraped_count': 2449,
 'log_count/ERROR': 54,
 'log_count/INFO': 62,
 'offsite/domains': 471,
 'offsite/filtered': 2062,
 'request_depth_max': 2,
 'response_received_count': 2599,
 'scheduler/dequeued': 3224,
 'scheduler/dequeued/memory': 3224,
 'scheduler/enqueued': 3224,
 'scheduler/enqueued/memory': 3224,
 'spider_exceptions/AttributeError': 6,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 0, 30, 25, 345939)}
2015-11-04 01:27:58 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 01:29:00 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 01:29:00 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 01:29:00 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 01:29:00 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 01:29:00 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 01:29:00 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 01:29:00 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 01:29:00 [scrapy] INFO: Spider opened
2015-11-04 01:29:00 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 01:30:19 [scrapy] INFO: Crawled 254 pages (at 254 pages/min), scraped 152 items (at 152 items/min)
2015-11-04 01:32:27 [scrapy] INFO: Crawled 342 pages (at 88 pages/min), scraped 222 items (at 70 items/min)
2015-11-04 01:32:43 [scrapy] ERROR: Spider error processing <GET https://www.oldfieldpartners.com/files/file/view/id/1187> (referer: https://www.oldfieldpartners.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/si2015-11-04 01:32:59 [scrapy] INFO: Crawled 3174 pages (at 80 pages/min), scraped 3071 items (at 80 items/min)
2015-11-04 01:34:02 [scrapy] INFO: Crawled 3222 pages (at 48 pages/min), scraped 3119 items (at 48 items/min)
2015-11-04 01:34:59 [scrapy] INFO: Crawled 3274 pages (at 52 pages/min), scraped 3173 items (at 54 items/min)
2015-11-04 01:35:38 [scrapy] ERROR: Spider error processing <GET http://guestofaguest.com/fashion/wardrobe-inspiration-for-an-endless-summer-from-carmella-by-katheryn-rice&slide=15> (referer: http://guestofaguest.com/fashion/wardrobe-inspiration-for-an-endless-summer-from-carmella-by-katheryn-rice)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 01:35:39 [scrapy] ERROR: Spider error processing <GET http://guestofaguest.com/fashion/wardrobe-inspiration-for-an-endless-summer-from-carmella-by-katheryn-rice&slide=14> (referer: http://guestofaguest.com/fashion/wardrobe-inspiration-for-an-endless-summer-from-carmella-by-katheryn-rice)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 01:35:40 [scrapy] ERROR: Spider error processing <GET http://guestofaguest.com/fashion/wardrobe-inspiration-for-an-endless-summer-from-carmella-by-katheryn-rice&slide=13> (referer: http://guestofaguest.com/fashion/wardrobe-inspiration-for-an-endless-summer-from-carmella-by-katheryn-rice)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.2015-11-04 01:35:41 [scrapy] INFO: Crawled 412 pages (at 55 pages/min), scraped 300 items (at 40 items/min)
2015-11-04 01:36:32 [scrapy] INFO: Crawled 413 pages (at 1 pages/min), scraped 322 items (at 22 items/min)
2015-11-04 01:37:21 [scrapy] INFO: Crawled 446 pages (at 33 pages/min), scraped 346 items (at 24 items/min)
2015-11-04 01:38:21 [scrapy] INFO: Crawled 479 pages (at 33 pages/min), scraped 383 items (at 37 items/min)
2015-11-04 01:39:00 [scrapy] INFO: Crawled 514 pages (at 35 pages/min), scraped 416 items (at 33 items/min)
2015-11-04 01:40:03 [scrapy] INFO: Crawled 567 pages (at 53 pages/min), scraped 463 items (at 47 items/min)
2015-11-04 01:41:27 [scrapy] INFO: Crawled 608 pages (at 41 pages/min), scraped 507 items (at 44 items/min)
2015-11-04 01:42:01 [scrapy] INFO: Crawled 618 pages (at 10 pages/min), scraped 521 items (at 14 items/min)
2015-11-04 01:43:10 [scrapy] INFO: Crawled 648 pages (at 30 pages/min), scraped 558 items (at 37 items/min)
2015-11-04 01:43:38 [scrapy] ERROR: Error downloading <GET mailto:internetsecurity@ubs.com>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 40, in download_request
    raise NotSupported("Unsupported URL scheme '%s': %s" % (scheme, msg))
NotSupported: Unsupported URL scheme 'mailto': no handler available for that scheme
2015-11-04 01:43:49 [scrapy] ERROR: Spider error processing <GET https://www.ubs.com/content/dam/static/wmamericas/statement_of_financial_condition.pdf> (referer: https://www.ubs.com/us/en/wealth/research/your-wealth-and-life.html?campID=INTERNAL-HPPROMOTEASER-us_house_view-en)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:44:02 [scrapy] INFO: Crawled 712 pages (at 64 pages/min), scraped 601 items (at 43 items/min)
2015-11-04 01:45:12 [scrapy] INFO: Crawled 737 pages (at 25 pages/min), scraped 646 items (at 45 items/min)
2015-11-04 01:46:01 [scrapy] INFO: Crawled 769 pages (at 32 pages/min), scraped 667 items (at 21 items/min)
2015-11-04 01:47:02 [scrapy] INFO: Crawled 800 pages (at 31 pages/min), scraped 696 items (at 29 items/min)
2015-11-04 01:47:34 [scrapy] ERROR: Error downloading <GET mailto:cr@ubs.com>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 45, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/__init__.py", line 40, in download_request
    raise NotSupported("Unsupported URL scheme '%s': %s" % (scheme, msg))
NotSupported: Unsupported URL scheme 'mailto': no handler available for that scheme
2015-11-04 01:48:01 [scrapy] INFO: Crawled 840 pages (at 40 pages/min), scraped 741 items (at 45 items/min)
2015-11-04 01:49:04 [scrapy] INFO: Crawled 866 pages (at 26 pages/min), scraped 771 items (at 30 items/min)
2015-11-04 01:50:16 [scrapy] INFO: Crawled 910 pages (at 44 pages/min), scraped 819 items (at 48 items/min)
2015-11-04 01:50:37 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/fulletnlist> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:50:43 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/MLPS/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:50:50 [scrapy] ERROR: Spider error processing <GET https://www.ubs.com/content/dam/WealthManagementAmericas/documents/statement-of-financial-condition-PuertoRico.pdf> (referer: https://www.ubs.com/us/en/wealth/research/your-wealth-and-life.html?campID=INTERNAL-HPPROMOTEASER-us_house_view-en)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:50:59 [scrapy] ERROR: Spider error processing <GET https://www.ubs.com/content/dam/static/wmamericas/bestexecution.pdf> (referer: https://www.ubs.com/us/en/wealth/research/your-wealth-and-life.html?campID=INTERNAL-HPPROMOTEASER-us_house_view-en)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:51:00 [scrapy] ERROR: Spider error processing <GET https://www.ubs.com/content/dam/static/wmamericas/annual_loan_disclosure.pdf> (referer: https://www.ubs.com/us/en/wealth/research/your-wealth-and-life.html?campID=INTERNAL-HPPROMOTEASER-us_house_view-en)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:51:05 [scrapy] INFO: Crawled 945 pages (at 35 pages/min), scraped 844 items (at 25 items/min)
2015-11-04 01:52:03 [scrapy] INFO: Crawled 990 pages (at 45 pages/min), scraped 879 items (at 35 items/min)
2015-11-04 01:53:04 [scrapy] INFO: Crawled 1011 pages (at 21 pages/min), scraped 910 items (at 31 items/min)
2015-11-04 01:54:10 [scrapy] INFO: Crawled 1031 pages (at 20 pages/min), scraped 935 items (at 25 items/min)
2015-11-04 01:54:48 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/UBC/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:54:50 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/UBM/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:54:56 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/UBG/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:54:57 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/USV/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:54:59 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/SPGH/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:55:00 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/FUD/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:55:13 [scrapy] ERROR: Spider error processing <GET http://etracs.ubs.com/docs/ussymbol/UAG/prospectus-supplement> (referer: https://www.ubs.com/global/en/about_ubs/about_us/news/news.html/en/2015/10/08/ubs-releases-statement-on-ubs-etracs-etns-and-launches-six-new-products.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=respon2015-11-04 01:56:01 [scrapy] INFO: Crawled 3919 pages (at 42 pages/min), scraped 3795 items (at 36 items/min)
2015-11-04 01:56:57 [scrapy] ERROR: Error downloading <GET https://modernteacher.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 01:56:57 [scrapy] INFO: Crawled 3987 pages (at 68 pages/min), scraped 3852 items (at 57 items/min)
2015-11-04 01:57:50 [scrapy] ERROR: Error downloading <GET https://www.c7.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert protocol version'), ('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 01:57:50 [scrapy] ERROR: Error downloading <GET https://clusterhq.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:58:54 [scrapy] INFO: Crawled 4016 pages (at 29 pages/min), scraped 3889 items (at 37 items/min)
2015-11-04 01:59:43 [PIL.ImageFile] ERROR: %s
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/ImageFile.py", line 100, in __init__
    self._open()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/JpegImagePlugin.py", line 308, in _open
    i = i8(s)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/_binary.py", line 18, in i8
    return ord(c)
TypeError: ord() expected a character, but string of length 0 found
2015-11-04 01:59:43 [PIL.ImageFile] ERROR: %s
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/ImageFile.py", line 100, in __init__
    self._open()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/JpegImagePlugin.py", line 308, in _open
    i = i8(s)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/PIL/_binary.py", line 18, in i8
    return ord(c)
TypeError: ord() expected a character, but string of length 0 found
2015-11-04 02:00:13 [scrapy] INFO: Crawled 4032 pages (at 16 pages/min), scraped 3905 items (at 16 items/min)
2015-11-04 02:00:43 [scrapy] ERROR: Error downloading <GET https://www.plumbee.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 02:01:35 [scrapy] INFO: Crawled 4078 pages (at 46 pages/min), scraped 3936 items (at 31 items/min)
2015-11-04 02:02:10 [scrapy] INFO: Crawled 4122 pages (at 44 pages/min), scraped 3966 items (at 30 items/min)
2015-11-04 02:03:07 [scrapy] INFO: Crawled 4144 pages (at 22 pages/min), scraped 4003 items (at 37 items/min)
2015-11-04 02:04:14 [scrapy] INFO: Crawled 4194 pages (at 50 pages/min), scraped 4064 items (at 61 items/min)
2015-11-04 02:05:32 [scrapy] INFO: Crawled 4194 pages (at 0 pages/min), scraped 4082 items (at 18 items/min)
2015-11-04 02:07:04 [scrapy] INFO: Crawled 4266 pages (at 72 pages/min), scraped 4140 items (at 58 items/min)
2015-11-04 02:08:41 [scrapy] INFO: Crawled 4301 pages (at 35 pages/min), scraped 4178 items (at 38 items/min)
2015-11-04 02:09:05 [scrapy] INFO: Crawled 4310 pages (at 9 pages/min), scraped 4189 items (at 11 items/min)
2015-11-04 02:10:27 [scrapy] INFO: Crawled 4357 pages (at 47 pages/min), scraped 4239 items (at 50 items/min)
2015-11-04 02:11:18 [scrapy] INFO: Crawled 4401 pages (at 44 pages/min), scraped 4275 items (at 36 items/min)
2015-11-04 02:12:27 [scrapy] INFO: Crawled 4421 pages (at 20 pages/min), scraped 4303 items (at 28 items/min)
2015-11-04 02:12:58 [scrapy] INFO: Crawled 4468 pages (at 47 pages/min), scraped 4336 items (at 33 items/min)
2015-11-04 02:14:09 [scrapy] INFO: Crawled 4498 pages (at 30 pages/min), scraped 4376 items (at 40 items/min)
2015-11-04 02:15:32 [scrapy] INFO: Crawled 4532 pages (at 34 pages/min), scraped 4394 items (at 18 items/min)
2015-11-04 02:16:25 [scrapy] INFO: Crawled 4537 pages (at 5 pages/min), scraped 4414 items (at 20 items/min)
2015-11-04 02:17:06 [scrapy] INFO: Crawled 4551 pages (at 14 pages/min), scraped 4429 items (at 15 items/min)
2015-11-04 02:18:05 [scrapy] INFO: Crawled 4576 pages (at 25 pages/min), scraped 4452 items (at 23 items/min)
2015-11-04 02:19:02 [scrapy] INFO: Crawled 4648 pages (at 72 pages/min), scraped 4505 items (at 53 items/min)
2015-11-04 02:19:59 [scrapy] INFO: Crawled 4688 pages (at 40 pages/min), scraped 4562 items (at 57 items/min)
2015-11-04 02:21:02 [scrapy] INFO: Crawled 4783 pages (at 95 pages/min), scraped 4644 items (at 82 items/min)
2015-11-04 02:22:01 [scrapy] INFO: Crawled 4838 pages (at 55 pages/min), scraped 4711 items (at 67 items/min)
2015-11-04 02:23:00 [scrapy] INFO: Crawled 4903 pages (at 65 pages/min), scraped 4780 items (at 69 items/min)
2015-11-04 02:24:00 [scrapy] INFO: Crawled 4971 pages (at 68 pages/min), scraped 4843 items (at 63 items/min)
2015-11-04 02:24:56 [scrapy] INFO: Crawled 5043 pages (at 72 pages/min), scraped 4919 items (at 76 items/min)
2015-11-04 02:26:01 [scrapy] INFO: Crawled 5134 pages (at 91 pages/min), scraped 5002 items (at 83 items/min)
2015-11-04 02:26:59 [scrapy] INFO: Crawled 5181 pages (at 47 pages/min), scraped 5059 items (at 57 items/min)
2015-11-04 02:28:06 [scrapy] INFO: Crawled 5258 pages (at 77 pages/min), scraped 5132 items (at 73 items/min)
2015-11-04 02:28:59 [scrapy] INFO: Crawled 5328 pages (at 70 pages/min), scraped 5200 items (at 68 items/min)
2015-11-04 02:30:02 [scrapy] INFO: Crawled 5389 pages (at 61 pages/min), scraped 5260 items (at 60 items/min)
2015-11-04 02:30:57 [scrapy] INFO: Crawled 5452 pages (at 63 pages/min), scraped 5326 items (at 66 items/min)
2015-11-04 02:31:56 [scrapy] INFO: Crawled 5520 pages (at 68 pages/min), scraped 5394 items (at 68 items/min)
2015-11-04 02:32:56 [scrapy] INFO: Crawled 5590 pages (at 70 pages/min), scraped 5463 items (at 69 items/min)
2015-11-04 02:34:01 [scrapy] INFO: Crawled 5701 pages (at 111 pages/min), scraped 5563 items (at 100 items/min)
2015-11-04 02:35:04 [scrapy] INFO: Crawled 5816 pages (at 115 pages/min), scraped 5655 items (at 92 items/min)
2015-11-04 02:36:36 [scrapy] INFO: Crawled 5857 pages (at 41 pages/min), scraped 5698 items (at 43 items/min)
2015-11-04 02:37:21 [scrapy] INFO: Crawled 5859 pages (at 2 pages/min), scraped 5726 items (at 28 items/min)
2015-11-04 02:37:57 [scrapy] INFO: Crawled 5904 pages (at 45 pages/min), scraped 5762 items (at 36 items/min)
2015-11-04 02:39:09 [scrapy] INFO: Crawled 5987 pages (at 83 pages/min), scraped 5834 items (at 72 items/min)
2015-11-04 02:39:55 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/login/?returnUri=http%253A%252F%252Fwww.meetup.com%252FNOVA-Brain-Computing%252Fabuse%252F>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 02:39:57 [scrapy] INFO: Crawled 6003 pages (at 16 pages/min), scraped 5872 items (at 38 items/min)
2015-11-04 02:40:57 [scrapy] INFO: Crawled 6068 pages (at 65 pages/min), scraped 5930 items (at 58 items/min)
2015-11-04 02:41:56 [scrapy] INFO: Crawled 6128 pages (at 60 pages/min), scraped 5995 items (at 65 items/min)
2015-11-04 02:43:03 [scrapy] INFO: Crawled 6227 pages (at 99 pages/min), scraped 6081 items (at 86 items/min)
2015-11-04 02:43:58 [scrapy] INFO: Crawled 6281 pages (at 54 pages/min), scraped 6141 items (at 60 items/min)
2015-11-04 02:44:57 [scrapy] INFO: Crawled 6322 pages (at 41 pages/min), scraped 6189 items (at 48 items/min)
2015-11-04 02:45:56 [scrapy] INFO: Crawled 6391 pages (at 69 pages/min), scraped 6260 items (at 71 items/min)
2015-11-04 02:46:59 [scrapy] INFO: Crawled 6473 pages (at 82 pages/min), scraped 6341 items (at 81 items/min)
2015-11-04 02:48:02 [scrapy] INFO: Crawled 6557 pages (at 84 pages/min), scraped 6408 items (at 67 items/min)
2015-11-04 02:48:56 [scrapy] INFO: Crawled 6568 pages (at 11 pages/min), scraped 6436 items (at 28 items/min)
2015-11-04 02:49:59 [scrapy] INFO: Crawled 6648 pages (at 80 pages/min), scraped 6507 items (at 71 items/min)
2015-11-04 02:50:56 [scrapy] INFO: Crawled 6719 pages (at 71 pages/min), scraped 6573 items (at 66 items/min)
2015-11-04 02:51:56 [scrapy] INFO: Crawled 6771 pages (at 52 pages/min), scraped 6637 items (at 64 items/min)
2015-11-04 02:53:05 [scrapy] INFO: Crawled 6860 pages (at 89 pages/min), scraped 6725 items (at 88 items/min)
2015-11-04 02:53:57 [scrapy] INFO: Crawled 6949 pages (at 89 pages/min), scraped 6794 items (at 69 items/min)
2015-11-04 02:54:57 [scrapy] INFO: Crawled 7026 pages (at 77 pages/min), scraped 6878 items (at 84 items/min)
2015-11-04 02:55:58 [scrapy] INFO: Crawled 7104 pages (at 78 pages/min), scraped 6963 items (at 85 items/min)
2015-11-04 02:57:08 [scrapy] INFO: Crawled 7203 pages (at 99 pages/min), scraped 7050 items (at 87 items/min)
2015-11-04 02:57:58 [scrapy] INFO: Crawled 7254 pages (at 51 pages/min), scraped 7110 items (at 60 items/min)
2015-11-04 02:59:02 [scrapy] INFO: Crawled 7346 pages (at 92 pages/min), scraped 7197 items (at 87 items/min)
2015-11-04 02:59:57 [scrapy] INFO: Crawled 7420 pages (at 74 pages/min), scraped 7278 items (at 81 items/min)
2015-11-04 03:00:51 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_AdministeringAccessAssuranceSuite.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:00:53 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_CourionEducation-Overview.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:00:56 [scrapy] INFO: Crawled 7499 pages (at 79 pages/min), scraped 7352 items (at 74 items/min)
2015-11-04 03:00:56 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_ARM_11-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:00:58 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_Professional_Services_10-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:00:58 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_PasswordCourier_11-15-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:00:58 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_Strategic_IAM_Workshop.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:01:02 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_AASuite_11-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:01:06 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_Methodology-R2D2.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:01:10 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_AccountCourier_11-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:01:11 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_ComplianceCourier_11-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:01:20 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/GovernanceBrochure_102013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:01:44 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_RoleCourier_11-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:01:48 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_AccessRiskAssessment_0414.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:02:01 [scrapy] INFO: Crawled 7617 pages (at 118 pages/min), scraped 7457 items (at 105 items/min)
2015-11-04 03:02:15 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_Provisioning_12-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:02:39 [scrapy] ERROR: Spider error processing <GET http://www.courion.com/images/ResourceCenter/Datasheets/Datasheet_AccessInsight_11-2013.pdf> (referer: http://www.courion.com/resources/datasheets.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:03:00 [scrapy] INFO: Crawled 7730 pages (at 113 pages/min), scraped 7559 items (at 102 items/min)
2015-11-04 03:04:00 [scrapy] INFO: Crawled 7816 pages (at 86 pages/min), scraped 7641 items (at 82 items/min)
2015-11-04 03:05:04 [scrapy] INFO: Crawled 7862 pages (at 46 pages/min), scraped 7694 items (at 53 items/min)
2015-11-04 03:06:08 [scrapy] INFO: Crawled 7923 pages (at 61 pages/min), scraped 7759 items (at 65 items/min)
2015-11-04 03:06:58 [scrapy] INFO: Crawled 7971 pages (at 48 pages/min), scraped 7804 items (at 45 items/min)
2015-11-04 03:08:07 [scrapy] INFO: Crawled 8007 pages (at 36 pages/min), scraped 7850 items (at 46 items/min)
2015-11-04 03:08:59 [scrapy] INFO: Crawled 8046 pages (at 39 pages/min), scraped 7888 items (at 38 items/min)
2015-11-04 03:09:57 [scrapy] INFO: Crawled 8118 pages (at 72 pages/min), scraped 7956 items (at 68 items/min)
2015-11-04 03:10:56 [scrapy] INFO: Crawled 8186 pages (at 68 pages/min), scraped 8022 items (at 66 items/min)
2015-11-04 03:11:56 [scrapy] INFO: Crawled 8250 pages (at 64 pages/min), scraped 8088 items (at 66 items/min)
2015-11-04 03:12:59 [scrapy] INFO: Crawled 8307 pages (at 57 pages/min), scraped 8140 items (at 52 items/min)
2015-11-04 03:13:58 [scrapy] INFO: Crawled 8378 pages (at 71 pages/min), scraped 8215 items (at 75 items/min)
2015-11-04 03:15:03 [scrapy] INFO: Crawled 8442 pages (at 64 pages/min), scraped 8280 items (at 65 items/min)
2015-11-04 03:15:59 [scrapy] INFO: Crawled 8510 pages (at 68 pages/min), scraped 8348 items (at 68 items/min)
2015-11-04 03:16:56 [scrapy] INFO: Crawled 8584 pages (at 74 pages/min), scraped 8419 items (at 71 items/min)
2015-11-04 03:18:02 [scrapy] INFO: Crawled 8655 pages (at 71 pages/min), scraped 8492 items (at 73 items/min)
2015-11-04 03:18:58 [scrapy] INFO: Crawled 8732 pages (at 77 pages/min), scraped 8561 items (at 69 items/min)
2015-11-04 03:19:58 [scrapy] INFO: Crawled 8784 pages (at 52 pages/min), scraped 8621 items (at 60 items/min)
2015-11-04 03:20:59 [scrapy] INFO: Crawled 8861 pages (at 77 pages/min), scraped 8692 items (at 71 items/min)
2015-11-04 03:21:58 [scrapy] INFO: Crawled 8939 pages (at 78 pages/min), scraped 8777 items (at 85 items/min)
2015-11-04 03:22:56 [scrapy] INFO: Crawled 8998 pages (at 59 pages/min), scraped 8828 items (at 51 items/min)
2015-11-04 03:23:58 [scrapy] INFO: Crawled 9063 pages (at 65 pages/min), scraped 8905 items (at 77 items/min)
2015-11-04 03:25:00 [scrapy] INFO: Crawled 9144 pages (at 81 pages/min), scraped 8977 items (at 72 items/min)
2015-11-04 03:25:23 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/mshare.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:24 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/digital-video-and-photos.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:24 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/custom-dvd.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:26 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/videotapes.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:26 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/how-it-works.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:29 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/film-reels.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:29 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/photos-and-slides.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:29 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/privacy.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:39 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/testimonials.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:39 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:39 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/hallmark.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:39 [scrapy] ERROR: Spider error processing <GET http://www.yesvideo.com/press-in-the-news.html> (referer: http://www.yesvideo.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:25:57 [scrapy] INFO: Crawled 9230 pages (at 86 pages/min), scraped 9048 items (at 71 items/min)
2015-11-04 03:27:09 [scrapy] INFO: Crawled 9314 pages (at 84 pages/min), scraped 9127 items (at 79 items/min)
2015-11-04 03:28:01 [scrapy] INFO: Crawled 9348 pages (at 34 pages/min), scraped 9170 items (at 43 items/min)
2015-11-04 03:29:19 [scrapy] INFO: Crawled 9399 pages (at 51 pages/min), scraped 9216 items (at 46 items/min)
2015-11-04 03:29:59 [scrapy] INFO: Crawled 9426 pages (at 27 pages/min), scraped 9240 items (at 24 items/min)
2015-11-04 03:30:59 [scrapy] INFO: Crawled 9469 pages (at 43 pages/min), scraped 9283 items (at 43 items/min)
2015-11-04 03:32:12 [scrapy] INFO: Crawled 9542 pages (at 73 pages/min), scraped 9345 items (at 62 items/min)
2015-11-04 03:32:32 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/register/?urlkey=&number=&eventId=226255832&c=18573322&returnUri=http%3A%2F%2Fwww.meetup.com%2FThe-Borg-Diplomatic-Corps%2Fevents%2F226255832%2F>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:32:32 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/login/?returnUri=http%253A%252F%252Fwww.meetup.com%252FNorthern-Virginia-Arabic-Lanaguage-Meetup-Group%252Fabuse%252F>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:33:11 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/register/?urlkey=&eventId=&a=&number=&gj=&c=4641072&newReg=1&returnUri=http%3A%2F%2Fwww.meetup.com%2FNorthern-Virginia-Arabic-Lanaguage-Meetup-Group%2Fjoin%2F%3FeventId%3D%26a%3D%26gj%3D%26newReg%3D1>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:33:12 [scrapy] INFO: Crawled 9590 pages (at 48 pages/min), scraped 9402 items (at 57 items/min)
2015-11-04 03:33:45 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/login/?returnUri=http%253A%252F%252Fwww.meetup.com%252FThe-Borg-Diplomatic-Corps%252Fabuse%252F>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:33:46 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/register/?urlkey=&eventId=&a=&number=&gj=&c=18573322&newReg=1&returnUri=http%3A%2F%2Fwww.meetup.com%2FThe-Borg-Diplomatic-Corps%2Fjoin%2F%3FeventId%3D%26a%3D%26gj%3D%26newReg%3D1>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:34:00 [scrapy] INFO: Crawled 9646 pages (at 56 pages/min), scraped 9435 items (at 33 items/min)
2015-11-04 03:34:18 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/register/?urlkey=&eventId=&a=&number=&gj=&c=1498974&newReg=1&returnUri=http%3A%2F%2Fwww.meetup.com%2FAshburn-Chess-Club%2Fjoin%2F%3FeventId%3D%26a%3D%26gj%3D%26newReg%3D1>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:35:21 [scrapy] INFO: Crawled 9689 pages (at 43 pages/min), scraped 9492 items (at 57 items/min)
2015-11-04 03:35:44 [scrapy] ERROR: Error downloading <GET https://secure.meetup.com/login/?returnUri=http%253A%252F%252Fwww.meetup.com%252FAshburn-Chess-Club%252Fabuse%252F>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:36:16 [scrapy] INFO: Crawled 9702 pages (at 13 pages/min), scraped 9518 items (at 26 items/min)
2015-11-04 03:37:24 [scrapy] INFO: Crawled 9728 pages (at 26 pages/min), scraped 9548 items (at 30 items/min)
2015-11-04 03:38:01 [scrapy] INFO: Crawled 9752 pages (at 24 pages/min), scraped 9568 items (at 20 items/min)
2015-11-04 03:39:03 [scrapy] INFO: Crawled 9776 pages (at 24 pages/min), scraped 9595 items (at 27 items/min)
2015-11-04 03:40:06 [scrapy] INFO: Crawled 9810 pages (at 34 pages/min), scraped 9632 items (at 37 items/min)
2015-11-04 03:40:59 [scrapy] INFO: Crawled 9835 pages (at 25 pages/min), scraped 9653 items (at 21 items/min)
2015-11-04 03:41:58 [scrapy] INFO: Crawled 9861 pages (at 26 pages/min), scraped 9676 items (at 23 items/min)
2015-11-04 03:42:58 [scrapy] INFO: Crawled 9884 pages (at 23 pages/min), scraped 9700 items (at 24 items/min)
2015-11-04 03:43:58 [scrapy] INFO: Crawled 9912 pages (at 28 pages/min), scraped 9730 items (at 30 items/min)
2015-11-04 03:44:05 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/funds/~/media/JumpStartInc/Images/Entrepreneur%20Programs/JumpStart%20Investment%20Terms.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/funds/ourfunds.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:06 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/funds/~/media/JumpStartInc/Images/Entrepreneur%20Programs/AdvisoryBoardBasics.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/funds/ourfunds.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:07 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/~/media/JumpStartInc/Images/Entrepreneur%20Programs/2011-UPDATE-orange-with_label-WEB-650w.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/apply.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:10 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/services/~/media/JumpStartInc/Images/Entrepreneur%20Programs/Collegiate-Ecosystem-Poster-feb2012.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/services/studententrepreneurs.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:13 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/~/media/B2F3AA9CBC1246B9B9CF28656996A9F7.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/toolkit.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:27 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/~/media/JumpStartInc/Images/Entrepreneur%20Programs/JumpStart-ExecutiveSummaryTemplate.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/toolkit.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:29 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/~/media/92F39498F59A4D389B7CA690E234248F.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/toolkit.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:32 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/~/media/DC08F0788AD649EFB633AF25B6D24BB8.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/toolkit.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:44:39 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/entrepreneursupport/~/media/JumpStartInc/Images/Entrepreneur%20Programs/protecting-your-position.ashx> (referer: http://www.jumpstartinc.org/entrepreneursupport/toolkit.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:45:04 [scrapy] INFO: Crawled 9932 pages (at 20 pages/min), scraped 9743 items (at 13 items/min)
2015-11-04 03:45:59 [scrapy] INFO: Crawled 9942 pages (at 10 pages/min), scraped 9753 items (at 10 items/min)
2015-11-04 03:47:03 [scrapy] INFO: Crawled 9956 pages (at 14 pages/min), scraped 9764 items (at 11 items/min)
2015-11-04 03:48:11 [scrapy] INFO: Crawled 9974 pages (at 18 pages/min), scraped 9784 items (at 20 items/min)
2015-11-04 03:49:18 [scrapy] INFO: Crawled 9985 pages (at 11 pages/min), scraped 9794 items (at 10 items/min)
2015-11-04 03:50:09 [scrapy] INFO: Crawled 9996 pages (at 11 pages/min), scraped 9807 items (at 13 items/min)
2015-11-04 03:51:02 [scrapy] INFO: Crawled 10017 pages (at 21 pages/min), scraped 9826 items (at 19 items/min)
2015-11-04 03:52:09 [scrapy] INFO: Crawled 10035 pages (at 18 pages/min), scraped 9844 items (at 18 items/min)
2015-11-04 03:53:06 [scrapy] INFO: Crawled 10045 pages (at 10 pages/min), scraped 9853 items (at 9 items/min)
2015-11-04 03:53:56 [scrapy] INFO: Crawled 10057 pages (at 12 pages/min), scraped 9862 items (at 9 items/min)
2015-11-04 03:55:12 [scrapy] INFO: Crawled 10071 pages (at 14 pages/min), scraped 9876 items (at 14 items/min)
2015-11-04 03:56:02 [scrapy] INFO: Crawled 10079 pages (at 8 pages/min), scraped 9886 items (at 10 items/min)
2015-11-04 03:57:05 [scrapy] INFO: Crawled 10103 pages (at 24 pages/min), scraped 9906 items (at 20 items/min)
2015-11-04 03:58:04 [scrapy] INFO: Crawled 10121 pages (at 18 pages/min), scraped 9928 items (at 22 items/min)
2015-11-04 03:58:58 [scrapy] INFO: Crawled 10125 pages (at 4 pages/min), scraped 9938 items (at 10 items/min)
2015-11-04 03:59:51 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/~/media/JumpStartInc/Images/Companies/cleantech-for-website.ashx> (referer: http://www.jumpstartinc.org/companies/portfolio.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 03:59:57 [scrapy] INFO: Crawled 10137 pages (at 12 pages/min), scraped 9946 items (at 8 items/min)
2015-11-04 04:00:08 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/~/media/JumpStartInc/Images/Companies/healthcare-aug2012-web.ashx> (referer: http://www.jumpstartinc.org/companies/portfolio.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:00:20 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/~/media/JumpStartInc/Images/Companies/IT-NEO_web_aug12.ashx> (referer: http://www.jumpstartinc.org/companies/portfolio.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:01:05 [scrapy] INFO: Crawled 10153 pages (at 16 pages/min), scraped 9957 items (at 11 items/min)
2015-11-04 04:01:42 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/~/media/JumpStartInc/Images/Companies/JS-portfolio_complete_july1.ashx> (referer: http://www.jumpstartinc.org/companies/portfolio.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:01:57 [scrapy] INFO: Crawled 10163 pages (at 10 pages/min), scraped 9971 items (at 14 items/min)
2015-11-04 04:03:00 [scrapy] INFO: Crawled 10192 pages (at 29 pages/min), scraped 9995 items (at 24 items/min)
2015-11-04 04:04:03 [scrapy] INFO: Crawled 10213 pages (at 21 pages/min), scraped 10019 items (at 24 items/min)
2015-11-04 04:04:58 [scrapy] INFO: Crawled 10233 pages (at 20 pages/min), scraped 10036 items (at 17 items/min)
2015-11-04 04:05:57 [scrapy] INFO: Crawled 10247 pages (at 14 pages/min), scraped 10049 items (at 13 items/min)
2015-11-04 04:06:27 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/~/media/JumpStartInc/Images/Results-Page/Midwest-Health-Care-Venture-Report-12.ashx> (referer: http://www.jumpstartinc.org/companies/vc-reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:06:31 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/dl/~/media/JumpStartInc/Images/Results-Page/2009VCReport.ashx> (referer: http://www.jumpstartinc.org/companies/vc-reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:06:34 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/sitecore/shell/Controls/Rich%20Text%20Editor/~/media/JumpStartInc/Images/Results-Page/impact_feb1_web.ashx> (referer: http://www.jumpstartinc.org/results/impact.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:06:42 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/~/media/JumpStartInc/Images/Results-Page/2012-VC_web.ashx> (referer: http://www.jumpstartinc.org/companies/vc-reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:06:48 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/~/media/JumpStartInc/Images/Results-Page/0711_VC_overview.ashx> (referer: http://www.jumpstartinc.org/companies/vc-reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:06:51 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/dl/~/media/JumpStartInc/Images/Results-Page/2011-NVCA-VentureImpactReport.ashx> (referer: http://www.jumpstartinc.org/companies/vc-reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:06:58 [scrapy] INFO: Crawled 10276 pages (at 29 pages/min), scraped 10078 items (at 29 items/min)
2015-11-04 04:07:01 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/results/dl/~/media/JumpStartInc/Images/Results-Page/2009VCReport.ashx> (referer: http://www.jumpstartinc.org/results/reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:07:07 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/results/~/media/JumpStartInc/Images/Results-Page/Midwest-Health-Care-Venture-Report-12.ashx> (referer: http://www.jumpstartinc.org/results/reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 04:07:18 [scrapy] ERROR: Spider error processing <GET http://www.jumpstartinc.org/companies/dl/~/media/JumpStartInc/Images/Results-Page/2010VCReport.ashx> (referer: http://www.jumpstartinc.org/companies/vc-reports.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/ho2015-11-04 04:08:01 [scrapy] ERROR: Error downloading <GET https://orchardplatform.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:08:08 [scrapy] INFO: Crawled 299 pages (at 60 pages/min), scraped 186 items (at 45 items/min)
2015-11-04 04:09:09 [scrapy] INFO: Crawled 360 pages (at 61 pages/min), scraped 223 items (at 37 items/min)
2015-11-04 04:10:22 [scrapy] INFO: Crawled 378 pages (at 18 pages/min), scraped 268 items (at 45 items/min)
2015-11-04 04:10:56 [scrapy] INFO: Crawled 389 pages (at 11 pages/min), scraped 278 items (at 10 items/min)
2015-11-04 04:11:48 [scrapy] INFO: Crawled 501 pages (at 112 pages/min), scraped 366 items (at 88 items/min)
2015-11-04 04:12:47 [scrapy] INFO: Crawled 594 pages (at 93 pages/min), scraped 486 items (at 120 items/min)
2015-11-04 04:13:47 [scrapy] INFO: Crawled 768 pages (at 174 pages/min), scraped 656 items (at 170 items/min)
2015-11-04 04:14:45 [scrapy] INFO: Crawled 900 pages (at 132 pages/min), scraped 771 items (at 115 items/min)
2015-11-04 04:15:45 [scrapy] INFO: Crawled 1004 pages (at 104 pages/min), scraped 867 items (at 96 items/min)
2015-11-04 04:16:50 [scrapy] INFO: Crawled 1203 pages (at 199 pages/min), scraped 975 items (at 108 items/min)
2015-11-04 04:17:47 [scrapy] INFO: Crawled 1324 pages (at 121 pages/min), scraped 1069 items (at 94 items/min)
2015-11-04 04:19:11 [scrapy] INFO: Crawled 1372 pages (at 48 pages/min), scraped 1150 items (at 81 items/min)
2015-11-04 04:19:15 [scrapy] ERROR: Spider error processing <GET http://www.eclectica-am.com/fund-prospectus-eagf/publications/585-eclectica-asset-management-april-2015-manager-commentary/file> (referer: http://www.eclectica-am.com/publications)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:19:23 [scrapy] ERROR: Spider error processing <GET http://www.eclectica-am.com/fund-prospectus-eagf/publications/602-eam-commentary-1505/file> (referer: http://www.eclectica-am.com/publications)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:19:30 [scrapy] ERROR: Spider error processing <GET http://www.eclectica-am.com/fund-prospectus-eagf/publications/611-eclectica-commentary-july-2015/file> (referer: http://www.eclectica-am.com/publications)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:19:32 [scrapy] ERROR: Spider error processing <GET http://www.eclectica-am.com/fund-prospectus-eagf/publications/604-commentary-june-2015/file> (referer: http://www.eclectica-am.com/publications)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:19:52 [scrapy] INFO: Crawled 1424 pages (at 52 pages/min), scraped 1200 items (at 50 items/min)
2015-11-04 04:19:56 [scrapy] ERROR: Spider error processing <GET http://www.eclectica-am.com/fund-prospectus-eagf/publications/612-eclectica-commentary-august-2015/file> (referer: http://www.eclectica-am.com/publications)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:19:58 [scrapy] ERROR: Spider error processing <GET http://www.eclectica-am.com/fund-prospectus-eagf/publications/620-eclectica-commentary-september-2015-1/file> (referer: http://www.eclectica-am.com/publications)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:20:03 [scrapy] ERROR: Error downloading <GET http://www.ccm>: DNS lookup failed: address 'www.ccm' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:03 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:03 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:03 [scrapy] ERROR: Error downloading <GET http://www.sco>: DNS lookup failed: address 'www.sco' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:03 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://www.northleafcapital.com/terms-use>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://www.northleafcapital.com/history-key-milestones>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://747capital.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://www.woodbinecapital.com>: DNS lookup failed: address 'www.woodbinecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://www.vnc>: DNS lookup failed: address 'www.vnc' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:06 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:20:08 [scrapy] ERROR: Error downloading <GET http://www.decuragroup.com>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 04:20:19 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 04:20:31 [scrapy] ERROR: Error downloading <GET https://www.bayviewassetmanagement.com/fund-management>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:20:31 [scrapy] ERROR: Error downloading <GET https://www.bayviewassetmanagement.com/loan-servicing>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:20:31 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 04:20:31 [scrapy] ERROR: Error downloading <GET https://www.bayviewassetmanagement.com/careers>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:20:31 [scrapy] ERROR: Error downloading <GET https://www.bayviewassetmanagement.com/contact-us>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:20:31 [scrapy] ERROR: Error downloading <GET https://www.bayviewassetmanagement.com/advisory-services>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:20:57 [scrapy] INFO: Crawled 1506 pages (at 82 pages/min), scraped 1273 items (at 73 items/min)
2015-11-04 04:21:44 [scrapy] INFO: Crawled 1558 pages (at 52 pages/min), scraped 1341 items (at 68 items/min)
2015-11-04 04:22:44 [scrapy] INFO: Crawled 1558 pages (at 0 pages/min), scraped 1341 items (at 0 items/min)
2015-11-04 04:23:44 [scrapy] INFO: Crawled 1558 pages (at 0 pages/min), scraped 1341 items (at 0 items/min)
2015-11-04 04:23:54 [scrapy] ERROR: Error downloading <GET http://www.altacomm.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 04:23:54 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 04:23:54 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 04:23:54 [scrapy] ERROR: Error downloading <GET http://www.emffp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 04:23:54 [scrapy] INFO: Closing spider (finished)
2015-11-04 04:23:54 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 152,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 10,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 36,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 12,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 91,
 'downloader/request_bytes': 635937,
 'downloader/request_count': 1893,
 'downloader/request_method_count/GET': 1893,
 'downloader/response_bytes': 20637117,
 'downloader/response_count': 1741,
 'downloader/response_status_count/200': 1423,
 'downloader/response_status_count/301': 104,
 'downloader/response_status_count/302': 70,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/400': 9,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 5,
 'downloader/response_status_count/404': 128,
 'dupefilter/filtered': 12255,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 4, 23, 54, 751289),
 'item_scraped_count': 1341,
 'log_count/ERROR': 35,
 'log_count/INFO': 25,
 'offsite/domains': 308,
 'offsite/filtered': 1629,
 'request_depth_max': 2,
 'response_received_count': 1558,
 'scheduler/dequeued': 1893,
 'scheduler/dequeued/memory': 1893,
 'scheduler/enqueued': 1893,
 'scheduler/enqueued/memory': 1893,
 'spider_exceptions/AttributeError': 6,
 'start_time': datetime.datetime(2015, 11, 4, 4, 5, 44, 712863)}
2015-11-04 04:23:54 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 04:24:57 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 04:24:57 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 04:24:57 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 04:24:57 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 04:24:57 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 04:24:57 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 04:24:57 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 04:24:57 [scrapy] INFO: Spider opened
2015-11-04 04:24:57 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 04:24:57 [scrapy] ERROR: Error downloading <GET http://www.formulainvesting.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 04:24:57 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 04:24:57 [scrapy] ERROR: Error downloading <GET http://www.key>: DNS lookup failed: address 'www.key' not found: [Errno -2] Name or service not known.
2015-11-04 04:24:59 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:24:59 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 04:24:59 [scrapy] ERROR: Error downloading <GET http://www.santanderasset.com>: DNS lookup failed: address 'www.santanderasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:24:59 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:00 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:00 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:00 [scrapy] ERROR: Error downloading <GET http://www.visicap.com>: DNS lookup failed: address 'www.visicap.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:00 [scrapy] ERROR: Error downloading <GET http://www.acc>: DNS lookup failed: address 'www.acc' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:00 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:03 [scrapy] ERROR: Error downloading <GET http://www.cqs>: DNS lookup failed: address 'www.cqs' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:03 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:03 [scrapy] ERROR: Error downloading <GET http://www.lp.lcpartners.com>: DNS lookup failed: address 'www.lp.lcpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:25:03 [scrapy] ERROR: Error downloading <GET http://www.beckerdrapkin.com>: DNS lookup failed: address 'www.beckerdrapkin.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:26:16 [scrapy] INFO: Crawled 204 pages (at 204 pages/min), scraped 105 items (at 105 items/min)
2015-11-04 04:27:01 [scrapy] INFO: Crawled 235 pages (at 31 pages/min), scraped 139 items (at 34 items/min)
2015-11-04 04:28:14 [scrapy] ERROR: Error downloading <GET https://www.aspectcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:28:14 [scrapy] INFO: Crawled 252 pages (at 17 pages/min), scraped 151 items (at 12 items/min)
2015-11-04 04:30:23 [scrapy] INFO: Crawled 253 pages (at 1 pages/min), scraped 169 items (at 18 items/min)
2015-11-04 04:31:16 [scrapy] INFO: Crawled 278 pages (at 25 pages/min), scraped 194 items (at 25 items/min)
2015-11-04 04:32:14 [scrapy] INFO: Crawled 303 pages (at 25 pages/min), scraped 219 items (at 25 items/min)
2015-11-04 04:33:06 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228798> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:33:08 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228799> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:33:22 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228801> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:33:22 [scrapy] INFO: Crawled 323 pages (at 20 pages/min), scraped 236 items (at 17 items/min)
2015-11-04 04:33:31 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228800> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:33:57 [scrapy] INFO: Crawled 345 pages (at 22 pages/min), scraped 249 items (at 13 items/min)
2015-11-04 04:34:57 [scrapy] INFO: Crawled 369 pages (at 24 pages/min), scraped 272 items (at 23 items/min)
2015-11-04 04:35:58 [scrapy] INFO: Crawled 385 pages (at 16 pages/min), scraped 285 items (at 13 items/min)
2015-11-04 04:37:08 [scrapy] INFO: Crawled 417 pages (at 32 pages/min), scraped 313 items (at 28 items/min)
2015-11-04 04:38:01 [scrapy] INFO: Crawled 417 pages (at 0 pages/min), scraped 329 items (at 16 items/min)
2015-11-04 04:39:18 [scrapy] INFO: Crawled 432 pages (at 15 pages/min), scraped 343 items (at 14 items/min)
2015-11-04 04:39:59 [scrapy] INFO: Crawled 453 pages (at 21 pages/min), scraped 358 items (at 15 items/min)
2015-11-04 04:41:01 [scrapy] INFO: Crawled 468 pages (at 15 pages/min), scraped 373 items (at 15 items/min)
2015-11-04 04:42:13 [scrapy] INFO: Crawled 479 pages (at 11 pages/min), scraped 381 items (at 8 items/min)
2015-11-04 04:43:01 [scrapy] INFO: Crawled 500 pages (at 21 pages/min), scraped 393 items (at 12 items/min)
2015-11-04 04:44:07 [scrapy] INFO: Crawled 529 pages (at 29 pages/min), scraped 426 items (at 33 items/min)
2015-11-04 04:45:11 [scrapy] INFO: Crawled 534 pages (at 5 pages/min), scraped 436 items (at 10 items/min)
2015-11-04 04:45:43 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=67e097fa-1a18-4d66-8197-64dff1c24583> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:46:20 [scrapy] INFO: Crawled 570 pages (at 36 pages/min), scraped 470 items (at 34 items/min)
2015-11-04 04:46:24 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=35f5f15a-af98-481a-b871-08a73855664b> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:46:29 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=2fcb50c7-2197-4e6a-a423-b77aa5ed2b28> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:46:29 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=b775d3c0-ef41-4046-80e7-229bd714c966> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:46:29 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=e1ddbe2b-42e2-4ced-8897-17b3f96a37b6> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:46:31 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=088fcf64-e894-46ca-a21e-03e41bdefe73> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:46:32 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=c12c83d1-6644-4e73-b933-674140d35e3b> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:47:13 [scrapy] INFO: Crawled 631 pages (at 61 pages/min), scraped 514 items (at 44 items/min)
2015-11-04 04:48:17 [scrapy] INFO: Crawled 670 pages (at 39 pages/min), scraped 564 items (at 50 items/min)
2015-11-04 04:49:12 [scrapy] INFO: Crawled 691 pages (at 21 pages/min), scraped 584 items (at 20 items/min)
2015-11-04 04:49:38 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index.jsp>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/index.jsp took longer than 180.0 seconds..
2015-11-04 04:50:02 [scrapy] INFO: Crawled 746 pages (at 55 pages/min), scraped 624 items (at 40 items/min)
2015-11-04 04:50:16 [scrapy] ERROR: Spider error processing <GET http://admin.rockbridgecapital.com/CMSPages/GetFile.aspx?nodeguid=3196bdfc-d777-4c55-983d-2d606a1d13e8> (referer: http://www.rockbridgecapital.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 04:50:59 [scrapy] INFO: Crawled 834 pages (at 88 pages/min), scraped 705 items (at 81 items/min)
2015-11-04 04:52:36 [scrapy] INFO: Crawled 952 pages (at 118 pages/min), scraped 822 items (at 117 items/min)
2015-11-04 04:53:26 [scrapy] INFO: Crawled 954 pages (at 2 pages/min), scraped 832 items (at 10 items/min)
2015-11-04 04:54:20 [scrapy] INFO: Crawled 1010 pages (at 56 pages/min), scraped 867 items (at 35 items/min)
2015-11-04 04:55:39 [scrapy] INFO: Crawled 1021 pages (at 11 pages/min), scraped 898 items (at 31 items/min)
2015-11-04 04:56:04 [scrapy] INFO: Crawled 1059 pages (at 38 pages/min), scraped 910 items (at 12 items/min)
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://www.taconiccap.com/terms-of-use/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://www.taconiccap.com/privacy-policy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://investors.taconiccap.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://www.taconiccap.com/team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://www.taconiccap.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://www.taconiccap.com/contact/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://www.taconiccap.com/careers/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:56:08 [scrapy] ERROR: Error downloading <GET https://www.taconiccap.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:17 [scrapy] INFO: Crawled 1075 pages (at 16 pages/min), scraped 944 items (at 34 items/min)
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET http://amherststage.amherst.com/about/leadership/>: DNS lookup failed: address 'amherststage.amherst.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:57:17 [scrapy] ERROR: Error downloading <GET http://amherststage.amherst.com/our-thinking/newsroom/>: DNS lookup failed: address 'amherststage.amherst.com' not found: [Errno -2] Name or service not known.
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET https://www.landmarkpartners.com/sunday-james.php>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET https://www.landmarkpartners.com/shanfield-robert.php>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/in-the-news/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/silverfern-advisory-board/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/our-resources/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/our-companies/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/what-makes-us-different/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/social-responsibility/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/private-equity/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://www.silfern.com/philosophy/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET http://capital.bosera.com/capital/index.jsp>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:57:36 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:58:40 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/juggernaut-capital-partners-invests-in-catapult-learning-inc/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:40 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/witt-associates-and-obriens-response-management-to-combine/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:40 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/voss-water-receives-18-million-in-growth-funding-from-new-investors/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:40 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/ddp-holdings-announces-new-investment-from-juggernaut-capital-partners/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:41 [scrapy] INFO: Crawled 1136 pages (at 61 pages/min), scraped 996 items (at 52 items/min)
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/scheduleBuy/scheduleBuyList>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/myFundList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1109_050029.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050026.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050018.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_160512.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050008.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:58:41 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050003>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] INFO: Crawled 1138 pages (at 2 pages/min), scraped 1013 items (at 17 items/min)
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=001055>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/privacy.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/portal.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.juggernautcap.com/investments/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/about-us/overview/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/juggernaut-capital-partners-invests-in-ceuta-healthcare-group/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://juggernautcap.com/juggernaut-capital-partners-invests-in-advantage-sales-marketing/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/copyright.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/../../../operating-system/environmental-stewardship.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/../../../operating-system/continuous-improvement.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/../../../operating-system/incentive-system.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/../../../operating-system/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.tenaskacapital.com/portfolio/power-generation/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.atlasholdingsllc.com/../../../operating-system/communication.aspx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/V3/doc/1.0.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_rights.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctQry/tradeRecordList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050004.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050010.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 04:59:34 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 04:59:40 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myAccountDetail>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:01:08 [scrapy] INFO: Crawled 1175 pages (at 37 pages/min), scraped 1038 items (at 25 items/min)
2015-11-04 05:01:58 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/userFeedback/feedbackForm>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctMgr/userFeedback/feedbackForm took longer than 180.0 seconds..
2015-11-04 05:02:25 [scrapy] INFO: Crawled 1175 pages (at 0 pages/min), scraped 1050 items (at 12 items/min)
2015-11-04 05:02:25 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=000936>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:02:27 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_risk.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:03:00 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050030>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/tradeMgr/buyFund?fundCode=050030 took longer than 180.0 seconds..
2015-11-04 05:03:00 [scrapy] INFO: Crawled 1215 pages (at 40 pages/min), scraped 1079 items (at 29 items/min)
2015-11-04 05:04:52 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050015>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/tradeMgr/buyFund?fundCode=050015 took longer than 180.0 seconds..
2015-11-04 05:04:52 [scrapy] INFO: Crawled 1260 pages (at 45 pages/min), scraped 1135 items (at 56 items/min)
2015-11-04 05:04:53 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index.jsp>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/index.jsp took longer than 180.0 seconds..
2015-11-04 05:04:58 [scrapy] INFO: Crawled 1281 pages (at 21 pages/min), scraped 1144 items (at 9 items/min)
2015-11-04 05:05:29 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=PA>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 05:05:57 [scrapy] INFO: Crawled 1341 pages (at 60 pages/min), scraped 1216 items (at 72 items/min)
2015-11-04 05:07:14 [scrapy] INFO: Crawled 1342 pages (at 1 pages/min), scraped 1217 items (at 1 items/min)
2015-11-04 05:07:14 [scrapy] INFO: Closing spider (finished)
2015-11-04 05:07:14 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 483,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 6,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 48,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 26,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 391,
 'downloader/request_bytes': 656344,
 'downloader/request_count': 1933,
 'downloader/request_method_count/GET': 1933,
 'downloader/response_bytes': 27135855,
 'downloader/response_count': 1450,
 'downloader/response_status_count/200': 1308,
 'downloader/response_status_count/301': 39,
 'downloader/response_status_count/302': 40,
 'downloader/response_status_count/400': 2,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 19,
 'downloader/response_status_count/404': 11,
 'downloader/response_status_count/500': 23,
 'downloader/response_status_count/503': 7,
 'dupefilter/filtered': 5366,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 5, 7, 14, 974155),
 'item_scraped_count': 1217,
 'log_count/ERROR': 105,
 'log_count/INFO': 47,
 'offsite/domains': 249,
 'offsite/filtered': 845,
 'request_depth_max': 2,
 'response_received_count': 1342,
 'scheduler/dequeued': 1933,
 'scheduler/dequeued/memory': 1933,
 'scheduler/enqueued': 1933,
 'scheduler/enqueued/memory': 1933,
 'spider_exceptions/AttributeError': 12,
 'start_time': datetime.datetime(2015, 11, 4, 4, 24, 57, 355080)}
2015-11-04 05:07:14 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 05:08:16 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 05:08:16 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 05:08:16 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 05:08:16 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 05:08:16 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 05:08:16 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 05:08:17 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 05:08:17 [scrapy] INFO: Spider opened
2015-11-04 05:08:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 05:08:17 [scrapy] ERROR: Error downloading <GET http://www.uni>: DNS lookup failed: address 'www.uni' not found: [Errno -2] Name or service not known.
2015-11-04 05:08:17 [scrapy] ERROR: Error downloading <GET http://www.tit>: DNS lookup failed: address 'www.tit' not found: [Errno -2] Name or service not known.
2015-11-04 05:08:17 [scrapy] ERROR: Error downloading <GET http://www.riv>: DNS lookup failed: address 'www.riv' not found: [Errno -2] Name or service not known.
2015-11-04 05:08:21 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 05:08:30 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=CSGM_RSC+-+add-on+-+Northwest+Comprehensive.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:09:06 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=DSLS_Sabre+-+add-on+-+FWT.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:09:06 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=LEVQ_Risk+-+add-on+-+Dubraski+FINAL.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:09:23 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=CXWN_AM+Conservation+-+Initial+Investment.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:09:27 [scrapy] INFO: Crawled 194 pages (at 194 pages/min), scraped 92 items (at 92 items/min)
2015-11-04 05:10:31 [scrapy] INFO: Crawled 249 pages (at 55 pages/min), scraped 160 items (at 68 items/min)
2015-11-04 05:10:40 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39827/17-spot.pdf> (referer: http://www.berkshire-group.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:11:27 [scrapy] ERROR: Spider error processing <GET http://blakesblog.com/> (referer: https://www.godaddy.com/?isc=instantpage_311&showip=true)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
  2015-11-04 05:12:09 [scrapy] INFO: Crawled 13022 pages (at 26 pages/min), scraped 12740 items (at 41 items/min)
2015-11-04 05:13:33 [scrapy] INFO: Crawled 13061 pages (at 39 pages/min), scraped 12781 items (at 41 items/min)
2015-11-04 05:13:57 [scrapy] INFO: Crawled 13082 pages (at 21 pages/min), scraped 12800 items (at 19 items/min)
2015-11-04 05:14:31 [scrapy] ERROR: Spider error processing <GET https://www.xplenty.com/signup/?utm_source=webapp&utm_medium=navigation&utm_campaign=signup> (referer: https://app.xplenty.com/login)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 05:15:00 [scrapy] INFO: Crawled 13131 pages (at 49 pages/min), scraped 12852 items (at 52 items/min)
2015-11-04 05:15:58 [scrapy] INFO: Crawled 13187 pages (at 56 pages/min), scraped 12909 items (at 57 items/min)
2015-11-04 05:16:59 [scrapy] INFO: Crawled 13263 pages (at 76 pages/min), scraped 12973 items (at 64 items/min)
2015-11-04 05:17:14 [scrapy] ERROR: Spider error processing <GET https://www.xplenty.com/> (referer: https://www.xplenty.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 05:18:10 [scrapy] INFO: Crawled 13346 pages (at 83 pages/min), scraped 13058 items (at 85 items/min)
2015-11-04 05:19:01 [scrapy] INFO: Crawled 13404 pages (at 58 pages/min), scraped 13109 items (at 51 items/min)
2015-11-04 05:19:16 [scrapy] ERROR: Error downloading <GET https://heckerty.com/heckerty_in_the_classroom/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:20:00 [scrapy] INFO: Crawled 13435 pages (at 31 pages/min), scraped 13155 items (at 46 items/min)
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 05:20:01 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception 2015-11-04 05:20:23 [scrapy] INFO: Crawled 930 pages (at 21 pages/min), scraped 796 items (at 44 items/min)
2015-11-04 05:21:19 [scrapy] INFO: Crawled 1016 pages (at 86 pages/min), scraped 873 items (at 77 items/min)
2015-11-04 05:22:19 [scrapy] INFO: Crawled 1101 pages (at 85 pages/min), scraped 939 items (at 66 items/min)
2015-11-04 05:23:23 [scrapy] INFO: Crawled 1176 pages (at 75 pages/min), scraped 1013 items (at 74 items/min)
2015-11-04 05:24:28 [scrapy] INFO: Crawled 1210 pages (at 34 pages/min), scraped 1066 items (at 53 items/min)
2015-11-04 05:24:40 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/home/?status=http%3A%2F%2Fwww.hsbc.com%2Fabout-hsbc%2Fstructure-and-network%2Fretail-banking-and-wealth-management>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:25:25 [scrapy] INFO: Crawled 1272 pages (at 62 pages/min), scraped 1122 items (at 56 items/min)
2015-11-04 05:26:29 [scrapy] INFO: Crawled 1345 pages (at 73 pages/min), scraped 1182 items (at 60 items/min)
2015-11-04 05:27:21 [scrapy] INFO: Crawled 1390 pages (at 45 pages/min), scraped 1236 items (at 54 items/min)
2015-11-04 05:27:53 [scrapy] ERROR: Spider error processing <GET http://www.oldhill.com/blog/> (referer: http://www.oldhill.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69,2015-11-04 05:27:57 [scrapy] INFO: Crawled 13817 pages (at 56 pages/min), scraped 13520 items (at 41 items/min)
2015-11-04 05:29:17 [scrapy] INFO: Crawled 13868 pages (at 51 pages/min), scraped 13577 items (at 57 items/min)
2015-11-04 05:30:08 [scrapy] INFO: Crawled 13906 pages (at 38 pages/min), scraped 13617 items (at 40 items/min)
2015-11-04 05:31:02 [scrapy] INFO: Crawled 13936 pages (at 30 pages/min), scraped 13653 items (at 36 items/min)
2015-11-04 05:32:00 [scrapy] INFO: Crawled 13982 pages (at 46 pages/min), scraped 13700 items (at 47 items/min)
2015-11-04 05:33:05 [scrapy] INFO: Crawled 14045 pages (at 63 pages/min), scraped 13760 items (at 60 items/min)
2015-11-04 05:34:07 [scrapy] INFO: Crawled 14089 pages (at 44 pages/min), scraped 13803 items (at 43 items/min)
2015-11-04 05:35:01 [scrapy] INFO: Crawled 14137 pages (at 48 pages/min), scraped 13850 items (at 47 items/min)
2015-11-04 05:35:57 [scrapy] INFO: Crawled 14180 pages (at 43 pages/min), scraped 13888 items (at 38 items/min)
2015-11-04 05:37:00 [scrapy] INFO: Crawled 14214 pages (at 34 pages/min), scraped 13926 items (at 38 items/min)
2015-11-04 05:38:04 [scrapy] INFO: Crawled 14268 pages (at 54 pages/min), scraped 13980 items (at 54 items/min)
2015-11-04 05:38:59 [scrapy] INFO: Crawled 14308 pages (at 40 pages/min), scraped 14018 items (at 38 items/min)
2015-11-04 05:39:58 [scrapy] INFO: Crawled 14372 pages (at 64 pages/min), scraped 14074 items (at 56 items/min)
2015-11-04 05:41:03 [scrapy] INFO: Crawled 14422 pages (at 50 pages/min), scraped 14120 items (at 46 items/min)
2015-11-04 05:42:04 [scrapy] INFO: Crawled 14463 pages (at 41 pages/min), scraped 14165 items (at 45 items/min)
2015-11-04 05:42:57 [scrapy] INFO: Crawled 14532 pages (at 69 pages/min), scraped 14223 items (at 58 items/min)
2015-11-04 05:43:57 [scrapy] INFO: Crawled 14579 pages (at 47 pages/min), scraped 14269 items (at 46 items/min)
2015-11-04 05:45:11 [scrapy] INFO: Crawled 14623 pages (at 44 pages/min), scraped 14330 items (at 61 items/min)
2015-11-04 05:46:03 [scrapy] INFO: Crawled 14671 pages (at 48 pages/min), scraped 14371 items (at 41 items/min)
2015-11-04 05:46:57 [scrapy] INFO: Crawled 14727 pages (at 56 pages/min), scraped 14429 items (at 58 items/min)
2015-11-04 05:48:01 [scrapy] INFO: Crawled 14810 pages (at 83 pages/min), scraped 14510 items (at 81 items/min)
2015-11-04 05:49:01 [scrapy] INFO: Crawled 14891 pages (at 81 pages/min), scraped 14584 items (at 74 items/min)
2015-11-04 05:50:02 [scrapy] INFO: Crawled 14956 pages (at 65 pages/min), scraped 14652 items (at 68 items/min)
2015-11-04 05:51:12 [scrapy] INFO: Crawled 15052 pages (at 96 pages/min), scraped 14747 items (at 95 items/min)
2015-11-04 05:52:15 [scrapy] INFO: Crawled 15098 pages (at 46 pages/min), scraped 14797 items (at 50 items/min)
2015-11-04 05:52:59 [scrapy] INFO: Crawled 15130 pages (at 32 pages/min), scraped 14835 items (at 38 items/min)
2015-11-04 05:54:05 [scrapy] INFO: Crawled 15222 pages (at 92 pages/min), scraped 14922 items (at 87 items/min)
2015-11-04 05:55:06 [scrapy] INFO: Crawled 15318 pages (at 96 pages/min), scraped 15018 items (at 96 items/min)
2015-11-04 05:55:58 [scrapy] INFO: Crawled 15379 pages (at 61 pages/min), scraped 15082 items (at 64 items/min)
2015-11-04 05:57:04 [scrapy] INFO: Crawled 15467 pages (at 88 pages/min), scraped 15150 items (at 68 items/min)
2015-11-04 05:57:57 [scrapy] INFO: Crawled 15540 pages (at 73 pages/min), scraped 15245 items (at 95 items/min)
2015-11-04 05:58:05 [scrapy] ERROR: Spider error processing <GET https://www.xplenty.com/integrations/rackspace-cloud-files/> (referer: https://www.xplenty.com/integrations/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 05:58:15 [scrapy] ERROR: Spider error processing <GET https://www.xplenty.com/integrations/postgresql/> (referer: https://www.xplenty.com/integrations/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/souppar2015-11-04 05:58:26 [scrapy] INFO: Crawled 1926 pages (at 54 pages/min), scraped 1708 items (at 42 items/min)
2015-11-04 05:58:46 [scrapy] ERROR: Spider error processing <GET https://www.backstopsolutions.com/documents/1701153> (referer: https://www.backstopsolutions.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 05:58:53 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/Scripts_Signup/Signup.dll?CMD=CMDChoosePackage&SUBMIT=SUBMIT&SEL_PACKAGE=26&SETCOUPON=>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 05:58:53 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/EJnDLLABAuQ>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 05:59:37 [scrapy] INFO: Crawled 2001 pages (at 75 pages/min), scraped 1783 items (at 75 items/min)
2015-11-04 06:00:47 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/jws/myEmailsLogin.action>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:00:47 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/Signup/SignupJavaScriptTest.ffhtml?PID=947&URL=%2f%7esite%2fSignup%2fStartCCRSignup%2effhtml>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:00:47 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/go/jump.ffhtml?SPTID=LoWebsitePackages_BuyGold&TARGET=/~site/Signup/gold.ffhtml>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:00:48 [scrapy] INFO: Crawled 2050 pages (at 49 pages/min), scraped 1825 items (at 42 items/min)
2015-11-04 06:01:29 [scrapy] INFO: Crawled 2075 pages (at 25 pages/min), scraped 1849 items (at 24 items/min)
2015-11-04 06:01:50 [scrapy] ERROR: Error downloading <GET http://splitrock.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:01:53 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f5188111c80>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 06:01:58 [scrapy] ERROR: Error downloading <GET https://www.skinmedica.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:02:26 [scrapy] INFO: Crawled 2150 pages (at 75 pages/min), scraped 1926 items (at 77 items/min)
2015-11-04 06:03:15 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/a65nHxoB--4?TB_iframe=&autoplay=1&controls=0&modestbranding=&rel=0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:03:25 [scrapy] INFO: Crawled 2250 pages (at 100 pages/min), scraped 2015 items (at 89 items/min)
2015-11-04 06:03:51 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39764/berkshireraises-1615mfornewstrategy.pdf> (referer: http://www.berkshire-group.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:03:56 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/C-YwlvPaKwU?TB_iframe=&autoplay=1&controls=0&modestbranding=&rel=0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:03:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f5188725320>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:03:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f51886607d0>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:03:57 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f5188479488>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 06:03:57 [scrapy] ERROR: Spider error processing <GET http://guggenheimpartners.com/getattachment/aa2ae2ff-ef23-453f-a316-869e81a35600/Guggenheim-Retail-Real-Estate-Overview-Brochure.pdf.aspx> (referer: http://guggenheimpartners.com/services/guggenheim-retail-partners)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:04:02 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase&esheet=51206683&id=smartlink&index=1&lan=en-US&md5=5b222d35082163093383787f2190a7f7&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com> (referer: http://springlakeequitypartners.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1: htmlParseEntityRef: no name
2015-11-04 06:04:03 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase&esheet=51155453&id=smartlink&index=1&lan=en-US&md5=afb50cfa7109b276baff40e02e4eb89d&newsitemid=20150804005417&url=http%3A%2F%2Fwww.netbase.com%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1: htmlParseEntityRef: no name
2015-11-04 06:04:03 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase&esheet=51168484&id=smartlink&index=1&lan=en-US&md5=9b593437d08385a91e0495900221720f&newsitemid=20150825005342&url=http%3A%2F%2Fwww.netbase.com%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1: htmlParseEntityRef: no name
2015-11-04 06:04:19 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=%40NetBase&esheet=51206683&id=smartlink&index=3&lan=en-US&md5=67c9a53f93d98177a86b9e3d18b9dbd1&newsitemid=20151021006518&url=https%3A%2F%2Ftwitter.com%2FNetBase> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 628: ID archives already defined
2015-11-04 06:04:20 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase+LIVE+Pulse&esheet=51206683&id=smartlink&index=4&lan=en-US&md5=b082c27a4dc5393f87d50dae85bc9e4a&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com%2Fproducts-overview%2Flive-pulse-product-suite%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 628: ID archives already defined
2015-11-04 06:04:20 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.netbase.com&esheet=51206683&id=smartlink&index=2&lan=en-US&md5=cadcbc9bd53a2cca5118452730a3db3f&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 628: ID archives already defined
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:04:22 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f518a813d70>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 06:04:36 [scrapy] INFO: Crawled 2321 pages (at 71 pages/min), scraped 2094 items (at 79 items/min)
2015-11-04 06:05:04 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/google+maps+50+Rowes+Wharf/data=!4m2!2m1!4b1?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:05:25 [scrapy] INFO: Crawled 2396 pages (at 75 pages/min), scraped 2159 items (at 65 items/min)
2015-11-04 06:06:09 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:06:18 [scrapy] ERROR: Error downloading <GET https://www.lightspeed.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:06:18 [scrapy] INFO: Crawled 2470 pages (at 74 pages/min), scraped 2228 items (at 69 items/min)
2015-11-04 06:07:18 [scrapy] INFO: Crawled 2554 pages (at 84 pages/min), scraped 2316 items (at 88 items/min)
2015-11-04 06:07:30 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/place/kellner+capital/@40.75866,-73.96885,1778m/data=!3m1!4b1!4m2!3m1!1s0x0:0x103a362094490e3f?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:08:19 [scrapy] INFO: Crawled 2644 pages (at 90 pages/min), scraped 2416 items (at 100 items/min)
2015-11-04 06:08:33 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39800/berkshireinter_am0615_2.pdf> (referer: http://www.berkshire-group.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:09:17 [scrapy] INFO: Crawled 2746 pages (at 102 pages/min), scraped 2513 items (at 97 items/min)
2015-11-04 06:10:18 [scrapy] INFO: Crawled 2839 pages (at 93 pages/min), scraped 2615 items (at 102 items/min)
2015-11-04 06:11:20 [scrapy] INFO: Crawled 2934 pages (at 95 pages/min), scraped 2698 items (at 83 items/min)
2015-11-04 06:12:19 [scrapy] INFO: Crawled 3015 pages (at 81 pages/min), scraped 2779 items (at 81 items/min)
2015-11-04 06:13:18 [scrapy] INFO: Crawled 3059 pages (at 44 pages/min), scraped 2826 items (at 47 items/min)
2015-11-04 06:14:16 [scrapy] ERROR: Spider error processing <GET http://www3.prudential.com/email/retirement/IMFPWeb/hosted_documents/0262290-00006-00.pdf> (referer: http://www.mesirowfinancial.com/structuredsettlements/default.jsp)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:14:17 [scrapy] INFO: Crawled 3147 pages (at 88 pages/min), scraped 2911 items (at 85 items/min)
2015-11-04 06:15:18 [scrapy] INFO: Crawled 3247 pages (at 100 pages/min), scraped 3000 items (at 89 items/min)
2015-11-04 06:16:26 [scrapy] INFO: Crawled 3299 pages (at 52 pages/min), scraped 3062 items (at 62 items/min)
2015-11-04 06:17:38 [scrapy] INFO: Crawled 3379 pages (at 80 pages/min), scraped 3125 items (at 63 items/min)
2015-11-04 06:18:47 [scrapy] INFO: Crawled 3413 pages (at 34 pages/min), scraped 3159 items (at 34 items/min)
2015-11-04 06:19:56 [scrapy] INFO: Crawled 3435 pages (at 22 pages/min), scraped 3194 items (at 35 items/min)
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 06:20:15 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f51880dcde8>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 06:20:42 [scrapy] INFO: Crawled 3525 pages (at 90 pages/min), scraped 3272 items (at 78 items/min)
2015-11-04 06:21:19 [scrapy] INFO: Crawled 3575 pages (at 50 pages/min), scraped 3305 items (at 33 items/min)
2015-11-04 06:22:25 [scrapy] INFO: Crawled 3614 pages (at 39 pages/min), scraped 3350 items (at 45 items/min)
2015-11-04 06:23:21 [scrapy] INFO: Crawled 3669 pages (at 55 pages/min), scraped 3402 items (at 52 items/min)
2015-11-04 06:23:59 [scrapy] ERROR: Error downloading <GET https://www.shire.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:24:15 [scrapy] ERROR: Error downloading <GET https://www.vantiv.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:24:15 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:24:15 [scrapy] ERROR: Error downloading <GET https://www.cetera.com>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:24:30 [scrapy] INFO: Crawled 3739 pages (at 70 pages/min), scraped 3473 items (at 71 items/min)
2015-11-04 06:24:31 [scrapy] ERROR: Error downloading <GET https://www.metropcs.com/metro/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:25:11 [scrapy] ERROR: Error downloading <GET https://www.sandisk.com/business/datacenter>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:25:32 [scrapy] INFO: Crawled 3806 pages (at 67 pages/min), scraped 3539 items (at 66 items/min)
2015-11-04 06:25:40 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=ASFQ_Sunspire+-+initial+investment+-+2015.06.08.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:25:50 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=APFR_Risk+-+add-on+-+MacCorkle+Insurance+Service.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:26:13 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=UOBN_Risk+-+liquidity+-+sale.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:26:13 [scrapy] ERROR: Spider error processing <GET http://www.expedia.com/> (referer: http://www.northskycapital.com/portfolio/fund-of-funds)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:26:13 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=TYHL_Aurora+-+Liquidity+-+Sale.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:26:14 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=IUPB_Osmose+-+Initial+Investment.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:26:31 [scrapy] INFO: Crawled 3884 pages (at 78 pages/min), scraped 3599 items (at 60 items/min)
2015-11-04 06:26:32 [scrapy] ERROR: Spider error processing <GET http://www.kohlberg.com/ViewDocument.aspx?f=OEMH_AM+Conservation+-+add-on+-+Goodcents.pdf> (referer: http://www.kohlberg.com/News/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:26:52 [scrapy] ERROR: Error downloading <GET http://www.legacyventure.com/password/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:27:09 [scrapy] ERROR: Error downloading <GET https://www.cdw.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:27:09 [scrapy] ERROR: Error downloading <GET https://www.groupon.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:27:24 [scrapy] INFO: Crawled 3942 pages (at 58 pages/min), scraped 3658 items (at 59 items/min)
2015-11-04 06:27:38 [scrapy] ERROR: Error downloading <GET https://m.starwoodhotels.com/westin/feature-phone/index.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:27:48 [scrapy] ERROR: Error downloading <GET https://www.mediaplatform.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:28:20 [scrapy] ERROR: Error downloading <GET https://www.fidelissecurity.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:28:20 [scrapy] INFO: Crawled 4009 pages (at 67 pages/min), scraped 3733 items (at 75 items/min)
2015-11-04 06:29:59 [scrapy] ERROR: Spider error processing <GET http://www.idgvc.com/> (referer: http://www.greenspringassociates.com/portfolio/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 06:30:03 [scrapy] ERROR: Error downloading <GET https://www.cloudflare.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:30:03 [scrapy] INFO: Crawled 4040 pages (at 31 pages/min), scraped 3766 items (at 33 items/min)
2015-11-04 06:30:24 [scrapy] INFO: Crawled 4057 pages (at 17 pages/min), scraped 3785 items (at 19 items/min)
2015-11-04 06:30:34 [scrapy] ERROR: Error downloading <GET https://www.jpmorgan.com/pages/cookies>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:30:34 [scrapy] ERROR: Error downloading <GET https://www.jpmorgan.com/pages/jpmorgan/bds/disclosures/protection>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:31:26 [scrapy] INFO: Crawled 4125 pages (at 68 pages/min), scraped 3845 items (at 60 items/min)
2015-11-04 06:32:22 [scrapy] INFO: Crawled 4163 pages (at 38 pages/min), scraped 3891 items (at 46 items/min)
2015-11-04 06:33:08 [scrapy] ERROR: Error downloading <GET https://support.google.com/plus/?hl=en&p=help_center>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:33:39 [scrapy] INFO: Crawled 4263 pages (at 100 pages/min), scraped 3973 items (at 82 items/min)
2015-11-04 06:33:39 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/stream?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:33:39 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/events?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:33:39 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/people?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:33:39 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/communities?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:34:14 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/me?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:34:14 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/settings/plus?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:35:10 [scrapy] INFO: Crawled 4298 pages (at 35 pages/min), scraped 4013 items (at 40 items/min)
2015-11-04 06:36:08 [scrapy] INFO: Crawled 4310 pages (at 12 pages/min), scraped 4032 items (at 19 items/min)
2015-11-04 06:36:08 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=JsN9QODSEfA>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:36:21 [scrapy] ERROR: Error downloading <GET https://www.reddit.com/submit?title=Lending+Club+CEO+Talks+To+CNBC+&url=http%3A%2F%2Fdirectlendingadvisors.com%2F%3Fp%3D333>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:36:21 [scrapy] INFO: Crawled 4324 pages (at 14 pages/min), scraped 4041 items (at 9 items/min)
2015-11-04 06:37:49 [scrapy] INFO: Crawled 4333 pages (at 9 pages/min), scraped 4054 items (at 13 items/min)
2015-11-04 06:38:59 [scrapy] INFO: Crawled 4358 pages (at 25 pages/min), scraped 4072 items (at 18 items/min)
2015-11-04 06:39:30 [scrapy] INFO: Crawled 4372 pages (at 14 pages/min), scraped 4088 items (at 16 items/min)
2015-11-04 06:41:16 [scrapy] INFO: Crawled 4379 pages (at 7 pages/min), scraped 4098 items (at 10 items/min)
2015-11-04 06:42:12 [scrapy] ERROR: Error downloading <GET https://support.twitter.com/groups/31-twitter-basics/topics/111-features/articles/119135-about-verified-accounts>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:42:12 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/collections?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:42:12 [scrapy] INFO: Crawled 4383 pages (at 4 pages/min), scraped 4109 items (at 11 items/min)
2015-11-04 06:42:21 [scrapy] INFO: Crawled 4404 pages (at 21 pages/min), scraped 4113 items (at 4 items/min)
2015-11-04 06:42:42 [scrapy] ERROR: Error downloading <GET https://www.facebook.com/woothemes>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:43:21 [scrapy] INFO: Crawled 4429 pages (at 25 pages/min), scraped 4145 items (at 32 items/min)
2015-11-04 06:44:10 [scrapy] ERROR: Error downloading <GET https://support.woothemes.com/hc/en-us>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:44:22 [scrapy] INFO: Crawled 4466 pages (at 37 pages/min), scraped 4173 items (at 28 items/min)
2015-11-04 06:45:17 [scrapy] INFO: Crawled 4524 pages (at 58 pages/min), scraped 4216 items (at 43 items/min)
2015-11-04 06:49:55 [scrapy] ERROR: Error downloading <GET https://www.addtoany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:55 [scrapy] ERROR: Error downloading <GET https://www.addtoany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:55 [scrapy] ERROR: Error downloading <GET https://www.addtoany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:55 [scrapy] ERROR: Error downloading <GET https://www.addtoany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:55 [scrapy] ERROR: Error downloading <GET https://www.addtoany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:55 [scrapy] ERROR: Error downloading <GET https://www.addtoany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:55 [scrapy] ERROR: Error downloading <GET https://www.addtoany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:49:56 [scrapy] INFO: Crawled 4548 pages (at 24 pages/min), scraped 4254 items (at 38 items/min)
2015-11-04 06:49:56 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/my-account/sign-up>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:50:19 [scrapy] INFO: Crawled 4617 pages (at 69 pages/min), scraped 4285 items (at 31 items/min)
2015-11-04 06:50:54 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/watermellon-rind-viagra.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:50:54 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/levitra-online-drug-stores-flonase-cialis.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:50:54 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:50:54 [scrapy] ERROR: Error downloading <GET http://bunkiechevroletservice.com/kepa/wirkung-von-viagra-video.php>: DNS lookup failed: address 'bunkiechevroletservice.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:50:54 [scrapy] ERROR: Error downloading <GET http://bunkiechevroletservice.com/kepa/nebenwirkungen-tadalafil-20mg.php>: DNS lookup failed: address 'bunkiechevroletservice.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:50:54 [scrapy] ERROR: Error downloading <GET http://louisvuittonoutleton.com/louis-vuitton-outlet.php>: DNS lookup failed: address 'louisvuittonoutleton.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/generic-viagra-24-hours.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/cialis-cant-cum.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/sex-drive-viagra.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://bunkiechevroletservice.com/kepa/viagra-fuer-die-frau-ohne-rezept-kaufen.php>: DNS lookup failed: address 'bunkiechevroletservice.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://louisvuittonsaleson.com/louis-vuitton-bags.php>: DNS lookup failed: address 'louisvuittonsaleson.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:51:15 [scrapy] ERROR: Error downloading <GET http://www.acielectronics.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET https://www.reddit.com/submit?title=Direct+Lending+and+Community+Banks+-+Getting+Money+to+The+People%21+&url=http%3A%2F%2Fdirectlendingadvisors.com%2F%3Fp%3D371>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/dir/''/threestory+studio/@37.4554585,-122.1316081,14z/data=!4m8!4m7!1m0!1m5!1m1!1s0x808fbae53ee4003d:0x2a340c8efc3fc668!2m2!1d-122.135942!2d37.439416>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET http://www.gcmlp.com/our-business/public-markets-overview/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET http://www.gcmlp.com/our-business/public-markets-overview/public-markets-approach/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET http://semconference.gcmlp.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET http://www.susafund.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/place/152+W+57th+St,+New+York,+NY+10019/@40.765054,-73.9796393,17z/data=!3m1!4b1!4m2!3m1!1s0x89c258f77bf5f87b:0x431cea4d583e0ca2>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET https://sorensoncapital.bamboohr.com/jobs>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET https://ua.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:51 [scrapy] ERROR: Error downloading <GET https://ru.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:51 [scrapy] INFO: Crawled 4673 pages (at 56 pages/min), scraped 4340 items (at 55 items/min)
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://www.paypal.com/us/cgi-bin/webscr?cmd=_refer-mrb&pal=PVTEFLZB2BY44>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://gr.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://tr.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://ve.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://uk.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://ch.godaddy.com/it>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://fi.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://ch.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://se.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/email/professional-email>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://ie.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://fr.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/business/email-marketing>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://media.twitter.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://business.twitter.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://dev.twitter.com/docs/embedded-tweets>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://dev.twitter.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://about.twitter.com/press/brand-assets>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/products/sensei/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://nz.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://nl.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://de.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://cl.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://dk.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://ca.godaddy.com/fr>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:51:56 [scrapy] ERROR: Error downloading <GET https://co.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://ads.twitter.com/start?ref=gl-tw-tw-twitter-advertise>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://support.twitter.com/articles/20170451>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://support.twitter.com/articles/20170514>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/hosting/premium-dns>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://lightserve.com/users/4567/login/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://my.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://it.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://mx.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://ch.godaddy.com/fr>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://sg.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://za.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://pl.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://pe.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://pt.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://ph.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://pk.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://at.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET https://no.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET http://www.aim13.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET http://www.heartflow.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:11 [scrapy] ERROR: Error downloading <GET http://ww3.paydayloanswed.com?kwrf=http%3A%2F%2Fdirectlendingadvisors.com%2Findex.php%2Femerald-asset-management-acquires-direct-lending-advisors%2F>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', '<html>')>]
2015-11-04 06:52:16 [scrapy] ERROR: Error downloading <GET https://www.reddit.com/submit?title=Podcast+With+LendingClub%27s+CMO+-+Scott+Sanbourne&url=http%3A%2F%2Fdirectlendingadvisors.com%2F%3Fp%3D455>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:16 [scrapy] ERROR: Error downloading <GET https://br.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:17 [scrapy] ERROR: Error downloading <GET https://ca.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:17 [scrapy] ERROR: Error downloading <GET https://es.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:17 [scrapy] ERROR: Error downloading <GET https://be.godaddy.com/fr>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/products/wooslider/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/plugins/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/develop-woocommerce/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/woocommerce/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://plus.google.com/114866167534836098189/posts>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/homesteadwebsites>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/press/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/about/meet-the-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/blog>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://plus.google.com/u/0/116036980713945338717>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?video=3000153388>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/find-viagra-cheap-overnight-mail.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://bunkiechevroletservice.com/kepa/tadalafil-wirkzeit.php>: DNS lookup failed: address 'bunkiechevroletservice.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://bunkiechevroletservice.com/kepa/wie-viel-kostet-eine-packung-viagra.php>: DNS lookup failed: address 'bunkiechevroletservice.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://bunkiechevroletservice.com/kepa/cialis-und-priligy-zusammen.php>: DNS lookup failed: address 'bunkiechevroletservice.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://louisvuittonoutleton.com/>: DNS lookup failed: address 'louisvuittonoutleton.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://be.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://ar.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://au.godaddy.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET https://plus.google.com/share?url=http%3A%2F%2Fwww.hsbc.com%2Fabout-hsbc%2Fstructure-and-network%2Fretail-banking-and-wealth-management>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/../financal-management.php>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/disclaimer.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/../lending-parameters.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/our-founder.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/privacy-policy.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/terms-of-use.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/../real-estate.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://www.oldhill.com/../quantitative-trading.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/cialis-and-rapid-heart.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://newsroom.businesswire.com/faqs>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:52:49 [scrapy] ERROR: Error downloading <GET http://louisvuittonsaleson.com/>: DNS lookup failed: address 'louisvuittonsaleson.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:52:49 [scrapy] INFO: Crawled 4719 pages (at 46 pages/min), scraped 4357 items (at 17 items/min)
2015-11-04 06:56:18 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/es/domains/personal-domains>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.twitter.com/woothemes>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.automattic.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://automattic.com/work-with-us/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://ideas.woothemes.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/contact-us/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.woothemes.com/product-category/woocommerce-extensions/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.woothemes.com/about/meet-the-team/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/woocommerce/download/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.woothemes.com/product-category/themes/woocommerce/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.woothemes.com/products/amazon-payments-advanced/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/cart/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/my-account/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] INFO: Crawled 4720 pages (at 1 pages/min), scraped 4378 items (at 21 items/min)
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?feature=youtu.be&utm_content=21206965&utm_medium=social&utm_source=twitter&v=O79AKeeDvaI>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://probablynaked.com/yegoa/best-price-generic-cialis.php>: Connection was refused by other side: 111: Connection refused.
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://docs.woothemes.com/documentation/plugins/woocommerce/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.paydayloansuol.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://plus.google.com/112968269532031325811>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/backstopsolutions>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?feature=youtu.be&v=ZRQEdbhiZzU>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=rhSMRUNWNvU>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://www.quintanacapitalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 06:56:48 [scrapy] ERROR: Error downloading <GET http://bunkiechevroletservice.com/kepa/viagra-25-wirkung.php>: DNS lookup failed: address 'bunkiechevroletservice.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:11 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/loanfilter.php?lender=IM-SHARKY&type=lender>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/loanfilter.php?lender=IM-SHARKY&type=lender took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.pymnts.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://jackkotlarzmd.com/jobs-online-dc>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://jackkotlarzmd.com/make-money-in-imobsters>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/bill-gross-warns-black-swans-can-turn-into-dragons/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/interview-with-direct-lending-advisors/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/an-interview-with-lending-club-ceo-renaud-laplanche/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/category/uncategorized/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/lending-club-adds-former-treasury-secretary-to-its-board/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://newsroom.businesswire.com/contact>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/Scripts_EmailManager/EmailManager.dll?CMD=CMDWebmailLogIn>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://techcrunch.com/2015/11/03/chef-announces-key-acquisition-and-new-compliance-tool/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://techcrunch.com/2015/11/03/relayrides-rebrands-as-turo-and-raises-47-million-led-by-kleiner/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://techcrunch.com/2015/11/03/body-labs-raises-8-million-to-create-ultra-realistic-3d-body-images/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?aq=&cid=16282048205514105173&f=q&geocode=&hl=en&hnear=Boston%2C+Suffolk%2C+Massachusetts&hq=saturn+partners&ie=UTF8&iwloc=A&ll=42.533856%2C-71.019745&q=saturn+partners+boston&sll=42.560949%2C-87.868472&source=embed&spn=0.354182%2C0.961304&sspn=0.076116%2C0.139475&t=h&z=10>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=Z650do2LGOk>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?list=PLTSgtmU9NJ3dfvwU-mE-Y9GaMaYLqGUjL&v=Z650do2LGOk>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=YXHaBpwvsCA>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=VpUyMt55GG0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/+guggenheimpartners/posts/8xLJcGxwGyW>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://support.twitter.com/articles/20069937>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/loanfilter.php?lender=blazing-revenue3&type=lender>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/loanfilter.php?lender=blazing-revenue3&type=lender took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/loanfilter.php?lender=natural-interest7&type=lender>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/loanfilter.php?lender=natural-interest7&type=lender took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/loanfilter.php?lender=Index_Plus&type=lender>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/loanfilter.php?lender=Index_Plus&type=lender took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.businesswire.com/portal/site/home/become-member/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.businesswire.com/portal/site/home/become-member/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/loanfilter.php?lender=OldManP&type=lender>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/loanfilter.php?lender=OldManP&type=lender took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/loanfilter.php?lender=Skeptical-one&type=lender>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/loanfilter.php?lender=Skeptical-one&type=lender took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/lc/alcloanfilter.php?sdm=01&sdy=2010&trm0=1>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/lc/alcloanfilter.php?sdm=01&sdy=2010&trm0=1 took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.famainvestimentos.com/novoUsuario.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.famainvestimentos.com/novoUsuario.php took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.famainvestimentos.com/foreignInvestors.php>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.famainvestimentos.com/foreignInvestors.php took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.famainvestimentos.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.famainvestimentos.com/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/consumer-direct-lending-continues-explosive-growth/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://directlendingadvisors.com/index.php/consumer-direct-lending-continues-explosive-growth/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/prosper-announces-executive-shakeup/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://directlendingadvisors.com/index.php/prosper-announces-executive-shakeup/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/2013/01/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://directlendingadvisors.com/index.php/2013/01/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.businesswire.com/portal/site/home/jobs/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.businesswire.com/portal/site/home/jobs/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.businesswire.com/portal/site/home/targeting/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.businesswire.com/portal/site/home/targeting/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.businesswire.com/portal/site/home/financial-disclosure/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.businesswire.com/portal/site/home/financial-disclosure/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.businesswire.com/portal/site/home/distribution/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.businesswire.com/portal/site/home/distribution/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.businesswire.com/blog>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.businesswire.com/blog took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.businesswire.com/portal/site/home/signup/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.businesswire.com/portal/site/home/signup/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.woothemes.com/my-account/sign-up/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.woothemes.com/my-account/sign-up/ took longer than 180.0 seconds..
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/Login/index.ffhtml>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://docs.woothemes.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.cloudflare.com/sign-up?utm_source=email_protection>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.zulily.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Ffeature%3Dplaylist%26hl%3Den%26action_handle_signin%3Dtrue%26next%3D%252Fuser%252FGuggenheimPtnrs%252F%26app%3Ddesktop&hl=en&passive=true&service=youtube&uilel=3>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?continue=https%3A%2F%2Fplus.google.com%2F%2Bguggenheimpartners%2F%3Fgpsrc%3Dgplp0&passive=1209600&service=oz>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://mail.google.com/mail/?tab=Xm>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/dashboard?ppsrc=gpnv0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://play.google.com/?hl=en&tab=X8>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/hangouts?n=1>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://drive.google.com/?tab=Xo>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.greenspringassociates.com/../../vcard/Jim%20Lim.vcf>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://news.google.com/nwshp?hl=en&tab=Xn>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?hl=en&tab=Xl>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.greenspringassociates.com/../../vcard/John_Avirett.vcf>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.trueventures.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.teladoc.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.walkme.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.meridianfundservices.com/>: DNS lookup failed: address 'www.meridianfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.banyanvc.com/>: DNS lookup failed: address 'www.banyanvc.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.cart.hostricity.com/whoiscart>: DNS lookup failed: address 'www.cart.hostricity.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:42 [scrapy] INFO: Crawled 4720 pages (at 0 pages/min), scraped 4394 items (at 16 items/min)
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?list=PLTSgtmU9NJ3frS0dyDKWvpN0-81BLWWWP&v=VpUyMt55GG0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://techcrunch.com/2015/10/29/the-sonos-play5-is-a-compact-wireless-music-machine-that-sounds-great/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=cJOp8bjpd0U>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=IZJRIx9RGXo>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/playlist?list=PLTSgtmU9NJ3frS0dyDKWvpN0-81BLWWWP>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.demandbase.com/press-release/demandbase-named-jmp-securities-hot-100-list-best-privately-held-software-companies/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.benchmark.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/101036911826502326955>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/111956473112185680805>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/105416385014586099466>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/+guggenheimpartners/posts/CYG6F2Pb5yb>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/+guggenheimpartners/posts/5ji7tRiciXT>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/channel/UCnnJ0xUjRrFtUvufGAUvDlg>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/GuggenheimPtnrs/discussion>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/GuggenheimPtnrs/channels>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/GuggenheimPtnrs/videos>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET https://plus.google.com/+guggenheimpartners/posts/5dzX6hm7cUs>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.cdnow.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:57:42 [scrapy] ERROR: Error downloading <GET http://www.lendstats.com/loansearch/lc/alcloanfilter.php?cg1=C5&cg2=C1&sdm=01&sdy=2010>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.lendstats.com/loansearch/lc/alcloanfilter.php?cg1=C5&cg2=C1&sdm=01&sdy=2010 took longer than 180.0 seconds..
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/inflations-insidious-effects/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/bank-of-england-director-says-mono-banking-culture-is-on-its-way-out/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/did-the-fiscal-cliff-deal-spring-a-leak-in-the-bond-bubble/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/direct-lending-exponential-growth/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://directlendingadvisors.com/index.php/another-top-industry-executive-joins-lending-club/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://www.fed>: DNS lookup failed: address 'www.fed' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://www.gra>: DNS lookup failed: address 'www.gra' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://www.ibinc.com/>: DNS lookup failed: address 'www.ibinc.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://www.lineagecapital.com>: DNS lookup failed: address 'www.lineagecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:57:43 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:21 [scrapy] ERROR: Error downloading <GET http://www.brownadvisory.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 06:58:21 [scrapy] ERROR: Error downloading <GET https://www.tdameritrade.com/home.page>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:58:21 [scrapy] ERROR: Error downloading <GET http://www.paracormedical.com/>: DNS lookup failed: address 'www.paracormedical.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:21 [scrapy] ERROR: Error downloading <GET http://www.vipshop.com/>: DNS lookup failed: address 'www.vipshop.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:21 [scrapy] INFO: Crawled 4784 pages (at 64 pages/min), scraped 4439 items (at 45 items/min)
2015-11-04 06:58:36 [scrapy] ERROR: Error downloading <GET http://www.costanoavc.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:58:40 [scrapy] ERROR: Error downloading <GET https://www.ziggo.com/nl/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 06:59:24 [scrapy] INFO: Crawled 4824 pages (at 40 pages/min), scraped 4484 items (at 45 items/min)
2015-11-04 06:59:41 [scrapy] ERROR: Error downloading <GET http://cloverpoint.com/News/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:00:01 [scrapy] ERROR: Error downloading <GET http://www.myogen.com/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:00:01 [scrapy] ERROR: Error downloading <GET http://www.digitalchocolate.com/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:00:01 [scrapy] ERROR: Error downloading <GET http://www.cw.com/new/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:00:39 [scrapy] INFO: Crawled 4850 pages (at 26 pages/min), scraped 4511 items (at 27 items/min)
2015-11-04 07:01:15 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:01:15 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1510,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 27,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 30,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 96,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 18,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 119,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1215,
 'downloader/request_bytes': 3261467,
 'downloader/request_count': 7464,
 'downloader/request_method_count/GET': 7464,
 'downloader/response_bytes': 110490875,
 'downloader/response_count': 5954,
 'downloader/response_status_count/200': 4674,
 'downloader/response_status_count/301': 517,
 'downloader/response_status_count/302': 412,
 'downloader/response_status_count/303': 5,
 'downloader/response_status_count/307': 4,
 'downloader/response_status_count/400': 27,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 14,
 'downloader/response_status_count/404': 122,
 'downloader/response_status_count/408': 69,
 'downloader/response_status_count/416': 11,
 'downloader/response_status_count/500': 31,
 'downloader/response_status_count/503': 6,
 'downloader/response_status_count/999': 60,
 'dupefilter/filtered': 14071,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 1, 15, 137115),
 'item_scraped_count': 4533,
 'log_count/CRITICAL': 6,
 'log_count/ERROR': 344,
 'log_count/INFO': 106,
 'offsite/domains': 158,
 'offsite/filtered': 686,
 'request_depth_max': 2,
 'response_received_count': 4864,
 'scheduler/dequeued': 7464,
 'scheduler/dequeued/memory': 7464,
 'scheduler/enqueued': 7464,
 'scheduler/enqueued/memory': 7464,
 'spider_exceptions/AttributeError': 13,
 'spider_exceptions/IndexError': 2,
 'spider_exceptions/TypeError': 6,
 'spider_exceptions/XMLSyntaxError': 6,
 'spider_exceptions/timeout': 1,
 'start_time': datetime.datetime(2015, 11, 4, 5, 8, 17, 122446)}
2015-11-04 07:01:15 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:02:17 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:02:17 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:02:17 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:02:17 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:02:17 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:02:17 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:02:17 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:02:17 [scrapy] INFO: Spider opened
2015-11-04 07:02:17 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:02:17 [scrapy] ERROR: Error downloading <GET http://www.arg>: DNS lookup failed: address 'www.arg' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:17 [scrapy] ERROR: Error downloading <GET http://www.atl>: DNS lookup failed: address 'www.atl' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:17 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:17 [scrapy] ERROR: Error downloading <GET http://www.pro>: DNS lookup failed: address 'www.pro' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:17 [scrapy] ERROR: Error downloading <GET http://www.aci>: DNS lookup failed: address 'www.aci' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:17 [scrapy] ERROR: Error downloading <GET http://www.lineagecapital.com>: DNS lookup failed: address 'www.lineagecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:18 [scrapy] ERROR: Error downloading <GET http://www.omn>: DNS lookup failed: address 'www.omn' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:18 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:18 [scrapy] ERROR: Error downloading <GET http://www.phi>: DNS lookup failed: address 'www.phi' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:18 [scrapy] ERROR: Error downloading <GET http://www.dai>: DNS lookup failed: address 'www.dai' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:18 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:18 [scrapy] ERROR: Error downloading <GET http://www.cshg.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:02:18 [scrapy] ERROR: Error downloading <GET http://www.wsc>: DNS lookup failed: address 'www.wsc' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:19 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:20 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:49 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:59 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:59 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:59 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:59 [scrapy] ERROR: Error downloading <GET http://www.rid>: DNS lookup failed: address 'www.rid' not found: [Errno -2] Name or service not known.
2015-11-04 07:02:59 [scrapy] ERROR: Error downloading <GET http://www.5tides.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:03:00 [scrapy] ERROR: Error downloading <GET http://www.emergingmanagersgroup.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:03:17 [scrapy] INFO: Crawled 148 pages (at 148 pages/min), scraped 71 items (at 71 items/min)
2015-11-04 07:04:17 [scrapy] INFO: Crawled 148 pages (at 0 pages/min), scraped 71 items (at 0 items/min)
2015-11-04 07:05:17 [scrapy] INFO: Crawled 148 pages (at 0 pages/min), scraped 71 items (at 0 items/min)
2015-11-04 07:06:17 [scrapy] INFO: Crawled 148 pages (at 0 pages/min), scraped 71 items (at 0 items/min)
2015-11-04 07:07:17 [scrapy] INFO: Crawled 148 pages (at 0 pages/min), scraped 71 items (at 0 items/min)
2015-11-04 07:08:17 [scrapy] INFO: Crawled 148 pages (at 0 pages/min), scraped 71 items (at 0 items/min)
2015-11-04 07:08:39 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:08:39 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:08:48 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:08:48 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:08:48 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 76,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 57,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 9,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 81858,
 'downloader/request_count': 292,
 'downloader/request_method_count/GET': 292,
 'downloader/response_bytes': 1049413,
 'downloader/response_count': 216,
 'downloader/response_status_count/200': 141,
 'downloader/response_status_count/301': 20,
 'downloader/response_status_count/302': 41,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 3,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 219,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 8, 48, 125868),
 'item_scraped_count': 71,
 'log_count/ERROR': 25,
 'log_count/INFO': 13,
 'offsite/domains': 79,
 'offsite/filtered': 338,
 'request_depth_max': 2,
 'response_received_count': 148,
 'scheduler/dequeued': 292,
 'scheduler/dequeued/memory': 292,
 'scheduler/enqueued': 292,
 'scheduler/enqueued/memory': 292,
 'start_time': datetime.datetime(2015, 11, 4, 7, 2, 17, 621619)}
2015-11-04 07:08:48 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:09:49 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:09:49 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:09:49 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:09:50 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:09:50 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:09:50 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:09:50 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:09:50 [scrapy] INFO: Spider opened
2015-11-04 07:09:50 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.rid>: DNS lookup failed: address 'www.rid' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.santanderasset.com>: DNS lookup failed: address 'www.santanderasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.inc>: DNS lookup failed: address 'www.inc' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.adamshillpartners.com>: DNS lookup failed: address 'www.adamshillpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:50 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:52 [scrapy] ERROR: Error downloading <GET http://www.investors.crystalfunds.com>: DNS lookup failed: address 'www.investors.crystalfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:53 [scrapy] ERROR: Error downloading <GET http://www.due>: DNS lookup failed: address 'www.due' not found: [Errno -2] Name or service not known.
2015-11-04 07:09:53 [scrapy] ERROR: Error downloading <GET http://www.formulainvesting.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:10:01 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:07 [scrapy] ERROR: Error downloading <GET http://www.wsc>: DNS lookup failed: address 'www.wsc' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:10 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:10 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:10 [scrapy] ERROR: Error downloading <GET http://www.uni>: DNS lookup failed: address 'www.uni' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:19 [scrapy] ERROR: Error downloading <GET http://www.alphatitans.com>: DNS lookup failed: address 'www.alphatitans.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:19 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:19 [scrapy] ERROR: Error downloading <GET http://www.tet>: DNS lookup failed: address 'www.tet' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:19 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:10:50 [scrapy] INFO: Crawled 282 pages (at 282 pages/min), scraped 180 items (at 180 items/min)
2015-11-04 07:11:56 [scrapy] INFO: Crawled 370 pages (at 88 pages/min), scraped 275 items (at 95 items/min)
2015-11-04 07:12:55 [scrapy] INFO: Crawled 441 pages (at 71 pages/min), scraped 350 items (at 75 items/min)
2015-11-04 07:12:55 [scrapy] ERROR: Error downloading <GET https://www.paymentnet.jpmorgan.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:13:51 [scrapy] INFO: Crawled 528 pages (at 87 pages/min), scraped 432 items (at 82 items/min)
2015-11-04 07:14:56 [scrapy] INFO: Crawled 632 pages (at 104 pages/min), scraped 530 items (at 98 items/min)
2015-11-04 07:15:57 [scrapy] INFO: Crawled 710 pages (at 78 pages/min), scraped 601 items (at 71 items/min)
2015-11-04 07:16:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:53 [scrapy] ERROR: Error downloading <GET https://www.highbridge.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:53 [scrapy] ERROR: Error downloading <GET https://gcp.jpmorgan.com/gcp/access.jsp>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:53 [scrapy] ERROR: Error downloading <GET https://imweb.jpmorgan.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:53 [scrapy] ERROR: Error downloading <GET https://asc.jpmorgan.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:16:53 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:16:54 [scrapy] INFO: Crawled 790 pages (at 80 pages/min), scraped 679 items (at 78 items/min)
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:05 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:11 [scrapy] ERROR: Error downloading <GET https://imweb.jpmorgan.com/imwebsmu/login_new.jsp>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:17:50 [scrapy] INFO: Crawled 805 pages (at 15 pages/min), scraped 714 items (at 35 items/min)
2015-11-04 07:18:50 [scrapy] INFO: Crawled 805 pages (at 0 pages/min), scraped 714 items (at 0 items/min)
2015-11-04 07:19:50 [scrapy] INFO: Crawled 805 pages (at 0 pages/min), scraped 714 items (at 0 items/min)
2015-11-04 07:20:11 [scrapy] ERROR: Error downloading <GET http://www.emffp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:12 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:12 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:12 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:12 [scrapy] ERROR: Error downloading <GET http://www.intrepidcap.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:12 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:20:12 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 160,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 4,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 60,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 15,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 78,
 'downloader/request_bytes': 374201,
 'downloader/request_count': 1037,
 'downloader/request_method_count/GET': 1037,
 'downloader/response_bytes': 10754133,
 'downloader/response_count': 877,
 'downloader/response_status_count/200': 789,
 'downloader/response_status_count/301': 41,
 'downloader/response_status_count/302': 22,
 'downloader/response_status_count/400': 6,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 3,
 'downloader/response_status_count/404': 15,
 'dupefilter/filtered': 2200,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 20, 12, 414211),
 'item_scraped_count': 714,
 'log_count/ERROR': 45,
 'log_count/INFO': 17,
 'offsite/domains': 242,
 'offsite/filtered': 1012,
 'request_depth_max': 2,
 'response_received_count': 805,
 'scheduler/dequeued': 1037,
 'scheduler/dequeued/memory': 1037,
 'scheduler/enqueued': 1037,
 'scheduler/enqueued/memory': 1037,
 'start_time': datetime.datetime(2015, 11, 4, 7, 9, 50, 300998)}
2015-11-04 07:20:12 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:21:14 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:21:14 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:21:14 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:21:14 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:21:14 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:21:14 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:21:14 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:21:14 [scrapy] INFO: Spider opened
2015-11-04 07:21:14 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.investors.crystalfunds.com>: DNS lookup failed: address 'www.investors.crystalfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.us.mcasset.com>: DNS lookup failed: address 'www.us.mcasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.5tides.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:21:14 [scrapy] ERROR: Error downloading <GET http://www.santanderasset.com>: DNS lookup failed: address 'www.santanderasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:15 [scrapy] ERROR: Error downloading <GET http://www.cqs>: DNS lookup failed: address 'www.cqs' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:15 [scrapy] ERROR: Error downloading <GET http://www.pro>: DNS lookup failed: address 'www.pro' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:15 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:16 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:16 [scrapy] ERROR: Error downloading <GET http://www.acc>: DNS lookup failed: address 'www.acc' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:17 [scrapy] ERROR: Error downloading <GET http://www.glaxisllc.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:21:20 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:21:21 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:21 [scrapy] ERROR: Error downloading <GET http://www.lan>: DNS lookup failed: address 'www.lan' not found: [Errno -2] Name or service not known.
2015-11-04 07:21:24 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:21:53 [scrapy] ERROR: Error downloading <GET http://www.polunin.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:22:13 [scrapy] ERROR: Error downloading <GET http://www.ecp.altareturn.com>: DNS lookup failed: address 'www.ecp.altareturn.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:22:13 [scrapy] ERROR: Error downloading <GET http://www.cfm>: DNS lookup failed: address 'www.cfm' not found: [Errno -2] Name or service not known.
2015-11-04 07:22:13 [scrapy] ERROR: Error downloading <GET http://www.tif>: DNS lookup failed: address 'www.tif' not found: [Errno -2] Name or service not known.
2015-11-04 07:22:13 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 07:22:23 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 07:22:34 [scrapy] INFO: Crawled 148 pages (at 148 pages/min), scraped 73 items (at 73 items/min)
2015-11-04 07:23:09 [scrapy] ERROR: Error downloading <GET http://www.pol>: DNS lookup failed: address 'www.pol' not found: [Errno -2] Name or service not known.
2015-11-04 07:23:44 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:23:44 [scrapy] INFO: Crawled 169 pages (at 21 pages/min), scraped 87 items (at 14 items/min)
2015-11-04 07:24:25 [scrapy] INFO: Crawled 178 pages (at 9 pages/min), scraped 97 items (at 10 items/min)
2015-11-04 07:25:14 [scrapy] INFO: Crawled 188 pages (at 10 pages/min), scraped 110 items (at 13 items/min)
2015-11-04 07:26:14 [scrapy] INFO: Crawled 188 pages (at 0 pages/min), scraped 110 items (at 0 items/min)
2015-11-04 07:27:14 [scrapy] INFO: Crawled 188 pages (at 0 pages/min), scraped 110 items (at 0 items/min)
2015-11-04 07:27:59 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:27:59 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:27:59 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:27:59 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 95,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 69,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 9,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 88635,
 'downloader/request_count': 324,
 'downloader/request_method_count/GET': 324,
 'downloader/response_bytes': 2092120,
 'downloader/response_count': 229,
 'downloader/response_status_count/200': 177,
 'downloader/response_status_count/301': 19,
 'downloader/response_status_count/302': 11,
 'downloader/response_status_count/400': 9,
 'downloader/response_status_count/401': 4,
 'downloader/response_status_count/403': 3,
 'downloader/response_status_count/404': 5,
 'downloader/response_status_count/408': 1,
 'dupefilter/filtered': 140,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 27, 59, 870454),
 'item_scraped_count': 110,
 'log_count/ERROR': 32,
 'log_count/INFO': 13,
 'offsite/domains': 97,
 'offsite/filtered': 399,
 'request_depth_max': 2,
 'response_received_count': 188,
 'scheduler/dequeued': 324,
 'scheduler/dequeued/memory': 324,
 'scheduler/enqueued': 324,
 'scheduler/enqueued/memory': 324,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 7, 21, 14, 588567)}
2015-11-04 07:27:59 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:29:01 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:29:01 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:29:01 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:29:01 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:29:01 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:29:01 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:29:01 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:29:01 [scrapy] INFO: Spider opened
2015-11-04 07:29:01 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.cor>: DNS lookup failed: address 'www.cor' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.inc>: DNS lookup failed: address 'www.inc' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.ccm>: DNS lookup failed: address 'www.ccm' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.cqs>: DNS lookup failed: address 'www.cqs' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.par>: DNS lookup failed: address 'www.par' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.cfm>: DNS lookup failed: address 'www.cfm' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.tet>: DNS lookup failed: address 'www.tet' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.zad>: DNS lookup failed: address 'www.zad' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.wellfieldpartners.com>: DNS lookup failed: address 'www.wellfieldpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.arg>: DNS lookup failed: address 'www.arg' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.investor.pccpllc.amiesdigital.com>: DNS lookup failed: address 'www.investor.pccpllc.amiesdigital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.phi>: DNS lookup failed: address 'www.phi' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.bpc>: DNS lookup failed: address 'www.bpc' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:02 [scrapy] ERROR: Error downloading <GET http://www.due>: DNS lookup failed: address 'www.due' not found: [Errno -2] Name or service not known.
2015-11-04 07:29:18 [scrapy] ERROR: Error downloading <GET http://www.sandsbros.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:29:18 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:29:34 [scrapy] ERROR: Error downloading <GET http://www.lightstreetcap.com>: DNS lookup failed: address 'www.lightstreetcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:30:01 [scrapy] INFO: Crawled 211 pages (at 211 pages/min), scraped 132 items (at 132 items/min)
2015-11-04 07:31:01 [scrapy] INFO: Crawled 211 pages (at 0 pages/min), scraped 132 items (at 0 items/min)
2015-11-04 07:32:01 [scrapy] INFO: Crawled 211 pages (at 0 pages/min), scraped 132 items (at 0 items/min)
2015-11-04 07:33:01 [scrapy] INFO: Crawled 211 pages (at 0 pages/min), scraped 132 items (at 0 items/min)
2015-11-04 07:34:01 [scrapy] INFO: Crawled 211 pages (at 0 pages/min), scraped 132 items (at 0 items/min)
2015-11-04 07:35:01 [scrapy] INFO: Crawled 211 pages (at 0 pages/min), scraped 132 items (at 0 items/min)
2015-11-04 07:35:23 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:35:24 [scrapy] ERROR: Error downloading <GET http://www.ironsidespartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:35:24 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:35:24 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 88,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 72,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 85146,
 'downloader/request_count': 335,
 'downloader/request_method_count/GET': 335,
 'downloader/response_bytes': 1630271,
 'downloader/response_count': 247,
 'downloader/response_status_count/200': 202,
 'downloader/response_status_count/301': 22,
 'downloader/response_status_count/302': 13,
 'downloader/response_status_count/401': 4,
 'downloader/response_status_count/404': 6,
 'dupefilter/filtered': 1924,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 35, 24, 285519),
 'item_scraped_count': 132,
 'log_count/ERROR': 29,
 'log_count/INFO': 13,
 'offsite/domains': 87,
 'offsite/filtered': 627,
 'request_depth_max': 2,
 'response_received_count': 211,
 'scheduler/dequeued': 335,
 'scheduler/dequeued/memory': 335,
 'scheduler/enqueued': 335,
 'scheduler/enqueued/memory': 335,
 'start_time': datetime.datetime(2015, 11, 4, 7, 29, 1, 972181)}
2015-11-04 07:35:24 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:36:26 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:36:26 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:36:26 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:36:26 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:36:26 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:36:26 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:36:26 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:36:26 [scrapy] INFO: Spider opened
2015-11-04 07:36:26 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:36:26 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:26 [scrapy] ERROR: Error downloading <GET http://www.aboutyou.bwater.com>: DNS lookup failed: address 'www.aboutyou.bwater.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.woodbinecapital.com>: DNS lookup failed: address 'www.woodbinecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.investor.gppfunds.com>: DNS lookup failed: address 'www.investor.gppfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.rid>: DNS lookup failed: address 'www.rid' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.atl>: DNS lookup failed: address 'www.atl' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.aetherip.applicationexperts.com>: DNS lookup failed: address 'www.aetherip.applicationexperts.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:27 [scrapy] ERROR: Error downloading <GET http://www.ellislake.com>: DNS lookup failed: address 'www.ellislake.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:28 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 07:36:28 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:37:46 [scrapy] INFO: Crawled 182 pages (at 182 pages/min), scraped 69 items (at 69 items/min)
2015-11-04 07:38:33 [scrapy] INFO: Crawled 197 pages (at 15 pages/min), scraped 98 items (at 29 items/min)
2015-11-04 07:39:35 [scrapy] INFO: Crawled 223 pages (at 26 pages/min), scraped 130 items (at 32 items/min)
2015-11-04 07:40:28 [scrapy] INFO: Crawled 271 pages (at 48 pages/min), scraped 177 items (at 47 items/min)
2015-11-04 07:41:31 [scrapy] INFO: Crawled 335 pages (at 64 pages/min), scraped 243 items (at 66 items/min)
2015-11-04 07:42:26 [scrapy] INFO: Crawled 405 pages (at 70 pages/min), scraped 306 items (at 63 items/min)
2015-11-04 07:43:30 [scrapy] INFO: Crawled 471 pages (at 66 pages/min), scraped 378 items (at 72 items/min)
2015-11-04 07:44:32 [scrapy] INFO: Crawled 543 pages (at 72 pages/min), scraped 450 items (at 72 items/min)
2015-11-04 07:45:31 [scrapy] INFO: Crawled 607 pages (at 64 pages/min), scraped 514 items (at 64 items/min)
2015-11-04 07:46:28 [scrapy] INFO: Crawled 671 pages (at 64 pages/min), scraped 578 items (at 64 items/min)
2015-11-04 07:47:31 [scrapy] INFO: Crawled 743 pages (at 72 pages/min), scraped 650 items (at 72 items/min)
2015-11-04 07:48:31 [scrapy] INFO: Crawled 812 pages (at 69 pages/min), scraped 720 items (at 70 items/min)
2015-11-04 07:49:34 [scrapy] INFO: Crawled 884 pages (at 72 pages/min), scraped 781 items (at 61 items/min)
2015-11-04 07:50:43 [scrapy] INFO: Crawled 916 pages (at 32 pages/min), scraped 815 items (at 34 items/min)
2015-11-04 07:51:36 [scrapy] INFO: Crawled 940 pages (at 24 pages/min), scraped 847 items (at 32 items/min)
2015-11-04 07:52:48 [scrapy] INFO: Crawled 996 pages (at 56 pages/min), scraped 898 items (at 51 items/min)
2015-11-04 07:53:34 [scrapy] INFO: Crawled 1031 pages (at 35 pages/min), scraped 934 items (at 36 items/min)
2015-11-04 07:54:34 [scrapy] INFO: Crawled 1064 pages (at 33 pages/min), scraped 967 items (at 33 items/min)
2015-11-04 07:55:39 [scrapy] INFO: Crawled 1104 pages (at 40 pages/min), scraped 1002 items (at 35 items/min)
2015-11-04 07:56:22 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228798> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:56:39 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228799> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:56:39 [scrapy] INFO: Crawled 1128 pages (at 24 pages/min), scraped 1033 items (at 31 items/min)
2015-11-04 07:56:45 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228800> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:56:52 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228801> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 07:57:59 [scrapy] INFO: Crawled 1171 pages (at 43 pages/min), scraped 1068 items (at 35 items/min)
2015-11-04 07:58:50 [scrapy] INFO: Crawled 1192 pages (at 21 pages/min), scraped 1088 items (at 20 items/min)
2015-11-04 08:00:01 [scrapy] INFO: Crawled 1219 pages (at 27 pages/min), scraped 1112 items (at 24 items/min)
2015-11-04 08:00:30 [scrapy] INFO: Crawled 1243 pages (at 24 pages/min), scraped 1133 items (at 21 items/min)
2015-11-04 08:02:10 [scrapy] INFO: Crawled 1248 pages (at 5 pages/min), scraped 1144 items (at 11 items/min)
2015-11-04 08:02:37 [scrapy] INFO: Crawled 1248 pages (at 0 pages/min), scraped 1156 items (at 12 items/min)
2015-11-04 08:03:48 [scrapy] INFO: Crawled 1293 pages (at 45 pages/min), scraped 1192 items (at 36 items/min)
2015-11-04 08:04:40 [scrapy] INFO: Crawled 1316 pages (at 23 pages/min), scraped 1213 items (at 21 items/min)
2015-11-04 08:06:05 [scrapy] INFO: Crawled 1340 pages (at 24 pages/min), scraped 1237 items (at 24 items/min)
2015-11-04 08:06:54 [scrapy] INFO: Crawled 1340 pages (at 0 pages/min), scraped 1248 items (at 11 items/min)
2015-11-04 08:07:35 [scrapy] INFO: Crawled 1387 pages (at 47 pages/min), scraped 1273 items (at 25 items/min)
2015-11-04 08:08:26 [scrapy] INFO: Crawled 1388 pages (at 1 pages/min), scraped 1295 items (at 22 items/min)
2015-11-04 08:10:16 [scrapy] INFO: Crawled 1447 pages (at 59 pages/min), scraped 1340 items (at 45 items/min)
2015-11-04 08:11:03 [scrapy] INFO: Crawled 1450 pages (at 3 pages/min), scraped 1355 items (at 15 items/min)
2015-11-04 08:11:33 [scrapy] INFO: Crawled 1487 pages (at 37 pages/min), scraped 1371 items (at 16 items/min)
2015-11-04 08:12:31 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/service/touziyulicaizhoukandingyuepingtai.html> (referer: http://www.bosera.com/service/xiazaizhongxinbiaogexiazai.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 08:12:31 [scrapy] INFO: Crawled 1491 pages (at 4 pages/min), scraped 1394 items (at 23 items/min)
2015-11-04 08:13:31 [scrapy] INFO: Crawled 1573 pages (at 82 pages/min), scraped 1464 items (at 70 items/min)
2015-11-04 08:14:31 [scrapy] INFO: Crawled 1606 pages (at 33 pages/min), scraped 1498 items (at 34 items/min)
2015-11-04 08:14:56 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:15:27 [scrapy] INFO: Crawled 1650 pages (at 44 pages/min), scraped 1544 items (at 46 items/min)
2015-11-04 08:16:27 [scrapy] INFO: Crawled 1705 pages (at 55 pages/min), scraped 1592 items (at 48 items/min)
2015-11-04 08:18:04 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/fund/boshixianjinbaohuobi.html> (referer: http://www.bosera.com/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 08:18:11 [scrapy] INFO: Crawled 1749 pages (at 44 pages/min), scraped 1638 items (at 46 items/min)
2015-11-04 08:18:35 [scrapy] INFO: Crawled 1749 pages (at 0 pages/min), scraped 1649 items (at 11 items/min)
2015-11-04 08:19:59 [scrapy] INFO: Crawled 1817 pages (at 68 pages/min), scraped 1705 items (at 56 items/min)
2015-11-04 08:20:39 [scrapy] INFO: Crawled 1845 pages (at 28 pages/min), scraped 1739 items (at 34 items/min)
2015-11-04 08:21:35 [scrapy] INFO: Crawled 1893 pages (at 48 pages/min), scraped 1780 items (at 41 items/min)
2015-11-04 08:22:53 [scrapy] INFO: Crawled 1934 pages (at 41 pages/min), scraped 1828 items (at 48 items/min)
2015-11-04 08:23:35 [scrapy] INFO: Crawled 1972 pages (at 38 pages/min), scraped 1862 items (at 34 items/min)
2015-11-04 08:24:33 [scrapy] INFO: Crawled 2006 pages (at 34 pages/min), scraped 1900 items (at 38 items/min)
2015-11-04 08:25:29 [scrapy] INFO: Crawled 2044 pages (at 38 pages/min), scraped 1934 items (at 34 items/min)
2015-11-04 08:26:41 [scrapy] INFO: Crawled 2078 pages (at 34 pages/min), scraped 1972 items (at 38 items/min)
2015-11-04 08:27:31 [scrapy] INFO: Crawled 2116 pages (at 38 pages/min), scraped 2006 items (at 34 items/min)
2015-11-04 08:29:16 [scrapy] INFO: Crawled 2158 pages (at 42 pages/min), scraped 2053 items (at 47 items/min)
2015-11-04 08:29:51 [scrapy] INFO: Crawled 2175 pages (at 17 pages/min), scraped 2063 items (at 10 items/min)
2015-11-04 08:30:30 [scrapy] INFO: Crawled 2215 pages (at 40 pages/min), scraped 2111 items (at 48 items/min)
2015-11-04 08:31:28 [scrapy] INFO: Crawled 2279 pages (at 64 pages/min), scraped 2175 items (at 64 items/min)
2015-11-04 08:33:33 [scrapy] INFO: Crawled 2337 pages (at 58 pages/min), scraped 2233 items (at 58 items/min)
2015-11-04 08:33:41 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:34:18 [scrapy] ERROR: Error downloading <GET http://www.valorep.com>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:35:06 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=000936>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:35:06 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/cashbox/myCashboxDetail>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:35:06 [scrapy] INFO: Crawled 2384 pages (at 47 pages/min), scraped 2276 items (at 43 items/min)
2015-11-04 08:35:06 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050014.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:35:06 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1101_050008.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 08:35:23 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050030>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:35:27 [scrapy] INFO: Crawled 2406 pages (at 22 pages/min), scraped 2294 items (at 18 items/min)
2015-11-04 08:35:45 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/scheduleBuy/scheduleBuyFundList>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 08:35:45 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_risk.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 08:36:25 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/acctAsset/specialFund/specialFundList> (referer: http://www.bosera.com/minisite/fundmanager/bosera_jingli.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 08:36:27 [scrapy] INFO: Crawled 2449 pages (at 43 pages/min), scraped 2336 items (at 42 items/min)
2015-11-04 08:36:31 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050015>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:37:38 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctQry/tradeRecordList>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctQry/tradeRecordList took longer than 180.0 seconds..
2015-11-04 08:37:38 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/ took longer than 180.0 seconds..
2015-11-04 08:37:38 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index_s.jsp?tgtUrl=%2FacctAsset%2FmyFund%2FmyFundList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:37:38 [scrapy] INFO: Crawled 2458 pages (at 9 pages/min), scraped 2352 items (at 16 items/min)
2015-11-04 08:38:12 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/acctAsset/myAccountDetail> (referer: http://www.bosera.com/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 08:38:12 [scrapy] ERROR: Error downloading <GET http://www.harvestmanagement.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 08:38:12 [scrapy] ERROR: Error downloading <GET http://www.permalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 08:38:12 [scrapy] ERROR: Error downloading <GET http://www.constellationcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 08:38:12 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/V3/doc/1.0.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/V3/doc/1.0.html took longer than 180.0 seconds..
2015-11-04 08:38:47 [scrapy] INFO: Crawled 2464 pages (at 6 pages/min), scraped 2354 items (at 2 items/min)
2015-11-04 08:39:50 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/www/fundInfoDetail?flag=info&fundCode=059056>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/www/fundInfoDetail?flag=info&fundCode=059056 took longer than 180.0 seconds..
2015-11-04 08:39:50 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_rights.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/notes/index_rights.html took longer than 180.0 seconds..
2015-11-04 08:39:50 [scrapy] INFO: Crawled 2464 pages (at 0 pages/min), scraped 2356 items (at 2 items/min)
2015-11-04 08:40:24 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index.jsp>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/index.jsp took longer than 180.0 seconds..
2015-11-04 08:40:58 [scrapy] INFO: Crawled 2467 pages (at 3 pages/min), scraped 2359 items (at 3 items/min)
2015-11-04 08:41:38 [scrapy] INFO: Crawled 2470 pages (at 3 pages/min), scraped 2362 items (at 3 items/min)
2015-11-04 08:42:26 [scrapy] INFO: Crawled 2473 pages (at 3 pages/min), scraped 2367 items (at 5 items/min)
2015-11-04 08:43:28 [scrapy] INFO: Crawled 2474 pages (at 1 pages/min), scraped 2368 items (at 1 items/min)
2015-11-04 08:44:26 [scrapy] INFO: Crawled 2476 pages (at 2 pages/min), scraped 2370 items (at 2 items/min)
2015-11-04 08:45:11 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=CEB>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:45:12 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=alipay>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:45:12 [scrapy] INFO: Closing spider (finished)
2015-11-04 08:45:12 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 190,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 48,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 9,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 23,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 110,
 'downloader/request_bytes': 1298883,
 'downloader/request_count': 2755,
 'downloader/request_method_count/GET': 2755,
 'downloader/response_bytes': 27446797,
 'downloader/response_count': 2565,
 'downloader/response_status_count/200': 2479,
 'downloader/response_status_count/301': 24,
 'downloader/response_status_count/302': 24,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 9,
 'downloader/response_status_count/500': 25,
 'downloader/response_status_count/503': 2,
 'dupefilter/filtered': 6594,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 8, 45, 12, 225004),
 'item_scraped_count': 2371,
 'log_count/ERROR': 46,
 'log_count/INFO': 74,
 'offsite/domains': 143,
 'offsite/filtered': 711,
 'request_depth_max': 2,
 'response_received_count': 2477,
 'scheduler/dequeued': 2755,
 'scheduler/dequeued/memory': 2755,
 'scheduler/enqueued': 2755,
 'scheduler/enqueued/memory': 2755,
 'spider_exceptions/AttributeError': 5,
 'spider_exceptions/SSLError': 2,
 'spider_exceptions/timeout': 2,
 'start_time': datetime.datetime(2015, 11, 4, 7, 36, 26, 497639)}
2015-11-04 08:45:12 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 08:46:14 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 08:46:14 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 08:46:14 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 08:46:14 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 08:46:14 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 08:46:14 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 08:46:14 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 08:46:14 [scrapy] INFO: Spider opened
2015-11-04 08:46:14 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 08:46:15 [scrapy] ERROR: Error downloading <GET http://www.careers.weissasset.com>: DNS lookup failed: address 'www.careers.weissasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.investor.gppfunds.com>: DNS lookup failed: address 'www.investor.gppfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.adamshillpartners.com>: DNS lookup failed: address 'www.adamshillpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.harvpartners.com>: DNS lookup failed: address 'www.harvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.ellislake.com>: DNS lookup failed: address 'www.ellislake.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.bpc>: DNS lookup failed: address 'www.bpc' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.uni>: DNS lookup failed: address 'www.uni' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.wsc>: DNS lookup failed: address 'www.wsc' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.atl>: DNS lookup failed: address 'www.atl' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.dis>: DNS lookup failed: address 'www.dis' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.us.mcasset.com>: DNS lookup failed: address 'www.us.mcasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:16 [scrapy] ERROR: Error downloading <GET http://www.citicapitaladvisors.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 08:46:17 [scrapy] ERROR: Error downloading <GET https://www.miopartners.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:17 [scrapy] ERROR: Error downloading <GET http://www.riverside-pm.com>: DNS lookup failed: address 'www.riverside-pm.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:46:17 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 08:46:22 [scrapy] ERROR: Error downloading <GET http://www.glaxisllc.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 08:46:23 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:23 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:23 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:23 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:46 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:52 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:52 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:52 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:52 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:47:17 [scrapy] INFO: Crawled 135 pages (at 135 pages/min), scraped 44 items (at 44 items/min)
2015-11-04 08:47:17 [scrapy] ERROR: Error downloading <GET http://www.lightstreetcap.com>: DNS lookup failed: address 'www.lightstreetcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:48:11 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:48:39 [scrapy] INFO: Crawled 179 pages (at 44 pages/min), scraped 80 items (at 36 items/min)
2015-11-04 08:49:38 [scrapy] INFO: Crawled 195 pages (at 16 pages/min), scraped 104 items (at 24 items/min)
2015-11-04 08:50:21 [scrapy] INFO: Crawled 204 pages (at 9 pages/min), scraped 125 items (at 21 items/min)
2015-11-04 08:51:14 [scrapy] INFO: Crawled 245 pages (at 41 pages/min), scraped 169 items (at 44 items/min)
2015-11-04 08:52:14 [scrapy] INFO: Crawled 245 pages (at 0 pages/min), scraped 169 items (at 0 items/min)
2015-11-04 08:53:14 [scrapy] INFO: Crawled 245 pages (at 0 pages/min), scraped 169 items (at 0 items/min)
2015-11-04 08:53:37 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 08:53:37 [scrapy] ERROR: Error downloading <GET http://www.seamarkcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 08:53:37 [scrapy] INFO: Closing spider (finished)
2015-11-04 08:53:37 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 130,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 76,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 39,
 'downloader/request_bytes': 122851,
 'downloader/request_count': 424,
 'downloader/request_method_count/GET': 424,
 'downloader/response_bytes': 1820935,
 'downloader/response_count': 294,
 'downloader/response_status_count/200': 239,
 'downloader/response_status_count/301': 20,
 'downloader/response_status_count/302': 23,
 'downloader/response_status_count/400': 6,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 4,
 'dupefilter/filtered': 719,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 8, 53, 37, 406620),
 'item_scraped_count': 169,
 'log_count/ERROR': 44,
 'log_count/INFO': 14,
 'offsite/domains': 95,
 'offsite/filtered': 466,
 'request_depth_max': 2,
 'response_received_count': 245,
 'scheduler/dequeued': 424,
 'scheduler/dequeued/memory': 424,
 'scheduler/enqueued': 424,
 'scheduler/enqueued/memory': 424,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 8, 46, 14, 698744)}
2015-11-04 08:53:37 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 08:54:39 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 08:54:39 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 08:54:39 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 08:54:39 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 08:54:39 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 08:54:39 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 08:54:39 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 08:54:39 [scrapy] INFO: Spider opened
2015-11-04 08:54:39 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 08:54:39 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:39 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.ellislake.com>: DNS lookup failed: address 'www.ellislake.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.pol>: DNS lookup failed: address 'www.pol' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.cshg.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.mountainpacificadvisors.com>: DNS lookup failed: address 'www.mountainpacificadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.beckerdrapkin.com>: DNS lookup failed: address 'www.beckerdrapkin.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.due>: DNS lookup failed: address 'www.due' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.bnp>: DNS lookup failed: address 'www.bnp' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.investor.gppfunds.com>: DNS lookup failed: address 'www.investor.gppfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.pia>: DNS lookup failed: address 'www.pia' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.phi>: DNS lookup failed: address 'www.phi' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.aci>: DNS lookup failed: address 'www.aci' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:40 [scrapy] ERROR: Error downloading <GET http://www.nom>: DNS lookup failed: address 'www.nom' not found: [Errno -2] Name or service not known.
2015-11-04 08:54:41 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 08:54:42 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 08:55:05 [scrapy] ERROR: Error downloading <GET https://www.miopartners.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:56:14 [scrapy] INFO: Crawled 189 pages (at 189 pages/min), scraped 83 items (at 83 items/min)
2015-11-04 08:56:49 [scrapy] INFO: Crawled 235 pages (at 46 pages/min), scraped 130 items (at 47 items/min)
2015-11-04 08:57:47 [scrapy] INFO: Crawled 301 pages (at 66 pages/min), scraped 211 items (at 81 items/min)
2015-11-04 08:58:45 [scrapy] INFO: Crawled 367 pages (at 66 pages/min), scraped 276 items (at 65 items/min)
2015-11-04 08:59:44 [scrapy] INFO: Crawled 431 pages (at 64 pages/min), scraped 340 items (at 64 items/min)
2015-11-04 09:00:43 [scrapy] INFO: Crawled 495 pages (at 64 pages/min), scraped 404 items (at 64 items/min)
2015-11-04 09:01:40 [scrapy] INFO: Crawled 569 pages (at 74 pages/min), scraped 476 items (at 72 items/min)
2015-11-04 09:02:42 [scrapy] INFO: Crawled 633 pages (at 64 pages/min), scraped 540 items (at 64 items/min)
2015-11-04 09:03:44 [scrapy] INFO: Crawled 705 pages (at 72 pages/min), scraped 612 items (at 72 items/min)
2015-11-04 09:04:05 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/926698.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/926698.pdf took longer than 180.0 seconds..
2015-11-04 09:04:30 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/922676.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/922676.pdf took longer than 180.0 seconds..
2015-11-04 09:04:30 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/922828.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/922828.pdf took longer than 180.0 seconds..
2015-11-04 09:04:30 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/926211.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/926211.pdf took longer than 180.0 seconds..
2015-11-04 09:04:45 [scrapy] INFO: Crawled 769 pages (at 64 pages/min), scraped 676 items (at 64 items/min)
2015-11-04 09:05:43 [scrapy] INFO: Crawled 833 pages (at 64 pages/min), scraped 740 items (at 64 items/min)
2015-11-04 09:06:46 [scrapy] INFO: Crawled 895 pages (at 62 pages/min), scraped 804 items (at 64 items/min)
2015-11-04 09:07:43 [scrapy] INFO: Crawled 953 pages (at 58 pages/min), scraped 860 items (at 56 items/min)
2015-11-04 09:08:42 [scrapy] INFO: Crawled 1016 pages (at 63 pages/min), scraped 923 items (at 63 items/min)
2015-11-04 09:09:44 [scrapy] INFO: Crawled 1080 pages (at 64 pages/min), scraped 987 items (at 64 items/min)
2015-11-04 09:10:45 [scrapy] INFO: Crawled 1144 pages (at 64 pages/min), scraped 1051 items (at 64 items/min)
2015-11-04 09:11:42 [scrapy] INFO: Crawled 1200 pages (at 56 pages/min), scraped 1107 items (at 56 items/min)
2015-11-04 09:12:44 [scrapy] INFO: Crawled 1271 pages (at 71 pages/min), scraped 1178 items (at 71 items/min)
2015-11-04 09:13:42 [scrapy] INFO: Crawled 1333 pages (at 62 pages/min), scraped 1240 items (at 62 items/min)
2015-11-04 09:14:44 [scrapy] INFO: Crawled 1405 pages (at 72 pages/min), scraped 1312 items (at 72 items/min)
2015-11-04 09:15:42 [scrapy] INFO: Crawled 1469 pages (at 64 pages/min), scraped 1376 items (at 64 items/min)
2015-11-04 09:16:41 [scrapy] INFO: Crawled 1532 pages (at 63 pages/min), scraped 1439 items (at 63 items/min)
2015-11-04 09:17:45 [scrapy] INFO: Crawled 1608 pages (at 76 pages/min), scraped 1515 items (at 76 items/min)
2015-11-04 09:18:43 [scrapy] INFO: Crawled 1662 pages (at 54 pages/min), scraped 1569 items (at 54 items/min)
2015-11-04 09:19:43 [scrapy] INFO: Crawled 1728 pages (at 66 pages/min), scraped 1633 items (at 64 items/min)
2015-11-04 09:20:47 [scrapy] INFO: Crawled 1787 pages (at 59 pages/min), scraped 1696 items (at 63 items/min)
2015-11-04 09:21:42 [scrapy] INFO: Crawled 1849 pages (at 62 pages/min), scraped 1752 items (at 56 items/min)
2015-11-04 09:22:41 [scrapy] INFO: Crawled 1903 pages (at 54 pages/min), scraped 1815 items (at 63 items/min)
2015-11-04 09:23:14 [scrapy] ERROR: Error downloading <GET http://www.meridianfunds.com>: DNS lookup failed: address 'www.meridianfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:23:46 [scrapy] INFO: Crawled 1970 pages (at 67 pages/min), scraped 1879 items (at 64 items/min)
2015-11-04 09:23:53 [scrapy] ERROR: Error downloading <GET https://www.rakuten-sec.co.jp/web/learn/seminar/etf_conference2015/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 09:24:39 [scrapy] INFO: Crawled 2013 pages (at 43 pages/min), scraped 1929 items (at 50 items/min)
2015-11-04 09:25:10 [scrapy] ERROR: Error downloading <GET http://www.nokomiscapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 09:25:39 [scrapy] INFO: Crawled 2013 pages (at 0 pages/min), scraped 1929 items (at 0 items/min)
2015-11-04 09:26:39 [scrapy] INFO: Crawled 2013 pages (at 0 pages/min), scraped 1929 items (at 0 items/min)
2015-11-04 09:27:08 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 09:27:13 [scrapy] ERROR: Error downloading <GET http://www.harvestmanagement.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 09:27:13 [scrapy] ERROR: Error downloading <GET http://www.vscapitalpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 09:27:13 [scrapy] INFO: Closing spider (finished)
2015-11-04 09:27:13 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 92,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 4,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 51,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 11,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 12,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 8,
 'downloader/request_bytes': 1087449,
 'downloader/request_count': 2205,
 'downloader/request_method_count/GET': 2205,
 'downloader/response_bytes': 16521670,
 'downloader/response_count': 2113,
 'downloader/response_status_count/200': 2022,
 'downloader/response_status_count/301': 23,
 'downloader/response_status_count/302': 52,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 8,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 4759,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 9, 27, 13, 405922),
 'item_scraped_count': 1929,
 'log_count/ERROR': 30,
 'log_count/INFO': 39,
 'offsite/domains': 86,
 'offsite/filtered': 500,
 'request_depth_max': 2,
 'response_received_count': 2013,
 'scheduler/dequeued': 2205,
 'scheduler/dequeued/memory': 2205,
 'scheduler/enqueued': 2205,
 'scheduler/enqueued/memory': 2205,
 'start_time': datetime.datetime(2015, 11, 4, 8, 54, 39, 815314)}
2015-11-04 09:27:13 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 09:28:15 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 09:28:15 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 09:28:15 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 09:28:15 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 09:28:15 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 09:28:15 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 09:28:15 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 09:28:15 [scrapy] INFO: Spider opened
2015-11-04 09:28:15 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 09:28:15 [scrapy] ERROR: Error downloading <GET http://www.zca>: DNS lookup failed: address 'www.zca' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:15 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:15 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:17 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:17 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:17 [scrapy] ERROR: Error downloading <GET http://www.fsc>: DNS lookup failed: address 'www.fsc' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:17 [scrapy] ERROR: Error downloading <GET http://www.santanderasset.com>: DNS lookup failed: address 'www.santanderasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:18 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:18 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:18 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:18 [scrapy] ERROR: Error downloading <GET http://www.investors.crystalfunds.com>: DNS lookup failed: address 'www.investors.crystalfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:18 [scrapy] ERROR: Error downloading <GET http://www.ecp.altareturn.com>: DNS lookup failed: address 'www.ecp.altareturn.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:18 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:19 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:19 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:19 [scrapy] ERROR: Error downloading <GET http://www.wellfieldpartners.com>: DNS lookup failed: address 'www.wellfieldpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:19 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:19 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:19 [scrapy] ERROR: Error downloading <GET http://www.key>: DNS lookup failed: address 'www.key' not found: [Errno -2] Name or service not known.
2015-11-04 09:28:19 [scrapy] ERROR: Err2015-11-04 09:28:27 [scrapy] INFO: Crawled 2973 pages (at 16 pages/min), scraped 2873 items (at 16 items/min)
2015-11-04 09:30:16 [scrapy] INFO: Crawled 2985 pages (at 12 pages/min), scraped 2889 items (at 16 items/min)
2015-11-04 09:31:10 [scrapy] INFO: Crawled 2993 pages (at 8 pages/min), scraped 2897 items (at 8 items/min)
2015-11-04 09:32:12 [scrapy] INFO: Crawled 3000 pages (at 7 pages/min), scraped 2905 items (at 8 items/min)
2015-11-04 09:33:10 [scrapy] INFO: Crawled 3007 pages (at 7 pages/min), scraped 2912 items (at 7 items/min)
2015-11-04 09:33:35 [scrapy] INFO: Crawled 3014 pages (at 7 pages/min), scraped 2919 items (at 7 items/min)
2015-11-04 09:35:03 [scrapy] INFO: Crawled 3028 pages (at 14 pages/min), scraped 2933 items (at 14 items/min)
2015-11-04 09:37:40 [scrapy] INFO: Crawled 3035 pages (at 7 pages/min), scraped 2940 items (at 7 items/min)
2015-11-04 09:38:39 [scrapy] INFO: Crawled 3049 pages (at 14 pages/min), scraped 2954 items (at 14 items/min)
2015-11-04 09:39:41 [scrapy] INFO: Crawled 3063 pages (at 14 pages/min), scraped 2968 items (at 14 items/min)
2015-11-04 09:40:35 [scrapy] INFO: Crawled 3077 pages (at 14 pages/min), scraped 2982 items (at 14 items/min)
2015-11-04 09:41:28 [scrapy] INFO: Crawled 3095 pages (at 18 pages/min), scraped 2996 items (at 14 items/min)
2015-11-04 09:42:23 [scrapy] INFO: Crawled 3109 pages (at 14 pages/min), scraped 3010 items (at 14 items/min)
2015-11-04 09:44:13 [scrapy] INFO: Crawled 3119 pages (at 10 pages/min), scraped 3024 items (at 14 items/min)
2015-11-04 09:44:46 [scrapy] INFO: Crawled 3126 pages (at 7 pages/min), scraped 3031 items (at 7 items/min)
2015-11-04 09:45:44 [scrapy] INFO: Crawled 3140 pages (at 14 pages/min), scraped 3045 items (at 14 items/min)
2015-11-04 09:47:16 [scrapy] INFO: Crawled 3154 pages (at 14 pages/min), scraped 3059 items (at 14 items/min)
2015-11-04 09:50:14 [scrapy] INFO: Crawled 3161 pages (at 7 pages/min), scraped 3066 items (at 7 items/min)
2015-11-04 09:50:41 [scrapy] INFO: Crawled 3168 pages (at 7 pages/min), scraped 3073 items (at 7 items/min)
2015-11-04 09:51:54 [scrapy] INFO: Crawled 3182 pages (at 14 pages/min), scraped 3087 items (at 14 items/min)
2015-11-04 09:52:44 [scrapy] INFO: Crawled 3196 pages (at 14 pages/min), scraped 3101 items (at 14 items/min)
2015-11-04 09:53:37 [scrapy] INFO: Crawled 3210 pages (at 14 pages/min), scraped 3115 items (at 14 items/min)
2015-11-04 09:54:42 [scrapy] INFO: Crawled 3224 pages (at 14 pages/min), scraped 3129 items (at 14 items/min)
2015-11-04 09:55:38 [scrapy] INFO: Crawled 3238 pages (at 14 pages/min), scraped 3143 items (at 14 items/min)
2015-11-04 09:56:36 [scrapy] INFO: Crawled 3252 pages (at 14 pages/min), scraped 3157 items (at 14 items/min)
2015-11-04 09:57:34 [scrapy] INFO: Crawled 3266 pages (at 14 pages/min), scraped 3171 items (at 14 items/min)
2015-11-04 09:58:28 [scrapy] INFO: Crawled 3284 pages (at 18 pages/min), scraped 3185 items (at 14 items/min)
2015-11-04 09:59:27 [scrapy] INFO: Crawled 3298 pages (at 14 pages/min), scraped 3199 items (at 14 items/min)
2015-11-04 10:00:21 [scrapy] INFO: Crawled 3312 pages (at 14 pages/min), scraped 3213 items (at 14 items/min)
2015-11-04 10:01:45 [scrapy] INFO: Crawled 3329 pages (at 17 pages/min), scraped 3234 items (at 21 items/min)
2015-11-04 10:02:42 [scrapy] INFO: Crawled 3343 pages (at 14 pages/min), scraped 3248 items (at 14 items/min)
2015-11-04 10:03:44 [scrapy] INFO: Crawled 3357 pages (at 14 pages/min), scraped 3262 items (at 14 items/min)
2015-11-04 10:04:45 [scrapy] INFO: Crawled 3371 pages (at 14 pages/min), scraped 3276 items (at 14 items/min)
2015-11-04 10:05:46 [scrapy] INFO: Crawled 3385 pages (at 14 pages/min), scraped 3290 items (at 14 items/min)
2015-11-04 10:06:46 [scrapy] INFO: Crawled 3399 pages (at 14 pages/min), scraped 3304 items (at 14 items/min)
2015-11-04 10:07:47 [scrapy] INFO: Crawled 3413 pages (at 14 pages/min), scraped 3318 items (at 14 items/min)
2015-11-04 10:08:39 [scrapy] INFO: Crawled 3427 pages (at 14 pages/min), scraped 3332 items (at 14 items/min)
2015-11-04 10:09:34 [scrapy] INFO: Crawled 3441 pages (at 14 pages/min), scraped 3346 items (at 14 items/min)
2015-11-04 10:10:34 [scrapy] INFO: Crawled 3459 pages (at 18 pages/min), scraped 3360 items (at 14 items/min)
2015-11-04 10:11:33 [scrapy] INFO: Crawled 3469 pages (at 10 pages/min), scraped 3374 items (at 14 items/min)
2015-11-04 10:12:39 [scrapy] INFO: Crawled 3483 pages (at 14 pages/min), scraped 3388 items (at 14 items/min)
2015-11-04 10:13:42 [scrapy] INFO: Crawled 3497 pages (at 14 pages/min), scraped 3402 items (at 14 items/min)
2015-11-04 10:14:39 [scrapy] INFO: Crawled 3511 pages (at 14 pages/min), scraped 3416 items (at 14 items/min)
2015-11-04 10:15:35 [scrapy] INFO: Crawled 3525 pages (at 14 pages/min), scraped 3430 items (at 14 items/min)
2015-11-04 10:16:33 [scrapy] INFO: Crawled 3543 pages (at 18 pages/min), scraped 3444 items (at 14 items/min)
2015-11-04 10:17:24 [scrapy] INFO: Crawled 3557 pages (at 14 pages/min), scraped 3458 items (at 14 items/min)
2015-11-04 10:18:42 [scrapy] INFO: Crawled 3574 pages (at 17 pages/min), scraped 3479 items (at 21 items/min)
2015-11-04 10:19:37 [scrapy] INFO: Crawled 3588 pages (at 14 pages/min), scraped 3493 items (at 14 items/min)
2015-11-04 10:20:29 [scrapy] INFO: Crawled 3606 pages (at 18 pages/min), scraped 3507 items (at 14 items/min)
2015-11-04 10:21:27 [scrapy] INFO: Crawled 3620 pages (at 14 pages/min), scraped 3521 items (at 14 items/min)
2015-11-04 10:22:34 [scrapy] INFO: Crawled 3634 pages (at 14 pages/min), scraped 3539 items (at 18 items/min)
2015-11-04 10:23:37 [scrapy] INFO: Crawled 3651 pages (at 17 pages/min), scraped 3556 items (at 17 items/min)
2015-11-04 10:24:28 [scrapy] INFO: Crawled 3669 pages (at 18 pages/min), scraped 3570 items (at 14 items/min)
2015-11-04 10:25:33 [scrapy] INFO: Crawled 3679 pages (at 10 pages/min), scraped 3584 items (at 14 items/min)
2015-11-04 10:26:26 [scrapy] INFO: Crawled 3697 pages (at 18 pages/min), scraped 3598 items (at 14 items/min)
2015-11-04 10:27:46 [scrapy] INFO: Crawled 3714 pages (at 17 pages/min), scraped 3619 items (at 21 items/min)
2015-11-04 10:28:46 [scrapy] INFO: Crawled 3728 pages (at 14 pages/min), scraped 3633 items (at 14 items/min)
2015-11-04 10:29:50 [scrapy] INFO: Crawled 3742 pages (at 14 pages/min), scraped 3647 items (at 14 items/min)
2015-11-04 10:30:27 [scrapy] INFO: Crawled 3753 pages (at 11 pages/min), scraped 3654 items (at 7 items/min)
2015-11-04 10:31:28 [scrapy] INFO: Crawled 3767 pages (at 14 pages/min), scraped 3668 items (at 14 items/min)
2015-11-04 10:32:21 [scrapy] INFO: Crawled 3781 pages (at 14 pages/min), scraped 3682 items (at 14 items/min)
2015-11-04 10:33:41 [scrapy] INFO: Crawled 3798 pages (at 17 pages/min), scraped 3703 items (at 21 items/min)
2015-11-04 10:34:46 [scrapy] INFO: Crawled 3812 pages (at 14 pages/min), scraped 3717 items (at 14 items/min)
2015-11-04 10:35:42 [scrapy] INFO: Crawled 3826 pages (at 14 pages/min), scraped 3731 items (at 14 items/min)
2015-11-04 10:36:41 [scrapy] INFO: Crawled 3840 pages (at 14 pages/min), scraped 3745 items (at 14 items/min)
2015-11-04 10:37:37 [scrapy] INFO: Crawled 3854 pages (at 14 pages/min), scraped 3759 items (at 14 items/min)
2015-11-04 10:38:30 [scrapy] INFO: Crawled 3872 pages (at 18 pages/min), scraped 3773 items (at 14 items/min)
2015-11-04 10:39:40 [scrapy] INFO: Crawled 3886 pages (at 14 pages/min), scraped 3791 items (at 18 items/min)
2015-11-04 10:40:35 [scrapy] INFO: Crawled 3900 pages (at 14 pages/min), scraped 3805 items (at 14 items/min)
2015-11-04 10:41:43 [scrapy] INFO: Crawled 3917 pages (at 17 pages/min), scraped 3822 items (at 17 items/min)
2015-11-04 10:42:38 [scrapy] INFO: Crawled 3931 pages (at 14 pages/min), scraped 3836 items (at 14 items/min)
2015-11-04 10:43:21 [scrapy] INFO: Crawled 3941 pages (at 10 pages/min), scraped 3843 items (at 7 items/min)
2015-11-04 10:44:27 [scrapy] INFO: Crawled 3955 pages (at 14 pages/min), scraped 3857 items (at 14 items/min)
2015-11-04 10:45:43 [scrapy] INFO: Crawled 3973 pages (at 18 pages/min), scraped 3878 items (at 21 items/min)
2015-11-04 10:46:37 [scrapy] INFO: Crawled 3987 pages (at 14 pages/min), scraped 3892 items (at 14 items/min)
2015-11-04 10:47:30 [scrapy] INFO: Crawled 4004 pages (at 17 pages/min), scraped 3906 items (at 14 items/min)
2015-11-04 10:48:24 [scrapy] INFO: Crawled 4018 pages (at 14 pages/min), scraped 3920 items (at 14 items/min)
2015-11-04 10:49:44 [scrapy] INFO: Crawled 4036 pages (at 18 pages/min), scraped 3941 items (at 21 items/min)
2015-11-04 10:50:40 [scrapy] INFO: Crawled 4050 pages (at 14 pages/min), scraped 3955 items (at 14 items/min)
2015-11-04 10:51:32 [scrapy] INFO: Crawled 4067 pages (at 17 pages/min), scraped 3969 items (at 14 items/min)
2015-11-04 10:52:25 [scrapy] INFO: Crawled 4081 pages (at 14 pages/min), scraped 3983 items (at 14 items/min)
2015-11-04 10:53:48 [scrapy] INFO: Crawled 4099 pages (at 18 pages/min), scraped 4004 items (at 21 items/min)
2015-11-04 10:54:40 [scrapy] INFO: Crawled 4113 pages (at 14 pages/min), scraped 4018 items (at 14 items/min)
2015-11-04 10:55:35 [scrapy] INFO: Crawled 4127 pages (at 14 pages/min), scraped 4032 items (at 14 items/min)
2015-11-04 10:56:26 [scrapy] INFO: Crawled 4144 pages (at 17 pages/min), scraped 4046 items (at 14 items/min)
2015-11-04 10:57:24 [scrapy] INFO: Crawled 4158 pages (at 14 pages/min), scraped 4060 items (at 14 items/min)
2015-11-04 10:58:22 [scrapy] INFO: Crawled 4172 pages (at 14 pages/min), scraped 4074 items (at 14 items/min)
2015-11-04 10:59:46 [scrapy] INFO: Crawled 4190 pages (at 18 pages/min), scraped 4095 items (at 21 items/min)
2015-11-04 11:00:38 [scrapy] INFO: Crawled 4204 pages (at 14 pages/min), scraped 4109 items (at 14 items/min)
2015-11-04 11:01:34 [scrapy] INFO: Crawled 4218 pages (at 14 pages/min), scraped 4123 items (at 14 items/min)
2015-11-04 11:02:32 [scrapy] INFO: Crawled 4235 pages (at 17 pages/min), scraped 4137 items (at 14 items/min)
2015-11-04 11:03:30 [scrapy] INFO: Crawled 4249 pages (at 14 pages/min), scraped 4151 items (at 14 items/min)
2015-11-04 11:04:31 [scrapy] INFO: Crawled 4263 pages (at 14 pages/min), scraped 4164 items (at 13 items/min)
2015-11-04 11:05:30 [scrapy] INFO: Crawled 4277 pages (at 14 pages/min), scraped 4178 items (at 14 items/min)
2015-11-04 11:06:24 [scrapy] INFO: Crawled 4291 pages (at 14 pages/min), scraped 4192 items (at 14 items/min)
2015-11-04 11:07:30 [scrapy] INFO: Crawled 4305 pages (at 14 pages/min), scraped 4206 items (at 14 items/min)
2015-11-04 11:08:24 [scrapy] INFO: Crawled 4319 pages (at 14 pages/min), scraped 4220 items (at 14 items/min)
2015-11-04 11:09:41 [scrapy] INFO: Crawled 4336 pages (at 17 pages/min), scraped 4241 items (at 21 items/min)
2015-11-04 11:10:38 [scrapy] INFO: Crawled 4350 pages (at 14 pages/min), scraped 4255 items (at 14 items/min)
2015-11-04 11:11:37 [scrapy] INFO: Crawled 4364 pages (at 14 pages/min), scraped 4269 items (at 14 items/min)
2015-11-04 11:12:27 [scrapy] INFO: Crawled 4382 pages (at 18 pages/min), scraped 4283 items (at 14 items/min)
2015-11-04 11:13:22 [scrapy] INFO: Crawled 4396 pages (at 14 pages/min), scraped 4297 items (at 14 items/min)
2015-11-04 11:14:40 [scrapy] INFO: Crawled 4413 pages (at 17 pages/min), scraped 4318 items (at 21 items/min)
2015-11-04 11:15:30 [scrapy] INFO: Crawled 4431 pages (at 18 pages/min), scraped 4332 items (at 14 items/min)
2015-11-04 11:16:30 [scrapy] INFO: Crawled 4445 pages (at 14 pages/min), scraped 4346 items (at 14 items/min)
2015-11-04 11:17:37 [scrapy] INFO: Crawled 4459 pages (at 14 pages/min), scraped 4364 items (at 18 items/min)
2015-11-04 11:18:46 [scrapy] INFO: Crawled 4476 pages (at 17 pages/min), scraped 4381 items (at 17 items/min)
2015-11-04 11:19:38 [scrapy] INFO: Crawled 4490 pages (at 14 pages/min), scraped 4395 items (at 14 items/min)
2015-11-04 11:20:30 [scrapy] INFO: Crawled 4508 pages (at 18 pages/min), scraped 4409 items (at 14 items/min)
2015-11-04 11:21:26 [scrapy] INFO: Crawled 4522 pages (at 14 pages/min), scraped 4423 items (at 14 items/min)
2015-11-04 11:22:25 [scrapy] INFO: Crawled 4536 pages (at 14 pages/min), scraped 4437 items (at 14 items/min)
2015-11-04 11:23:22 [scrapy] INFO: Crawled 4550 pages (at 14 pages/min), scraped 4451 items (at 14 items/min)
2015-11-04 11:24:46 [scrapy] INFO: Crawled 4567 pages (at 17 pages/min), scraped 4472 items (at 21 items/min)
2015-11-04 11:25:41 [scrapy] INFO: Crawled 4581 pages (at 14 pages/min), scraped 4486 items (at 14 items/min)
2015-11-04 11:26:33 [scrapy] INFO: Crawled 4595 pages (at 14 pages/min), scraped 4500 items (at 14 items/min)
2015-11-04 11:27:30 [scrapy] INFO: Crawled 4613 pages (at 18 pages/min), scraped 4514 items (at 14 items/min)
2015-11-04 11:28:27 [scrapy] INFO: Crawled 4627 pages (at 14 pages/min), scraped 4528 items (at 14 items/min)
2015-11-04 11:29:24 [scrapy] INFO: Crawled 4641 pages (at 14 pages/min), scraped 4542 items (at 14 items/min)
2015-11-04 11:30:44 [scrapy] INFO: Crawled 4658 pages (at 17 pages/min), scraped 4563 items (at 21 items/min)
2015-11-04 11:31:42 [scrapy] INFO: Crawled 4672 pages (at 14 pages/min), scraped 4577 items (at 14 items/min)
2015-11-04 11:32:42 [scrapy] INFO: Crawled 4686 pages (at 14 pages/min), scraped 4591 items (at 14 items/min)
2015-11-04 11:33:43 [scrapy] INFO: Crawled 4700 pages (at 14 pages/min), scraped 4605 items (at 14 items/min)
2015-11-04 11:34:52 [scrapy] INFO: Crawled 4714 pages (at 14 pages/min), scraped 4619 items (at 14 items/min)
2015-11-04 11:35:31 [scrapy] INFO: Crawled 4725 pages (at 11 pages/min), scraped 4626 items (at 7 items/min)
2015-11-04 11:36:33 [scrapy] INFO: Crawled 4735 pages (at 10 pages/min), scraped 4640 items (at 14 items/min)
2015-11-04 11:37:31 [scrapy] INFO: Crawled 4753 pages (at 18 pages/min), scraped 4654 items (at 14 items/min)
2015-11-04 11:38:34 [scrapy] INFO: Crawled 4763 pages (at 10 pages/min), scraped 4668 items (at 14 items/min)
2015-11-04 11:39:36 [scrapy] INFO: Crawled 4777 pages (at 14 pages/min), scraped 4682 items (at 14 items/min)
2015-11-04 11:40:35 [scrapy] INFO: Crawled 4791 pages (at 14 pages/min), scraped 4696 items (at 14 items/min)
2015-11-04 11:41:36 [scrapy] INFO: Crawled 4805 pages (at 14 pages/min), scraped 4710 items (at 14 items/min)
2015-11-04 11:42:31 [scrapy] INFO: Crawled 4819 pages (at 14 pages/min), scraped 4724 items (at 14 items/min)
2015-11-04 11:43:25 [scrapy] INFO: Crawled 4837 pages (at 18 pages/min), scraped 4738 items (at 14 items/min)
2015-11-04 11:44:35 [scrapy] INFO: Crawled 4851 pages (at 14 pages/min), scraped 4756 items (at 18 items/min)
2015-11-04 11:45:43 [scrapy] INFO: Crawled 4868 pages (at 17 pages/min), scraped 4773 items (at 17 items/min)
2015-11-04 11:46:35 [scrapy] INFO: Crawled 4882 pages (at 14 pages/min), scraped 4787 items (at 14 items/min)
2015-11-04 11:47:31 [scrapy] INFO: Crawled 4900 pages (at 18 pages/min), scraped 4801 items (at 14 items/min)
2015-11-04 11:48:24 [scrapy] INFO: Crawled 4914 pages (at 14 pages/min), scraped 4815 items (at 14 items/min)
2015-11-04 11:49:21 [scrapy] INFO: Crawled 4928 pages (at 14 pages/min), scraped 4829 items (at 14 items/min)
2015-11-04 11:50:40 [scrapy] INFO: Crawled 4945 pages (at 17 pages/min), scraped 4850 items (at 21 items/min)
2015-11-04 11:51:47 [scrapy] INFO: Crawled 4959 pages (at 14 pages/min), scraped 4864 items (at 14 items/min)
2015-11-04 11:52:40 [scrapy] INFO: Crawled 4973 pages (at 14 pages/min), scraped 4878 items (at 14 items/min)
2015-11-04 11:53:42 [scrapy] INFO: Crawled 4987 pages (at 14 pages/min), scraped 4892 items (at 14 items/min)
2015-11-04 11:54:32 [scrapy] INFO: Crawled 5001 pages (at 14 pages/min), scraped 4906 items (at 14 items/min)
2015-11-04 11:55:25 [scrapy] INFO: Crawled 5019 pages (at 18 pages/min), scraped 4920 items (at 14 items/min)
2015-11-04 11:56:46 [scrapy] INFO: Crawled 5036 pages (at 17 pages/min), scraped 4941 items (at 21 items/min)
2015-11-04 11:57:41 [scrapy] INFO: Crawled 5050 pages (at 14 pages/min), scraped 4955 items (at 14 items/min)
2015-11-04 11:58:36 [scrapy] INFO: Crawled 5064 pages (at 14 pages/min), scraped 4969 items (at 14 items/min)
2015-11-04 11:59:33 [scrapy] INFO: Crawled 5078 pages (at 14 pages/min), scraped 4983 items (at 14 items/min)
2015-11-04 12:00:32 [scrapy] INFO: Crawled 5096 pages (at 18 pages/min), scraped 4997 items (at 14 items/min)
2015-11-04 12:01:26 [scrapy] INFO: Crawled 5110 pages (at 14 pages/min), scraped 5011 items (at 14 items/min)
2015-11-04 12:02:22 [scrapy] INFO: Crawled 5124 pages (at 14 pages/min), scraped 5025 items (at 14 items/min)
2015-11-04 12:03:46 [scrapy] INFO: Crawled 5141 pages (at 17 pages/min), scraped 5046 items (at 21 items/min)
2015-11-04 12:05:04 [scrapy] INFO: Crawled 5155 pages (at 14 pages/min), scraped 5060 items (at 14 items/min)
2015-11-04 12:05:32 [scrapy] INFO: Crawled 5164 pages (at 9 pages/min), scraped 5067 items (at 7 items/min)
2015-11-04 12:06:27 [scrapy] INFO: Crawled 5178 pages (at 14 pages/min), scraped 5081 items (at 14 items/min)
2015-11-04 12:07:24 [scrapy] INFO: Crawled 5192 pages (at 14 pages/min), scraped 5095 items (at 14 items/min)
2015-11-04 12:08:24 [scrapy] INFO: Crawled 5206 pages (at 14 pages/min), scraped 5109 items (at 14 items/min)
2015-11-04 12:09:46 [scrapy] INFO: Crawled 5225 pages (at 19 pages/min), scraped 5130 items (at 21 items/min)
2015-11-04 12:10:45 [scrapy] INFO: Crawled 5239 pages (at 14 pages/min), scraped 5144 items (at 14 items/min)
2015-11-04 12:11:45 [scrapy] INFO: Crawled 5252 pages (at 13 pages/min), scraped 5157 items (at 13 items/min)
2015-11-04 12:12:43 [scrapy] INFO: Crawled 5266 pages (at 14 pages/min), scraped 5171 items (at 14 items/min)
2015-11-04 12:13:36 [scrapy] INFO: Crawled 5280 pages (at 14 pages/min), scraped 5185 items (at 14 items/min)
2015-11-04 12:14:35 [scrapy] INFO: Crawled 5294 pages (at 14 pages/min), scraped 5199 items (at 14 items/min)
2015-11-04 12:15:34 [scrapy] INFO: Crawled 5311 pages (at 17 pages/min), scraped 5213 items (at 14 items/min)
2015-11-04 12:16:30 [scrapy] INFO: Crawled 5325 pages (at 14 pages/min), scraped 5227 items (at 14 items/min)
2015-11-04 12:17:29 [scrapy] INFO: Crawled 5339 pages (at 14 pages/min), scraped 5241 items (at 14 items/min)
2015-11-04 12:18:21 [scrapy] INFO: Crawled 5353 pages (at 14 pages/min), scraped 5255 items (at 14 items/min)
2015-11-04 12:19:45 [scrapy] INFO: Crawled 5371 pages (at 18 pages/min), scraped 5276 items (at 21 items/min)
2015-11-04 12:20:42 [scrapy] INFO: Crawled 5385 pages (at 14 pages/min), scraped 5290 items (at 14 items/min)
2015-11-04 12:21:42 [scrapy] INFO: Crawled 5399 pages (at 14 pages/min), scraped 5304 items (at 14 items/min)
2015-11-04 12:22:48 [scrapy] INFO: Crawled 5413 pages (at 14 pages/min), scraped 5318 items (at 14 items/min)
2015-11-04 12:23:41 [scrapy] INFO: Crawled 5427 pages (at 14 pages/min), scraped 5332 items (at 14 items/min)
2015-11-04 12:24:51 [scrapy] INFO: Crawled 5441 pages (at 14 pages/min), scraped 5346 items (at 14 items/min)
2015-11-04 12:25:24 [scrapy] INFO: Crawled 5451 pages (at 10 pages/min), scraped 5353 items (at 7 items/min)
2015-11-04 12:26:21 [scrapy] INFO: Crawled 5465 pages (at 14 pages/min), scraped 5367 items (at 14 items/min)
2015-11-04 12:27:21 [scrapy] INFO: Crawled 5479 pages (at 14 pages/min), scraped 5381 items (at 14 items/min)
2015-11-04 12:28:29 [scrapy] INFO: Crawled 5500 pages (at 21 pages/min), scraped 5402 items (at 21 items/min)
2015-11-04 12:29:40 [scrapy] INFO: Crawled 5518 pages (at 18 pages/min), scraped 5423 items (at 21 items/min)
2015-11-04 12:30:33 [scrapy] INFO: Crawled 5535 pages (at 17 pages/min), scraped 5437 items (at 14 items/min)
2015-11-04 12:31:35 [scrapy] INFO: Crawled 5549 pages (at 14 pages/min), scraped 5451 items (at 14 items/min)
2015-11-04 12:32:27 [scrapy] INFO: Crawled 5563 pages (at 14 pages/min), scraped 5465 items (at 14 items/min)
2015-11-04 12:33:25 [scrapy] INFO: Crawled 5577 pages (at 14 pages/min), scraped 5479 items (at 14 items/min)
2015-11-04 12:34:45 [scrapy] INFO: Crawled 5595 pages (at 18 pages/min), scraped 5500 items (at 21 items/min)
2015-11-04 12:35:46 [scrapy] INFO: Crawled 5609 pages (at 14 pages/min), scraped 5514 items (at 14 items/min)
2015-11-04 12:36:45 [scrapy] INFO: Crawled 5623 pages (at 14 pages/min), scraped 5528 items (at 14 items/min)
2015-11-04 12:37:48 [scrapy] INFO: Crawled 5637 pages (at 14 pages/min), scraped 5542 items (at 14 items/min)
2015-11-04 12:38:42 [scrapy] INFO: Crawled 5651 pages (at 14 pages/min), scraped 5556 items (at 14 items/min)
2015-11-04 12:39:37 [scrapy] INFO: Crawled 5665 pages (at 14 pages/min), scraped 5570 items (at 14 items/min)
2015-11-04 12:40:38 [scrapy] INFO: Crawled 5679 pages (at 14 pages/min), scraped 5584 items (at 14 items/min)
2015-11-04 12:41:36 [scrapy] INFO: Crawled 5693 pages (at 14 pages/min), scraped 5598 items (at 14 items/min)
2015-11-04 12:42:36 [scrapy] INFO: Crawled 5710 pages (at 17 pages/min), scraped 5612 items (at 14 items/min)
2015-11-04 12:43:30 [scrapy] INFO: Crawled 5724 pages (at 14 pages/min), scraped 5626 items (at 14 items/min)
2015-11-04 12:44:43 [scrapy] INFO: Crawled 5735 pages (at 11 pages/min), scraped 5640 items (at 14 items/min)
2015-11-04 12:45:41 [scrapy] INFO: Crawled 5749 pages (at 14 pages/min), scraped 5654 items (at 14 items/min)
2015-11-04 12:46:36 [scrapy] INFO: Crawled 5763 pages (at 14 pages/min), scraped 5668 items (at 14 items/min)
2015-11-04 12:47:32 [scrapy] INFO: Crawled 5780 pages (at 17 pages/min), scraped 5682 items (at 14 items/min)
2015-11-04 12:48:32 [scrapy] INFO: Crawled 5794 pages (at 14 pages/min), scraped 5696 items (at 14 items/min)
2015-11-04 12:49:29 [scrapy] INFO: Crawled 5808 pages (at 14 pages/min), scraped 5710 items (at 14 items/min)
2015-11-04 12:50:26 [scrapy] INFO: Crawled 5821 pages (at 13 pages/min), scraped 5724 items (at 14 items/min)
2015-11-04 12:51:45 [scrapy] INFO: Crawled 5831 pages (at 10 pages/min), scraped 5736 items (at 12 items/min)
2015-11-04 12:52:29 [scrapy] INFO: Crawled 5842 pages (at 11 pages/min), scraped 5743 items (at 7 items/min)
2015-11-04 12:53:49 [scrapy] INFO: Crawled 5852 pages (at 10 pages/min), scraped 5757 items (at 14 items/min)
2015-11-04 12:54:33 [scrapy] INFO: Crawled 5863 pages (at 11 pages/min), scraped 5764 items (at 7 items/min)
2015-11-04 12:55:23 [scrapy] INFO: Crawled 5877 pages (at 14 pages/min), scraped 5777 items (at 13 items/min)
2015-11-04 12:56:22 [scrapy] INFO: Crawled 5891 pages (at 14 pages/min), scraped 5791 items (at 14 items/min)
2015-11-04 12:57:22 [scrapy] INFO: Crawled 5905 pages (at 14 pages/min), scraped 5805 items (at 14 items/min)
2015-11-04 12:58:42 [scrapy] INFO: Crawled 5922 pages (at 17 pages/min), scraped 5826 items (at 21 items/min)
2015-11-04 12:59:46 [scrapy] INFO: Crawled 5936 pages (at 14 pages/min), scraped 5840 items (at 14 items/min)
2015-11-04 13:00:40 [scrapy] INFO: Crawled 5950 pages (at 14 pages/min), scraped 5854 items (at 14 items/min)
2015-11-04 13:01:43 [scrapy] INFO: Crawled 5964 pages (at 14 pages/min), scraped 5868 items (at 14 items/min)
2015-11-04 13:02:40 [scrapy] INFO: Crawled 5978 pages (at 14 pages/min), scraped 5882 items (at 14 items/min)
2015-11-04 13:03:39 [scrapy] INFO: Crawled 5992 pages (at 14 pages/min), scraped 5896 items (at 14 items/min)
2015-11-04 13:04:37 [scrapy] INFO: Crawled 6006 pages (at 14 pages/min), scraped 5910 items (at 14 items/min)
2015-11-04 13:05:35 [scrapy] INFO: Crawled 6024 pages (at 18 pages/min), scraped 5924 items (at 14 items/min)
2015-11-04 13:07:10 [scrapy] INFO: Crawled 6034 pages (at 10 pages/min), scraped 5938 items (at 14 items/min)
2015-11-04 13:07:51 [scrapy] INFO: Crawled 6039 pages (at 5 pages/min), scraped 5945 items (at 7 items/min)
2015-11-04 13:09:03 [scrapy] INFO: Crawled 6051 pages (at 12 pages/min), scraped 5956 items (at 11 items/min)
2015-11-04 13:09:30 [scrapy] INFO: Crawled 6058 pages (at 7 pages/min), scraped 5962 items (at 6 items/min)
2015-11-04 13:10:34 [scrapy] INFO: Crawled 6065 pages (at 7 pages/min), scraped 5969 items (at 7 items/min)
2015-11-04 13:11:37 [scrapy] INFO: Crawled 6079 pages (at 14 pages/min), scraped 5983 items (at 14 items/min)
2015-11-04 13:12:32 [scrapy] INFO: Crawled 6093 pages (at 14 pages/min), scraped 5997 items (at 14 items/min)
2015-11-04 13:13:23 [scrapy] INFO: Crawled 6113 pages (at 20 pages/min), scraped 6011 items (at 14 items/min)
2015-11-04 13:14:48 [scrapy] INFO: Crawled 6128 pages (at 15 pages/min), scraped 6032 items (at 21 items/min)
2015-11-04 13:15:43 [scrapy] INFO: Crawled 6142 pages (at 14 pages/min), scraped 6046 items (at 14 items/min)
2015-11-04 13:16:44 [scrapy] INFO: Crawled 6156 pages (at 14 pages/min), scraped 6060 items (at 14 items/min)
2015-11-04 13:17:35 [scrapy] INFO: Crawled 6170 pages (at 14 pages/min), scraped 6074 items (at 14 items/min)
2015-11-04 13:18:38 [scrapy] INFO: Crawled 6184 pages (at 14 pages/min), scraped 6088 items (at 14 items/min)
2015-11-04 13:19:26 [scrapy] INFO: Crawled 6198 pages (at 14 pages/min), scraped 6102 items (at 14 items/min)
2015-11-04 13:20:46 [scrapy] INFO: Crawled 6219 pages (at 21 pages/min), scraped 6123 items (at 21 items/min)
2015-11-04 13:21:50 [scrapy] INFO: Crawled 6233 pages (at 14 pages/min), scraped 6137 items (at 14 items/min)
2015-11-04 13:22:26 [scrapy] INFO: Crawled 6245 pages (at 12 pages/min), scraped 6144 items (at 7 items/min)
2015-11-04 13:23:23 [scrapy] INFO: Crawled 6259 pages (at 14 pages/min), scraped 6158 items (at 14 items/min)
2015-11-04 13:24:45 [scrapy] INFO: Crawled 6275 pages (at 16 pages/min), scraped 6179 items (at 21 items/min)
2015-11-04 13:25:38 [scrapy] INFO: Crawled 6289 pages (at 14 pages/min), scraped 6193 items (at 14 items/min)
2015-11-04 13:26:31 [scrapy] INFO: Crawled 6303 pages (at 14 pages/min), scraped 6207 items (at 14 items/min)
2015-11-04 13:27:22 [scrapy] INFO: Crawled 6322 pages (at 19 pages/min), scraped 6221 items (at 14 items/min)
2015-11-04 13:28:43 [scrapy] INFO: Crawled 6338 pages (at 16 pages/min), scraped 6242 items (at 21 items/min)
2015-11-04 13:29:33 [scrapy] INFO: Crawled 6352 pages (at 14 pages/min), scraped 6256 items (at 14 items/min)
2015-11-04 13:30:28 [scrapy] INFO: Crawled 6366 pages (at 14 pages/min), scraped 6270 items (at 14 items/min)
2015-11-04 13:31:21 [scrapy] INFO: Crawled 6385 pages (at 19 pages/min), scraped 6284 items (at 14 items/min)
2015-11-04 13:32:38 [scrapy] INFO: Crawled 6401 pages (at 16 pages/min), scraped 6305 items (at 21 items/min)
2015-11-04 13:33:33 [scrapy] INFO: Crawled 6415 pages (at 14 pages/min), scraped 6319 items (at 14 items/min)
2015-11-04 13:34:24 [scrapy] INFO: Crawled 6434 pages (at 19 pages/min), scraped 6333 items (at 14 items/min)
2015-11-04 13:35:42 [scrapy] INFO: Crawled 6450 pages (at 16 pages/min), scraped 6354 items (at 21 items/min)
2015-11-04 13:36:35 [scrapy] INFO: Crawled 6464 pages (at 14 pages/min), scraped 6368 items (at 14 items/min)
2015-11-04 13:37:27 [scrapy] INFO: Crawled 6483 pages (at 19 pages/min), scraped 6382 items (at 14 items/min)
2015-11-04 13:38:41 [scrapy] INFO: Crawled 6505 pages (at 22 pages/min), scraped 6403 items (at 21 items/min)
2015-11-04 13:39:22 [scrapy] INFO: Crawled 6519 pages (at 14 pages/min), scraped 6416 items (at 13 items/min)
2015-11-04 13:40:27 [scrapy] INFO: Crawled 6541 pages (at 22 pages/min), scraped 6433 items (at 17 items/min)
2015-11-04 13:41:32 [scrapy] INFO: Crawled 6556 pages (at 15 pages/min), scraped 6454 items (at 21 items/min)
2015-11-04 13:42:40 [scrapy] INFO: Crawled 6573 pages (at 17 pages/min), scraped 6476 items (at 22 items/min)
2015-11-04 13:43:52 [scrapy] INFO: Crawled 6593 pages (at 20 pages/min), scraped 6497 items (at 21 items/min)
2015-11-04 13:44:21 [scrapy] INFO: Crawled 6604 pages (at 11 pages/min), scraped 6504 items (at 7 items/min)
2015-11-04 13:45:44 [scrapy] INFO: Crawled 6621 pages (at 17 pages/min), scraped 6525 items (at 21 items/min)
2015-11-04 13:46:36 [scrapy] INFO: Crawled 6635 pages (at 14 pages/min), scraped 6539 items (at 14 items/min)
2015-11-04 13:47:28 [scrapy] INFO: Crawled 6652 pages (at 17 pages/min), scraped 6553 items (at 14 items/min)
2015-11-04 13:48:48 [scrapy] INFO: Crawled 6678 pages (at 26 pages/min), scraped 6577 items (at 24 items/min)
2015-11-04 13:49:24 [scrapy] INFO: Crawled 6691 pages (at 13 pages/min), scraped 6589 items (at 12 items/min)
2015-11-04 13:50:42 [scrapy] INFO: Crawled 6710 pages (at 19 pages/min), scraped 6614 items (at 25 items/min)
2015-11-04 13:51:31 [scrapy] INFO: Crawled 6732 pages (at 22 pages/min), scraped 6628 items (at 14 items/min)
2015-11-04 13:52:34 [scrapy] INFO: Crawled 6750 pages (at 18 pages/min), scraped 6647 items (at 19 items/min)
2015-11-04 13:53:41 [scrapy] INFO: Crawled 6768 pages (at 18 pages/min), scraped 6669 items (at 22 items/min)
2015-11-04 13:54:53 [scrapy] INFO: Crawled 6787 pages (at 19 pages/min), scraped 6691 items (at 22 items/min)
2015-11-04 13:55:51 [scrapy] INFO: Crawled 6809 pages (at 22 pages/min), scraped 6712 items (at 21 items/min)
2015-11-04 13:56:48 [scrapy] INFO: Crawled 6829 pages (at 20 pages/min), scraped 6730 items (at 18 items/min)
2015-11-04 13:57:26 [scrapy] INFO: Crawled 6832 pages (at 3 pages/min), scraped 6740 items (at 10 items/min)
2015-11-04 13:58:25 [scrapy] INFO: Crawled 6853 pages (at 21 pages/min), scraped 6759 items (at 19 items/min)
2015-11-04 13:59:33 [scrapy] INFO: Crawled 6874 pages (at 21 pages/min), scraped 6783 items (at 24 items/min)
2015-11-04 14:00:24 [scrapy] INFO: Crawled 6908 pages (at 34 pages/min), scraped 6811 items (at 28 items/min)
2015-11-04 14:01:20 [scrapy] INFO: Crawled 6936 pages (at 28 pages/min), scraped 6840 items (at 29 items/min)
2015-11-04 14:02:21 [scrapy] INFO: Crawled 6982 pages (at 46 pages/min), scraped 6887 items (at 47 items/min)
2015-11-04 14:03:21 [scrapy] INFO: Crawled 7024 pages (at 42 pages/min), scraped 6927 items (at 40 items/min)
2015-11-04 14:04:38 [scrapy] INFO: Crawled 7066 pages (at 42 pages/min), scraped 6969 items (at 42 items/min)
2015-11-04 14:05:21 [scrapy] INFO: Crawled 7090 pages (at 24 pages/min), scraped 6991 items (at 22 items/min)
2015-11-04 14:06:25 [scrapy] INFO: Crawled 7129 pages (at 39 pages/min), scraped 7020 items (at 29 items/min)
2015-11-04 14:07:39 [scrapy] INFO: Crawled 7160 pages (at 31 pages/min), scraped 7059 items (at 39 items/min)
2015-11-04 14:08:54 [scrapy] INFO: Crawled 7184 pages (at 24 pages/min), scraped 7083 items (at 24 items/min)
2015-11-04 14:10:02 [scrapy] INFO: Crawled 7208 pages (at 24 pages/min), scraped 7106 items (at 23 items/min)
2015-11-04 14:10:24 [scrapy] INFO: Crawled 7216 pages (at 8 pages/min), scraped 7115 items (at 9 items/min)
2015-11-04 14:11:31 [scrapy] INFO: Crawled 7258 pages (at 42 pages/min), scraped 7149 items (at 34 items/min)
2015-11-04 14:11:33 [scrapy] ERROR: Error downloading <GET https://jaman.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:12:26 [scrapy] INFO: Crawled 7292 pages (at 34 pages/min), scraped 7181 items (at 32 items/min)
2015-11-04 14:13:29 [scrapy] INFO: Crawled 7327 pages (at 35 pages/min), scraped 7217 items (at 36 items/min)
2015-11-04 14:14:28 [scrapy] INFO: Crawled 7356 pages (at 29 pages/min), scraped 7249 items (at 32 items/min)
2015-11-04 14:15:59 [scrapy] INFO: Crawled 7395 pages (at 39 pages/min), scraped 7287 items (at 38 items/min)
2015-11-04 14:16:21 [scrapy] INFO: Crawled 7409 pages (at 14 pages/min), scraped 7299 items (at 12 items/min)
2015-11-04 14:17:12 [scrapy] ERROR: Spider error processing <GET http://community.loggly.com/customer/portal/articles/1225909?b_id=50> (referer: https://www.loggly.com/docs/loggly-libraries-catalog/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 99, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 14:17:28 [scrapy] INFO: Crawled 7423 pages (at 14 pages/min), scraped 7317 items (at 18 items/min)
2015-11-04 14:18:28 [scrapy] INFO: Crawled 7438 pages (at 15 pages/min), scraped 7332 items (at 15 items/min)
2015-11-04 14:19:28 [scrapy] INFO: Crawled 7462 pages (at 24 pages/min), scraped 7350 items (at 18 items/min)
2015-11-04 14:20:31 [scrapy] INFO: Crawled 7484 pages (at 22 pages/min), scraped 7368 items (at 18 items/min)
2015-11-04 14:21:14 [scrapy] ERROR: Error downloading <GET http://loggly.wpengine.com/login/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 14:21:45 [scrapy] INFO: Crawled 7510 pages (at 26 pages/min), scraped 7389 items (at 21 items/min)
2015-11-04 14:22:30 [scrapy] INFO: Crawled 7532 pages (at 22 pages/min), scraped 7403 items (at 14 items/min)
2015-11-04 14:23:23 [scrapy] ERROR: Error downloading <GET https://www.thepaymentscompany.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:23:23 [scrapy] INFO: Crawled 7554 pages (at 22 pages/min), scraped 7418 items (at 15 items/min)
2015-11-04 14:24:12 [scrapy] ERROR: Error downloading <GET http://www.jc258.cn/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 14:24:52 [scrapy] INFO: Crawled 7566 pages (at 12 pages/min), scraped 7442 items (at 24 items/min)
2015-11-04 14:25:23 [scrapy] INFO: Crawled 7583 pages (at 17 pages/min), scraped 7449 items (at 7 items/min)
2015-11-04 14:26:25 [scrapy] INFO: Crawled 7590 pages (at 7 pages/min), scraped 7463 items (at 14 items/min)
2015-11-04 14:28:00 [scrapy] INFO: Crawled 7607 pages (at 17 pages/min), scraped 7478 items (at 15 items/min)
2015-11-04 14:28:34 [scrapy] ERROR: Error downloading <GET http://loggly.wpengine.com/blog/loggly-real-time-anomaly-detection-know-the-unknown/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 14:28:34 [scrapy] INFO: Crawled 7613 pages (at 6 pages/min), scraped 7490 items (at 12 items/min)
2015-11-04 14:29:54 [scrapy] INFO: Crawled 7637 pages (at 24 pages/min), scraped 7509 items (at 19 items/min)
2015-11-04 14:30:34 [scrapy] INFO: Crawled 7646 pages (at 9 pages/min), scraped 7517 items (at 8 items/min)
2015-11-04 14:31:31 [scrapy] INFO: Crawled 7657 pages (at 11 pages/min), scraped 7534 items (at 17 items/min)
2015-11-04 14:31:52 [scrapy] ERROR: Spider error processing <GET https://www.loggly.com/wp-content/uploads/2014/10/Loggly-Case-Study-Segment-R4-Web.pdf> (referer: https://www.loggly.com/customers/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 14:32:22 [scrapy] INFO: Crawled 7676 pages (at 19 pages/min), scraped 7541 items (at 7 items/min)
2015-11-04 14:33:48 [scrapy] INFO: Crawled 7693 pages (at 17 pages/min), scraped 7558 items (at 17 items/min)
2015-11-04 14:34:45 [scrapy] INFO: Crawled 7701 pages (at 8 pages/min), scraped 7575 items (at 17 items/min)
2015-11-04 14:35:35 [scrapy] INFO: Crawled 7717 pages (at 16 pages/min), scraped 7591 items (at 16 items/min)
2015-11-04 14:36:32 [scrapy] INFO: Crawled 7733 pages (at 16 pages/min), scraped 7606 items (at 15 items/min)
2015-11-04 14:37:44 [scrapy] INFO: Crawled 7749 pages (at 16 pages/min), scraped 7623 items (at 17 items/min)
2015-11-04 14:38:33 [scrapy] INFO: Crawled 7765 pages (at 16 pages/min), scraped 7639 items (at 16 items/min)
2015-11-04 14:39:24 [scrapy] INFO: Crawled 7781 pages (at 16 pages/min), scraped 7655 items (at 16 items/min)
2015-11-04 14:40:40 [scrapy] INFO: Crawled 7805 pages (at 24 pages/min), scraped 7679 items (at 24 items/min)
2015-11-04 14:41:33 [scrapy] INFO: Crawled 7821 pages (at 16 pages/min), scraped 7695 items (at 16 items/min)
2015-11-04 14:42:22 [scrapy] INFO: Crawled 7837 pages (at 16 pages/min), scraped 7711 items (at 16 items/min)
2015-11-04 14:43:37 [scrapy] INFO: Crawled 7861 pages (at 24 pages/min), scraped 7735 items (at 24 items/min)
2015-11-04 14:44:41 [scrapy] INFO: Crawled 7877 pages (at 16 pages/min), scraped 7751 items (at 16 items/min)
2015-11-04 14:45:34 [scrapy] INFO: Crawled 7893 pages (at 16 pages/min), scraped 7767 items (at 16 items/min)
2015-11-04 14:46:30 [scrapy] INFO: Crawled 7909 pages (at 16 pages/min), scraped 7783 items (at 16 items/min)
2015-11-04 14:47:21 [scrapy] INFO: Crawled 7925 pages (at 16 pages/min), scraped 7798 items (at 15 items/min)
2015-11-04 14:48:20 [scrapy] INFO: Crawled 7941 pages (at 16 pages/min), scraped 7815 items (at 17 items/min)
2015-11-04 14:49:27 [scrapy] INFO: Crawled 7957 pages (at 16 pages/min), scraped 7831 items (at 16 items/min)
2015-11-04 14:50:23 [scrapy] INFO: Crawled 7973 pages (at 16 pages/min), scraped 7847 items (at 16 items/min)
2015-11-04 14:51:22 [scrapy] INFO: Crawled 7989 pages (at 16 pages/min), scraped 7863 items (at 16 items/min)
2015-11-04 14:52:41 [scrapy] INFO: Crawled 8013 pages (at 24 pages/min), scraped 7887 items (at 24 items/min)
2015-11-04 14:53:35 [scrapy] INFO: Crawled 8029 pages (at 16 pages/min), scraped 7903 items (at 16 items/min)
2015-11-04 14:54:31 [scrapy] INFO: Crawled 8045 pages (at 16 pages/min), scraped 7919 items (at 16 items/min)
2015-11-04 14:55:23 [scrapy] INFO: Crawled 8061 pages (at 16 pages/min), scraped 7935 items (at 16 items/min)
2015-11-04 14:56:57 [scrapy] INFO: Crawled 8083 pages (at 22 pages/min), scraped 7957 items (at 22 items/min)
2015-11-04 14:58:09 [scrapy] INFO: Crawled 8093 pages (at 10 pages/min), scraped 7967 items (at 10 items/min)
2015-11-04 14:58:35 [scrapy] INFO: Crawled 8101 pages (at 8 pages/min), scraped 7975 items (at 8 items/min)
2015-11-04 14:59:45 [scrapy] INFO: Crawled 8117 pages (at 16 pages/min), scraped 7991 items (at 16 items/min)
2015-11-04 15:00:39 [scrapy] INFO: Crawled 8133 pages (at 16 pages/min), scraped 8007 items (at 16 items/min)
2015-11-04 15:01:30 [scrapy] INFO: Crawled 8152 pages (at 19 pages/min), scraped 8023 items (at 16 items/min)
2015-11-04 15:02:52 [scrapy] INFO: Crawled 8173 pages (at 21 pages/min), scraped 8047 items (at 24 items/min)
2015-11-04 15:03:42 [scrapy] INFO: Crawled 8189 pages (at 16 pages/min), scraped 8063 items (at 16 items/min)
2015-11-04 15:04:32 [scrapy] INFO: Crawled 8208 pages (at 19 pages/min), scraped 8079 items (at 16 items/min)
2015-11-04 15:05:23 [scrapy] INFO: Crawled 8224 pages (at 16 pages/min), scraped 8095 items (at 16 items/min)
2015-11-04 15:06:40 [scrapy] INFO: Crawled 8245 pages (at 21 pages/min), scraped 8119 items (at 24 items/min)
2015-11-04 15:07:44 [scrapy] INFO: Crawled 8261 pages (at 16 pages/min), scraped 8135 items (at 16 items/min)
2015-11-04 15:08:40 [scrapy] INFO: Crawled 8276 pages (at 15 pages/min), scraped 8151 items (at 16 items/min)
2015-11-04 15:09:50 [scrapy] INFO: Crawled 8290 pages (at 14 pages/min), scraped 8164 items (at 13 items/min)
2015-11-04 15:10:43 [scrapy] INFO: Crawled 8306 pages (at 16 pages/min), scraped 8180 items (at 16 items/min)
2015-11-04 15:11:28 [scrapy] INFO: Crawled 8326 pages (at 20 pages/min), scraped 8196 items (at 16 items/min)
2015-11-04 15:12:43 [scrapy] INFO: Crawled 8346 pages (at 20 pages/min), scraped 8220 items (at 24 items/min)
2015-11-04 15:13:34 [scrapy] INFO: Crawled 8362 pages (at 16 pages/min), scraped 8236 items (at 16 items/min)
2015-11-04 15:14:43 [scrapy] INFO: Crawled 8378 pages (at 16 pages/min), scraped 8252 items (at 16 items/min)
2015-11-04 15:15:38 [scrapy] INFO: Crawled 8394 pages (at 16 pages/min), scraped 8268 items (at 16 items/min)
2015-11-04 15:16:29 [scrapy] INFO: Crawled 8413 pages (at 19 pages/min), scraped 8284 items (at 16 items/min)
2015-11-04 15:17:28 [scrapy] INFO: Crawled 8428 pages (at 15 pages/min), scraped 8300 items (at 16 items/min)
2015-11-04 15:18:21 [scrapy] INFO: Crawled 8444 pages (at 16 pages/min), scraped 8315 items (at 15 items/min)
2015-11-04 15:19:47 [scrapy] INFO: Crawled 8463 pages (at 19 pages/min), scraped 8337 items (at 22 items/min)
2015-11-04 15:20:40 [scrapy] INFO: Crawled 8479 pages (at 16 pages/min), scraped 8353 items (at 16 items/min)
2015-11-04 15:21:35 [scrapy] INFO: Crawled 8493 pages (at 14 pages/min), scraped 8369 items (at 16 items/min)
2015-11-04 15:22:44 [scrapy] INFO: Crawled 8509 pages (at 16 pages/min), scraped 8383 items (at 14 items/min)
2015-11-04 15:23:35 [scrapy] INFO: Crawled 8527 pages (at 18 pages/min), scraped 8398 items (at 15 items/min)
2015-11-04 15:24:26 [scrapy] INFO: Crawled 8543 pages (at 16 pages/min), scraped 8414 items (at 16 items/min)
2015-11-04 15:25:39 [scrapy] INFO: Crawled 8562 pages (at 19 pages/min), scraped 8436 items (at 22 items/min)
2015-11-04 15:26:32 [scrapy] INFO: Crawled 8578 pages (at 16 pages/min), scraped 8452 items (at 16 items/min)
2015-11-04 15:27:26 [scrapy] INFO: Crawled 8597 pages (at 19 pages/min), scraped 8468 items (at 16 items/min)
2015-11-04 15:28:30 [scrapy] INFO: Crawled 8612 pages (at 15 pages/min), scraped 8484 items (at 16 items/min)
2015-11-04 15:29:41 [scrapy] INFO: Crawled 8628 pages (at 16 pages/min), scraped 8499 items (at 15 items/min)
2015-11-04 15:30:32 [scrapy] INFO: Crawled 8644 pages (at 16 pages/min), scraped 8515 items (at 16 items/min)
2015-11-04 15:31:24 [scrapy] INFO: Crawled 8659 pages (at 15 pages/min), scraped 8531 items (at 16 items/min)
2015-11-04 15:32:36 [scrapy] INFO: Crawled 8673 pages (at 14 pages/min), scraped 8545 items (at 14 items/min)
2015-11-04 15:33:22 [scrapy] INFO: Crawled 8681 pages (at 8 pages/min), scraped 8553 items (at 8 items/min)
2015-11-04 15:34:41 [scrapy] INFO: Crawled 8702 pages (at 21 pages/min), scraped 8576 items (at 23 items/min)
2015-11-04 15:35:33 [scrapy] INFO: Crawled 8721 pages (at 19 pages/min), scraped 8592 items (at 16 items/min)
2015-11-04 15:36:30 [scrapy] INFO: Crawled 8737 pages (at 16 pages/min), scraped 8608 items (at 16 items/min)
2015-11-04 15:37:24 [scrapy] INFO: Crawled 8753 pages (at 16 pages/min), scraped 8623 items (at 15 items/min)
2015-11-04 15:38:24 [scrapy] INFO: Crawled 8769 pages (at 16 pages/min), scraped 8639 items (at 16 items/min)
2015-11-04 15:39:41 [scrapy] INFO: Crawled 8788 pages (at 19 pages/min), scraped 8663 items (at 24 items/min)
2015-11-04 15:40:50 [scrapy] INFO: Crawled 8804 pages (at 16 pages/min), scraped 8678 items (at 15 items/min)
2015-11-04 15:41:57 [scrapy] INFO: Crawled 8819 pages (at 15 pages/min), scraped 8693 items (at 15 items/min)
2015-11-04 15:42:37 [scrapy] INFO: Crawled 8827 pages (at 8 pages/min), scraped 8701 items (at 8 items/min)
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:58 [scrapy] INFO: Closing spider (shutdown)
arseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 10:09:35 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7effc74c79b0>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 10:09:42 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:10:31 [scrapy] INFO: Crawled 172 pages (at 172 pages/min), scraped 90 items (at 90 items/min)
2015-11-04 10:11:31 [scrapy] INFO: Crawled 172 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2015-11-04 10:12:31 [scrapy] INFO: Crawled 172 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2015-11-04 10:12:40 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:13:31 [scrapy] INFO: Crawled 172 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2015-11-04 10:14:31 [scrapy] INFO: Crawled 172 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2015-11-04 10:15:31 [scrapy] INFO: Crawled 172 pages (at 0 pages/min), scraped 90 items (at 0 items/min)
2015-11-04 10:15:53 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:53 [scrapy] ERROR: Error downloading <GET http://www.adelphi-europe.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:53 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:53 [scrapy] ERROR: Error downloading <GET http://www.harvestmanagement.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:53 [scrapy] ERROR: Error downloading <GET http://www.quintanacapitalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:54 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:55 [scrapy] ERROR: Error downloading <GET http://www.altacomm.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:56 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:15:56 [scrapy] INFO: Closing spider (finished)
2015-11-04 10:15:56 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 57,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 24,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 24,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/request_bytes': 69360,
 'downloader/request_count': 274,
 'downloader/request_method_count/GET': 274,
 'downloader/response_bytes': 1050444,
 'downloader/response_count': 217,
 'downloader/response_status_count/200': 167,
 'downloader/response_status_count/301': 18,
 'downloader/response_status_count/302': 19,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 3,
 'downloader/response_status_count/500': 3,
 'dupefilter/filtered': 273,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 10, 15, 56, 157653),
 'item_scraped_count': 90,
 'log_count/CRITICAL': 3,
 'log_count/ERROR': 19,
 'log_count/INFO': 13,
 'offsite/domains': 77,
 'offsite/filtered': 335,
 'request_depth_max': 2,
 'response_received_count': 172,
 'scheduler/dequeued': 274,
 'scheduler/dequeued/memory': 274,
 'scheduler/enqueued': 274,
 'scheduler/enqueued/memory': 274,
 'start_time': datetime.datetime(2015, 11, 4, 10, 9, 31, 850242)}
2015-11-04 10:15:56 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 10:16:58 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 10:16:58 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 10:16:58 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 10:16:58 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 10:16:58 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 10:16:58 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 10:16:58 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 10:16:58 [scrapy] INFO: Spider opened
2015-11-04 10:16:58 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 10:16:58 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:16:58 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 10:16:58 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 10:16:58 [scrapy] ERROR: Error downloading <GET http://www.bpc>: DNS lookup failed: address 'www.bpc' not found: [Errno -2] Name or service not known.
2015-11-04 10:16:58 [scrapy] ERROR: Error downloading <GET http://emergingcapitalmarket.com>: DNS lookup failed: address 'emergingcapitalmarket.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:16:59 [scrapy] ERROR: Error downloading <GET http://www.cshg.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:16:59 [scrapy] ERROR: Error downloading <GET https://www.magnitudecapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]
2015-11-04 10:17:01 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:17:07 [scrapy] ERROR: Error downloading <GET http://www.meridianfunds.com>: DNS lookup failed: address 'www.meridianfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:12 [scrapy] ERROR: Error downloading <GET http://www.polunin.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 10:17:12 [scrapy] ERROR: Error downloading <GET http://www.harvpartners.com>: DNS lookup failed: address 'www.harvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:13 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET http://www.tet>: DNS lookup failed: address 'www.tet' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET http://www.mountainpacificadvisors.com>: DNS lookup failed: address 'www.mountainpacificadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET http://www.clairvuecapital.com>: DNS lookup failed: address 'www.clairvuecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET http://www.portal.krfs.com>: DNS lookup failed: address 'www.portal.krfs.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:14 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:17:59 [scrapy] INFO: Crawled 253 pages (at 253 pages/min), scraped 159 items (at 159 items/min)
2015-11-04 10:18:58 [scrapy] INFO: Crawled 273 pages (at 20 pages/min), scraped 185 items (at 26 items/min)
2015-11-04 10:19:58 [scrapy] INFO: Crawled 273 pages (at 0 pages/min), scraped 185 items (at 0 items/min)
2015-11-04 10:20:58 [scrapy] INFO: Crawled 273 pages (at 0 pages/min), scraped 185 items (at 0 items/min)
2015-11-04 10:21:31 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 10:21:58 [scrapy] INFO: Crawled 273 pages (at 0 pages/min), scraped 185 items (at 0 items/min)
2015-11-04 10:22:58 [scrapy] INFO: Crawled 273 pages (at 0 pages/min), scraped 185 items (at 0 items/min)
2015-11-04 10:23:20 [scrapy] ERROR: Error downloading <GET http://www.adelphi-europe.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:23:23 [scrapy] ERROR: Error downloading <GET http://www.cornwallcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:23:23 [scrapy] INFO: Closing spider (finished)
2015-11-04 10:23:23 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 98,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 6,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 39,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 8,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 39,
 'downloader/request_bytes': 113796,
 'downloader/request_count': 420,
 'downloader/request_method_count/GET': 420,
 'downloader/response_bytes': 1739690,
 'downloader/response_count': 322,
 'downloader/response_status_count/200': 260,
 'downloader/response_status_count/301': 14,
 'downloader/response_status_count/302': 28,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 4,
 'downloader/response_status_count/404': 7,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 750,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 10, 23, 23, 774028),
 'item_scraped_count': 185,
 'log_count/ERROR': 32,
 'log_count/INFO': 13,
 'offsite/domains': 288,
 'offsite/filtered': 750,
 'request_depth_max': 2,
 'response_received_count': 273,
 'scheduler/dequeued': 420,
 'scheduler/dequeued/memory': 420,
 'scheduler/enqueued': 420,
 'scheduler/enqueued/memory': 420,
 'start_time': datetime.datetime(2015, 11, 4, 10, 16, 58, 403761)}
2015-11-04 10:23:23 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 10:24:25 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 10:24:25 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 10:24:25 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 10:24:26 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 10:24:26 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 10:24:26 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 10:24:26 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 10:24:26 [scrapy] INFO: Spider opened
2015-11-04 10:24:26 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.investor.pccpllc.amiesdigital.com>: DNS lookup failed: address 'www.investor.pccpllc.amiesdigital.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.alphatitans.com>: DNS lookup failed: address 'www.alphatitans.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.lp.lcpartners.com>: DNS lookup failed: address 'www.lp.lcpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.lineagecapital.com>: DNS lookup failed: address 'www.lineagecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.acc>: DNS lookup failed: address 'www.acc' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.adamshillpartners.com>: DNS lookup failed: address 'www.adamshillpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.nom>: DNS lookup failed: address 'www.nom' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:26 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:30 [scrapy] ERROR: Error downloading <GET http://emergingcapitalmarket.com>: DNS lookup failed: address 'emergingcapitalmarket.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:30 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:24:30 [scrapy] ERROR: Error downloading <GET http://www.sandsbros.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:24:30 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:24:31 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:24:33 [scrapy] ERROR: Error downloading <GET http://www.cshg.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:24:33 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:25:27 [scrapy] INFO: Crawled 176 pages (at 176 pages/min), scraped 91 items (at 91 items/min)
2015-11-04 10:26:46 [scrapy] INFO: Crawled 209 pages (at 33 pages/min), scraped 120 items (at 29 items/min)
2015-11-04 10:27:31 [scrapy] INFO: Crawled 209 pages (at 0 pages/min), scraped 136 items (at 16 items/min)
2015-11-04 10:28:44 [scrapy] INFO: Crawled 262 pages (at 53 pages/min), scraped 181 items (at 45 items/min)
2015-11-04 10:28:53 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:29:22 [scrapy] ERROR: Error downloading <GET http://www.arg>: DNS lookup failed: address 'www.arg' not found: [Errno -2] Name or service not known.
2015-11-04 10:29:27 [scrapy] INFO: Crawled 337 pages (at 75 pages/min), scraped 254 items (at 73 items/min)
2015-11-04 10:30:26 [scrapy] INFO: Crawled 411 pages (at 74 pages/min), scraped 336 items (at 82 items/min)
2015-11-04 10:31:24 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:31:26 [scrapy] INFO: Crawled 411 pages (at 0 pages/min), scraped 336 items (at 0 items/min)
2015-11-04 10:32:26 [scrapy] INFO: Crawled 411 pages (at 0 pages/min), scraped 336 items (at 0 items/min)
2015-11-04 10:33:26 [scrapy] INFO: Crawled 411 pages (at 0 pages/min), scraped 336 items (at 0 items/min)
2015-11-04 10:33:30 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:33:30 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:33:30 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:33:30 [scrapy] ERROR: Error downloading <GET http://www.constellationcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:33:30 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:33:30 [scrapy] INFO: Closing spider (finished)
2015-11-04 10:33:30 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 103,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 63,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 15,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 12,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 7,
 'downloader/request_bytes': 213444,
 'downloader/request_count': 575,
 'downloader/request_method_count/GET': 575,
 'downloader/response_bytes': 6003757,
 'downloader/response_count': 472,
 'downloader/response_status_count/200': 403,
 'downloader/response_status_count/301': 20,
 'downloader/response_status_count/302': 38,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 7,
 'dupefilter/filtered': 487,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 10, 33, 30, 879773),
 'item_scraped_count': 336,
 'log_count/ERROR': 33,
 'log_count/INFO': 16,
 'offsite/domains': 91,
 'offsite/filtered': 533,
 'request_depth_max': 2,
 'response_received_count': 411,
 'scheduler/dequeued': 575,
 'scheduler/dequeued/memory': 575,
 'scheduler/enqueued': 575,
 'scheduler/enqueued/memory': 575,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 10, 24, 26, 325423)}
2015-11-04 10:33:30 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 10:34:32 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 10:34:32 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 10:34:32 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 10:34:32 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 10:34:32 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 10:34:32 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 10:34:33 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 10:34:33 [scrapy] INFO: Spider opened
2015-11-04 10:34:33 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.phi>: DNS lookup failed: address 'www.phi' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.investor.pccpllc.amiesdigital.com>: DNS lookup failed: address 'www.investor.pccpllc.amiesdigital.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.coo>: DNS lookup failed: address 'www.coo' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.mad>: DNS lookup failed: address 'www.mad' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:33 [scrapy] ERROR: Error downloading <GET http://www.gim>: DNS lookup failed: address 'www.gim' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:34 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:34 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:34 [scrapy] ERROR: Error downloading <GET http://www.cmsco.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 10:34:35 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:38 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:39 [scrapy] ERROR: Error downloading <GET http://www.aim13.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:34:41 [scrapy] ERROR: Error downloading <GET http://www.sandsbros.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:34:42 [scrapy] ERROR: Error downloading <GET http://www.portal.krfs.com>: DNS lookup failed: address 'www.portal.krfs.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:34:44 [scrapy] ERROR: Error downloading <GET http://www.lar>: DNS lookup failed: address 'www.lar' not found: [Errno -2] Name or service not known.
2015-11-04 10:35:30 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 10:35:30 [scrapy] ERROR: Error downloading <GET http://www.tet>: DNS lookup failed: address 'www.tet' not found: [Errno -2] Name or service not known.
2015-11-04 10:35:33 [scrapy] INFO: Crawled 158 pages (at 158 pages/min), scraped 79 items (at 79 items/min)
2015-11-04 10:36:33 [scrapy] INFO: Crawled 160 pages (at 2 pages/min), scraped 81 items (at 2 items/min)
2015-11-04 10:37:33 [scrapy] INFO: Crawled 160 pages (at 0 pages/min), scraped 81 items (at 0 items/min)
2015-11-04 10:37:43 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:38:33 [scrapy] INFO: Crawled 160 pages (at 0 pages/min), scraped 81 items (at 0 items/min)
2015-11-04 10:39:12 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:39:33 [scrapy] INFO: Crawled 160 pages (at 0 pages/min), scraped 81 items (at 0 items/min)
2015-11-04 10:40:33 [scrapy] INFO: Crawled 160 pages (at 0 pages/min), scraped 81 items (at 0 items/min)
2015-11-04 10:40:55 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:41:06 [scrapy] ERROR: Error downloading <GET http://www.oldmutualus.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:41:12 [scrapy] ERROR: Error downloading <GET http://www.quintanacapitalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:41:33 [scrapy] INFO: Crawled 160 pages (at 0 pages/min), scraped 81 items (at 0 items/min)
2015-11-04 10:41:52 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:41:52 [scrapy] INFO: Closing spider (finished)
2015-11-04 10:41:52 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 76,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 4,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 48,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 14,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 66227,
 'downloader/request_count': 271,
 'downloader/request_method_count/GET': 271,
 'downloader/response_bytes': 1073514,
 'downloader/response_count': 195,
 'downloader/response_status_count/200': 152,
 'downloader/response_status_count/301': 11,
 'downloader/response_status_count/302': 18,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 3,
 'downloader/response_status_count/404': 6,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 252,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 10, 41, 52, 509694),
 'item_scraped_count': 81,
 'log_count/ERROR': 25,
 'log_count/INFO': 14,
 'offsite/domains': 62,
 'offsite/filtered': 360,
 'request_depth_max': 2,
 'response_received_count': 160,
 'scheduler/dequeued': 271,
 'scheduler/dequeued/memory': 271,
 'scheduler/enqueued': 271,
 'scheduler/enqueued/memory': 271,
 'start_time': datetime.datetime(2015, 11, 4, 10, 34, 33, 120057)}
2015-11-04 10:41:52 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 10:42:54 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 10:42:54 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 10:42:54 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 10:42:54 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 10:42:54 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 10:42:54 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 10:42:54 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 10:42:54 [scrapy] INFO: Spider opened
2015-11-04 10:42:54 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 10:42:55 [scrapy] ERROR: Error downloading <GET http://www.torshencapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 10:42:56 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:56 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:56 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:56 [scrapy] ERROR: Error downloading <GET http://www.coo>: DNS lookup failed: address 'www.coo' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:56 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.cfm>: DNS lookup failed: address 'www.cfm' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.sco>: DNS lookup failed: address 'www.sco' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.vnc>: DNS lookup failed: address 'www.vnc' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.gra>: DNS lookup failed: address 'www.gra' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.sec>: DNS lookup failed: address 'www.sec' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.woodbinecapital.com>: DNS lookup failed: address 'www.woodbinecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.careers.weissasset.com>: DNS lookup failed: address 'www.careers.weissasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:57 [scrapy] ERROR: Error downloading <GET http://www.key>: DNS lookup failed: address 'www.key' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:58 [scrapy] ERROR: Error downloading <GET http://www.nom>: DNS lookup failed: address 'www.nom' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:58 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 10:42:58 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 10:46:09 [scrapy] INFO: Crawled 158 pages (at 158 pages/min), scraped 41 items (at 41 items/min)
2015-11-04 10:50:11 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.preludecapital.com took longer than 180.0 seconds..
2015-11-04 10:50:11 [scrapy] INFO: Crawled 158 pages (at 0 pages/min), scraped 52 items (at 11 items/min)
2015-11-04 10:54:20 [scrapy] ERROR: Error downloading <GET http://www.gbcredit.com/%20http://gordonbrothers.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.gbcredit.com/%20http://gordonbrothers.com/ took longer than 180.0 seconds..
2015-11-04 10:54:20 [scrapy] INFO: Crawled 158 pages (at 0 pages/min), scraped 76 items (at 24 items/min)
2015-11-04 10:54:57 [scrapy] INFO: Crawled 268 pages (at 110 pages/min), scraped 166 items (at 90 items/min)
2015-11-04 10:55:58 [scrapy] INFO: Crawled 392 pages (at 124 pages/min), scraped 293 items (at 127 items/min)
2015-11-04 10:56:54 [scrapy] INFO: Crawled 445 pages (at 53 pages/min), scraped 354 items (at 61 items/min)
2015-11-04 10:57:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 10:57:55 [scrapy] ERROR: Error downloading <GET http://www.feplp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:57:55 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:57:59 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:58:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 10:59:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 11:00:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 11:01:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 11:02:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 11:03:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 11:04:54 [scrapy] INFO: Crawled 445 pages (at 0 pages/min), scraped 354 items (at 0 items/min)
2015-11-04 11:04:57 [scrapy] ERROR: Error downloading <GET http://madisonint.com/de/our-business/recapitalizations/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 11:04:57 [scrapy] ERROR: Error downloading <GET http://madisonint.com/de/limited-partners/about-our-limited-partners/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 11:14:30 [scrapy] INFO: Crawled 470 pages (at 25 pages/min), scraped 379 items (at 25 items/min)
2015-11-04 11:14:31 [scrapy] INFO: Closing spider (finished)
2015-11-04 11:14:31 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 163,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 12,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 58,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 57,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 24,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 10,
 'downloader/request_bytes': 250540,
 'downloader/request_count': 691,
 'downloader/request_method_count/GET': 691,
 'downloader/response_bytes': 6373838,
 'downloader/response_count': 528,
 'downloader/response_status_count/200': 454,
 'downloader/response_status_count/301': 29,
 'downloader/response_status_count/302': 23,
 'downloader/response_status_count/400': 5,
 'downloader/response_status_count/401': 4,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 11,
 'dupefilter/filtered': 1214,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 11, 14, 31, 66522),
 'item_scraped_count': 379,
 'log_count/ERROR': 27,
 'log_count/INFO': 22,
 'offsite/domains': 191,
 'offsite/filtered': 672,
 'request_depth_max': 2,
 'response_received_count': 470,
 'scheduler/dequeued': 691,
 'scheduler/dequeued/memory': 691,
 'scheduler/enqueued': 691,
 'scheduler/enqueued/memory': 691,
 'start_time': datetime.datetime(2015, 11, 4, 10, 42, 54, 913157)}
2015-11-04 11:14:31 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 11:15:32 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 11:15:32 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 11:15:32 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 11:15:33 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 11:15:33 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 11:15:33 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 11:15:33 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 11:15:33 [scrapy] INFO: Spider opened
2015-11-04 11:15:33 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.jrc>: DNS lookup failed: address 'www.jrc' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.alphatitans.com>: DNS lookup failed: address 'www.alphatitans.com' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.wellfieldpartners.com>: DNS lookup failed: address 'www.wellfieldpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.wsc>: DNS lookup failed: address 'www.wsc' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.tit>: DNS lookup failed: address 'www.tit' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.tia>: DNS lookup failed: address 'www.tia' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.dis>: DNS lookup failed: address 'www.dis' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:33 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:34 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:34 [scrapy] ERROR: Error downloading <GET http://www.cor>: DNS lookup failed: address 'www.cor' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:34 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 11:15:38 [scrapy] ERROR: Error downloading <GET http://www.sandsbros.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 11:16:43 [scrapy] INFO: Crawled 183 pages (at 183 pages/min), scraped 99 items (at 99 items/min)
2015-11-04 11:17:09 [scrapy] ERROR: Spider error processing <GET http://assets.legal.web.com/TermsOfUse.pdf> (referer: http://www.networksolutions.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:17:13 [scrapy] ERROR: Spider error processing <GET http://assets.legal.web.com/PrivacyPolicy.pdf> (referer: http://www.networksolutions.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:17:29 [scrapy] ERROR: Error downloading <GET http://www.encorecm.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:17:45 [scrapy] INFO: Crawled 234 pages (at 51 pages/min), scraped 156 items (at 57 items/min)
2015-11-04 11:18:19 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:18:34 [scrapy] INFO: Crawled 271 pages (at 37 pages/min), scraped 186 items (at 30 items/min)
2015-11-04 11:18:58 [scrapy] ERROR: Error downloading <GET http://patch.com>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:19:26 [scrapy] ERROR: Error downloading <GET https://www.invesco.com/portal/site/us/psgateway>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:19:38 [scrapy] INFO: Crawled 304 pages (at 33 pages/min), scraped 218 items (at 32 items/min)
2015-11-04 11:20:36 [scrapy] INFO: Crawled 334 pages (at 30 pages/min), scraped 253 items (at 35 items/min)
2015-11-04 11:21:40 [scrapy] INFO: Crawled 394 pages (at 60 pages/min), scraped 305 items (at 52 items/min)
2015-11-04 11:22:47 [scrapy] INFO: Crawled 450 pages (at 56 pages/min), scraped 361 items (at 56 items/min)
2015-11-04 11:23:49 [scrapy] INFO: Crawled 511 pages (at 61 pages/min), scraped 420 items (at 59 items/min)
2015-11-04 11:24:51 [scrapy] INFO: Crawled 549 pages (at 38 pages/min), scraped 459 items (at 39 items/min)
2015-11-04 11:25:34 [scrapy] INFO: Crawled 590 pages (at 41 pages/min), scraped 496 items (at 37 items/min)
2015-11-04 11:26:42 [scrapy] INFO: Crawled 644 pages (at 54 pages/min), scraped 556 items (at 60 items/min)
2015-11-04 11:27:03 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/open-letter-shareholders.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:27:39 [scrapy] INFO: Crawled 717 pages (at 73 pages/min), scraped 610 items (at 54 items/min)
2015-11-04 11:27:46 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY11-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:28:35 [scrapy] INFO: Crawled 773 pages (at 56 pages/min), scraped 673 items (at 63 items/min)
2015-11-04 11:29:36 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Prelim-release-Q1FY12.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:29:47 [scrapy] INFO: Crawled 871 pages (at 98 pages/min), scraped 760 items (at 87 items/min)
2015-11-04 11:30:36 [scrapy] INFO: Crawled 906 pages (at 35 pages/min), scraped 810 items (at 50 items/min)
2015-11-04 11:30:40 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY12-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:30:55 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:31:25 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:31:37 [scrapy] INFO: Crawled 971 pages (at 65 pages/min), scraped 874 items (at 64 items/min)
2015-11-04 11:32:34 [scrapy] INFO: Crawled 1044 pages (at 73 pages/min), scraped 945 items (at 71 items/min)
2015-11-04 11:33:45 [scrapy] INFO: Crawled 1102 pages (at 58 pages/min), scraped 1010 items (at 65 items/min)
2015-11-04 11:33:51 [scrapy] ERROR: Error downloading <GET https://www.codasip.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:34:34 [scrapy] INFO: Crawled 1181 pages (at 79 pages/min), scraped 1087 items (at 77 items/min)
2015-11-04 11:35:46 [scrapy] INFO: Crawled 1273 pages (at 92 pages/min), scraped 1177 items (at 90 items/min)
2015-11-04 11:36:37 [scrapy] INFO: Crawled 1312 pages (at 39 pages/min), scraped 1221 items (at 44 items/min)
2015-11-04 11:37:01 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:37:02 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:37:37 [scrapy] INFO: Crawled 1388 pages (at 76 pages/min), scraped 1283 items (at 62 items/min)
2015-11-04 11:38:26 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:38:36 [scrapy] INFO: Crawled 1444 pages (at 56 pages/min), scraped 1344 items (at 61 items/min)
2015-11-04 11:39:08 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/misc/company/news/embedded-extends-support-development-boards.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:39:41 [scrapy] INFO: Crawled 1509 pages (at 65 pages/min), scraped 1402 items (at 58 items/min)
2015-11-04 11:40:21 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:40:27 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:40:31 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:40:35 [scrapy] INFO: Crawled 1565 pages (at 56 pages/min), scraped 1466 items (at 64 items/min)
2015-11-04 11:41:30 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:41:34 [scrapy] INFO: Crawled 1659 pages (at 94 pages/min), scraped 1546 items (at 80 items/min)
2015-11-04 11:42:20 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:42:48 [scrapy] INFO: Crawled 1746 pages (at 87 pages/min), scraped 1631 items (at 85 items/min)
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 11:42:58 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f4b8fca6488>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 11:43:09 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:43:51 [scrapy] INFO: Crawled 1828 pages (at 82 pages/min), scraped 1707 items (at 76 items/min)
2015-11-04 11:44:40 [scrapy] INFO: Crawled 1870 pages (at 42 pages/min), scraped 1749 items (at 42 items/min)
2015-11-04 11:45:23 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:45:31 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:45:48 [scrapy] INFO: Crawled 1923 pages (at 53 pages/min), scraped 1794 items (at 45 items/min)
2015-11-04 11:45:49 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:45:49 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:45:59 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:45 [scrapy] INFO: Crawled 1961 pages (at 38 pages/min), scraped 1830 items (at 36 items/min)
2015-11-04 11:46:47 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2016-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:47 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2016-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:47:41 [scrapy] INFO: Crawled 1992 pages (at 31 pages/min), scraped 1867 items (at 37 items/min)
2015-11-04 11:48:36 [scrapy] INFO: Crawled 2062 pages (at 70 pages/min), scraped 1920 items (at 53 items/min)
2015-11-04 11:48:36 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/questa-formal-Based-technology-ds.pdf> (referer: https://www.mentor.com/products/fv/questa-secure-check)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:49:15 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/presentation/products/fv/questa-covercheck/adv-verification-mgmt-coverage-closure.pdf> (referer: https://www.mentor.com/products/fv/questa-covercheck/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:49:43 [scrapy] INFO: Crawled 2138 pages (at 76 pages/min), scraped 1995 items (at 75 items/min)
2015-11-04 11:50:39 [scrapy] INFO: Crawled 2190 pages (at 52 pages/min), scraped 2058 items (at 63 items/min)
2015-11-04 11:50:52 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/transforming-verification.pdf> (referer: https://www.mentor.com/products/fv/questa-secure-check)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:51:07 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/channel/UCr7fIBWawPC6KcPJ0c9VAEA>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:51:34 [scrapy] INFO: Crawled 2296 pages (at 106 pages/min), scraped 2139 items (at 81 items/min)
2015-11-04 11:52:37 [scrapy] INFO: Crawled 2369 pages (at 73 pages/min), scraped 2224 items (at 85 items/min)
2015-11-04 11:53:34 [scrapy] INFO: Crawled 2435 pages (at 66 pages/min), scraped 2298 items (at 74 items/min)
2015-11-04 11:54:35 [scrapy] INFO: Crawled 2497 pages (at 62 pages/min), scraped 2354 items (at 56 items/min)
2015-11-04 11:55:40 [scrapy] INFO: Crawled 2545 pages (at 48 pages/min), scraped 2402 items (at 48 items/min)
2015-11-04 11:56:36 [scrapy] INFO: Crawled 2592 pages (at 47 pages/min), scraped 2452 items (at 50 items/min)
2015-11-04 11:57:44 [scrapy] INFO: Crawled 2707 pages (at 115 pages/min), scraped 2550 items (at 98 items/min)
2015-11-04 11:58:03 [scrapy] ERROR: Spider error processing <GET http://investments.voya.com/idc/idcplg?IdcService=GET_FILE&Rendition=Primary&RevisionSelectionMethod=LatestReleased&dDocName=UCMDEV_062188> (referer: http://investments.voya.com/Company/Legal/Privacy-Notice/index.htm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:58:39 [scrapy] INFO: Crawled 2791 pages (at 84 pages/min), scraped 2626 items (at 76 items/min)
2015-11-04 11:58:46 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39827/17-spot.pdf> (referer: http://www.berkshire-group.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:59:38 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39764/berkshireraises-1615mfornewstrategy.pdf> (referer: http://www.berkshire-group.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:59:39 [scrapy] INFO: Crawled 2848 pages (at 57 pages/min), scraped 2688 items (at 62 items/min)
2015-11-04 12:00:39 [scrapy] INFO: Crawled 2882 pages (at 34 pages/min), scraped 2730 items (at 42 items/min)
2015-11-04 12:01:40 [scrapy] INFO: Crawled 2946 pages (at 64 pages/min), scraped 2782 items (at 52 items/min)
2015-11-04 12:02:45 [scrapy] INFO: Crawled 2996 pages (at 50 pages/min), scraped 2838 items (at 56 items/min)
2015-11-04 12:03:45 [scrapy] INFO: Crawled 3040 pages (at 44 pages/min), scraped 2883 items (at 45 items/min)
2015-11-04 12:04:36 [scrapy] INFO: Crawled 3083 pages (at 43 pages/min), scraped 2917 items (at 34 items/min)
2015-11-04 12:05:33 [scrapy] INFO: Crawled 3118 pages (at 35 pages/min), scraped 2959 items (at 42 items/min)
2015-11-04 12:06:48 [scrapy] INFO: Crawled 3171 pages (at 53 pages/min), scraped 3009 items (at 50 items/min)
2015-11-04 12:07:34 [scrapy] INFO: Crawled 3202 pages (at 31 pages/min), scraped 3042 items (at 33 items/min)
2015-11-04 12:08:22 [scrapy] ERROR: Error downloading <GET http://www.berkshire-group.com/media/39800/berkshireinter_am0615_2.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.berkshire-group.com/media/39800/berkshireinter_am0615_2.pdf took longer than 180.0 seconds..
2015-11-04 12:08:34 [scrapy] INFO: Crawled 3242 pages (at 40 pages/min), scraped 3084 items (at 42 items/min)
2015-11-04 12:09:39 [scrapy] INFO: Crawled 3289 pages (at 47 pages/min), scraped 3125 items (at 41 items/min)
2015-11-04 12:10:42 [scrapy] INFO: Crawled 3324 pages (at 35 pages/min), scraped 3164 items (at 39 items/min)
2015-11-04 12:11:45 [scrapy] INFO: Crawled 3390 pages (at 66 pages/min), scraped 3205 items (at 41 items/min)
2015-11-04 12:12:42 [scrapy] INFO: Crawled 3415 pages (at 25 pages/min), scraped 3241 items (at 36 items/min)
2015-11-04 12:13:41 [scrapy] INFO: Crawled 3486 pages (at 71 pages/min), scraped 3297 items (at 56 items/min)
2015-11-04 12:14:45 [scrapy] INFO: Crawled 3531 pages (at 45 pages/min), scraped 3358 items (at 61 items/min)
2015-11-04 12:15:39 [scrapy] INFO: Crawled 3572 pages (at 41 pages/min), scraped 3398 items (at 40 items/min)
2015-11-04 12:16:40 [scrapy] INFO: Crawled 3605 pages (at 33 pages/min), scraped 3437 items (at 39 items/min)
2015-11-04 12:17:00 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/questa-cdc-ds.pdf> (referer: https://www.mentor.com/products/fv/resource-center/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:34 [scrapy] INFO: Crawled 3643 pages (at 38 pages/min), scraped 3473 items (at 36 items/min)
2015-11-04 12:18:16 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader/?promoid=JZEFU>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:18:38 [scrapy] INFO: Crawled 3692 pages (at 49 pages/min), scraped 3519 items (at 46 items/min)
2015-11-04 12:19:52 [scrapy] INFO: Crawled 3745 pages (at 53 pages/min), scraped 3567 items (at 48 items/min)
2015-11-04 12:20:37 [scrapy] INFO: Crawled 3794 pages (at 49 pages/min), scraped 3605 items (at 38 items/min)
2015-11-04 12:21:45 [scrapy] INFO: Crawled 3856 pages (at 62 pages/min), scraped 3678 items (at 73 items/min)
2015-11-04 12:21:55 [scrapy] ERROR: Spider error processing <GET http://www.zend.com/> (referer: http://sapphireventures.com/companies/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 12:22:37 [scrapy] INFO: Crawled 3887 pages (at 31 pages/min), scraped 3721 items (at 43 items/min)
2015-11-04 12:23:37 [scrapy] INFO: Crawled 3975 pages (at 88 pages/min), scraped 3785 items (at 64 items/min)
2015-11-04 12:24:09 [scrapy] ERROR: Error downloading <GET https://squareup.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:24:36 [scrapy] INFO: Crawled 4041 pages (at 66 pages/min), scraped 3858 items (at 73 items/min)
2015-11-04 12:25:35 [scrapy] INFO: Crawled 4151 pages (at 110 pages/min), scraped 3943 items (at 85 items/min)
2015-11-04 12:26:35 [scrapy] ERROR: Error downloading <GET https://www.currencycloud.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:26:35 [scrapy] ERROR: Error downloading <GET https://www.jibe.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:26:35 [scrapy] INFO: Crawled 4202 pages (at 51 pages/min), scraped 3993 items (at 50 items/min)
2015-11-04 12:27:32 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/industry_article/resources/industry_articles/fv-thine-electronics.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/veloce)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:27:32 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/industry_article/products/fv/hardware-emulators.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/veloce)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:27:35 [scrapy] INFO: Crawled 4256 pages (at 54 pages/min), scraped 4057 items (at 64 items/min)
2015-11-04 12:27:35 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/certus-ds.pdf> (referer: https://www.mentor.com/products/fv/certus-silicon-debug)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:26 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/visualizer-debug-environment.pdf> (referer: https://www.mentor.com/products/fv/visualizer-debug)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:35 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce2-brochure.pdf> (referer: https://www.mentor.com/products/fv/verification-debug)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:35 [scrapy] INFO: Crawled 4327 pages (at 71 pages/min), scraped 4118 items (at 61 items/min)
2015-11-04 12:28:46 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/testbench-xpress-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/testbench-xpress)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:48 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-sata-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:50 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-multimedia-exerciser-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:50 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-usb-peripheral.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:52 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/virtual-pcie-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:53 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-ethernet-switch.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:54 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-usb-3-0-host-controller-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:55 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-arm-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:56 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-softmodel-memories-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:57 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-pci-express-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:28:59 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-multimedia-analyzer-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:00 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-sas-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:02 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtual-usb-3-0-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:03 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtualab-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:04 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-usb-host-controller-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:05 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtual-ethernet-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:06 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtual-usb-2-0-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:07 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/virtual-mulitmedia-device.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:09 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve_custom_solutions-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:29:35 [scrapy] INFO: Crawled 4411 pages (at 84 pages/min), scraped 4179 items (at 61 items/min)
2015-11-04 12:30:35 [scrapy] INFO: Crawled 4499 pages (at 88 pages/min), scraped 4264 items (at 85 items/min)
2015-11-04 12:30:36 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/presentation/products/fv/formal-model-checking.pdf> (referer: https://www.mentor.com/products/fv/questa-property-checking)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:30:49 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/ic_nanometer_design/analog-mixed-signal-verification/advance-ms/ADMS_Datasheet.pdf> (referer: https://www.mentor.com/products/fv/advance_ms/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:31:11 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/codelink/codelink-hve-ds.pdf> (referer: https://www.mentor.com/products/fv/codelink/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:31:33 [scrapy] INFO: Crawled 4590 pages (at 91 pages/min), scraped 4346 items (at 82 items/min)
2015-11-04 12:32:34 [scrapy] INFO: Crawled 4656 pages (at 66 pages/min), scraped 4411 items (at 65 items/min)
2015-11-04 12:33:11 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/tannereda/mems-design-ds.pdf> (referer: https://www.mentor.com/tannereda/mems-design)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:33:34 [scrapy] INFO: Crawled 4756 pages (at 100 pages/min), scraped 4507 items (at 96 items/min)
2015-11-04 12:34:25 [scrapy] ERROR: Error downloading <GET http://www.tataelxsi.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:34:25 [scrapy] ERROR: Error downloading <GET http://www.%20pronesistech.com/>: DNS lookup failed: address 'www.%20pronesistech.com' not found: [Errno -5] No address associated with hostname.
2015-11-04 12:34:25 [scrapy] ERROR: Error downloading <GET http://www.%20sondrel.com/>: DNS lookup failed: address 'www.%20sondrel.com' not found: [Errno -5] No address associated with hostname.
2015-11-04 12:34:34 [scrapy] ERROR: Error downloading <GET http://www.verifasttech.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:34:34 [scrapy] ERROR: Error downloading <GET http://udfddesign.com/project-request>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:34:34 [scrapy] ERROR: Error downloading <GET http://www.orora.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:34:34 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/company.html?promoid=KLXND>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:34:34 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/elements-family.html?promoid=KQQSD>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:34:34 [scrapy] INFO: Crawled 4828 pages (at 72 pages/min), scraped 4564 items (at 57 items/min)
2015-11-04 12:35:01 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?video=3000412270>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:35:01 [scrapy] ERROR: Error downloading <GET http://www.aumraj.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 12:35:01 [scrapy] ERROR: Error downloading <GET http://www.masamb.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 12:35:01 [scrapy] ERROR: Error downloading <GET http://www.inno-logic.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 12:35:01 [scrapy] ERROR: Error downloading <GET http://www.%20logicircuit.com/>: DNS lookup failed: address 'www.%20logicircuit.com' not found: [Errno -5] No address associated with hostname.
2015-11-04 12:35:02 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?aq=&cid=16282048205514105173&f=q&geocode=&hl=en&hnear=Boston%2C+Suffolk%2C+Massachusetts&hq=saturn+partners&ie=UTF8&iwloc=A&ll=42.533856%2C-71.019745&q=saturn+partners+boston&sll=42.560949%2C-87.868472&source=embed&spn=0.354182%2C0.961304&sspn=0.076116%2C0.139475&t=h&z=10>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:02 [scrapy] ERROR: Error downloading <GET http://www.ibtimes.com/federal-reserve-system-fomc-says-rate-hike-approaching-inflation-conditions-not-yet-2060780>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:02 [scrapy] ERROR: Error downloading <GET http://www.pivotalpath.com/team/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:02 [scrapy] ERROR: Error downloading <GET http://www.pivotalpath.com/about-us/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:02 [scrapy] ERROR: Error downloading <GET http://www.pivotalpath.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/catalog/software._sl_id-contentfilter_sl_catalog_sl_software_sl_mostpopular.html?promoid=KLXMI>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/digital-publishing-solution.html?promoid=KLXMT>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud.html?promoid=KLXMJ>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/online-marketing-solutions.html?promoid=KRVUY>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?hl=en&hnear=17+State+St%2C+New+York%2C+10004&ll=40.716883%2C-73.989744&oq=17+&q=17+State+St%2C+New+York%2C+NY&sll=40.75701%2C-73.972063&spn=0.046839%2C0.104628&sspn=0.023406%2C0.052314&t=m&z=14>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.semiwiki.com/forum/content/5043-something-old-something-new%C2%85eda-verification.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.semiwiki.com/forum/content/4787-power-management-gets-tricky-ip-driven-world.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET http://www.pivotalpath.com/contact>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET http://compliancetechtalks.com/palo-alto/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET http://www.pivotalpath.com/contact-us/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/web-analytics.html?promoid=KOUEP>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/catalog/software._sl_id-contentfilter_sl_catalog_sl_software_sl_mostpopular.html?promoid=KLXMV>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud.html?promoid=KLXLZ>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:15 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/online-advertising-management.html?promoid=KOUES>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/shockwave?promoid=KLXMH>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/air?promoid=KLXMG>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer?promoid=KLXMF>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader?promoid=KLXME>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET https://www.semiwiki.com/forum/content/5126-shifting-low-power-verification-ip-soc-flow.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET http://techcrunch.com/2015/09/26/closer-look-at-european-investing/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET https://www.saastr.com/the-dry-bubble-we-may-be-in-what-that-means/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET http://www.socrata.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET http://abovethecrowd.com/2015/02/25/investors-beware/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:31 [scrapy] ERROR: Error downloading <GET http://techcrunch.com/2015/08/26/two-lies-and-a-truth-about-the-bubble/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.cbinsights.com/blog/misinterpretations-cltv-cac-saas-metrics/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/enterprise-content-management.html?promoid=KOUER>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/primetime-tv-platform.html?promoid=KOUEV>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/data-management-platform.html?promoid=KRVUX>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/digital-publishing-solution.html?promoid=KLXMC>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET http://recommind.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:35:44 [scrapy] INFO: Crawled 4861 pages (at 33 pages/min), scraped 4609 items (at 45 items/min)
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.cbinsights.com/blog/special-purpose-vehicles-in-venture-pros-and-cons/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/campaign-management.html?promoid=KOUEQ>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/elements-family.html?promoid=KQQSC>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/buy/students.html?mv=other&promoid=MH16SLW8>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:44 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/aftereffects.html?promoid=KLXLW>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:35:56 [scrapy] ERROR: Error downloading <GET http://www.fsc>: DNS lookup failed: address 'www.fsc' not found: [Errno -2] Name or service not known.
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/business/teams.html?promoid=KQQSE>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/photography.html?mv=other&promoid=MLR7SH57>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/catalog/desktop.html?promoid=KOVFF>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/indesign.html?promoid=KLXLU>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/premiere.html?promoid=KLXLV>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/photoshop.html?promoid=KLXLS>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/illustrator.html?promoid=KLXLT>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/photoshop-lightroom.html?promoid=KLXLX>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:36:35 [scrapy] INFO: Crawled 4900 pages (at 39 pages/min), scraped 4646 items (at 37 items/min)
2015-11-04 12:37:29 [scrapy] ERROR: Error downloading <GET https://mentor1.adobeconnect.com/_a781163502/p51755750/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:38:23 [scrapy] INFO: Crawled 4940 pages (at 40 pages/min), scraped 4678 items (at 32 items/min)
2015-11-04 12:38:24 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/tannereda/t-spice-ams-simulation.pdf> (referer: https://www.mentor.com/tannereda/ams-ic)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:38:43 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/tannereda/ams-ic-design-flow-ds.pdf> (referer: https://www.mentor.com/tannereda/ams-ic)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.feplp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.harvestmanagement.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.cornwallcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.seamarkcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:45 [scrapy] INFO: Crawled 4949 pages (at 9 pages/min), scraped 4687 items (at 9 items/min)
2015-11-04 12:39:52 [scrapy] INFO: Crawled 4998 pages (at 49 pages/min), scraped 4722 items (at 35 items/min)
2015-11-04 12:39:53 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/silicon-yield/products/ijtag-ds.pdf> (referer: https://www.mentor.com/products/silicon-yield/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:41:12 [scrapy] INFO: Crawled 5011 pages (at 13 pages/min), scraped 4743 items (at 21 items/min)
2015-11-04 12:41:37 [scrapy] INFO: Crawled 5039 pages (at 28 pages/min), scraped 4756 items (at 13 items/min)
2015-11-04 12:43:10 [scrapy] ERROR: Error downloading <GET http://s3.mentor.com/public_documents/datasheet/products/silicon-yield/products/tessent.pdf>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:43:10 [scrapy] ERROR: Error downloading <GET http://s3.mentor.com/public_documents/datasheet/pcb-manufacturing-assembly/valor-mss-brochure-2.pdf>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:43:11 [scrapy] INFO: Crawled 5053 pages (at 14 pages/min), scraped 4774 items (at 18 items/min)
2015-11-04 12:43:47 [scrapy] INFO: Crawled 5060 pages (at 7 pages/min), scraped 4789 items (at 15 items/min)
2015-11-04 12:44:38 [scrapy] INFO: Crawled 5077 pages (at 17 pages/min), scraped 4805 items (at 16 items/min)
2015-11-04 12:44:40 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2011-Q3+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:44:46 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2011-Q4+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:01 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q1+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:01 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q2+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:23 [scrapy] ERROR: Error downloading <GET https://assetmanagement.gs.com/content/gsam/worldwide.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:45:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:26 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:45 [scrapy] INFO: Crawled 5088 pages (at 11 pages/min), scraped 4822 items (at 17 items/min)
2015-11-04 12:45:50 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:45:51 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:46:17 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-11+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:46:17 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-10+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:46:17 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-12+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:46:21 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-01+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:47:02 [scrapy] INFO: Crawled 5116 pages (at 28 pages/min), scraped 4835 items (at 13 items/min)
2015-11-04 12:47:06 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:47:06 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-02+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:47:09 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:47:26 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:48:23 [scrapy] INFO: Crawled 5147 pages (at 31 pages/min), scraped 4862 items (at 27 items/min)
2015-11-04 12:48:33 [scrapy] INFO: Crawled 5157 pages (at 10 pages/min), scraped 4872 items (at 10 items/min)
2015-11-04 12:48:34 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q3+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:48:37 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q4+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:48:38 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q1+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:48:38 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q3+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:48:42 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q2+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:48:44 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q4+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:48:46 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-02+February+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-03+March+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-06+June+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-07+July+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:11 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-08+August+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:14 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-09+September+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:14 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-10+October+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:14 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-04+April+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:15 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-01+January+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:15 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-11+November+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:16 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-12+December+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:34 [scrapy] INFO: Crawled 5213 pages (at 56 pages/min), scraped 4903 items (at 31 items/min)
2015-11-04 12:49:34 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-02+February+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:35 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-03+March+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:39 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-06+June+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:39 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-04+April+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:39 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-05+May+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:55 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-08++Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:55 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-07++Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:49:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-09+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:50:43 [scrapy] INFO: Crawled 5280 pages (at 67 pages/min), scraped 4957 items (at 54 items/min)
2015-11-04 12:51:11 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:51:20 [scrapy] ERROR: Error downloading <GET https://twitter.com/news_pdi>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:51:20 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-11+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:51:34 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-10+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:51:34 [scrapy] ERROR: Spider error processing <GET http://www.goldmansachs.com/citizenship/goldman-sachs-gives/index.html> (referer: http://www.goldmansachs.com/sitemap/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 12:51:46 [scrapy] INFO: Crawled 5321 pages (at 41 pages/min), scraped 5000 items (at 43 items/min)
2015-11-04 12:51:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:51:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:51:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:51:57 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:23 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:37 [scrapy] INFO: Crawled 5349 pages (at 28 pages/min), scraped 5029 items (at 29 items/min)
2015-11-04 12:52:37 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-01+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:38 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-02+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:41 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:55 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-12+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:55 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:52:59 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:14 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-11+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:33 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-10+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:34 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-02+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:39 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-01+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:39 [scrapy] INFO: Crawled 5408 pages (at 59 pages/min), scraped 5064 items (at 35 items/min)
2015-11-04 12:53:42 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:45 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:46 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:51 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:51 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:51 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:52 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:53:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:54:08 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-12+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:54:20 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2011-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:54:24 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:54:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:54:40 [scrapy] INFO: Crawled 5518 pages (at 110 pages/min), scraped 5150 items (at 86 items/min)
2015-11-04 12:54:40 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:54:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2011-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:55:13 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:55:15 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:55:39 [scrapy] INFO: Crawled 5541 pages (at 23 pages/min), scraped 5184 items (at 34 items/min)
2015-11-04 12:55:59 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/GoldmanSachs>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:55:59 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:56:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:56:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2015-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:56:15 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:56:15 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:56:29 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:56:55 [scrapy] INFO: Crawled 5575 pages (at 34 pages/min), scraped 5219 items (at 35 items/min)
2015-11-04 12:56:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2015-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:56:57 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2015-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:05 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:05 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q2+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:13 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q1+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:21 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q4+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:24 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q1+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q3+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:32 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q4+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:35 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2015+Q1+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:48 [scrapy] INFO: Crawled 5603 pages (at 28 pages/min), scraped 5238 items (at 19 items/min)
2015-11-04 12:57:53 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2015+Q3+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:57:54 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2015+Q2+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:58:16 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q2+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:59:20 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q3+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:59:21 [scrapy] INFO: Crawled 5662 pages (at 59 pages/min), scraped 5282 items (at 44 items/min)
2015-11-04 13:00:02 [scrapy] INFO: Crawled 5677 pages (at 15 pages/min), scraped 5301 items (at 19 items/min)
2015-11-04 13:00:37 [scrapy] INFO: Crawled 5694 pages (at 17 pages/min), scraped 5316 items (at 15 items/min)
2015-11-04 13:01:19 [scrapy] ERROR: Error downloading <GET https://plus.google.com/+GoldmanSachs>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:01:19 [scrapy] ERROR: Error downloading <GET https://plus.google.com/105278084975905168483/posts>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:01:19 [scrapy] ERROR: Error downloading <GET http://www.tranistional-funding.com/>: DNS lookup failed: address 'www.tranistional-funding.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:01:20 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/place/601+Carlson+Pkwy+#1125,+Hopkins,+MN+55305/@44.9728673,-93.4659062,17z/data=!3m1!4b1!4m2!3m1!1s0x52b34b1c56a9df0b:0x779178290d7dff1e>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:01:35 [scrapy] ERROR: Error downloading <GET https://www.recapitalnews.com/event/germanyforum/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:01:35 [scrapy] ERROR: Error downloading <GET https://www.recapitalnews.com/event/europeforum/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:01:35 [scrapy] INFO: Crawled 5716 pages (at 22 pages/min), scraped 5337 items (at 21 items/min)
2015-11-04 13:01:38 [scrapy] ERROR: Error downloading <GET http://www.utsandiego.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 13:03:04 [scrapy] ERROR: Spider error processing <GET http://www.fosun.com/about/about.html> (referer: http://www.fosun.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:03:13 [scrapy] INFO: Crawled 5754 pages (at 38 pages/min), scraped 5374 items (at 37 items/min)
2015-11-04 13:03:39 [scrapy] INFO: Crawled 5754 pages (at 0 pages/min), scraped 5379 items (at 5 items/min)
2015-11-04 13:03:39 [scrapy] ERROR: Error downloading <GET https://twitter.com/thebondbuyer>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:03:39 [scrapy] ERROR: Error downloading <GET http://www.fosun.com/news/news_2.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 13:03:39 [scrapy] ERROR: Error downloading <GET http://www.fosun.com/investment/investment.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 13:03:39 [scrapy] ERROR: Error downloading <GET http://www.fosun.com/news/news.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 13:03:39 [scrapy] ERROR: Error downloading <GET http://www.fosun.com/investment/investment_1.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 13:03:39 [scrapy] ERROR: Error downloading <GET http://www.fosun.com/news/news_1.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 13:03:39 [scrapy] ERROR: Error downloading <GET http://www.fosun.com/investment/investment_9.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 13:04:47 [scrapy] INFO: Crawled 5788 pages (at 34 pages/min), scraped 5409 items (at 30 items/min)
2015-11-04 13:04:49 [scrapy] ERROR: Error downloading <GET https://globalcash.gs.com/smartplus/start>: TCP connection timed out: 110: Connection timed out.
2015-11-04 13:05:35 [scrapy] INFO: Crawled 5811 pages (at 23 pages/min), scraped 5437 items (at 28 items/min)
2015-11-04 13:06:50 [scrapy] INFO: Crawled 5867 pages (at 56 pages/min), scraped 5484 items (at 47 items/min)
2015-11-04 13:07:43 [scrapy] INFO: Crawled 5897 pages (at 30 pages/min), scraped 5507 items (at 23 items/min)
2015-11-04 13:08:46 [scrapy] INFO: Crawled 5922 pages (at 25 pages/min), scraped 5531 items (at 24 items/min)
2015-11-04 13:09:39 [scrapy] INFO: Crawled 5948 pages (at 26 pages/min), scraped 5563 items (at 32 items/min)
2015-11-04 13:10:54 [scrapy] INFO: Crawled 5974 pages (at 26 pages/min), scraped 5592 items (at 29 items/min)
2015-11-04 13:11:36 [scrapy] INFO: Crawled 5998 pages (at 24 pages/min), scraped 5617 items (at 25 items/min)
2015-11-04 13:12:03 [scrapy] INFO: Closing spider (finished)
2015-11-04 13:12:03 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 547,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 10,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 54,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 30,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 13,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 434,
 'downloader/request_bytes': 4328034,
 'downloader/request_count': 8159,
 'downloader/request_method_count/GET': 8159,
 'downloader/response_bytes': 203383962,
 'downloader/response_count': 7612,
 'downloader/response_status_count/200': 5890,
 'downloader/response_status_count/301': 824,
 'downloader/response_status_count/302': 625,
 'downloader/response_status_count/303': 3,
 'downloader/response_status_count/307': 7,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 10,
 'downloader/response_status_count/404': 101,
 'downloader/response_status_count/408': 122,
 'downloader/response_status_count/410': 2,
 'downloader/response_status_count/500': 7,
 'downloader/response_status_count/503': 3,
 'downloader/response_status_count/999': 12,
 'dupefilter/filtered': 12127,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 13, 12, 3, 383419),
 'item_scraped_count': 5637,
 'log_count/CRITICAL': 1,
 'log_count/ERROR': 284,
 'log_count/INFO': 123,
 'offsite/domains': 62,
 'offsite/filtered': 381,
 'request_depth_max': 2,
 'response_received_count': 6011,
 'scheduler/dequeued': 8159,
 'scheduler/dequeued/memory': 8159,
 'scheduler/enqueued': 8159,
 'scheduler/enqueued/memory': 8159,
 'spider_exceptions/AttributeError': 34,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/TypeError': 128,
 'spider_exceptions/error': 1,
 'spider_exceptions/timeout': 1,
 'start_time': datetime.datetime(2015, 11, 4, 11, 15, 33, 322904)}
2015-11-04 13:12:03 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 13:13:05 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 13:13:05 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 13:13:05 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 13:13:05 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 13:13:05 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 13:13:05 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 13:13:05 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 13:13:05 [scrapy] INFO: Spider opened
2015-11-04 13:13:05 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 13:13:05 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:05 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:05 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:05 [scrapy] ERROR: Error downloading <GET http://www.portal.krfs.com>: DNS lookup failed: address 'www.portal.krfs.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:05 [scrapy] ERROR: Error downloading <GET http://www.arb>: DNS lookup failed: address 'www.arb' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:05 [scrapy] ERROR: Error downloading <GET http://www.tif>: DNS lookup failed: address 'www.tif' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:06 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:06 [scrapy] ERROR: Error downloading <GET http://www.zad>: DNS lookup failed: address 'www.zad' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:06 [scrapy] ERROR: Error downloading <GET http://www.ecp.altareturn.com>: DNS lookup failed: address 'www.ecp.altareturn.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:13:10 [scrapy] ERROR: Error downloading <GET https://www.magnitudecapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 13:13:15 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7f3fce6de488>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 13:14:21 [scrapy] INFO: Crawled 198 pages (at 198 pages/min), scraped 134 items (at 134 items/min)
2015-11-04 13:14:32 [scrapy] ERROR: Error downloading <GET https://deutscheawm.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:15:23 [scrapy] INFO: Crawled 261 pages (at 63 pages/min), scraped 197 items (at 63 items/min)
2015-11-04 13:16:12 [scrapy] INFO: Crawled 326 pages (at 65 pages/min), scraped 260 items (at 63 items/min)
2015-11-04 13:16:23 [scrapy] ERROR: Error downloading <GET https://plus.google.com/108306343581548568740>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:16:23 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.godaddy.mobile.android>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 13:18:27 [scrapy] ERROR: Spider error processing <GET http://www.nblife.com/> (referer: http://www.carlyle.com/our-business/corporate-private-equity/asia-buyout)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 15:42:13 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM twice, forcing unclean shutdown
