usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
2015-11-04 00:31:04 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 00:31:04 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 00:31:04 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 00:31:04 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 00:31:04 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 00:31:04 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 00:31:04 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 00:31:04 [scrapy] INFO: Spider opened
2015-11-04 00:31:04 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 00:32:27 [scrapy] INFO: Crawled 155 pages (at 155 pages/min), scraped 87 items (at 87 items/min)
2015-11-04 00:33:18 [scrapy] INFO: Crawled 201 pages (at 46 pages/min), scraped 121 items (at 34 items/min)
2015-11-04 00:34:19 [scrapy] INFO: Crawled 237 pages (at 36 pages/min), scraped 152 items (at 31 items/min)
2015-11-04 00:35:15 [scrapy] ERROR: Error downloading <GET https://downtyme.uservoice.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:35:16 [scrapy] INFO: Crawled 286 pages (at 49 pages/min), scraped 191 items (at 39 items/min)
2015-11-04 00:35:16 [scrapy] ERROR: Error downloading <GET https://roller.software/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:36:07 [scrapy] INFO: Crawled 302 pages (at 16 pages/min), scraped 224 items (at 33 items/min)
2015-11-04 00:37:20 [scrapy] INFO: Crawled 348 pages (at 46 pages/min), scraped 269 items (at 45 items/min)
2015-11-04 00:38:10 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=460526786545-r7bv8fqdnsu2v53pie90mm786fo4ec2m.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fplus.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.profile&redirect_uri=https%3A%2F%2Fwww.mobypark.com%2Fsocial_oauth%2Flogin%2Fcheck-google&request_visible_actions=http%3A%2F%2Fschemas.google.com%2FAddActivity+http%3A%2F%2Fschemas.google.com%2FCommentActivity>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 00:38:23 [scrapy] INFO: Crawled 386 pages (at 38 pages/min), scraped 314 items (at 45 items/min)
2015-11-04 00:39:14 [scrapy] INFO: Crawled 423 pages (at 37 pages/min), scraped 348 items (at 34 items/min)
2015-11-04 00:40:17 [scrapy] INFO: Crawled 444 pages (at 21 pages/min), scraped 379 items (at 31 items/min)
2015-11-04 00:41:16 [scrapy] INFO: Crawled 477 pages (at 33 pages/min), scraped 405 items (at 26 items/min)
2015-11-04 00:42:10 [scrapy] INFO: Crawled 500 pages (at 23 pages/min), scraped 431 items (at 26 items/min)
2015-11-04 00:43:06 [scrapy] INFO: Crawled 524 pages (at 24 pages/min), scraped 455 items (at 24 items/min)
2015-11-04 00:44:05 [scrapy] INFO: Crawled 549 pages (at 25 pages/min), scraped 479 items (at 24 items/min)
2015-11-04 00:45:08 [scrapy] INFO: Crawled 582 pages (at 33 pages/min), scraped 508 items (at 29 items/min)
2015-11-04 00:46:09 [scrapy] INFO: Crawled 606 pages (at 24 pages/min), scraped 538 items (at 30 items/min)
2015-11-04 00:47:10 [scrapy] INFO: Crawled 633 pages (at 27 pages/min), scraped 567 items (at 29 items/min)
2015-11-04 00:48:06 [scrapy] INFO: Crawled 663 pages (at 30 pages/min), scraped 594 items (at 27 items/min)
2015-11-04 00:49:09 [scrapy] INFO: Crawled 691 pages (at 28 pages/min), scraped 622 items (at 28 items/min)
2015-11-04 00:49:27 [scrapy] ERROR: Error downloading <GET https://www.wandoujia.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.wandoujia.com/ took longer than 180.0 seconds..
2015-11-04 00:50:05 [scrapy] INFO: Crawled 725 pages (at 34 pages/min), scraped 651 items (at 29 items/min)
2015-11-04 00:51:13 [scrapy] INFO: Crawled 752 pages (at 27 pages/min), scraped 683 items (at 32 items/min)
2015-11-04 00:52:08 [scrapy] INFO: Crawled 776 pages (at 24 pages/min), scraped 707 items (at 24 items/min)
2015-11-04 00:53:11 [scrapy] INFO: Crawled 802 pages (at 26 pages/min), scraped 736 items (at 29 items/min)
2015-11-04 00:54:11 [scrapy] INFO: Crawled 825 pages (at 23 pages/min), scraped 759 items (at 23 items/min)
2015-11-04 00:55:07 [scrapy] INFO: Crawled 851 pages (at 26 pages/min), scraped 783 items (at 24 items/min)
2015-11-04 00:56:05 [scrapy] INFO: Crawled 878 pages (at 27 pages/min), scraped 808 items (at 25 items/min)
2015-11-04 00:57:05 [scrapy] INFO: Crawled 906 pages (at 28 pages/min), scraped 835 items (at 27 items/min)
2015-11-04 00:58:10 [scrapy] INFO: Crawled 931 pages (at 25 pages/min), scraped 866 items (at 31 items/min)
2015-11-04 00:59:13 [scrapy] INFO: Crawled 964 pages (at 33 pages/min), scraped 898 items (at 32 items/min)
2015-11-04 01:00:05 [scrapy] INFO: Crawled 987 pages (at 23 pages/min), scraped 921 items (at 23 items/min)
2015-11-04 01:01:09 [scrapy] INFO: Crawled 1019 pages (at 32 pages/min), scraped 953 items (at 32 items/min)
2015-11-04 01:02:06 [scrapy] INFO: Crawled 1043 pages (at 24 pages/min), scraped 977 items (at 24 items/min)
2015-11-04 01:03:10 [scrapy] INFO: Crawled 1080 pages (at 37 pages/min), scraped 1007 items (at 30 items/min)
2015-11-04 01:04:09 [scrapy] INFO: Crawled 1132 pages (at 52 pages/min), scraped 1041 items (at 34 items/min)
2015-11-04 01:05:18 [scrapy] INFO: Crawled 1169 pages (at 37 pages/min), scraped 1077 items (at 36 items/min)
2015-11-04 01:06:08 [scrapy] INFO: Crawled 1186 pages (at 17 pages/min), scraped 1096 items (at 19 items/min)
2015-11-04 01:07:17 [scrapy] INFO: Crawled 1237 pages (at 51 pages/min), scraped 1127 items (at 31 items/min)
2015-11-04 01:08:26 [scrapy] INFO: Crawled 1256 pages (at 19 pages/min), scraped 1156 items (at 29 items/min)
2015-11-04 01:09:21 [scrapy] INFO: Crawled 1279 pages (at 23 pages/min), scraped 1174 items (at 18 items/min)
2015-11-04 01:10:27 [scrapy] INFO: Crawled 1292 pages (at 13 pages/min), scraped 1196 items (at 22 items/min)
2015-11-04 01:11:48 [scrapy] INFO: Crawled 1316 pages (at 24 pages/min), scraped 1220 items (at 24 items/min)
2015-11-04 01:12:36 [scrapy] INFO: Crawled 1324 pages (at 8 pages/min), scraped 1228 items (at 8 items/min)
2015-11-04 01:13:13 [scrapy] INFO: Crawled 1335 pages (at 11 pages/min), scraped 1236 items (at 8 items/min)
2015-11-04 01:14:41 [scrapy] INFO: Crawled 1348 pages (at 13 pages/min), scraped 1252 items (at 16 items/min)
2015-11-04 01:15:22 [scrapy] INFO: Crawled 1358 pages (at 10 pages/min), scraped 1260 items (at 8 items/min)
2015-11-04 01:16:30 [scrapy] INFO: Crawled 1369 pages (at 11 pages/min), scraped 1274 items (at 14 items/min)
2015-11-04 01:17:39 [scrapy] INFO: Crawled 1383 pages (at 14 pages/min), scraped 1288 items (at 14 items/min)
2015-11-04 01:18:10 [scrapy] INFO: Crawled 1393 pages (at 10 pages/min), scraped 1295 items (at 7 items/min)
2015-11-04 01:19:23 [scrapy] INFO: Crawled 1413 pages (at 20 pages/min), scraped 1317 items (at 22 items/min)
2015-11-04 01:20:14 [scrapy] INFO: Crawled 1433 pages (at 20 pages/min), scraped 1333 items (at 16 items/min)
2015-11-04 01:21:23 [scrapy] INFO: Crawled 1453 pages (at 20 pages/min), scraped 1357 items (at 24 items/min)
2015-11-04 01:22:16 [scrapy] INFO: Crawled 1477 pages (at 24 pages/min), scraped 1375 items (at 18 items/min)
2015-11-04 01:23:11 [scrapy] INFO: Crawled 1515 pages (at 38 pages/min), scraped 1397 items (at 22 items/min)
2015-11-04 01:24:32 [scrapy] INFO: Crawled 1539 pages (at 24 pages/min), scraped 1432 items (at 35 items/min)
2015-11-04 01:25:43 [scrapy] INFO: Crawled 1563 pages (at 24 pages/min), scraped 1450 items (at 18 items/min)
2015-11-04 01:26:48 [scrapy] INFO: Crawled 1580 pages (at 17 pages/min), scraped 1474 items (at 24 items/min)
2015-11-04 01:28:07 [scrapy] INFO: Crawled 1596 pages (at 16 pages/min), scraped 1491 items (at 17 items/min)
2015-11-04 01:29:07 [scrapy] INFO: Crawled 1622 pages (at 26 pages/min), scraped 1514 items (at 23 items/min)
2015-11-04 01:30:27 [scrapy] INFO: Crawled 1656 pages (at 34 pages/min), scraped 1546 items (at 32 items/min)
2015-11-04 01:31:10 [scrapy] INFO: Crawled 1679 pages (at 23 pages/min), scraped 1562 items (at 16 items/min)
2015-11-04 01:32:08 [scrapy] INFO: Crawled 1710 pages (at 31 pages/min), scraped 1587 items (at 25 items/min)
2015-11-04 01:33:45 [scrapy] INFO: Crawled 1732 pages (at 22 pages/min), scraped 1616 items (at 29 items/min)
2015-11-04 01:34:47 [scrapy] INFO: Crawled 1748 pages (at 16 pages/min), scraped 1640 items (at 24 items/min)
2015-11-04 01:35:27 [scrapy] INFO: Crawled 1765 pages (at 17 pages/min), scraped 1654 items (at 14 items/min)
2015-11-04 01:36:17 [scrapy] INFO: Crawled 1798 pages (at 33 pages/min), scraped 1671 items (at 17 items/min)
2015-11-04 01:37:08 [scrapy] INFO: Crawled 1806 pages (at 8 pages/min), scraped 1692 items (at 21 items/min)
2015-11-04 01:38:07 [scrapy] INFO: Crawled 1828 pages (at 22 pages/min), scraped 1714 items (at 22 items/min)
2015-11-04 01:39:07 [scrapy] INFO: Crawled 1856 pages (at 28 pages/min), scraped 1747 items (at 33 items/min)
2015-11-04 01:40:12 [scrapy] INFO: Crawled 1883 pages (at 27 pages/min), scraped 1780 items (at 33 items/min)
2015-11-04 01:41:25 [scrapy] INFO: Crawled 1927 pages (at 44 pages/min), scraped 1817 items (at 37 items/min)
2015-11-04 01:42:47 [scrapy] INFO: Crawled 1954 pages (at 27 pages/min), scraped 1841 items (at 24 items/min)
2015-11-04 01:43:19 [scrapy] INFO: Crawled 1963 pages (at 9 pages/min), scraped 1859 items (at 18 items/min)
2015-11-04 01:44:09 [scrapy] INFO: Crawled 1988 pages (at 25 pages/min), scraped 1889 items (at 30 items/min)
2015-11-04 01:45:13 [scrapy] INFO: Crawled 2027 pages (at 39 pages/min), scraped 1927 items (at 38 items/min)
2015-11-04 01:46:10 [scrapy] INFO: Crawled 2063 pages (at 36 pages/min), scraped 1960 items (at 33 items/min)
2015-11-04 01:47:09 [scrapy] INFO: Crawled 2086 pages (at 23 pages/min), scraped 1986 items (at 26 items/min)
2015-11-04 01:48:05 [scrapy] INFO: Crawled 2113 pages (at 27 pages/min), scraped 2016 items (at 30 items/min)
2015-11-04 01:49:07 [scrapy] INFO: Crawled 2142 pages (at 29 pages/min), scraped 2046 items (at 30 items/min)
2015-11-04 01:50:09 [scrapy] INFO: Crawled 2174 pages (at 32 pages/min), scraped 2075 items (at 29 items/min)
2015-11-04 01:51:04 [scrapy] INFO: Crawled 2196 pages (at 22 pages/min), scraped 2098 items (at 23 items/min)
2015-11-04 01:52:08 [scrapy] INFO: Crawled 2229 pages (at 33 pages/min), scraped 2130 items (at 32 items/min)
2015-11-04 01:53:08 [scrapy] INFO: Crawled 2254 pages (at 25 pages/min), scraped 2156 items (at 26 items/min)
2015-11-04 01:54:07 [scrapy] INFO: Crawled 2278 pages (at 24 pages/min), scraped 2183 items (at 27 items/min)
2015-11-04 01:55:10 [scrapy] INFO: Crawled 2312 pages (at 34 pages/min), scraped 2213 items (at 30 items/min)
2015-11-04 01:56:11 [scrapy] INFO: Crawled 2340 pages (at 28 pages/min), scraped 2244 items (at 31 items/min)
2015-11-04 01:57:05 [scrapy] INFO: Crawled 2367 pages (at 27 pages/min), scraped 2268 items (at 24 items/min)
2015-11-04 01:58:10 [scrapy] INFO: Crawled 2393 pages (at 26 pages/min), scraped 2297 items (at 29 items/min)
2015-11-04 01:59:08 [scrapy] INFO: Crawled 2423 pages (at 30 pages/min), scraped 2325 items (at 28 items/min)
2015-11-04 02:00:07 [scrapy] INFO: Crawled 2450 pages (at 27 pages/min), scraped 2351 items (at 26 items/min)
2015-11-04 02:01:11 [scrapy] INFO: Crawled 2481 pages (at 31 pages/min), scraped 2382 items (at 31 items/min)
2015-11-04 02:02:07 [scrapy] INFO: Crawled 2507 pages (at 26 pages/min), scraped 2409 items (at 27 items/min)
2015-11-04 02:03:07 [scrapy] INFO: Crawled 2538 pages (at 31 pages/min), scraped 2439 items (at 30 items/min)
2015-11-04 02:04:12 [scrapy] INFO: Crawled 2564 pages (at 26 pages/min), scraped 2467 items (at 28 items/min)
2015-11-04 02:05:11 [scrapy] INFO: Crawled 2592 pages (at 28 pages/min), scraped 2497 items (at 30 items/min)
2015-11-04 02:06:05 [scrapy] INFO: Crawled 2616 pages (at 24 pages/min), scraped 2520 items (at 23 items/min)
2015-11-04 02:07:06 [scrapy] INFO: Crawled 2647 pages (at 31 pages/min), scraped 2549 items (at 29 items/min)
2015-11-04 02:08:11 [scrapy] INFO: Crawled 2680 pages (at 33 pages/min), scraped 2580 items (at 31 items/min)
2015-11-04 02:09:17 [scrapy] INFO: Crawled 2702 pages (at 22 pages/min), scraped 2606 items (at 26 items/min)
2015-11-04 02:10:05 [scrapy] INFO: Crawled 2728 pages (at 26 pages/min), scraped 2629 items (at 23 items/min)
2015-11-04 02:11:04 [scrapy] INFO: Crawled 2750 pages (at 22 pages/min), scraped 2654 items (at 25 items/min)
2015-11-04 02:12:09 [scrapy] INFO: Crawled 2783 pages (at 33 pages/min), scraped 2682 items (at 28 items/min)
2015-11-04 02:13:07 [scrapy] INFO: Crawled 2809 pages (at 26 pages/min), scraped 2710 items (at 28 items/min)
2015-11-04 02:14:06 [scrapy] INFO: Crawled 2838 pages (at 29 pages/min), scraped 2738 items (at 28 items/min)
2015-11-04 02:15:15 [scrapy] INFO: Crawled 2865 pages (at 27 pages/min), scraped 2770 items (at 32 items/min)
2015-11-04 02:16:06 [scrapy] INFO: Crawled 2892 pages (at 27 pages/min), scraped 2792 items (at 22 items/min)
2015-11-04 02:17:06 [scrapy] INFO: Crawled 2920 pages (at 28 pages/min), scraped 2821 items (at 29 items/min)
2015-11-04 02:18:13 [scrapy] INFO: Crawled 2962 pages (at 42 pages/min), scraped 2858 items (at 37 items/min)
2015-11-04 02:20:41 [scrapy] INFO: Crawled 2970 pages (at 8 pages/min), scraped 2867 items (at 9 items/min)
2015-11-04 02:21:13 [scrapy] INFO: Crawled 2989 pages (at 19 pages/min), scraped 2886 items (at 19 items/min)
2015-11-04 02:22:07 [scrapy] INFO: Crawled 3025 pages (at 36 pages/min), scraped 2924 items (at 38 items/min)
2015-11-04 02:23:14 [scrapy] INFO: Crawled 3077 pages (at 52 pages/min), scraped 2962 items (at 38 items/min)
2015-11-04 02:24:05 [scrapy] INFO: Crawled 3104 pages (at 27 pages/min), scraped 3001 items (at 39 items/min)
2015-11-04 02:25:07 [scrapy] INFO: Crawled 3154 pages (at 50 pages/min), scraped 3049 items (at 48 items/min)
2015-11-04 02:26:09 [scrapy] INFO: Crawled 3192 pages (at 38 pages/min), scraped 3093 items (at 44 items/min)
2015-11-04 02:27:05 [scrapy] INFO: Crawled 3228 pages (at 36 pages/min), scraped 3125 items (at 32 items/min)
2015-11-04 02:28:18 [scrapy] INFO: Crawled 3292 pages (at 64 pages/min), scraped 3174 items (at 49 items/min)
2015-11-04 02:29:08 [scrapy] INFO: Crawled 3330 pages (at 38 pages/min), scraped 3207 items (at 33 items/min)
2015-11-04 02:30:14 [scrapy] INFO: Crawled 3388 pages (at 58 pages/min), scraped 3280 items (at 73 items/min)
2015-11-04 02:31:44 [scrapy] INFO: Crawled 3422 pages (at 34 pages/min), scraped 3313 items (at 33 items/min)
2015-11-04 02:32:14 [scrapy] INFO: Crawled 3440 pages (at 18 pages/min), scraped 3326 items (at 13 items/min)
2015-11-04 02:33:10 [scrapy] INFO: Crawled 3458 pages (at 18 pages/min), scraped 3355 items (at 29 items/min)
2015-11-04 02:34:07 [scrapy] INFO: Crawled 3535 pages (at 77 pages/min), scraped 3425 items (at 70 items/min)
2015-11-04 02:35:14 [scrapy] INFO: Crawled 3649 pages (at 114 pages/min), scraped 3524 items (at 99 items/min)
2015-11-04 02:36:23 [scrapy] INFO: Crawled 3719 pages (at 70 pages/min), scraped 3581 items (at 57 items/min)
2015-11-04 02:37:21 [scrapy] INFO: Crawled 3727 pages (at 8 pages/min), scraped 3609 items (at 28 items/min)
2015-11-04 02:38:17 [scrapy] INFO: Crawled 3738 pages (at 11 pages/min), scraped 3635 items (at 26 items/min)
2015-11-04 02:39:04 [scrapy] INFO: Crawled 3784 pages (at 46 pages/min), scraped 3661 items (at 26 items/min)
2015-11-04 02:39:56 [scrapy] ERROR: Error downloading <GET https://stackengine.com/signup/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 02:40:07 [scrapy] INFO: Crawled 3829 pages (at 45 pages/min), scraped 3702 items (at 41 items/min)
2015-11-04 02:41:08 [scrapy] INFO: Crawled 3850 pages (at 21 pages/min), scraped 3750 items (at 48 items/min)
2015-11-04 02:42:20 [scrapy] INFO: Crawled 3903 pages (at 53 pages/min), scraped 3797 items (at 47 items/min)
2015-11-04 02:43:10 [scrapy] INFO: Crawled 3942 pages (at 39 pages/min), scraped 3822 items (at 25 items/min)
2015-11-04 02:44:07 [scrapy] INFO: Crawled 3974 pages (at 32 pages/min), scraped 3858 items (at 36 items/min)
2015-11-04 02:44:52 [scrapy] ERROR: Error downloading <GET http://www.new.chemistdirect.co.uk/privacy-and-cookies>: DNS lookup failed: address 'www.new.chemistdirect.co.uk' not found: [Errno -2] Name or service not known.
2015-11-04 02:44:52 [scrapy] ERROR: Error downloading <GET http://www.new.chemistdirect.co.uk/how-to-shop>: DNS lookup failed: address 'www.new.chemistdirect.co.uk' not found: [Errno -2] Name or service not known.
2015-11-04 02:44:52 [scrapy] ERROR: Error downloading <GET http://www.new.chemistdirect.co.uk/delivery-information>: DNS lookup failed: address 'www.new.chemistdirect.co.uk' not found: [Errno -2] Name or service not known.
2015-11-04 02:44:52 [scrapy] ERROR: Error downloading <GET http://www.new.chemistdirect.co.uk/returns>: DNS lookup failed: address 'www.new.chemistdirect.co.uk' not found: [Errno -2] Name or service not known.
2015-11-04 02:45:04 [scrapy] INFO: Crawled 4001 pages (at 27 pages/min), scraped 3888 items (at 30 items/min)
2015-11-04 02:46:06 [scrapy] INFO: Crawled 4023 pages (at 22 pages/min), scraped 3915 items (at 27 items/min)
2015-11-04 02:47:05 [scrapy] INFO: Crawled 4047 pages (at 24 pages/min), scraped 3939 items (at 24 items/min)
2015-11-04 02:48:08 [scrapy] INFO: Crawled 4065 pages (at 18 pages/min), scraped 3960 items (at 21 items/min)
2015-11-04 02:49:13 [scrapy] INFO: Crawled 4092 pages (at 27 pages/min), scraped 3989 items (at 29 items/min)
2015-11-04 02:50:07 [scrapy] INFO: Crawled 4122 pages (at 30 pages/min), scraped 4015 items (at 26 items/min)
2015-11-04 02:51:09 [scrapy] INFO: Crawled 4156 pages (at 34 pages/min), scraped 4050 items (at 35 items/min)
2015-11-04 02:52:04 [scrapy] INFO: Crawled 4185 pages (at 29 pages/min), scraped 4075 items (at 25 items/min)
2015-11-04 02:53:04 [scrapy] INFO: Crawled 4217 pages (at 32 pages/min), scraped 4107 items (at 32 items/min)
2015-11-04 02:54:08 [scrapy] INFO: Crawled 4252 pages (at 35 pages/min), scraped 4143 items (at 36 items/min)
2015-11-04 02:55:12 [scrapy] INFO: Crawled 4293 pages (at 41 pages/min), scraped 4175 items (at 32 items/min)
2015-11-04 02:56:10 [scrapy] INFO: Crawled 4336 pages (at 43 pages/min), scraped 4214 items (at 39 items/min)
2015-11-04 02:57:10 [scrapy] INFO: Crawled 4380 pages (at 44 pages/min), scraped 4253 items (at 39 items/min)
2015-11-04 02:58:09 [scrapy] INFO: Crawled 4415 pages (at 35 pages/min), scraped 4280 items (at 27 items/min)
2015-11-04 02:59:06 [scrapy] INFO: Crawled 4451 pages (at 36 pages/min), scraped 4319 items (at 39 items/min)
2015-11-04 03:00:06 [scrapy] INFO: Crawled 4487 pages (at 36 pages/min), scraped 4353 items (at 34 items/min)
2015-11-04 03:01:09 [scrapy] INFO: Crawled 4522 pages (at 35 pages/min), scraped 4390 items (at 37 items/min)
2015-11-04 03:02:09 [scrapy] INFO: Crawled 4554 pages (at 32 pages/min), scraped 4421 items (at 31 items/min)
2015-11-04 03:03:07 [scrapy] INFO: Crawled 4586 pages (at 32 pages/min), scraped 4454 items (at 33 items/min)
2015-11-04 03:04:10 [scrapy] INFO: Crawled 4618 pages (at 32 pages/min), scraped 4486 items (at 32 items/min)
2015-11-04 03:05:06 [scrapy] INFO: Crawled 4650 pages (at 32 pages/min), scraped 4512 items (at 26 items/min)
2015-11-04 03:06:04 [scrapy] INFO: Crawled 4674 pages (at 24 pages/min), scraped 4536 items (at 24 items/min)
2015-11-04 03:07:08 [scrapy] INFO: Crawled 4700 pages (at 26 pages/min), scraped 4566 items (at 30 items/min)
2015-11-04 03:08:10 [scrapy] INFO: Crawled 4725 pages (at 25 pages/min), scraped 4592 items (at 26 items/min)
2015-11-04 03:09:08 [scrapy] INFO: Crawled 4759 pages (at 34 pages/min), scraped 4618 items (at 26 items/min)
2015-11-04 03:10:24 [scrapy] INFO: Crawled 4791 pages (at 32 pages/min), scraped 4655 items (at 37 items/min)
2015-11-04 03:11:07 [scrapy] INFO: Crawled 4819 pages (at 28 pages/min), scraped 4678 items (at 23 items/min)
2015-11-04 03:12:06 [scrapy] INFO: Crawled 4859 pages (at 40 pages/min), scraped 4714 items (at 36 items/min)
2015-11-04 03:13:16 [scrapy] INFO: Crawled 4897 pages (at 38 pages/min), scraped 4755 items (at 41 items/min)
2015-11-04 03:14:21 [scrapy] INFO: Crawled 4975 pages (at 78 pages/min), scraped 4818 items (at 63 items/min)
2015-11-04 03:15:13 [scrapy] INFO: Crawled 5051 pages (at 76 pages/min), scraped 4881 items (at 63 items/min)
2015-11-04 03:15:20 [scrapy] ERROR: Spider error processing <GET http://members.localmaven.com/> (referer: http://localmaven.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 45, in parse_items
    il = ItemLoader(item=SuspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 03:16:38 [scrapy] INFO: Crawled 5101 pages (at 50 pages/min), scraped 4933 items (at 52 items/min)
2015-11-04 03:17:28 [scrapy] INFO: Crawled 5114 pages (at 13 pages/min), scraped 4955 items (at 22 items/min)
2015-11-04 03:18:25 [scrapy] INFO: Crawled 5126 pages (at 12 pages/min), scraped 4973 items (at 18 items/min)
2015-11-04 03:19:21 [scrapy] ERROR: Error downloading <GET https://www.newsvine.com/_nv/api/accounts/tenderLogin?to=http%3A%2F%2Fsupport.newsvine.com%2Fdiscussion%2Fnew>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 03:19:21 [scrapy] INFO: Crawled 5133 pages (at 7 pages/min), scraped 4984 items (at 11 items/min)
2015-11-04 03:20:32 [scrapy] INFO: Crawled 5140 pages (at 7 pages/min), scraped 4996 items (at 12 items/min)
2015-11-04 03:20:50 [scrapy] ERROR: Error downloading <GET http://www.chemistdirect.co.uk/medigro-men-advanced-hair-loss-treatment/prd-3hrh>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 03:20:50 [scrapy] ERROR: Error downloading <GET http://www.chemistdirect.co.uk/bioburn-natural-fat-burner-and-xls-medical-fat/prd-3me5>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 03:22:00 [scrapy] INFO: Crawled 5164 pages (at 24 pages/min), scraped 5011 items (at 15 items/min)
2015-11-04 03:23:05 [scrapy] INFO: Crawled 5174 pages (at 10 pages/min), scraped 5028 items (at 17 items/min)
2015-11-04 03:24:17 [scrapy] ERROR: Error downloading <GET https://www.newsvine.com/_nv/api/accounts/tenderLogin?to=http%3A%2F%2Fsupport.newsvine.com%2Fdiscussions%2Fbugs%2F645-report-bugs-in-the-updated-commenting-system-here%3Funresolve%3Dtrue>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 03:24:17 [scrapy] INFO: Crawled 5197 pages (at 23 pages/min), scraped 5044 items (at 16 items/min)
2015-11-04 03:25:16 [scrapy] INFO: Crawled 5215 pages (at 18 pages/min), scraped 5068 items (at 24 items/min)
2015-11-04 03:26:07 [scrapy] INFO: Crawled 5242 pages (at 27 pages/min), scraped 5097 items (at 29 items/min)
2015-11-04 03:27:13 [scrapy] INFO: Crawled 5294 pages (at 52 pages/min), scraped 5145 items (at 48 items/min)
2015-11-04 03:27:14 [scrapy] ERROR: Error downloading <GET https://secure.newsvine.com/_tps/accounts/login?redirect=https%3A%2F%2Fwww.newsvine.com%2F_nv%2Fapi%2Faccounts%2FtenderLogin%3Fto%3Dhttp%253A%252F%252Fsupport.newsvine.com%252Fkb%252Fgetting-started&tender=newsvine&vid=1c2641f4edb4b6c788c05e57d771ab94&statusCode=success&statusMessage=Your+request+has+been+successfully+processed>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:27:38 [scrapy] ERROR: Error downloading <GET https://secure.newsvine.com/_tps/accounts/login?redirect=http%3A%2F%2Fwww.newsvine.com%2F_tps%2F_publish%2Fseed>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 03:28:14 [scrapy] INFO: Crawled 5339 pages (at 45 pages/min), scraped 5192 items (at 47 items/min)
2015-11-04 03:28:20 [scrapy] ERROR: Error downloading <GET https://secure.newsvine.com/_tps/accounts/login?redirect=https%3A%2F%2Fwww.newsvine.com%2F_nv%2Fapi%2Faccounts%2FtenderLogin%3Fto%3Dhttp%253A%252F%252Fsupport.newsvine.com%252Fdiscussions%252Fgeneral%252F74915-spammer&tender=newsvine&vid=1c2641f4edb4b6c788c05e57d771ab94&statusCode=success&statusMessage=Your+request+has+been+successfully+processed>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:28:20 [scrapy] ERROR: Error downloading <GET https://www.newsvine.com/_nv/api/accounts/tenderLogin?to=http%3A%2F%2Fsupport.newsvine.com%2Fdiscussions%2Fgeneral%2F74873-delete>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:28:20 [scrapy] ERROR: Error downloading <GET https://secure.newsvine.com/_tps/accounts/login?redirect=https%3A%2F%2Fwww.newsvine.com%2F_nv%2Fapi%2Faccounts%2FtenderLogin%3Fto%3Dhttp%253A%252F%252Fsupport.newsvine.com%252Fdiscussions%252Fgeneral%252F74865-it-said-i-was-sent-a-email-so-i-could-finish-opening-my-account&tender=newsvine&vid=1c2641f4edb4b6c788c05e57d771ab94&statusCode=success&statusMessage=Your+request+has+been+successfully+processed>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:28:20 [scrapy] ERROR: Error downloading <GET https://www.newsvine.com/_nv/api/accounts/tenderLogin?to=http%3A%2F%2Fsupport.newsvine.com%2Fdiscussions%2Fgeneral%2F74913-lost-seedmissing-comments>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:28:47 [scrapy] ERROR: Error downloading <GET https://secure.newsvine.com/_tps/accounts/login?redirect=https%3A%2F%2Fwww.newsvine.com%2F_nv%2Fapi%2Faccounts%2FtenderLogin%3Fto%3Dhttp%253A%252F%252Fsupport.newsvine.com%252Fhome&tender=newsvine&vid=1c2641f4edb4b6c788c05e57d771ab94&statusCode=success&statusMessage=Your+request+has+been+successfully+processed>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:29:26 [scrapy] INFO: Crawled 5397 pages (at 58 pages/min), scraped 5248 items (at 56 items/min)
2015-11-04 03:30:19 [scrapy] INFO: Crawled 5456 pages (at 59 pages/min), scraped 5283 items (at 35 items/min)
2015-11-04 03:31:11 [scrapy] INFO: Crawled 5509 pages (at 53 pages/min), scraped 5342 items (at 59 items/min)
2015-11-04 03:32:07 [scrapy] INFO: Crawled 5573 pages (at 64 pages/min), scraped 5414 items (at 72 items/min)
2015-11-04 03:33:12 [scrapy] INFO: Crawled 5620 pages (at 47 pages/min), scraped 5467 items (at 53 items/min)
2015-11-04 03:34:08 [scrapy] INFO: Crawled 5681 pages (at 61 pages/min), scraped 5514 items (at 47 items/min)
2015-11-04 03:35:17 [scrapy] INFO: Crawled 5725 pages (at 44 pages/min), scraped 5571 items (at 57 items/min)
2015-11-04 03:35:17 [scrapy] ERROR: Error downloading <GET https://iryo-de-hatarako.net/newusers/index1?lpp=formaf_12&re_adpcnt=1Vk_Wegc>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 03:36:05 [scrapy] INFO: Crawled 5770 pages (at 45 pages/min), scraped 5618 items (at 47 items/min)
2015-11-04 03:37:08 [scrapy] INFO: Crawled 5795 pages (at 25 pages/min), scraped 5649 items (at 31 items/min)
2015-11-04 03:38:11 [scrapy] INFO: Crawled 5832 pages (at 37 pages/min), scraped 5684 items (at 35 items/min)
2015-11-04 03:39:06 [scrapy] INFO: Crawled 5866 pages (at 34 pages/min), scraped 5710 items (at 26 items/min)
2015-11-04 03:40:10 [scrapy] INFO: Crawled 5898 pages (at 32 pages/min), scraped 5740 items (at 30 items/min)
2015-11-04 03:41:06 [scrapy] INFO: Crawled 5930 pages (at 32 pages/min), scraped 5772 items (at 32 items/min)
2015-11-04 03:42:06 [scrapy] INFO: Crawled 5967 pages (at 37 pages/min), scraped 5806 items (at 34 items/min)
2015-11-04 03:43:10 [scrapy] INFO: Crawled 6002 pages (at 35 pages/min), scraped 5841 items (at 35 items/min)
2015-11-04 03:44:04 [scrapy] INFO: Crawled 6033 pages (at 31 pages/min), scraped 5872 items (at 31 items/min)
2015-11-04 03:45:06 [scrapy] INFO: Crawled 6073 pages (at 40 pages/min), scraped 5912 items (at 40 items/min)
2015-11-04 03:46:10 [scrapy] INFO: Crawled 6117 pages (at 44 pages/min), scraped 5956 items (at 44 items/min)
2015-11-04 03:47:05 [scrapy] INFO: Crawled 6149 pages (at 32 pages/min), scraped 5983 items (at 27 items/min)
2015-11-04 03:48:05 [scrapy] INFO: Crawled 6185 pages (at 36 pages/min), scraped 6017 items (at 34 items/min)
2015-11-04 03:49:06 [scrapy] INFO: Crawled 6217 pages (at 32 pages/min), scraped 6049 items (at 32 items/min)
2015-11-04 03:50:09 [scrapy] INFO: Crawled 6253 pages (at 36 pages/min), scraped 6085 items (at 36 items/min)
2015-11-04 03:51:06 [scrapy] INFO: Crawled 6285 pages (at 32 pages/min), scraped 6115 items (at 30 items/min)
2015-11-04 03:52:11 [scrapy] INFO: Crawled 6313 pages (at 28 pages/min), scraped 6143 items (at 28 items/min)
2015-11-04 03:53:06 [scrapy] INFO: Crawled 6345 pages (at 32 pages/min), scraped 6175 items (at 32 items/min)
2015-11-04 03:54:13 [scrapy] INFO: Crawled 6377 pages (at 32 pages/min), scraped 6207 items (at 32 items/min)
2015-11-04 03:55:05 [scrapy] INFO: Crawled 6409 pages (at 32 pages/min), scraped 6239 items (at 32 items/min)
2015-11-04 03:56:07 [scrapy] INFO: Crawled 6441 pages (at 32 pages/min), scraped 6270 items (at 31 items/min)
2015-11-04 03:57:08 [scrapy] INFO: Crawled 6481 pages (at 40 pages/min), scraped 6302 items (at 32 items/min)
2015-11-04 03:58:12 [scrapy] INFO: Crawled 6556 pages (at 75 pages/min), scraped 6377 items (at 75 items/min)
2015-11-04 03:59:05 [scrapy] INFO: Crawled 6613 pages (at 57 pages/min), scraped 6425 items (at 48 items/min)
2015-11-04 04:00:13 [scrapy] INFO: Crawled 6673 pages (at 60 pages/min), scraped 6492 items (at 67 items/min)
2015-11-04 04:01:06 [scrapy] INFO: Crawled 6736 pages (at 63 pages/min), scraped 6555 items (at 63 items/min)
2015-11-04 04:02:15 [scrapy] INFO: Crawled 6785 pages (at 49 pages/min), scraped 6607 items (at 52 items/min)
2015-11-04 04:03:27 [scrapy] INFO: Crawled 6844 pages (at 59 pages/min), scraped 6665 items (at 58 items/min)
2015-11-04 04:04:24 [scrapy] INFO: Crawled 6874 pages (at 30 pages/min), scraped 6700 items (at 35 items/min)
2015-11-04 04:05:07 [scrapy] INFO: Crawled 6902 pages (at 28 pages/min), scraped 6730 items (at 30 items/min)
2015-11-04 04:06:13 [scrapy] INFO: Crawled 6939 pages (at 37 pages/min), scraped 6761 items (at 31 items/min)
2015-11-04 04:07:14 [scrapy] INFO: Crawled 6971 pages (at 32 pages/min), scraped 6793 items (at 32 items/min)
2015-11-04 04:08:09 [scrapy] INFO: Crawled 7003 pages (at 32 pages/min), scraped 6825 items (at 32 items/min)
2015-11-04 04:09:05 [scrapy] INFO: Crawled 7035 pages (at 32 pages/min), scraped 6857 items (at 32 items/min)
2015-11-04 04:10:09 [scrapy] INFO: Crawled 7067 pages (at 32 pages/min), scraped 6889 items (at 32 items/min)
2015-11-04 04:11:10 [scrapy] INFO: Crawled 7099 pages (at 32 pages/min), scraped 6921 items (at 32 items/min)
2015-11-04 04:12:07 [scrapy] INFO: Crawled 7124 pages (at 25 pages/min), scraped 6951 items (at 30 items/min)
2015-11-04 04:13:09 [scrapy] INFO: Crawled 7156 pages (at 32 pages/min), scraped 6985 items (at 34 items/min)
2015-11-04 04:14:05 [scrapy] INFO: Crawled 7204 pages (at 48 pages/min), scraped 7020 items (at 35 items/min)
2015-11-04 04:15:12 [scrapy] INFO: Crawled 7236 pages (at 32 pages/min), scraped 7060 items (at 40 items/min)
2015-11-04 04:16:06 [scrapy] INFO: Crawled 7276 pages (at 40 pages/min), scraped 7092 items (at 32 items/min)
2015-11-04 04:17:25 [scrapy] INFO: Crawled 7311 pages (at 35 pages/min), scraped 7139 items (at 47 items/min)
2015-11-04 04:18:19 [scrapy] INFO: Crawled 7343 pages (at 32 pages/min), scraped 7172 items (at 33 items/min)
2015-11-04 04:19:08 [scrapy] INFO: Crawled 7389 pages (at 46 pages/min), scraped 7204 items (at 32 items/min)
2015-11-04 04:20:09 [scrapy] INFO: Crawled 7411 pages (at 22 pages/min), scraped 7238 items (at 34 items/min)
2015-11-04 04:21:10 [scrapy] INFO: Crawled 7474 pages (at 63 pages/min), scraped 7285 items (at 47 items/min)
2015-11-04 04:22:10 [scrapy] INFO: Crawled 7517 pages (at 43 pages/min), scraped 7335 items (at 50 items/min)
2015-11-04 04:23:10 [scrapy] INFO: Crawled 7563 pages (at 46 pages/min), scraped 7388 items (at 53 items/min)
2015-11-04 04:24:08 [scrapy] INFO: Crawled 7592 pages (at 29 pages/min), scraped 7416 items (at 28 items/min)
2015-11-04 04:25:10 [scrapy] INFO: Crawled 7624 pages (at 32 pages/min), scraped 7453 items (at 37 items/min)
2015-11-04 04:26:05 [scrapy] INFO: Crawled 7654 pages (at 30 pages/min), scraped 7481 items (at 28 items/min)
2015-11-04 04:27:15 [scrapy] INFO: Crawled 7692 pages (at 38 pages/min), scraped 7519 items (at 38 items/min)
2015-11-04 04:28:04 [scrapy] INFO: Crawled 7726 pages (at 34 pages/min), scraped 7547 items (at 28 items/min)
2015-11-04 04:29:10 [scrapy] INFO: Crawled 7762 pages (at 36 pages/min), scraped 7583 items (at 36 items/min)
2015-11-04 04:30:06 [scrapy] INFO: Crawled 7806 pages (at 44 pages/min), scraped 7611 items (at 28 items/min)
2015-11-04 04:31:11 [scrapy] INFO: Crawled 7838 pages (at 32 pages/min), scraped 7647 items (at 36 items/min)
2015-11-04 04:32:04 [scrapy] INFO: Crawled 7870 pages (at 32 pages/min), scraped 7675 items (at 28 items/min)
2015-11-04 04:33:05 [scrapy] INFO: Crawled 7898 pages (at 28 pages/min), scraped 7703 items (at 28 items/min)
2015-11-04 04:34:05 [scrapy] INFO: Crawled 7934 pages (at 36 pages/min), scraped 7736 items (at 33 items/min)
2015-11-04 04:35:08 [scrapy] INFO: Crawled 7966 pages (at 32 pages/min), scraped 7770 items (at 34 items/min)
2015-11-04 04:36:09 [scrapy] INFO: Crawled 8001 pages (at 35 pages/min), scraped 7807 items (at 37 items/min)
2015-11-04 04:37:07 [scrapy] INFO: Crawled 8038 pages (at 37 pages/min), scraped 7842 items (at 35 items/min)
2015-11-04 04:38:11 [scrapy] INFO: Crawled 8073 pages (at 35 pages/min), scraped 7879 items (at 37 items/min)
2015-11-04 04:39:07 [scrapy] INFO: Crawled 8110 pages (at 37 pages/min), scraped 7914 items (at 35 items/min)
2015-11-04 04:40:04 [scrapy] INFO: Crawled 8150 pages (at 40 pages/min), scraped 7952 items (at 38 items/min)
2015-11-04 04:41:09 [scrapy] INFO: Crawled 8185 pages (at 35 pages/min), scraped 7989 items (at 37 items/min)
2015-11-04 04:42:06 [scrapy] INFO: Crawled 8222 pages (at 37 pages/min), scraped 8024 items (at 35 items/min)
2015-11-04 04:43:07 [scrapy] INFO: Crawled 8255 pages (at 33 pages/min), scraped 8061 items (at 37 items/min)
2015-11-04 04:44:05 [scrapy] INFO: Crawled 8292 pages (at 37 pages/min), scraped 8093 items (at 32 items/min)
2015-11-04 04:45:07 [scrapy] INFO: Crawled 8328 pages (at 36 pages/min), scraped 8122 items (at 29 items/min)
2015-11-04 04:46:06 [scrapy] INFO: Crawled 8368 pages (at 40 pages/min), scraped 8162 items (at 40 items/min)
2015-11-04 04:47:10 [scrapy] INFO: Crawled 8408 pages (at 40 pages/min), scraped 8202 items (at 40 items/min)
2015-11-04 04:48:06 [scrapy] INFO: Crawled 8445 pages (at 37 pages/min), scraped 8231 items (at 29 items/min)
2015-11-04 04:49:14 [scrapy] INFO: Crawled 8479 pages (at 34 pages/min), scraped 8266 items (at 35 items/min)
2015-11-04 04:50:07 [scrapy] INFO: Crawled 8509 pages (at 30 pages/min), scraped 8292 items (at 26 items/min)
2015-11-04 04:51:10 [scrapy] INFO: Crawled 8543 pages (at 34 pages/min), scraped 8330 items (at 38 items/min)
2015-11-04 04:52:05 [scrapy] INFO: Crawled 8567 pages (at 24 pages/min), scraped 8352 items (at 22 items/min)
2015-11-04 04:53:05 [scrapy] INFO: Crawled 8605 pages (at 38 pages/min), scraped 8386 items (at 34 items/min)
2015-11-04 04:54:10 [scrapy] INFO: Crawled 8639 pages (at 34 pages/min), scraped 8422 items (at 36 items/min)
2015-11-04 04:55:10 [scrapy] INFO: Crawled 8671 pages (at 32 pages/min), scraped 8454 items (at 32 items/min)
2015-11-04 04:56:16 [scrapy] INFO: Crawled 8703 pages (at 32 pages/min), scraped 8485 items (at 31 items/min)
2015-11-04 04:57:12 [scrapy] INFO: Crawled 8735 pages (at 32 pages/min), scraped 8517 items (at 32 items/min)
2015-11-04 04:58:05 [scrapy] INFO: Crawled 8765 pages (at 30 pages/min), scraped 8542 items (at 25 items/min)
2015-11-04 04:59:05 [scrapy] INFO: Crawled 8797 pages (at 32 pages/min), scraped 8574 items (at 32 items/min)
2015-11-04 05:00:07 [scrapy] INFO: Crawled 8829 pages (at 32 pages/min), scraped 8606 items (at 32 items/min)
2015-11-04 05:01:04 [scrapy] INFO: Crawled 8864 pages (at 35 pages/min), scraped 8637 items (at 31 items/min)
2015-11-04 05:02:12 [scrapy] INFO: Crawled 8899 pages (at 35 pages/min), scraped 8669 items (at 32 items/min)
2015-11-04 05:03:05 [scrapy] INFO: Crawled 8930 pages (at 31 pages/min), scraped 8697 items (at 28 items/min)
2015-11-04 05:04:15 [scrapy] INFO: Crawled 8978 pages (at 48 pages/min), scraped 8744 items (at 47 items/min)
2015-11-04 05:05:15 [scrapy] INFO: Crawled 9013 pages (at 35 pages/min), scraped 8783 items (at 39 items/min)
2015-11-04 05:06:09 [scrapy] INFO: Crawled 9047 pages (at 34 pages/min), scraped 8817 items (at 34 items/min)
2015-11-04 05:07:06 [scrapy] INFO: Crawled 9070 pages (at 23 pages/min), scraped 8840 items (at 23 items/min)
2015-11-04 05:08:07 [scrapy] INFO: Crawled 9098 pages (at 28 pages/min), scraped 8864 items (at 24 items/min)
2015-11-04 05:09:10 [scrapy] INFO: Crawled 9116 pages (at 18 pages/min), scraped 8888 items (at 24 items/min)
2015-11-04 05:10:05 [scrapy] INFO: Crawled 9144 pages (at 28 pages/min), scraped 8912 items (at 24 items/min)
2015-11-04 05:11:18 [scrapy] INFO: Crawled 9166 pages (at 22 pages/min), scraped 8936 items (at 24 items/min)
2015-11-04 05:12:08 [scrapy] INFO: Crawled 9184 pages (at 18 pages/min), scraped 8952 items (at 16 items/min)
2015-11-04 05:13:05 [scrapy] INFO: Crawled 9219 pages (at 35 pages/min), scraped 8987 items (at 35 items/min)
2015-11-04 05:14:10 [scrapy] INFO: Crawled 9249 pages (at 30 pages/min), scraped 9019 items (at 32 items/min)
2015-11-04 05:15:09 [scrapy] INFO: Crawled 9278 pages (at 29 pages/min), scraped 9050 items (at 31 items/min)
2015-11-04 05:16:06 [scrapy] INFO: Crawled 9310 pages (at 32 pages/min), scraped 9078 items (at 28 items/min)
2015-11-04 05:17:09 [scrapy] INFO: Crawled 9344 pages (at 34 pages/min), scraped 9116 items (at 38 items/min)
2015-11-04 05:18:06 [scrapy] INFO: Crawled 9374 pages (at 30 pages/min), scraped 9142 items (at 26 items/min)
2015-11-04 05:19:06 [scrapy] INFO: Crawled 9399 pages (at 25 pages/min), scraped 9171 items (at 29 items/min)
2015-11-04 05:20:08 [scrapy] INFO: Crawled 9436 pages (at 37 pages/min), scraped 9204 items (at 33 items/min)
2015-11-04 05:21:11 [scrapy] INFO: Crawled 9462 pages (at 26 pages/min), scraped 9234 items (at 30 items/min)
2015-11-04 05:22:09 [scrapy] INFO: Crawled 9494 pages (at 32 pages/min), scraped 9266 items (at 32 items/min)
2015-11-04 05:23:07 [scrapy] INFO: Crawled 9526 pages (at 32 pages/min), scraped 9298 items (at 32 items/min)
2015-11-04 05:24:10 [scrapy] INFO: Crawled 9566 pages (at 40 pages/min), scraped 9338 items (at 40 items/min)
2015-11-04 05:25:11 [scrapy] INFO: Crawled 9612 pages (at 46 pages/min), scraped 9378 items (at 40 items/min)
2015-11-04 05:26:07 [scrapy] INFO: Crawled 9642 pages (at 30 pages/min), scraped 9410 items (at 32 items/min)
2015-11-04 05:27:12 [scrapy] INFO: Crawled 9676 pages (at 34 pages/min), scraped 9448 items (at 38 items/min)
2015-11-04 05:28:08 [scrapy] INFO: Crawled 9706 pages (at 30 pages/min), scraped 9476 items (at 28 items/min)
2015-11-04 05:29:05 [scrapy] INFO: Crawled 9738 pages (at 32 pages/min), scraped 9508 items (at 32 items/min)
2015-11-04 05:30:07 [scrapy] INFO: Crawled 9774 pages (at 36 pages/min), scraped 9544 items (at 36 items/min)
2015-11-04 05:31:13 [scrapy] INFO: Crawled 9808 pages (at 34 pages/min), scraped 9578 items (at 34 items/min)
2015-11-04 05:32:06 [scrapy] INFO: Crawled 9836 pages (at 28 pages/min), scraped 9606 items (at 28 items/min)
2015-11-04 05:33:09 [scrapy] INFO: Crawled 9876 pages (at 40 pages/min), scraped 9646 items (at 40 items/min)
2015-11-04 05:34:10 [scrapy] INFO: Crawled 9917 pages (at 41 pages/min), scraped 9687 items (at 41 items/min)
2015-11-04 05:35:04 [scrapy] INFO: Crawled 9949 pages (at 32 pages/min), scraped 9719 items (at 32 items/min)
2015-11-04 05:36:08 [scrapy] INFO: Crawled 9994 pages (at 45 pages/min), scraped 9764 items (at 45 items/min)
2015-11-04 05:37:08 [scrapy] INFO: Crawled 10022 pages (at 28 pages/min), scraped 9792 items (at 28 items/min)
2015-11-04 05:38:08 [scrapy] INFO: Crawled 10058 pages (at 36 pages/min), scraped 9828 items (at 36 items/min)
2015-11-04 05:39:04 [scrapy] INFO: Crawled 10093 pages (at 35 pages/min), scraped 9863 items (at 35 items/min)
2015-11-04 05:40:05 [scrapy] INFO: Crawled 10129 pages (at 36 pages/min), scraped 9899 items (at 36 items/min)
2015-11-04 05:41:05 [scrapy] INFO: Crawled 10157 pages (at 28 pages/min), scraped 9927 items (at 28 items/min)
2015-11-04 05:42:06 [scrapy] INFO: Crawled 10197 pages (at 40 pages/min), scraped 9967 items (at 40 items/min)
2015-11-04 05:43:09 [scrapy] INFO: Crawled 10229 pages (at 32 pages/min), scraped 9999 items (at 32 items/min)
2015-11-04 05:44:07 [scrapy] INFO: Crawled 10261 pages (at 32 pages/min), scraped 10031 items (at 32 items/min)
2015-11-04 05:45:07 [scrapy] INFO: Crawled 10293 pages (at 32 pages/min), scraped 10063 items (at 32 items/min)
2015-11-04 05:46:08 [scrapy] INFO: Crawled 10325 pages (at 32 pages/min), scraped 10095 items (at 32 items/min)
2015-11-04 05:47:09 [scrapy] INFO: Crawled 10353 pages (at 28 pages/min), scraped 10123 items (at 28 items/min)
2015-11-04 05:48:06 [scrapy] INFO: Crawled 10393 pages (at 40 pages/min), scraped 10163 items (at 40 items/min)
2015-11-04 05:49:08 [scrapy] INFO: Crawled 10433 pages (at 40 pages/min), scraped 10203 items (at 40 items/min)
2015-11-04 05:50:05 [scrapy] INFO: Crawled 10461 pages (at 28 pages/min), scraped 10231 items (at 28 items/min)
2015-11-04 05:51:09 [scrapy] INFO: Crawled 10489 pages (at 28 pages/min), scraped 10259 items (at 28 items/min)
2015-11-04 05:52:08 [scrapy] INFO: Crawled 10529 pages (at 40 pages/min), scraped 10299 items (at 40 items/min)
2015-11-04 05:53:09 [scrapy] INFO: Crawled 10569 pages (at 40 pages/min), scraped 10339 items (at 40 items/min)
2015-11-04 05:54:09 [scrapy] INFO: Crawled 10597 pages (at 28 pages/min), scraped 10367 items (at 28 items/min)
2015-11-04 05:55:12 [scrapy] INFO: Crawled 10625 pages (at 28 pages/min), scraped 10395 items (at 28 items/min)
2015-11-04 05:56:10 [scrapy] INFO: Crawled 10653 pages (at 28 pages/min), scraped 10423 items (at 28 items/min)
2015-11-04 05:57:06 [scrapy] INFO: Crawled 10685 pages (at 32 pages/min), scraped 10455 items (at 32 items/min)
2015-11-04 05:58:09 [scrapy] INFO: Crawled 10721 pages (at 36 pages/min), scraped 10491 items (at 36 items/min)
2015-11-04 05:59:10 [scrapy] INFO: Crawled 10753 pages (at 32 pages/min), scraped 10523 items (at 32 items/min)
2015-11-04 06:00:07 [scrapy] INFO: Crawled 10785 pages (at 32 pages/min), scraped 10555 items (at 32 items/min)
2015-11-04 06:01:10 [scrapy] INFO: Crawled 10813 pages (at 28 pages/min), scraped 10583 items (at 28 items/min)
2015-11-04 06:02:06 [scrapy] INFO: Crawled 10837 pages (at 24 pages/min), scraped 10607 items (at 24 items/min)
2015-11-04 06:03:06 [scrapy] INFO: Crawled 10869 pages (at 32 pages/min), scraped 10639 items (at 32 items/min)
2015-11-04 06:04:09 [scrapy] INFO: Crawled 10905 pages (at 36 pages/min), scraped 10675 items (at 36 items/min)
2015-11-04 06:05:06 [scrapy] INFO: Crawled 10941 pages (at 36 pages/min), scraped 10711 items (at 36 items/min)
2015-11-04 06:06:07 [scrapy] INFO: Crawled 10973 pages (at 32 pages/min), scraped 10743 items (at 32 items/min)
2015-11-04 06:07:09 [scrapy] INFO: Crawled 11013 pages (at 40 pages/min), scraped 10783 items (at 40 items/min)
2015-11-04 06:08:08 [scrapy] INFO: Crawled 11049 pages (at 36 pages/min), scraped 10819 items (at 36 items/min)
2015-11-04 06:09:08 [scrapy] INFO: Crawled 11081 pages (at 32 pages/min), scraped 10851 items (at 32 items/min)
2015-11-04 06:10:07 [scrapy] INFO: Crawled 11113 pages (at 32 pages/min), scraped 10883 items (at 32 items/min)
2015-11-04 06:11:09 [scrapy] INFO: Crawled 11141 pages (at 28 pages/min), scraped 10911 items (at 28 items/min)
2015-11-04 06:12:12 [scrapy] INFO: Crawled 11169 pages (at 28 pages/min), scraped 10939 items (at 28 items/min)
2015-11-04 06:13:08 [scrapy] INFO: Crawled 11197 pages (at 28 pages/min), scraped 10967 items (at 28 items/min)
2015-11-04 06:14:07 [scrapy] INFO: Crawled 11229 pages (at 32 pages/min), scraped 10999 items (at 32 items/min)
2015-11-04 06:15:04 [scrapy] INFO: Crawled 11257 pages (at 28 pages/min), scraped 11027 items (at 28 items/min)
2015-11-04 06:16:11 [scrapy] INFO: Crawled 11293 pages (at 36 pages/min), scraped 11063 items (at 36 items/min)
2015-11-04 06:17:10 [scrapy] INFO: Crawled 11321 pages (at 28 pages/min), scraped 11091 items (at 28 items/min)
2015-11-04 06:18:08 [scrapy] INFO: Crawled 11345 pages (at 24 pages/min), scraped 11115 items (at 24 items/min)
2015-11-04 06:19:07 [scrapy] INFO: Crawled 11377 pages (at 32 pages/min), scraped 11147 items (at 32 items/min)
2015-11-04 06:20:09 [scrapy] INFO: Crawled 11409 pages (at 32 pages/min), scraped 11179 items (at 32 items/min)
2015-11-04 06:21:05 [scrapy] INFO: Crawled 11437 pages (at 28 pages/min), scraped 11207 items (at 28 items/min)
2015-11-04 06:22:05 [scrapy] INFO: Crawled 11470 pages (at 33 pages/min), scraped 11239 items (at 32 items/min)
2015-11-04 06:23:12 [scrapy] INFO: Crawled 11502 pages (at 32 pages/min), scraped 11272 items (at 33 items/min)
2015-11-04 06:24:12 [scrapy] INFO: Crawled 11526 pages (at 24 pages/min), scraped 11296 items (at 24 items/min)
2015-11-04 06:25:05 [scrapy] INFO: Crawled 11554 pages (at 28 pages/min), scraped 11324 items (at 28 items/min)
2015-11-04 06:26:08 [scrapy] INFO: Crawled 11586 pages (at 32 pages/min), scraped 11356 items (at 32 items/min)
2015-11-04 06:27:05 [scrapy] INFO: Crawled 11618 pages (at 32 pages/min), scraped 11388 items (at 32 items/min)
2015-11-04 06:28:08 [scrapy] INFO: Crawled 11650 pages (at 32 pages/min), scraped 11420 items (at 32 items/min)
2015-11-04 06:29:05 [scrapy] INFO: Crawled 11686 pages (at 36 pages/min), scraped 11453 items (at 33 items/min)
2015-11-04 06:30:11 [scrapy] INFO: Crawled 11726 pages (at 40 pages/min), scraped 11492 items (at 39 items/min)
2015-11-04 06:31:09 [scrapy] INFO: Crawled 11748 pages (at 22 pages/min), scraped 11518 items (at 26 items/min)
2015-11-04 06:32:10 [scrapy] INFO: Crawled 11775 pages (at 27 pages/min), scraped 11545 items (at 27 items/min)
2015-11-04 06:33:08 [scrapy] INFO: Crawled 11803 pages (at 28 pages/min), scraped 11573 items (at 28 items/min)
2015-11-04 06:34:05 [scrapy] INFO: Crawled 11830 pages (at 27 pages/min), scraped 11601 items (at 28 items/min)
2015-11-04 06:35:11 [scrapy] INFO: Crawled 11865 pages (at 35 pages/min), scraped 11635 items (at 34 items/min)
2015-11-04 06:36:12 [scrapy] INFO: Crawled 11897 pages (at 32 pages/min), scraped 11667 items (at 32 items/min)
2015-11-04 06:37:09 [scrapy] INFO: Crawled 11925 pages (at 28 pages/min), scraped 11695 items (at 28 items/min)
2015-11-04 06:38:06 [scrapy] INFO: Crawled 11963 pages (at 38 pages/min), scraped 11724 items (at 29 items/min)
2015-11-04 06:39:08 [scrapy] INFO: Crawled 12024 pages (at 61 pages/min), scraped 11788 items (at 64 items/min)
2015-11-04 06:40:09 [scrapy] INFO: Crawled 12099 pages (at 75 pages/min), scraped 11861 items (at 73 items/min)
2015-11-04 06:41:08 [scrapy] INFO: Crawled 12156 pages (at 57 pages/min), scraped 11920 items (at 59 items/min)
2015-11-04 06:42:09 [scrapy] INFO: Crawled 12219 pages (at 63 pages/min), scraped 11978 items (at 58 items/min)
2015-11-04 06:42:52 [scrapy] ERROR: Error downloading <GET http://www.touchstoneh.com/blog/?page=6>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py", line 46, in process_response
    response = method(request=request, response=response, spider=spider)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 27, in process_response
    decoded_body = self._decode(response.body, encoding.lower())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 54, in _decode
    body = zlib.decompress(body, -15)
error: Error -5 while decompressing data: incomplete or truncated stream
2015-11-04 06:43:09 [scrapy] INFO: Crawled 12269 pages (at 50 pages/min), scraped 12036 items (at 58 items/min)
2015-11-04 06:44:12 [scrapy] INFO: Crawled 12330 pages (at 61 pages/min), scraped 12092 items (at 56 items/min)
2015-11-04 06:44:39 [scrapy] ERROR: Error downloading <GET http://www.touchstoneh.com/blog/?tag=%2FWomen%27s+Health>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py", line 46, in process_response
    response = method(request=request, response=response, spider=spider)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 27, in process_response
    decoded_body = self._decode(response.body, encoding.lower())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 54, in _decode
    body = zlib.decompress(body, -15)
error: Error -5 while decompressing data: incomplete or truncated stream
2015-11-04 06:44:46 [scrapy] ERROR: Error downloading <GET http://www.touchstoneh.com/blog/?tag=%2FMedicare-part-B>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py", line 46, in process_response
    response = method(request=request, response=response, spider=spider)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 27, in process_response
    decoded_body = self._decode(response.body, encoding.lower())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 54, in _decode
    body = zlib.decompress(body, -15)
error: Error -5 while decompressing data: incomplete or truncated stream
2015-11-04 06:45:20 [scrapy] INFO: Crawled 12379 pages (at 49 pages/min), scraped 12144 items (at 52 items/min)
2015-11-04 06:46:08 [scrapy] INFO: Crawled 12403 pages (at 24 pages/min), scraped 12171 items (at 27 items/min)
2015-11-04 06:46:23 [scrapy] ERROR: Error downloading <GET http://www.touchstoneh.com/blog/?tag=%2FAwareness>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py", line 46, in process_response
    response = method(request=request, response=response, spider=spider)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 27, in process_response
    decoded_body = self._decode(response.body, encoding.lower())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/downloadermiddlewares/httpcompression.py", line 54, in _decode
    body = zlib.decompress(body, -15)
error: Error -5 while decompressing data: incomplete or truncated stream
2015-11-04 06:47:08 [scrapy] INFO: Crawled 12469 pages (at 66 pages/min), scraped 12225 items (at 54 items/min)
2015-11-04 06:48:05 [scrapy] INFO: Crawled 12538 pages (at 69 pages/min), scraped 12291 items (at 66 items/min)
2015-11-04 06:49:13 [scrapy] INFO: Crawled 12624 pages (at 86 pages/min), scraped 12383 items (at 92 items/min)
2015-11-04 06:50:07 [scrapy] INFO: Crawled 12708 pages (at 84 pages/min), scraped 12461 items (at 78 items/min)
2015-11-04 06:51:06 [scrapy] INFO: Crawled 12802 pages (at 94 pages/min), scraped 12555 items (at 94 items/min)
2015-11-04 06:52:12 [scrapy] INFO: Crawled 12839 pages (at 37 pages/min), scraped 12591 items (at 36 items/min)
2015-11-04 06:53:42 [scrapy] INFO: Crawled 12882 pages (at 43 pages/min), scraped 12633 items (at 42 items/min)
2015-11-04 06:55:03 [scrapy] INFO: Crawled 12890 pages (at 8 pages/min), scraped 12647 items (at 14 items/min)
2015-11-04 06:55:10 [scrapy] INFO: Crawled 12890 pages (at 0 pages/min), scraped 12655 items (at 8 items/min)
2015-11-04 06:56:17 [scrapy] INFO: Crawled 12926 pages (at 36 pages/min), scraped 12682 items (at 27 items/min)
2015-11-04 06:57:53 [scrapy] INFO: Crawled 12934 pages (at 8 pages/min), scraped 12689 items (at 7 items/min)
2015-11-04 06:59:01 [scrapy] INFO: Crawled 12934 pages (at 0 pages/min), scraped 12695 items (at 6 items/min)
2015-11-04 07:00:29 [scrapy] INFO: Crawled 12934 pages (at 0 pages/min), scraped 12703 items (at 8 items/min)
2015-11-04 07:01:15 [scrapy] INFO: Crawled 12949 pages (at 15 pages/min), scraped 12705 items (at 2 items/min)
2015-11-04 07:02:19 [scrapy] INFO: Crawled 12957 pages (at 8 pages/min), scraped 12710 items (at 5 items/min)
2015-11-04 07:03:26 [scrapy] INFO: Crawled 12957 pages (at 0 pages/min), scraped 12718 items (at 8 items/min)
2015-11-04 07:04:36 [scrapy] INFO: Crawled 12957 pages (at 0 pages/min), scraped 12726 items (at 8 items/min)
2015-11-04 07:05:19 [scrapy] INFO: Crawled 12977 pages (at 20 pages/min), scraped 12733 items (at 7 items/min)
2015-11-04 07:06:14 [scrapy] INFO: Crawled 12979 pages (at 2 pages/min), scraped 12738 items (at 5 items/min)
2015-11-04 07:07:36 [scrapy] INFO: Crawled 12979 pages (at 0 pages/min), scraped 12746 items (at 8 items/min)
2015-11-04 07:08:04 [scrapy] INFO: Crawled 12979 pages (at 0 pages/min), scraped 12748 items (at 2 items/min)
2015-11-04 07:09:41 [scrapy] INFO: Crawled 13000 pages (at 21 pages/min), scraped 12755 items (at 7 items/min)
2015-11-04 07:11:05 [scrapy] INFO: Crawled 13000 pages (at 0 pages/min), scraped 12761 items (at 6 items/min)
2015-11-04 07:12:21 [scrapy] INFO: Crawled 13000 pages (at 0 pages/min), scraped 12769 items (at 8 items/min)
2015-11-04 07:13:29 [scrapy] INFO: Crawled 13017 pages (at 17 pages/min), scraped 12773 items (at 4 items/min)
2015-11-04 07:14:25 [scrapy] INFO: Crawled 13022 pages (at 5 pages/min), scraped 12779 items (at 6 items/min)
2015-11-04 07:15:40 [scrapy] INFO: Crawled 13022 pages (at 0 pages/min), scraped 12786 items (at 7 items/min)
2015-11-04 07:16:21 [scrapy] INFO: Crawled 13022 pages (at 0 pages/min), scraped 12791 items (at 5 items/min)
2015-11-04 07:17:07 [scrapy] INFO: Crawled 13041 pages (at 19 pages/min), scraped 12795 items (at 4 items/min)
2015-11-04 07:18:15 [scrapy] INFO: Crawled 13042 pages (at 1 pages/min), scraped 12802 items (at 7 items/min)
2015-11-04 07:19:38 [scrapy] INFO: Crawled 13042 pages (at 0 pages/min), scraped 12810 items (at 8 items/min)
2015-11-04 07:20:09 [scrapy] INFO: Crawled 13055 pages (at 13 pages/min), scraped 12812 items (at 2 items/min)
2015-11-04 07:21:22 [scrapy] INFO: Crawled 13063 pages (at 8 pages/min), scraped 12817 items (at 5 items/min)
2015-11-04 07:24:20 [scrapy] INFO: Crawled 13063 pages (at 0 pages/min), scraped 12832 items (at 15 items/min)
2015-11-04 07:25:31 [scrapy] INFO: Crawled 13078 pages (at 15 pages/min), scraped 12836 items (at 4 items/min)
2015-11-04 07:27:31 [scrapy] INFO: Crawled 13084 pages (at 6 pages/min), scraped 12847 items (at 11 items/min)
2015-11-04 07:28:22 [scrapy] INFO: Crawled 13084 pages (at 0 pages/min), scraped 12852 items (at 5 items/min)
2015-11-04 07:29:12 [scrapy] INFO: Crawled 13099 pages (at 15 pages/min), scraped 12856 items (at 4 items/min)
2015-11-04 07:31:12 [scrapy] INFO: Crawled 13105 pages (at 6 pages/min), scraped 12868 items (at 12 items/min)
2015-11-04 07:32:07 [scrapy] INFO: Crawled 13105 pages (at 0 pages/min), scraped 12874 items (at 6 items/min)
2015-11-04 07:34:18 [scrapy] INFO: Crawled 13127 pages (at 22 pages/min), scraped 12885 items (at 11 items/min)
2015-11-04 07:36:15 [scrapy] INFO: Crawled 13127 pages (at 0 pages/min), scraped 12896 items (at 11 items/min)
2015-11-04 07:37:36 [scrapy] INFO: Crawled 13147 pages (at 20 pages/min), scraped 12901 items (at 5 items/min)
2015-11-04 07:39:04 [scrapy] INFO: Crawled 13147 pages (at 0 pages/min), scraped 12909 items (at 8 items/min)
2015-11-04 07:40:23 [scrapy] INFO: Crawled 13147 pages (at 0 pages/min), scraped 12916 items (at 7 items/min)
2015-11-04 07:41:22 [scrapy] INFO: Crawled 13167 pages (at 20 pages/min), scraped 12920 items (at 4 items/min)
2015-11-04 07:42:28 [scrapy] INFO: Crawled 13167 pages (at 0 pages/min), scraped 12928 items (at 8 items/min)
2015-11-04 07:43:30 [scrapy] INFO: Crawled 13167 pages (at 0 pages/min), scraped 12936 items (at 8 items/min)
2015-11-04 07:44:08 [scrapy] INFO: Crawled 13176 pages (at 9 pages/min), scraped 12937 items (at 1 items/min)
2015-11-04 07:45:58 [scrapy] INFO: Crawled 13188 pages (at 12 pages/min), scraped 12945 items (at 8 items/min)
2015-11-04 07:47:16 [scrapy] INFO: Crawled 13188 pages (at 0 pages/min), scraped 12951 items (at 6 items/min)
2015-11-04 07:48:20 [scrapy] INFO: Crawled 13188 pages (at 0 pages/min), scraped 12957 items (at 6 items/min)
2015-11-04 07:50:33 [scrapy] INFO: Crawled 13206 pages (at 18 pages/min), scraped 12969 items (at 12 items/min)
2015-11-04 07:51:31 [scrapy] INFO: Crawled 13206 pages (at 0 pages/min), scraped 12975 items (at 6 items/min)
2015-11-04 07:52:36 [scrapy] INFO: Crawled 13226 pages (at 20 pages/min), scraped 12983 items (at 8 items/min)
2015-11-04 07:53:04 [scrapy] INFO: Crawled 13226 pages (at 0 pages/min), scraped 12987 items (at 4 items/min)
2015-11-04 07:54:36 [scrapy] INFO: Crawled 13226 pages (at 0 pages/min), scraped 12995 items (at 8 items/min)
2015-11-04 07:55:09 [scrapy] INFO: Crawled 13235 pages (at 9 pages/min), scraped 12996 items (at 1 items/min)
2015-11-04 07:56:10 [scrapy] INFO: Crawled 13283 pages (at 48 pages/min), scraped 13044 items (at 48 items/min)
2015-11-04 07:57:10 [scrapy] INFO: Crawled 13348 pages (at 65 pages/min), scraped 13109 items (at 65 items/min)
2015-11-04 07:59:22 [scrapy] INFO: Crawled 13412 pages (at 64 pages/min), scraped 13173 items (at 64 items/min)
2015-11-04 08:00:51 [scrapy] INFO: Crawled 13427 pages (at 15 pages/min), scraped 13181 items (at 8 items/min)
2015-11-04 08:02:07 [scrapy] INFO: Crawled 13427 pages (at 0 pages/min), scraped 13188 items (at 7 items/min)
2015-11-04 08:03:17 [scrapy] INFO: Crawled 13427 pages (at 0 pages/min), scraped 13196 items (at 8 items/min)
2015-11-04 08:04:58 [scrapy] INFO: Crawled 13444 pages (at 17 pages/min), scraped 13205 items (at 9 items/min)
2015-11-04 08:05:11 [scrapy] INFO: Crawled 13452 pages (at 8 pages/min), scraped 13206 items (at 1 items/min)
2015-11-04 08:06:27 [scrapy] INFO: Crawled 13460 pages (at 8 pages/min), scraped 13213 items (at 7 items/min)
2015-11-04 08:07:48 [scrapy] INFO: Crawled 13460 pages (at 0 pages/min), scraped 13221 items (at 8 items/min)
2015-11-04 08:09:32 [scrapy] INFO: Crawled 13460 pages (at 0 pages/min), scraped 13229 items (at 8 items/min)
2015-11-04 08:10:24 [scrapy] INFO: Crawled 13472 pages (at 12 pages/min), scraped 13231 items (at 2 items/min)
2015-11-04 08:11:31 [scrapy] INFO: Crawled 13479 pages (at 7 pages/min), scraped 13236 items (at 5 items/min)
2015-11-04 08:12:26 [scrapy] INFO: Crawled 13479 pages (at 0 pages/min), scraped 13241 items (at 5 items/min)
2015-11-04 08:13:50 [scrapy] INFO: Crawled 13479 pages (at 0 pages/min), scraped 13248 items (at 7 items/min)
2015-11-04 08:14:13 [scrapy] INFO: Crawled 13488 pages (at 9 pages/min), scraped 13249 items (at 1 items/min)
2015-11-04 08:15:36 [scrapy] INFO: Crawled 13495 pages (at 7 pages/min), scraped 13257 items (at 8 items/min)
2015-11-04 08:16:52 [scrapy] INFO: Crawled 13495 pages (at 0 pages/min), scraped 13264 items (at 7 items/min)
2015-11-04 08:17:04 [scrapy] INFO: Crawled 13495 pages (at 0 pages/min), scraped 13264 items (at 0 items/min)
2015-11-04 08:19:17 [scrapy] INFO: Crawled 13519 pages (at 24 pages/min), scraped 13273 items (at 9 items/min)
2015-11-04 08:20:31 [scrapy] INFO: Crawled 13519 pages (at 0 pages/min), scraped 13280 items (at 7 items/min)
2015-11-04 08:22:00 [scrapy] INFO: Crawled 13519 pages (at 0 pages/min), scraped 13288 items (at 8 items/min)
2015-11-04 08:22:22 [scrapy] INFO: Crawled 13531 pages (at 12 pages/min), scraped 13289 items (at 1 items/min)
2015-11-04 08:24:15 [scrapy] INFO: Crawled 13542 pages (at 11 pages/min), scraped 13300 items (at 11 items/min)
2015-11-04 08:25:59 [scrapy] INFO: Crawled 13542 pages (at 0 pages/min), scraped 13311 items (at 11 items/min)
2015-11-04 08:26:21 [scrapy] INFO: Crawled 13554 pages (at 12 pages/min), scraped 13312 items (at 1 items/min)
2015-11-04 08:27:07 [scrapy] INFO: Crawled 13562 pages (at 8 pages/min), scraped 13316 items (at 4 items/min)
2015-11-04 08:28:15 [scrapy] INFO: Crawled 13562 pages (at 0 pages/min), scraped 13323 items (at 7 items/min)
2015-11-04 08:29:08 [scrapy] INFO: Crawled 13585 pages (at 23 pages/min), scraped 13338 items (at 15 items/min)
2015-11-04 08:30:05 [scrapy] INFO: Crawled 13641 pages (at 56 pages/min), scraped 13402 items (at 64 items/min)
2015-11-04 08:31:08 [scrapy] INFO: Crawled 13708 pages (at 67 pages/min), scraped 13469 items (at 67 items/min)
2015-11-04 08:32:07 [scrapy] INFO: Crawled 13772 pages (at 64 pages/min), scraped 13533 items (at 64 items/min)
2015-11-04 08:33:11 [scrapy] INFO: Crawled 13844 pages (at 72 pages/min), scraped 13605 items (at 72 items/min)
2015-11-04 08:34:10 [scrapy] INFO: Crawled 13914 pages (at 70 pages/min), scraped 13669 items (at 64 items/min)
2015-11-04 08:35:06 [scrapy] INFO: Crawled 13978 pages (at 64 pages/min), scraped 13733 items (at 64 items/min)
2015-11-04 08:36:06 [scrapy] INFO: Crawled 14044 pages (at 66 pages/min), scraped 13805 items (at 72 items/min)
2015-11-04 08:37:10 [scrapy] INFO: Crawled 14108 pages (at 64 pages/min), scraped 13869 items (at 64 items/min)
2015-11-04 08:38:10 [scrapy] INFO: Crawled 14172 pages (at 64 pages/min), scraped 13933 items (at 64 items/min)
2015-11-04 08:40:19 [scrapy] INFO: Crawled 14228 pages (at 56 pages/min), scraped 13989 items (at 56 items/min)
2015-11-04 08:41:43 [scrapy] INFO: Crawled 14228 pages (at 0 pages/min), scraped 13997 items (at 8 items/min)
2015-11-04 08:42:08 [scrapy] INFO: Crawled 14237 pages (at 9 pages/min), scraped 13998 items (at 1 items/min)
2015-11-04 08:43:23 [scrapy] INFO: Crawled 14244 pages (at 7 pages/min), scraped 14006 items (at 8 items/min)
2015-11-04 08:44:34 [scrapy] INFO: Crawled 14244 pages (at 0 pages/min), scraped 14013 items (at 7 items/min)
2015-11-04 08:45:31 [scrapy] INFO: Crawled 14269 pages (at 25 pages/min), scraped 14022 items (at 9 items/min)
2015-11-04 08:46:08 [scrapy] INFO: Crawled 14301 pages (at 32 pages/min), scraped 14062 items (at 40 items/min)
2015-11-04 08:47:05 [scrapy] INFO: Crawled 14365 pages (at 64 pages/min), scraped 14126 items (at 64 items/min)
2015-11-04 08:48:10 [scrapy] INFO: Crawled 14445 pages (at 80 pages/min), scraped 14198 items (at 72 items/min)
2015-11-04 08:49:06 [scrapy] INFO: Crawled 14509 pages (at 64 pages/min), scraped 14262 items (at 64 items/min)
2015-11-04 08:50:04 [scrapy] INFO: Crawled 14573 pages (at 64 pages/min), scraped 14327 items (at 65 items/min)
2015-11-04 08:51:08 [scrapy] INFO: Crawled 14645 pages (at 72 pages/min), scraped 14398 items (at 71 items/min)
2015-11-04 08:52:05 [scrapy] INFO: Crawled 14709 pages (at 64 pages/min), scraped 14462 items (at 64 items/min)
2015-11-04 08:53:09 [scrapy] INFO: Crawled 14773 pages (at 64 pages/min), scraped 14534 items (at 72 items/min)
2015-11-04 08:54:10 [scrapy] INFO: Crawled 14853 pages (at 80 pages/min), scraped 14606 items (at 72 items/min)
2015-11-04 08:55:05 [scrapy] INFO: Crawled 14909 pages (at 56 pages/min), scraped 14670 items (at 64 items/min)
2015-11-04 08:56:46 [scrapy] INFO: Crawled 14925 pages (at 16 pages/min), scraped 14686 items (at 16 items/min)
2015-11-04 08:58:11 [scrapy] INFO: Crawled 14925 pages (at 0 pages/min), scraped 14694 items (at 8 items/min)
2015-11-04 09:00:00 [scrapy] INFO: Crawled 14949 pages (at 24 pages/min), scraped 14702 items (at 8 items/min)
2015-11-04 09:01:39 [scrapy] INFO: Crawled 14949 pages (at 0 pages/min), scraped 14710 items (at 8 items/min)
2015-11-04 09:03:07 [scrapy] INFO: Crawled 14949 pages (at 0 pages/min), scraped 14718 items (at 8 items/min)
2015-11-04 09:05:08 [scrapy] INFO: Crawled 14975 pages (at 26 pages/min), scraped 14728 items (at 10 items/min)
2015-11-04 09:06:36 [scrapy] INFO: Crawled 14975 pages (at 0 pages/min), scraped 14736 items (at 8 items/min)
2015-11-04 09:08:08 [scrapy] INFO: Crawled 14975 pages (at 0 pages/min), scraped 14744 items (at 8 items/min)
2015-11-04 09:09:07 [scrapy] INFO: Crawled 15024 pages (at 49 pages/min), scraped 14777 items (at 33 items/min)
2015-11-04 09:10:08 [scrapy] INFO: Crawled 15088 pages (at 64 pages/min), scraped 14841 items (at 64 items/min)
2015-11-04 09:11:04 [scrapy] INFO: Crawled 15152 pages (at 64 pages/min), scraped 14905 items (at 64 items/min)
2015-11-04 09:12:10 [scrapy] INFO: Crawled 15216 pages (at 64 pages/min), scraped 14977 items (at 72 items/min)
2015-11-04 09:13:09 [scrapy] INFO: Crawled 15280 pages (at 64 pages/min), scraped 15041 items (at 64 items/min)
2015-11-04 09:14:08 [scrapy] INFO: Crawled 15344 pages (at 64 pages/min), scraped 15105 items (at 64 items/min)
2015-11-04 09:15:07 [scrapy] INFO: Crawled 15409 pages (at 65 pages/min), scraped 15170 items (at 65 items/min)
2015-11-04 09:16:11 [scrapy] INFO: Crawled 15481 pages (at 72 pages/min), scraped 15242 items (at 72 items/min)
2015-11-04 09:17:08 [scrapy] INFO: Crawled 15545 pages (at 64 pages/min), scraped 15306 items (at 64 items/min)
2015-11-04 09:18:10 [scrapy] INFO: Crawled 15617 pages (at 72 pages/min), scraped 15378 items (at 72 items/min)
2015-11-04 09:20:15 [scrapy] INFO: Crawled 15648 pages (at 31 pages/min), scraped 15410 items (at 32 items/min)
2015-11-04 09:21:12 [scrapy] INFO: Crawled 15648 pages (at 0 pages/min), scraped 15417 items (at 7 items/min)
2015-11-04 09:22:09 [scrapy] INFO: Crawled 15662 pages (at 14 pages/min), scraped 15426 items (at 9 items/min)
2015-11-04 09:23:13 [scrapy] INFO: Crawled 15735 pages (at 73 pages/min), scraped 15496 items (at 70 items/min)
2015-11-04 09:24:05 [scrapy] INFO: Crawled 15799 pages (at 64 pages/min), scraped 15552 items (at 56 items/min)
2015-11-04 09:25:05 [scrapy] INFO: Crawled 15855 pages (at 56 pages/min), scraped 15616 items (at 64 items/min)
2015-11-04 09:26:05 [scrapy] INFO: Crawled 15927 pages (at 72 pages/min), scraped 15683 items (at 67 items/min)
2015-11-04 09:27:07 [scrapy] INFO: Crawled 15999 pages (at 72 pages/min), scraped 15752 items (at 69 items/min)
2015-11-04 09:30:03 [scrapy] INFO: Crawled 16045 pages (at 46 pages/min), scraped 15806 items (at 54 items/min)
2015-11-04 09:30:08 [scrapy] INFO: Crawled 16045 pages (at 0 pages/min), scraped 15813 items (at 7 items/min)
2015-11-04 09:32:16 [scrapy] INFO: Crawled 16052 pages (at 7 pages/min), scraped 15814 items (at 1 items/min)
2015-11-04 09:33:48 [scrapy] INFO: Crawled 16060 pages (at 8 pages/min), scraped 15815 items (at 1 items/min)
2015-11-04 09:34:55 [scrapy] INFO: Crawled 16060 pages (at 0 pages/min), scraped 15821 items (at 6 items/min)
2015-11-04 09:36:15 [scrapy] INFO: Crawled 16060 pages (at 0 pages/min), scraped 15829 items (at 8 items/min)
2015-11-04 09:37:09 [scrapy] INFO: Crawled 16073 pages (at 13 pages/min), scraped 15832 items (at 3 items/min)
2015-11-04 09:41:59 [scrapy] INFO: Crawled 16074 pages (at 1 pages/min), scraped 15836 items (at 4 items/min)
2015-11-04 09:53:46 [scrapy] INFO: Crawled 16074 pages (at 0 pages/min), scraped 15842 items (at 6 items/min)
2015-11-04 09:54:05 [scrapy] INFO: Crawled 16074 pages (at 0 pages/min), scraped 15843 items (at 1 items/min)
2015-11-04 09:55:55 [scrapy] INFO: Crawled 16090 pages (at 16 pages/min), scraped 15851 items (at 8 items/min)
2015-11-04 09:57:06 [scrapy] INFO: Crawled 16090 pages (at 0 pages/min), scraped 15859 items (at 8 items/min)
2015-11-04 09:58:33 [scrapy] INFO: Crawled 16111 pages (at 21 pages/min), scraped 15866 items (at 7 items/min)
2015-11-04 09:59:33 [scrapy] INFO: Crawled 16111 pages (at 0 pages/min), scraped 15873 items (at 7 items/min)
2015-11-04 10:00:54 [scrapy] INFO: Crawled 16111 pages (at 0 pages/min), scraped 15880 items (at 7 items/min)
2015-11-04 10:01:16 [scrapy] INFO: Crawled 16118 pages (at 7 pages/min), scraped 15881 items (at 1 items/min)
2015-11-04 10:02:32 [scrapy] INFO: Crawled 16129 pages (at 11 pages/min), scraped 15887 items (at 6 items/min)
2015-11-04 10:03:41 [scrapy] INFO: Crawled 16129 pages (at 0 pages/min), scraped 15892 items (at 5 items/min)
2015-11-04 10:04:42 [scrapy] INFO: Crawled 16129 pages (at 0 pages/min), scraped 15898 items (at 6 items/min)
2015-11-04 10:05:29 [scrapy] INFO: Crawled 16149 pages (at 20 pages/min), scraped 15902 items (at 4 items/min)
2015-11-04 10:06:37 [scrapy] INFO: Crawled 16152 pages (at 3 pages/min), scraped 15910 items (at 8 items/min)
2015-11-04 10:07:56 [scrapy] INFO: Crawled 16152 pages (at 0 pages/min), scraped 15918 items (at 8 items/min)
2015-11-04 10:08:28 [scrapy] INFO: Crawled 16152 pages (at 0 pages/min), scraped 15921 items (at 3 items/min)
2015-11-04 10:10:25 [scrapy] INFO: Crawled 16175 pages (at 23 pages/min), scraped 15931 items (at 10 items/min)
2015-11-04 10:11:13 [scrapy] INFO: Crawled 16175 pages (at 0 pages/min), scraped 15936 items (at 5 items/min)
2015-11-04 10:12:30 [scrapy] INFO: Crawled 16175 pages (at 0 pages/min), scraped 15944 items (at 8 items/min)
2015-11-04 10:13:42 [scrapy] INFO: Crawled 16195 pages (at 20 pages/min), scraped 15950 items (at 6 items/min)
2015-11-04 10:15:24 [scrapy] INFO: Crawled 16195 pages (at 0 pages/min), scraped 15958 items (at 8 items/min)
2015-11-04 10:16:31 [scrapy] INFO: Crawled 16195 pages (at 0 pages/min), scraped 15964 items (at 6 items/min)
2015-11-04 10:17:20 [scrapy] INFO: Crawled 16210 pages (at 15 pages/min), scraped 15969 items (at 5 items/min)
2015-11-04 10:18:32 [scrapy] INFO: Crawled 16213 pages (at 3 pages/min), scraped 15979 items (at 10 items/min)
2015-11-04 10:19:07 [scrapy] INFO: Crawled 16213 pages (at 0 pages/min), scraped 15982 items (at 3 items/min)
2015-11-04 10:20:41 [scrapy] INFO: Crawled 16230 pages (at 17 pages/min), scraped 15990 items (at 8 items/min)
2015-11-04 10:21:57 [scrapy] INFO: Crawled 16230 pages (at 0 pages/min), scraped 15999 items (at 9 items/min)
2015-11-04 10:22:11 [scrapy] INFO: Crawled 16239 pages (at 9 pages/min), scraped 16000 items (at 1 items/min)
2015-11-04 10:23:17 [scrapy] INFO: Crawled 16255 pages (at 16 pages/min), scraped 16008 items (at 8 items/min)
2015-11-04 10:24:41 [scrapy] INFO: Crawled 16255 pages (at 0 pages/min), scraped 16016 items (at 8 items/min)
2015-11-04 10:26:09 [scrapy] INFO: Crawled 16255 pages (at 0 pages/min), scraped 16024 items (at 8 items/min)
2015-11-04 10:27:09 [scrapy] INFO: Crawled 16266 pages (at 11 pages/min), scraped 16026 items (at 2 items/min)
2015-11-04 10:28:13 [scrapy] INFO: Crawled 16273 pages (at 7 pages/min), scraped 16032 items (at 6 items/min)
2015-11-04 10:29:39 [scrapy] INFO: Crawled 16273 pages (at 0 pages/min), scraped 16042 items (at 10 items/min)
2015-11-04 10:30:17 [scrapy] INFO: Crawled 16281 pages (at 8 pages/min), scraped 16043 items (at 1 items/min)
2015-11-04 10:31:08 [scrapy] INFO: Crawled 16316 pages (at 35 pages/min), scraped 16077 items (at 34 items/min)
2015-11-04 10:32:12 [scrapy] INFO: Crawled 16380 pages (at 64 pages/min), scraped 16141 items (at 64 items/min)
2015-11-04 10:33:09 [scrapy] INFO: Crawled 16436 pages (at 56 pages/min), scraped 16197 items (at 56 items/min)
2015-11-04 10:34:06 [scrapy] INFO: Crawled 16492 pages (at 56 pages/min), scraped 16253 items (at 56 items/min)
2015-11-04 10:35:11 [scrapy] INFO: Crawled 16555 pages (at 63 pages/min), scraped 16316 items (at 63 items/min)
2015-11-04 10:36:11 [scrapy] INFO: Crawled 16612 pages (at 57 pages/min), scraped 16373 items (at 57 items/min)
2015-11-04 10:37:15 [scrapy] INFO: Crawled 16668 pages (at 56 pages/min), scraped 16429 items (at 56 items/min)
2015-11-04 10:38:07 [scrapy] INFO: Crawled 16716 pages (at 48 pages/min), scraped 16477 items (at 48 items/min)
2015-11-04 10:39:11 [scrapy] INFO: Crawled 16779 pages (at 63 pages/min), scraped 16540 items (at 63 items/min)
2015-11-04 10:40:04 [scrapy] INFO: Crawled 16835 pages (at 56 pages/min), scraped 16589 items (at 49 items/min)
2015-11-04 10:41:32 [scrapy] INFO: Crawled 16860 pages (at 25 pages/min), scraped 16621 items (at 32 items/min)
2015-11-04 10:42:51 [scrapy] INFO: Crawled 16860 pages (at 0 pages/min), scraped 16629 items (at 8 items/min)
2015-11-04 10:43:17 [scrapy] INFO: Crawled 16872 pages (at 12 pages/min), scraped 16630 items (at 1 items/min)
2015-11-04 10:44:08 [scrapy] INFO: Crawled 16912 pages (at 40 pages/min), scraped 16665 items (at 35 items/min)
2015-11-04 10:45:05 [scrapy] INFO: Crawled 16968 pages (at 56 pages/min), scraped 16721 items (at 56 items/min)
2015-11-04 10:46:11 [scrapy] INFO: Crawled 17032 pages (at 64 pages/min), scraped 16785 items (at 64 items/min)
2015-11-04 10:47:08 [scrapy] INFO: Crawled 17080 pages (at 48 pages/min), scraped 16841 items (at 56 items/min)
2015-11-04 10:48:08 [scrapy] INFO: Crawled 17144 pages (at 64 pages/min), scraped 16897 items (at 56 items/min)
2015-11-04 10:49:09 [scrapy] INFO: Crawled 17200 pages (at 56 pages/min), scraped 16953 items (at 56 items/min)
2015-11-04 10:50:08 [scrapy] INFO: Crawled 17256 pages (at 56 pages/min), scraped 17009 items (at 56 items/min)
2015-11-04 10:51:11 [scrapy] INFO: Crawled 17312 pages (at 56 pages/min), scraped 17065 items (at 56 items/min)
2015-11-04 10:52:04 [scrapy] INFO: Crawled 17360 pages (at 48 pages/min), scraped 17113 items (at 48 items/min)
2015-11-04 10:53:05 [scrapy] INFO: Crawled 17416 pages (at 56 pages/min), scraped 17169 items (at 56 items/min)
2015-11-04 10:54:07 [scrapy] INFO: Crawled 17432 pages (at 16 pages/min), scraped 17193 items (at 24 items/min)
2015-11-04 10:55:32 [scrapy] INFO: Crawled 17432 pages (at 0 pages/min), scraped 17201 items (at 8 items/min)
2015-11-04 10:57:18 [scrapy] INFO: Crawled 17457 pages (at 25 pages/min), scraped 17210 items (at 9 items/min)
2015-11-04 10:58:30 [scrapy] INFO: Crawled 17457 pages (at 0 pages/min), scraped 17218 items (at 8 items/min)
2015-11-04 10:59:57 [scrapy] INFO: Crawled 17457 pages (at 0 pages/min), scraped 17226 items (at 8 items/min)
2015-11-04 11:00:08 [scrapy] INFO: Crawled 17463 pages (at 6 pages/min), scraped 17227 items (at 1 items/min)
2015-11-04 11:01:10 [scrapy] INFO: Crawled 17479 pages (at 16 pages/min), scraped 17232 items (at 5 items/min)
2015-11-04 11:02:35 [scrapy] INFO: Crawled 17479 pages (at 0 pages/min), scraped 17240 items (at 8 items/min)
2015-11-04 11:03:21 [scrapy] INFO: Crawled 17479 pages (at 0 pages/min), scraped 17248 items (at 8 items/min)
2015-11-04 11:05:25 [scrapy] INFO: Crawled 17501 pages (at 22 pages/min), scraped 17258 items (at 10 items/min)
2015-11-04 11:07:30 [scrapy] INFO: Crawled 17501 pages (at 0 pages/min), scraped 17270 items (at 12 items/min)
2015-11-04 11:08:07 [scrapy] INFO: Crawled 17518 pages (at 17 pages/min), scraped 17272 items (at 2 items/min)
2015-11-04 11:09:22 [scrapy] INFO: Crawled 17524 pages (at 6 pages/min), scraped 17279 items (at 7 items/min)
2015-11-04 11:10:49 [scrapy] INFO: Crawled 17524 pages (at 0 pages/min), scraped 17287 items (at 8 items/min)
2015-11-04 11:11:47 [scrapy] INFO: Crawled 17524 pages (at 0 pages/min), scraped 17293 items (at 6 items/min)
2015-11-04 11:12:13 [scrapy] INFO: Crawled 17539 pages (at 15 pages/min), scraped 17295 items (at 2 items/min)
2015-11-04 11:13:14 [scrapy] INFO: Crawled 17547 pages (at 8 pages/min), scraped 17300 items (at 5 items/min)
2015-11-04 11:14:35 [scrapy] INFO: Crawled 17547 pages (at 0 pages/min), scraped 17308 items (at 8 items/min)
2015-11-04 11:16:07 [scrapy] INFO: Crawled 17547 pages (at 0 pages/min), scraped 17316 items (at 8 items/min)
2015-11-04 11:17:05 [scrapy] INFO: Crawled 17575 pages (at 28 pages/min), scraped 17330 items (at 14 items/min)
2015-11-04 11:18:06 [scrapy] INFO: Crawled 17631 pages (at 56 pages/min), scraped 17386 items (at 56 items/min)
2015-11-04 11:19:05 [scrapy] INFO: Crawled 17687 pages (at 56 pages/min), scraped 17442 items (at 56 items/min)
2015-11-04 11:20:10 [scrapy] INFO: Crawled 17745 pages (at 58 pages/min), scraped 17506 items (at 64 items/min)
2015-11-04 11:21:08 [scrapy] INFO: Crawled 17801 pages (at 56 pages/min), scraped 17562 items (at 56 items/min)
2015-11-04 11:22:08 [scrapy] INFO: Crawled 17857 pages (at 56 pages/min), scraped 17618 items (at 56 items/min)
2015-11-04 11:23:08 [scrapy] INFO: Crawled 17913 pages (at 56 pages/min), scraped 17674 items (at 56 items/min)
2015-11-04 11:24:11 [scrapy] INFO: Crawled 17969 pages (at 56 pages/min), scraped 17730 items (at 56 items/min)
2015-11-04 11:25:05 [scrapy] INFO: Crawled 18023 pages (at 54 pages/min), scraped 17778 items (at 48 items/min)
2015-11-04 11:26:04 [scrapy] INFO: Crawled 18079 pages (at 56 pages/min), scraped 17834 items (at 56 items/min)
2015-11-04 11:27:13 [scrapy] INFO: Crawled 18113 pages (at 34 pages/min), scraped 17874 items (at 40 items/min)
2015-11-04 11:28:35 [scrapy] INFO: Crawled 18113 pages (at 0 pages/min), scraped 17882 items (at 8 items/min)
2015-11-04 11:29:26 [scrapy] INFO: Crawled 18126 pages (at 13 pages/min), scraped 17885 items (at 3 items/min)
2015-11-04 11:30:56 [scrapy] INFO: Crawled 18133 pages (at 7 pages/min), scraped 17890 items (at 5 items/min)
2015-11-04 11:31:51 [scrapy] INFO: Crawled 18133 pages (at 0 pages/min), scraped 17895 items (at 5 items/min)
2015-11-04 11:33:07 [scrapy] INFO: Crawled 18133 pages (at 0 pages/min), scraped 17902 items (at 7 items/min)
2015-11-04 11:34:08 [scrapy] INFO: Crawled 18150 pages (at 17 pages/min), scraped 17904 items (at 2 items/min)
2015-11-04 11:35:11 [scrapy] INFO: Crawled 18155 pages (at 5 pages/min), scraped 17911 items (at 7 items/min)
2015-11-04 11:36:42 [scrapy] INFO: Crawled 18155 pages (at 0 pages/min), scraped 17919 items (at 8 items/min)
2015-11-04 11:37:11 [scrapy] INFO: Crawled 18192 pages (at 37 pages/min), scraped 17945 items (at 26 items/min)
2015-11-04 11:38:04 [scrapy] INFO: Crawled 18232 pages (at 40 pages/min), scraped 17993 items (at 48 items/min)
2015-11-04 11:39:09 [scrapy] INFO: Crawled 18296 pages (at 64 pages/min), scraped 18049 items (at 56 items/min)
2015-11-04 11:40:18 [scrapy] INFO: Crawled 18352 pages (at 56 pages/min), scraped 18113 items (at 64 items/min)
2015-11-04 11:41:11 [scrapy] INFO: Crawled 18400 pages (at 48 pages/min), scraped 18161 items (at 48 items/min)
2015-11-04 11:42:10 [scrapy] INFO: Crawled 18464 pages (at 64 pages/min), scraped 18217 items (at 56 items/min)
2015-11-04 11:43:11 [scrapy] INFO: Crawled 18512 pages (at 48 pages/min), scraped 18273 items (at 56 items/min)
2015-11-04 11:44:07 [scrapy] INFO: Crawled 18568 pages (at 56 pages/min), scraped 18325 items (at 52 items/min)
2015-11-04 11:45:11 [scrapy] INFO: Crawled 18632 pages (at 64 pages/min), scraped 18385 items (at 60 items/min)
2015-11-04 11:46:10 [scrapy] INFO: Crawled 18680 pages (at 48 pages/min), scraped 18441 items (at 56 items/min)
2015-11-04 11:47:44 [scrapy] INFO: Crawled 18728 pages (at 48 pages/min), scraped 18481 items (at 40 items/min)
2015-11-04 11:48:52 [scrapy] INFO: Crawled 18728 pages (at 0 pages/min), scraped 18489 items (at 8 items/min)
2015-11-04 11:50:12 [scrapy] INFO: Crawled 18728 pages (at 0 pages/min), scraped 18497 items (at 8 items/min)
2015-11-04 11:51:13 [scrapy] INFO: Crawled 18739 pages (at 11 pages/min), scraped 18500 items (at 3 items/min)
2015-11-04 11:52:59 [scrapy] INFO: Crawled 18746 pages (at 7 pages/min), scraped 18507 items (at 7 items/min)
2015-11-04 11:54:34 [scrapy] INFO: Crawled 18746 pages (at 0 pages/min), scraped 18515 items (at 8 items/min)
2015-11-04 11:55:08 [scrapy] INFO: Crawled 18754 pages (at 8 pages/min), scraped 18516 items (at 1 items/min)
2015-11-04 11:56:06 [scrapy] INFO: Crawled 18767 pages (at 13 pages/min), scraped 18523 items (at 7 items/min)
2015-11-04 11:58:15 [scrapy] INFO: Crawled 18767 pages (at 0 pages/min), scraped 18536 items (at 13 items/min)
2015-11-04 11:59:10 [scrapy] INFO: Crawled 18795 pages (at 28 pages/min), scraped 18548 items (at 12 items/min)
2015-11-04 12:00:11 [scrapy] INFO: Crawled 18843 pages (at 48 pages/min), scraped 18604 items (at 56 items/min)
2015-11-04 12:01:13 [scrapy] INFO: Crawled 18907 pages (at 64 pages/min), scraped 18660 items (at 56 items/min)
2015-11-04 12:02:06 [scrapy] INFO: Crawled 18947 pages (at 40 pages/min), scraped 18708 items (at 48 items/min)
2015-11-04 12:03:10 [scrapy] INFO: Crawled 19004 pages (at 57 pages/min), scraped 18765 items (at 57 items/min)
2015-11-04 12:04:05 [scrapy] INFO: Crawled 19052 pages (at 48 pages/min), scraped 18813 items (at 48 items/min)
2015-11-04 12:05:14 [scrapy] INFO: Crawled 19111 pages (at 59 pages/min), scraped 18872 items (at 59 items/min)
2015-11-04 12:06:14 [scrapy] INFO: Crawled 19155 pages (at 44 pages/min), scraped 18921 items (at 49 items/min)
2015-11-04 12:07:08 [scrapy] INFO: Crawled 19213 pages (at 58 pages/min), scraped 18966 items (at 45 items/min)
2015-11-04 12:08:04 [scrapy] INFO: Crawled 19256 pages (at 43 pages/min), scraped 19009 items (at 43 items/min)
2015-11-04 12:10:42 [scrapy] INFO: Crawled 19298 pages (at 42 pages/min), scraped 19057 items (at 48 items/min)
2015-11-04 12:12:03 [scrapy] INFO: Crawled 19298 pages (at 0 pages/min), scraped 19065 items (at 8 items/min)
2015-11-04 12:12:17 [scrapy] INFO: Crawled 19298 pages (at 0 pages/min), scraped 19067 items (at 2 items/min)
2015-11-04 12:13:12 [scrapy] INFO: Crawled 19355 pages (at 57 pages/min), scraped 19108 items (at 41 items/min)
2015-11-04 12:14:11 [scrapy] INFO: Crawled 19399 pages (at 44 pages/min), scraped 19152 items (at 44 items/min)
2015-11-04 12:15:06 [scrapy] INFO: Crawled 19439 pages (at 40 pages/min), scraped 19200 items (at 48 items/min)
2015-11-04 12:16:05 [scrapy] INFO: Crawled 19480 pages (at 41 pages/min), scraped 19233 items (at 33 items/min)
2015-11-04 12:17:15 [scrapy] INFO: Crawled 19529 pages (at 49 pages/min), scraped 19282 items (at 49 items/min)
2015-11-04 12:18:09 [scrapy] INFO: Crawled 19577 pages (at 48 pages/min), scraped 19331 items (at 49 items/min)
2015-11-04 12:19:10 [scrapy] INFO: Crawled 19620 pages (at 43 pages/min), scraped 19373 items (at 42 items/min)
2015-11-04 12:20:09 [scrapy] INFO: Crawled 19660 pages (at 40 pages/min), scraped 19421 items (at 48 items/min)
2015-11-04 12:21:09 [scrapy] INFO: Crawled 19712 pages (at 52 pages/min), scraped 19473 items (at 52 items/min)
2015-11-04 12:22:09 [scrapy] INFO: Crawled 19752 pages (at 40 pages/min), scraped 19513 items (at 40 items/min)
2015-11-04 12:23:07 [scrapy] INFO: Crawled 19752 pages (at 0 pages/min), scraped 19521 items (at 8 items/min)
2015-11-04 12:25:33 [scrapy] INFO: Crawled 19770 pages (at 18 pages/min), scraped 19530 items (at 9 items/min)
2015-11-04 12:26:43 [scrapy] INFO: Crawled 19770 pages (at 0 pages/min), scraped 19536 items (at 6 items/min)
2015-11-04 12:27:19 [scrapy] INFO: Crawled 19770 pages (at 0 pages/min), scraped 19539 items (at 3 items/min)
2015-11-04 12:29:12 [scrapy] INFO: Crawled 19785 pages (at 15 pages/min), scraped 19549 items (at 10 items/min)
2015-11-04 12:30:04 [scrapy] INFO: Crawled 19787 pages (at 2 pages/min), scraped 19554 items (at 5 items/min)
2015-11-04 12:31:49 [scrapy] INFO: Crawled 19795 pages (at 8 pages/min), scraped 19563 items (at 9 items/min)
2015-11-04 12:32:14 [scrapy] INFO: Crawled 19804 pages (at 9 pages/min), scraped 19565 items (at 2 items/min)
2015-11-04 12:33:55 [scrapy] INFO: Crawled 19815 pages (at 11 pages/min), scraped 19573 items (at 8 items/min)
2015-11-04 12:34:54 [scrapy] INFO: Crawled 19815 pages (at 0 pages/min), scraped 19580 items (at 7 items/min)
2015-11-04 12:35:04 [scrapy] INFO: Crawled 19815 pages (at 0 pages/min), scraped 19584 items (at 4 items/min)
2015-11-04 12:36:06 [scrapy] INFO: Crawled 19898 pages (at 83 pages/min), scraped 19651 items (at 67 items/min)
2015-11-04 12:37:09 [scrapy] INFO: Crawled 19930 pages (at 32 pages/min), scraped 19699 items (at 48 items/min)
2015-11-04 12:38:05 [scrapy] INFO: Crawled 19984 pages (at 54 pages/min), scraped 19737 items (at 38 items/min)
2015-11-04 12:39:10 [scrapy] INFO: Crawled 20026 pages (at 42 pages/min), scraped 19793 items (at 56 items/min)
2015-11-04 12:40:07 [scrapy] INFO: Crawled 20086 pages (at 60 pages/min), scraped 19847 items (at 54 items/min)
2015-11-04 12:41:16 [scrapy] INFO: Crawled 20145 pages (at 59 pages/min), scraped 19898 items (at 51 items/min)
2015-11-04 12:42:07 [scrapy] INFO: Crawled 20186 pages (at 41 pages/min), scraped 19939 items (at 41 items/min)
2015-11-04 12:43:11 [scrapy] INFO: Crawled 20230 pages (at 44 pages/min), scraped 19997 items (at 58 items/min)
2015-11-04 12:44:08 [scrapy] INFO: Crawled 20289 pages (at 59 pages/min), scraped 20050 items (at 53 items/min)
2015-11-04 12:45:07 [scrapy] INFO: Crawled 20329 pages (at 40 pages/min), scraped 20090 items (at 40 items/min)
2015-11-04 12:46:26 [scrapy] INFO: Crawled 20329 pages (at 0 pages/min), scraped 20098 items (at 8 items/min)
2015-11-04 12:47:16 [scrapy] INFO: Crawled 20338 pages (at 9 pages/min), scraped 20100 items (at 2 items/min)
2015-11-04 12:48:49 [scrapy] INFO: Crawled 20346 pages (at 8 pages/min), scraped 20107 items (at 7 items/min)
2015-11-04 12:50:15 [scrapy] INFO: Crawled 20355 pages (at 9 pages/min), scraped 20115 items (at 8 items/min)
2015-11-04 12:52:16 [scrapy] INFO: Crawled 20355 pages (at 0 pages/min), scraped 20124 items (at 9 items/min)
2015-11-04 12:54:02 [scrapy] INFO: Crawled 20374 pages (at 19 pages/min), scraped 20129 items (at 5 items/min)
2015-11-04 12:55:42 [scrapy] INFO: Crawled 20374 pages (at 0 pages/min), scraped 20137 items (at 8 items/min)
2015-11-04 12:56:46 [scrapy] INFO: Crawled 20374 pages (at 0 pages/min), scraped 20143 items (at 6 items/min)
2015-11-04 12:57:30 [scrapy] INFO: Crawled 20389 pages (at 15 pages/min), scraped 20148 items (at 5 items/min)
2015-11-04 12:58:41 [scrapy] INFO: Crawled 20391 pages (at 2 pages/min), scraped 20158 items (at 10 items/min)
2015-11-04 12:59:21 [scrapy] INFO: Crawled 20401 pages (at 10 pages/min), scraped 20162 items (at 4 items/min)
2015-11-04 13:01:27 [scrapy] INFO: Crawled 20417 pages (at 16 pages/min), scraped 20170 items (at 8 items/min)
2015-11-04 13:02:44 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20178 items (at 8 items/min)
2015-11-04 13:04:16 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20186 items (at 8 items/min)
2015-11-04 13:05:04 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20186 items (at 0 items/min)
2015-11-04 13:06:04 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20186 items (at 0 items/min)
2015-11-04 13:07:04 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20186 items (at 0 items/min)
2015-11-04 13:08:04 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20186 items (at 0 items/min)
2015-11-04 13:09:04 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20186 items (at 0 items/min)
2015-11-04 13:10:04 [scrapy] INFO: Crawled 20417 pages (at 0 pages/min), scraped 20186 items (at 0 items/min)
2015-11-04 13:11:09 [scrapy] INFO: Crawled 20442 pages (at 25 pages/min), scraped 20201 items (at 15 items/min)
2015-11-04 13:12:12 [scrapy] INFO: Crawled 20506 pages (at 64 pages/min), scraped 20259 items (at 58 items/min)
2015-11-04 13:13:11 [scrapy] INFO: Crawled 20554 pages (at 48 pages/min), scraped 20307 items (at 48 items/min)
2015-11-04 13:14:08 [scrapy] INFO: Crawled 20608 pages (at 54 pages/min), scraped 20361 items (at 54 items/min)
2015-11-04 13:15:04 [scrapy] INFO: Crawled 20648 pages (at 40 pages/min), scraped 20401 items (at 40 items/min)
2015-11-04 13:16:10 [scrapy] INFO: Crawled 20691 pages (at 43 pages/min), scraped 20449 items (at 48 items/min)
2015-11-04 13:17:05 [scrapy] INFO: Crawled 20741 pages (at 50 pages/min), scraped 20502 items (at 53 items/min)
2015-11-04 13:18:05 [scrapy] INFO: Crawled 20781 pages (at 40 pages/min), scraped 20542 items (at 40 items/min)
2015-11-04 13:19:11 [scrapy] INFO: Crawled 20831 pages (at 50 pages/min), scraped 20599 items (at 57 items/min)
2015-11-04 13:20:06 [scrapy] INFO: Crawled 20888 pages (at 57 pages/min), scraped 20649 items (at 50 items/min)
2015-11-04 13:21:15 [scrapy] INFO: Crawled 20928 pages (at 40 pages/min), scraped 20681 items (at 32 items/min)
2015-11-04 13:22:05 [scrapy] INFO: Crawled 20941 pages (at 13 pages/min), scraped 20698 items (at 17 items/min)
2015-11-04 13:23:09 [scrapy] INFO: Crawled 20994 pages (at 53 pages/min), scraped 20755 items (at 57 items/min)
2015-11-04 13:24:06 [scrapy] INFO: Crawled 21043 pages (at 49 pages/min), scraped 20796 items (at 41 items/min)
2015-11-04 13:25:05 [scrapy] INFO: Crawled 21079 pages (at 36 pages/min), scraped 20848 items (at 52 items/min)
2015-11-04 13:26:06 [scrapy] INFO: Crawled 21154 pages (at 75 pages/min), scraped 20907 items (at 59 items/min)
2015-11-04 13:27:16 [scrapy] INFO: Crawled 21210 pages (at 56 pages/min), scraped 20963 items (at 56 items/min)
2015-11-04 13:28:06 [scrapy] INFO: Crawled 21226 pages (at 16 pages/min), scraped 20995 items (at 32 items/min)
2015-11-04 13:29:06 [scrapy] INFO: Crawled 21286 pages (at 60 pages/min), scraped 21047 items (at 52 items/min)
2015-11-04 13:30:05 [scrapy] INFO: Crawled 21347 pages (at 61 pages/min), scraped 21100 items (at 53 items/min)
2015-11-04 13:31:11 [scrapy] INFO: Crawled 21387 pages (at 40 pages/min), scraped 21148 items (at 48 items/min)
2015-11-04 13:32:04 [scrapy] INFO: Crawled 21405 pages (at 18 pages/min), scraped 21174 items (at 26 items/min)
2015-11-04 13:33:04 [scrapy] INFO: Crawled 21405 pages (at 0 pages/min), scraped 21174 items (at 0 items/min)
2015-11-04 13:34:04 [scrapy] INFO: Crawled 21405 pages (at 0 pages/min), scraped 21174 items (at 0 items/min)
2015-11-04 13:35:04 [scrapy] INFO: Crawled 21405 pages (at 0 pages/min), scraped 21174 items (at 0 items/min)
2015-11-04 13:36:04 [scrapy] INFO: Crawled 21405 pages (at 0 pages/min), scraped 21174 items (at 0 items/min)
2015-11-04 13:37:04 [scrapy] INFO: Crawled 21405 pages (at 0 pages/min), scraped 21174 items (at 0 items/min)
2015-11-04 13:38:08 [scrapy] INFO: Crawled 21476 pages (at 71 pages/min), scraped 21237 items (at 63 items/min)
2015-11-04 13:39:13 [scrapy] INFO: Crawled 21521 pages (at 45 pages/min), scraped 21282 items (at 45 items/min)
2015-11-04 13:40:10 [scrapy] INFO: Crawled 21562 pages (at 41 pages/min), scraped 21315 items (at 33 items/min)
2015-11-04 13:41:06 [scrapy] INFO: Crawled 21611 pages (at 49 pages/min), scraped 21364 items (at 49 items/min)
2015-11-04 13:42:05 [scrapy] INFO: Crawled 21658 pages (at 47 pages/min), scraped 21411 items (at 47 items/min)
2015-11-04 13:43:11 [scrapy] INFO: Crawled 21714 pages (at 56 pages/min), scraped 21475 items (at 64 items/min)
2015-11-04 13:44:14 [scrapy] INFO: Crawled 21762 pages (at 48 pages/min), scraped 21515 items (at 40 items/min)
2015-11-04 13:45:04 [scrapy] INFO: Crawled 21778 pages (at 16 pages/min), scraped 21547 items (at 32 items/min)
2015-11-04 13:46:10 [scrapy] INFO: Crawled 21837 pages (at 59 pages/min), scraped 21605 items (at 58 items/min)
2015-11-04 13:47:11 [scrapy] INFO: Crawled 21905 pages (at 68 pages/min), scraped 21658 items (at 53 items/min)
2015-11-04 13:49:25 [scrapy] INFO: Crawled 21913 pages (at 8 pages/min), scraped 21674 items (at 16 items/min)
2015-11-04 13:50:54 [scrapy] INFO: Crawled 21913 pages (at 0 pages/min), scraped 21682 items (at 8 items/min)
2015-11-04 13:51:12 [scrapy] INFO: Crawled 21922 pages (at 9 pages/min), scraped 21683 items (at 1 items/min)
2015-11-04 13:52:58 [scrapy] INFO: Crawled 21937 pages (at 15 pages/min), scraped 21691 items (at 8 items/min)
2015-11-04 13:54:38 [scrapy] INFO: Crawled 21937 pages (at 0 pages/min), scraped 21698 items (at 7 items/min)
2015-11-04 13:56:40 [scrapy] INFO: Crawled 21937 pages (at 0 pages/min), scraped 21706 items (at 8 items/min)
2015-11-04 13:57:06 [scrapy] INFO: Crawled 21947 pages (at 10 pages/min), scraped 21707 items (at 1 items/min)
2015-11-04 13:58:53 [scrapy] INFO: Crawled 21954 pages (at 7 pages/min), scraped 21716 items (at 9 items/min)
2015-11-04 13:59:23 [scrapy] INFO: Crawled 21954 pages (at 0 pages/min), scraped 21718 items (at 2 items/min)
2015-11-04 14:00:13 [scrapy] INFO: Crawled 21962 pages (at 8 pages/min), scraped 21724 items (at 6 items/min)
2015-11-04 14:02:15 [scrapy] INFO: Crawled 21964 pages (at 2 pages/min), scraped 21731 items (at 7 items/min)
2015-11-04 14:03:04 [scrapy] INFO: Crawled 21964 pages (at 0 pages/min), scraped 21733 items (at 2 items/min)
2015-11-04 14:04:04 [scrapy] INFO: Crawled 21964 pages (at 0 pages/min), scraped 21733 items (at 0 items/min)
2015-11-04 14:05:04 [scrapy] INFO: Crawled 21964 pages (at 0 pages/min), scraped 21733 items (at 0 items/min)
2015-11-04 14:06:04 [scrapy] INFO: Crawled 21964 pages (at 0 pages/min), scraped 21733 items (at 0 items/min)
2015-11-04 14:07:07 [scrapy] INFO: Crawled 21976 pages (at 12 pages/min), scraped 21736 items (at 3 items/min)
2015-11-04 14:08:19 [scrapy] INFO: Crawled 22039 pages (at 63 pages/min), scraped 21800 items (at 64 items/min)
2015-11-04 14:09:10 [scrapy] INFO: Crawled 22064 pages (at 25 pages/min), scraped 21833 items (at 33 items/min)
2015-11-04 14:10:11 [scrapy] INFO: Crawled 22133 pages (at 69 pages/min), scraped 21886 items (at 53 items/min)
2015-11-04 14:11:13 [scrapy] INFO: Crawled 22188 pages (at 55 pages/min), scraped 21941 items (at 55 items/min)
2015-11-04 14:12:09 [scrapy] INFO: Crawled 22228 pages (at 40 pages/min), scraped 21981 items (at 40 items/min)
2015-11-04 14:13:08 [scrapy] INFO: Crawled 22279 pages (at 51 pages/min), scraped 22032 items (at 51 items/min)
2015-11-04 14:14:13 [scrapy] INFO: Crawled 22335 pages (at 56 pages/min), scraped 22088 items (at 56 items/min)
2015-11-04 14:15:13 [scrapy] INFO: Crawled 22380 pages (at 45 pages/min), scraped 22136 items (at 48 items/min)
2015-11-04 14:16:09 [scrapy] INFO: Crawled 22439 pages (at 59 pages/min), scraped 22192 items (at 56 items/min)
2015-11-04 14:18:16 [scrapy] INFO: Crawled 22479 pages (at 40 pages/min), scraped 22232 items (at 40 items/min)
2015-11-04 14:19:26 [scrapy] INFO: Crawled 22479 pages (at 0 pages/min), scraped 22240 items (at 8 items/min)
2015-11-04 14:21:08 [scrapy] INFO: Crawled 22479 pages (at 0 pages/min), scraped 22248 items (at 8 items/min)
2015-11-04 14:23:32 [scrapy] INFO: Crawled 22497 pages (at 18 pages/min), scraped 22257 items (at 9 items/min)
2015-11-04 14:25:09 [scrapy] INFO: Crawled 22497 pages (at 0 pages/min), scraped 22266 items (at 9 items/min)
2015-11-04 14:26:04 [scrapy] INFO: Crawled 22497 pages (at 0 pages/min), scraped 22266 items (at 0 items/min)
2015-11-04 14:27:04 [scrapy] INFO: Crawled 22497 pages (at 0 pages/min), scraped 22266 items (at 0 items/min)
2015-11-04 14:28:08 [scrapy] INFO: Crawled 22540 pages (at 43 pages/min), scraped 22301 items (at 35 items/min)
2015-11-04 14:29:08 [scrapy] INFO: Crawled 22588 pages (at 48 pages/min), scraped 22341 items (at 40 items/min)
2015-11-04 14:30:06 [scrapy] INFO: Crawled 22624 pages (at 36 pages/min), scraped 22391 items (at 50 items/min)
2015-11-04 14:31:19 [scrapy] INFO: Crawled 22693 pages (at 69 pages/min), scraped 22449 items (at 58 items/min)
2015-11-04 14:32:11 [scrapy] INFO: Crawled 22717 pages (at 24 pages/min), scraped 22486 items (at 37 items/min)
2015-11-04 14:33:09 [scrapy] INFO: Crawled 22784 pages (at 67 pages/min), scraped 22537 items (at 51 items/min)
2015-11-04 14:34:13 [scrapy] INFO: Crawled 22833 pages (at 49 pages/min), scraped 22586 items (at 49 items/min)
2015-11-04 14:35:05 [scrapy] INFO: Crawled 22874 pages (at 41 pages/min), scraped 22627 items (at 41 items/min)
2015-11-04 14:36:07 [scrapy] INFO: Crawled 22935 pages (at 61 pages/min), scraped 22688 items (at 61 items/min)
2015-11-04 14:37:10 [scrapy] INFO: Crawled 22975 pages (at 40 pages/min), scraped 22728 items (at 40 items/min)
2015-11-04 14:38:09 [scrapy] INFO: Crawled 23003 pages (at 28 pages/min), scraped 22760 items (at 32 items/min)
2015-11-04 14:39:51 [scrapy] INFO: Crawled 23003 pages (at 0 pages/min), scraped 22768 items (at 8 items/min)
2015-11-04 14:40:29 [scrapy] INFO: Crawled 23003 pages (at 0 pages/min), scraped 22772 items (at 4 items/min)
2015-11-04 14:41:04 [scrapy] INFO: Crawled 23003 pages (at 0 pages/min), scraped 22772 items (at 0 items/min)
2015-11-04 14:42:04 [scrapy] INFO: Crawled 23003 pages (at 0 pages/min), scraped 22772 items (at 0 items/min)
2015-11-04 14:43:11 [scrapy] INFO: Crawled 23009 pages (at 6 pages/min), scraped 22774 items (at 2 items/min)
2015-11-04 14:44:08 [scrapy] INFO: Crawled 23023 pages (at 14 pages/min), scraped 22785 items (at 11 items/min)
2015-11-04 14:45:05 [scrapy] INFO: Crawled 23080 pages (at 57 pages/min), scraped 22833 items (at 48 items/min)
2015-11-04 14:46:13 [scrapy] INFO: Crawled 23121 pages (at 41 pages/min), scraped 22884 items (at 51 items/min)
2015-11-04 14:47:10 [scrapy] INFO: Crawled 23182 pages (at 61 pages/min), scraped 22940 items (at 56 items/min)
2015-11-04 14:48:14 [scrapy] INFO: Crawled 23219 pages (at 37 pages/min), scraped 22980 items (at 40 items/min)
2015-11-04 14:49:04 [scrapy] INFO: Crawled 23255 pages (at 36 pages/min), scraped 23024 items (at 44 items/min)
2015-11-04 14:50:10 [scrapy] INFO: Crawled 23330 pages (at 75 pages/min), scraped 23083 items (at 59 items/min)
2015-11-04 14:51:10 [scrapy] INFO: Crawled 23374 pages (at 44 pages/min), scraped 23127 items (at 44 items/min)
2015-11-04 14:52:07 [scrapy] INFO: Crawled 23422 pages (at 48 pages/min), scraped 23175 items (at 48 items/min)
2015-11-04 14:53:11 [scrapy] INFO: Crawled 23486 pages (at 64 pages/min), scraped 23239 items (at 64 items/min)
2015-11-04 14:54:49 [scrapy] INFO: Crawled 23522 pages (at 36 pages/min), scraped 23275 items (at 36 items/min)
2015-11-04 14:55:27 [scrapy] INFO: Crawled 23522 pages (at 0 pages/min), scraped 23283 items (at 8 items/min)
2015-11-04 14:56:06 [scrapy] INFO: Crawled 23564 pages (at 42 pages/min), scraped 23317 items (at 34 items/min)
2015-11-04 14:57:04 [scrapy] INFO: Crawled 23599 pages (at 35 pages/min), scraped 23368 items (at 51 items/min)
2015-11-04 14:58:10 [scrapy] INFO: Crawled 23676 pages (at 77 pages/min), scraped 23437 items (at 69 items/min)
2015-11-04 14:59:16 [scrapy] INFO: Crawled 23724 pages (at 48 pages/min), scraped 23485 items (at 48 items/min)
2015-11-04 15:00:04 [scrapy] INFO: Crawled 23765 pages (at 41 pages/min), scraped 23518 items (at 33 items/min)
2015-11-04 15:01:09 [scrapy] INFO: Crawled 23820 pages (at 55 pages/min), scraped 23574 items (at 56 items/min)
2015-11-04 15:02:13 [scrapy] INFO: Crawled 23870 pages (at 50 pages/min), scraped 23631 items (at 57 items/min)
2015-11-04 15:03:05 [scrapy] INFO: Crawled 23924 pages (at 54 pages/min), scraped 23678 items (at 47 items/min)
2015-11-04 15:04:05 [scrapy] INFO: Crawled 23964 pages (at 40 pages/min), scraped 23717 items (at 39 items/min)
2015-11-04 15:05:13 [scrapy] INFO: Crawled 24011 pages (at 47 pages/min), scraped 23772 items (at 55 items/min)
2015-11-04 15:06:36 [scrapy] INFO: Crawled 24019 pages (at 8 pages/min), scraped 23780 items (at 8 items/min)
2015-11-04 15:07:58 [scrapy] INFO: Crawled 24019 pages (at 0 pages/min), scraped 23788 items (at 8 items/min)
2015-11-04 15:08:04 [scrapy] INFO: Crawled 24019 pages (at 0 pages/min), scraped 23788 items (at 0 items/min)
2015-11-04 15:09:10 [scrapy] INFO: Crawled 24024 pages (at 5 pages/min), scraped 23790 items (at 2 items/min)
2015-11-04 15:10:04 [scrapy] INFO: Crawled 24024 pages (at 0 pages/min), scraped 23793 items (at 3 items/min)
2015-11-04 15:11:04 [scrapy] INFO: Crawled 24024 pages (at 0 pages/min), scraped 23793 items (at 0 items/min)
2015-11-04 15:12:04 [scrapy] INFO: Crawled 24024 pages (at 0 pages/min), scraped 23793 items (at 0 items/min)
2015-11-04 15:13:04 [scrapy] INFO: Crawled 24024 pages (at 0 pages/min), scraped 23793 items (at 0 items/min)
2015-11-04 15:14:04 [scrapy] INFO: Crawled 24024 pages (at 0 pages/min), scraped 23793 items (at 0 items/min)
2015-11-04 15:15:47 [scrapy] INFO: Crawled 24045 pages (at 21 pages/min), scraped 23799 items (at 6 items/min)
2015-11-04 15:16:47 [scrapy] INFO: Crawled 24045 pages (at 0 pages/min), scraped 23806 items (at 7 items/min)
2015-11-04 15:17:45 [scrapy] INFO: Crawled 24045 pages (at 0 pages/min), scraped 23814 items (at 8 items/min)
2015-11-04 15:18:17 [scrapy] INFO: Crawled 24061 pages (at 16 pages/min), scraped 23816 items (at 2 items/min)
2015-11-04 15:19:42 [scrapy] INFO: Crawled 24069 pages (at 8 pages/min), scraped 23823 items (at 7 items/min)
2015-11-04 15:21:00 [scrapy] INFO: Crawled 24069 pages (at 0 pages/min), scraped 23830 items (at 7 items/min)
2015-11-04 15:22:09 [scrapy] INFO: Crawled 24069 pages (at 0 pages/min), scraped 23838 items (at 8 items/min)
2015-11-04 15:23:47 [scrapy] INFO: Crawled 24094 pages (at 25 pages/min), scraped 23847 items (at 9 items/min)
2015-11-04 15:25:09 [scrapy] INFO: Crawled 24094 pages (at 0 pages/min), scraped 23855 items (at 8 items/min)
2015-11-04 15:26:31 [scrapy] INFO: Crawled 24094 pages (at 0 pages/min), scraped 23863 items (at 8 items/min)
2015-11-04 15:27:17 [scrapy] INFO: Crawled 24111 pages (at 17 pages/min), scraped 23868 items (at 5 items/min)
2015-11-04 15:29:23 [scrapy] INFO: Crawled 24113 pages (at 2 pages/min), scraped 23880 items (at 12 items/min)
2015-11-04 15:30:22 [scrapy] INFO: Crawled 24121 pages (at 8 pages/min), scraped 23883 items (at 3 items/min)
2015-11-04 15:32:06 [scrapy] INFO: Crawled 24130 pages (at 9 pages/min), scraped 23890 items (at 7 items/min)
2015-11-04 15:33:45 [scrapy] INFO: Crawled 24130 pages (at 0 pages/min), scraped 23899 items (at 9 items/min)
2015-11-04 15:34:11 [scrapy] INFO: Crawled 24141 pages (at 11 pages/min), scraped 23900 items (at 1 items/min)
2015-11-04 15:36:10 [scrapy] INFO: Crawled 24147 pages (at 6 pages/min), scraped 23910 items (at 10 items/min)
2015-11-04 15:37:08 [scrapy] INFO: Crawled 24147 pages (at 0 pages/min), scraped 23916 items (at 6 items/min)
2015-11-04 15:38:22 [scrapy] INFO: Crawled 24172 pages (at 25 pages/min), scraped 23925 items (at 9 items/min)
2015-11-04 15:39:30 [scrapy] INFO: Crawled 24172 pages (at 0 pages/min), scraped 23933 items (at 8 items/min)
2015-11-04 15:40:23 [scrapy] INFO: Crawled 24172 pages (at 0 pages/min), scraped 23941 items (at 8 items/min)
2015-11-04 15:42:20 [scrapy] INFO: Crawled 24184 pages (at 12 pages/min), scraped 23950 items (at 9 items/min)
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:45 [scrapy] INFO: Closing spider (shutdown)
2015-11-04 15:43:04 [scrapy] INFO: Crawled 24184 pages (at 0 pages/min), scraped 23953 items (at 3 items/min)
 "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:45:53 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12885_2015_11_03_06_45_46_233.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:45:54 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11623_2015_11_03_12_58_43_587.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:45:54 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13493_2015_11_03_03_34_56_070.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:45:59 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11606_2015_11_03_12_58_37_580.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:46:01 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12817_2015_10_21_05_08_16_253.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:46:37 [scrapy] INFO: Crawled 872 pages (at 58 pages/min), scraped 728 items (at 33 items/min)
2015-11-04 00:47:33 [scrapy] INFO: Crawled 905 pages (at 33 pages/min), scraped 769 items (at 41 items/min)
2015-11-04 00:48:35 [scrapy] INFO: Crawled 947 pages (at 42 pages/min), scraped 806 items (at 37 items/min)
2015-11-04 00:48:37 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37196_2015_11_03_04_02_41_057.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:41 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11610_2015_11_03_12_58_41_620.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:42 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12274_2015_11_03_05_24_16_397.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:43 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11594_2015_11_03_12_58_35_520.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:43 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12805_2015_11_03_09_28_59_663.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:44 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13432_2015_11_03_03_32_34_590.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:45 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12252_2015_11_03_12_58_50_200.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:45 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11596_2015_11_03_05_22_07_863.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:46 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11609_2015_11_03_12_58_39_653.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:47 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12273_2015_11_03_05_24_14_133.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:47 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12807_2015_11_03_06_41_23_053.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:48 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37189_2015_11_03_03_58_25_633.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:49 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12850_2015_11_03_06_43_28_340.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:49 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12272_2015_11_03_05_24_11_980.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:50 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12855_2015_11_03_09_30_55_060.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:50 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12346_2015_11_03_01_07_18_637.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:51 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/15405_2015_11_03_08_18_19_683.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:52 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12254_2015_11_03_12_58_52_447.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:52 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11856_2015_11_03_03_32_36_787.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:53 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12249_2015_11_03_12_58_47_610.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:53 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11607_2015_11_03_05_22_10_203.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:54 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/32281_2015_11_03_08_19_50_790.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:55 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/32283_2015_11_03_08_19_52_740.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:56 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/39889_2015_09_09_06_00_38_357.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:57 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12818_2015_10_21_04_03_44_960.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:58 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/32280_2015_11_03_08_19_48_823.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:59 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/39720_2015_11_02_08_40_41_433.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:48:59 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13458_2015_11_03_08_17_45_907.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:00 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12342_2015_11_03_05_29_46_080.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:01 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37554_2015_11_03_08_17_56_703.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:01 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37555_2015_11_03_03_35_11_670.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:02 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11862_2015_11_03_08_17_38_717.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:07 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12287_2015_11_03_05_25_45_507.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:08 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/32282_2015_11_03_03_37_22_947.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:09 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13463_2015_11_03_08_17_48_013.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:09 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/15397_2015_11_03_05_29_48_093.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:10 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37193_2015_11_03_04_02_35_813.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:10 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11624_2015_11_03_05_24_01_560.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:49:37 [scrapy] INFO: Crawled 1010 pages (at 63 pages/min), scraped 830 items (at 24 items/min)
2015-11-04 00:50:33 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12281_2015_11_03_01_02_20_933.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:34 [scrapy] INFO: Crawled 1048 pages (at 38 pages/min), scraped 871 items (at 41 items/min)
2015-11-04 00:50:41 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/13430_2015_11_03_03_32_29_487.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:41 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37192_2015_11_02_11_46_43_027.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:42 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12803_2015_11_03_06_41_21_057.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:43 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12882_2015_11_03_02_06_09_060.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:49 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11613_2015_11_03_05_22_16_803.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:52 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12341_2015_11_03_05_29_43_897.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:52 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12950_2015_11_03_09_32_49_053.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:53 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11592_2015_11_03_12_55_49_110.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:50:59 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12824_2015_09_08_08_23_38_677.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:00 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11595_2015_11_03_05_22_05_710.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:01 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/13429_2015_11_03_03_32_16_303.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:02 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12339_2015_11_03_05_29_41_790.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:03 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12280_2015_11_03_01_02_18_920.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:04 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12338_2015_11_03_05_28_08_827.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:05 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37191_2015_11_03_03_58_30_407.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:06 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11612_2015_11_03_05_22_14_447.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:07 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11855_2015_11_03_08_14_13_443.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:08 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12943_2015_11_03_02_08_35_610.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:08 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12802_2015_11_03_02_03_40_310.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:09 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12279_2015_11_03_01_02_16_923.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:10 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12257_2015_11_03_01_02_05_003.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:10 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12334_2015_11_03_05_28_04_193.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:11 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12806_2015_11_03_09_29_01_787.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:13 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11591_2015_11_03_12_55_47_160.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:20 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12335_2015_11_03_05_28_06_533.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:31 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11611_2015_11_03_05_22_12_293.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:51:35 [scrapy] INFO: Crawled 1109 pages (at 61 pages/min), scraped 899 items (at 28 items/min)
2015-11-04 00:51:55 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/29996_2015_09_09_03_11_47_483.gzip.pdf> (referer: https://www.henderson.com/depi)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:52:33 [scrapy] INFO: Crawled 1139 pages (at 30 pages/min), scraped 939 items (at 40 items/min)
2015-11-04 00:53:10 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12246_2015_11_03_05_24_05_960.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:22 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12897_2015_11_03_02_06_18_607.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:22 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/14957_2015_11_03_05_29_50_137.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:23 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/15001_2015_11_03_03_35_16_630.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:24 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/14941_2015_11_03_01_07_20_803.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:25 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/14949_2015_11_03_08_19_46_937.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:25 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/13455_2015_11_03_08_17_43_723.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:27 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12245_2015_11_03_05_24_03_683.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:28 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13454_2015_11_03_08_17_41_243.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:29 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/25278_2015_11_03_03_35_00_797.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:29 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12895_2015_11_03_02_06_16_300.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:30 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12294_2015_11_03_05_25_53_960.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:31 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12296_2015_11_03_05_25_56_193.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:32 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12846_2015_10_21_04_08_00_837.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:32 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13437_2015_11_03_08_17_36_657.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:41 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/44746_2015_11_03_06_59_12_950.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:42 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/44745_2015_11_03_02_19_27_337.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:42 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/44751_2015_11_03_07_01_34_900.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:43 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12865_2015_11_03_02_03_42_650.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:44 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11615_2015_09_08_07_50_25_750.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:45 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11605_2015_09_08_07_50_08_557.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:45 [scrapy] INFO: Crawled 1201 pages (at 62 pages/min), scraped 973 items (at 34 items/min)
2015-11-04 00:53:46 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13473_2015_11_03_08_17_51_913.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:46 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/44749_2015_11_03_06_59_29_877.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:47 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12278_2015_09_08_08_04_03_637.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:49 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/48059_2015_11_03_06_01_26_083.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:50 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37198_2015_11_03_04_02_43_253.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:53 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37538_2015_10_21_09_37_44_143.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:53 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12816_2015_11_03_09_29_04_097.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:53:54 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37537_2015_10_21_09_37_51_490.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:54:11 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12264_2015_09_08_08_03_36_727.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:54:18 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37199_2015_11_03_04_02_45_533.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:54:37 [scrapy] INFO: Crawled 1245 pages (at 44 pages/min), scraped 1004 items (at 31 items/min)
2015-11-04 00:54:58 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/6356_2015_10_29_02_30_05_490.gzip.pdf> (referer: https://www.henderson.com/sgpi)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:55:13 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/38010_2015_09_09_05_33_54_277.gzip.pdf> (referer: https://www.henderson.com/sgpi)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:55:15 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/40295_2015_09_09_06_10_10_377.gzip.pdf> (referer: https://www.henderson.com/sgpi)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:55:16 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/39736_2015_09_09_05_55_02_247.gzip.pdf> (referer: https://www.henderson.com/sgpi)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:55:35 [scrapy] INFO: Crawled 1288 pages (at 43 pages/min), scraped 1046 items (at 42 items/min)
2015-11-04 00:56:00 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12319_2015_11_03_05_27_56_067.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:01 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12315_2015_11_03_01_04_57_547.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:01 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12926_2015_11_03_02_08_23_787.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:02 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12921_2015_11_03_06_48_12_767.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:02 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12312_2015_11_03_01_04_55_563.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:02 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/14043_2015_11_03_01_04_53_503.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:02 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12314_2015_11_03_05_27_50_247.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:02 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12313_2015_11_03_05_27_48_343.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:03 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12271_2015_11_03_01_02_08_997.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:04 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/18275_2015_11_03_06_53_10_877.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:04 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37190_2015_11_03_03_58_27_770.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:04 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12918_2015_11_03_06_48_10_647.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:05 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11590_2015_11_03_12_55_45_163.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:05 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/15523_2015_11_03_06_53_16_823.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:05 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/15526_2015_11_03_06_53_31_767.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:06 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/15524_2015_11_03_06_53_29_753.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:06 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12909_2015_11_03_09_31_02_987.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:07 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/15522_2015_11_03_02_11_04_567.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:08 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13428_2015_11_03_03_32_13_950.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:09 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37197_2015_11_02_11_46_47_130.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:09 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11890_2015_11_03_03_35_09_407.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:10 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11885_2015_11_03_03_35_05_307.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:11 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12302_2015_11_03_01_04_51_337.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:11 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12910_2015_11_03_06_46_00_430.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:12 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12329_2015_11_03_01_05_05_720.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:12 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12938_2015_11_03_06_48_26_857.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:13 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11915_2015_11_03_03_35_14_243.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:14 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12902_2015_11_03_02_08_19_340.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:15 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12330_2015_11_03_01_05_07_780.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:15 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12301_2015_11_03_05_26_03_477.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:16 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12276_2015_11_03_01_02_13_007.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:17 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12801_2015_10_21_04_03_12_790.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:18 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11878_2015_11_03_03_35_02_810.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:19 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12270_2015_11_03_05_24_09_890.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:20 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12304_2015_11_03_05_26_05_520.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:20 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11886_2015_11_03_03_35_07_410.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:21 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12300_2015_11_03_05_26_01_123.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:21 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12939_2015_11_03_06_48_28_837.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:22 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11877_2015_11_03_08_18_11_603.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:33 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12328_2015_11_03_05_28_02_120.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:34 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13477_2015_11_03_03_32_45_477.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:35 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12935_2015_11_03_09_31_15_373.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:37 [scrapy] INFO: Crawled 1377 pages (at 89 pages/min), scraped 1084 items (at 38 items/min)
2015-11-04 00:56:37 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12326_2015_11_03_01_05_03_630.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:38 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12869_2015_11_03_06_43_49_463.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:39 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12268_2015_11_03_05_24_07_893.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:42 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11593_2015_11_03_05_22_03_433.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:45 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13431_2015_11_03_03_32_32_107.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:46 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12345_2015_11_03_01_07_14_313.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:46 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12267_2015_11_03_01_02_07_047.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:49 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13476_2015_11_03_03_32_43_480.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:50 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12868_2015_11_03_06_43_47_450.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:50 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12282_2015_11_03_05_24_18_627.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:51 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12804_2015_11_03_09_28_57_450.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:53 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11917_2015_11_03_08_18_17_547.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:53 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12344_2015_11_03_01_05_09_823.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:56:54 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12292_2015_11_03_05_25_51_917.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:11 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37195_2015_11_03_04_02_38_090.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:19 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12298_2015_11_03_05_25_58_783.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:20 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13498_2015_11_03_03_34_58_393.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:21 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12289_2015_11_03_05_25_47_720.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:21 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12291_2015_11_03_05_25_49_907.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:22 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12932_2015_11_03_02_08_26_343.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:30 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13497_2015_11_03_08_18_09_637.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:30 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12931_2015_11_03_06_48_24_827.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:31 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12322_2015_11_03_01_04_59_497.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:32 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12323_2015_11_03_05_27_58_033.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:32 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/12933_2015_11_03_09_31_12_970.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:33 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12324_2015_11_03_01_05_01_570.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:34 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12934_2015_11_03_02_08_28_873.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:35 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12889_2015_11_03_06_45_50_290.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:35 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12288_2015_11_03_01_02_24_940.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:40 [scrapy] INFO: Crawled 1445 pages (at 68 pages/min), scraped 1121 items (at 37 items/min)
2015-11-04 00:57:43 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/28108_2015_09_09_02_24_18_023.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:46 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/49847_2015_10_29_11_28_35_047.gzip.pdf> (referer: https://www.henderson.com/ir)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:47 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12325_2015_11_03_05_28_00_030.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:49 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/14804_2015_09_08_09_17_28_880.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:50 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/14797_2015_10_21_04_09_26_187.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:50 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12915_2015_10_21_04_08_26_953.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:57:51 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12840_2015_10_21_04_08_05_097.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:58:09 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/46058_2015_07_30_07_05_12_327.gzip.xlsx> (referer: https://www.henderson.com/ir)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:58:17 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/28106_2015_09_09_02_24_13_903.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:58:42 [scrapy] INFO: Crawled 1562 pages (at 117 pages/min), scraped 1230 items (at 109 items/min)
2015-11-04 00:58:42 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/46059_2015_09_09_07_38_35_620.gzip.pdf> (referer: https://www.henderson.com/ir)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:58:52 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/46060_2015_09_09_07_38_45_213.gzip.pdf> (referer: https://www.henderson.com/ir)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:16 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12845_2015_10_21_04_04_23_383.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:17 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12844_2015_10_21_04_04_13_977.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:18 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13452_2015_09_08_08_43_37_007.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:19 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13436_2015_09_08_08_43_15_117.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:26 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13453_2015_09_08_08_43_39_127.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:27 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11882_2015_09_08_07_58_33_360.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:28 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13449_2015_09_08_08_43_29_923.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:32 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13448_2015_09_08_08_43_27_880.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:32 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12838_2015_10_21_04_04_20_997.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:37 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12836_2015_10_21_04_04_16_330.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:38 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13446_2015_09_08_08_43_23_853.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:38 [scrapy] INFO: Crawled 1661 pages (at 99 pages/min), scraped 1311 items (at 81 items/min)
2015-11-04 00:59:44 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/21466_2015_09_08_11_22_48_383.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:45 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11857_2015_09_08_07_57_59_817.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:46 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12843_2015_10_21_04_04_07_877.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:51 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12917_2015_10_21_04_08_31_540.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:51 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13447_2015_09_08_08_43_25_897.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:54 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12831_2015_10_21_04_04_03_693.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 00:59:55 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12832_2015_10_21_04_04_11_963.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

2015-11-04 00:59:55 [twisted] CRITICAL: Error during info_callback
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 415, in dataReceived
    self._write(bytes)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/protocols/tls.py", line 554, in _write
    sent = self._tlsConnection.send(toSend)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 949, in send
    result = _lib.SSL_write(self._ssl, buf, len(buf))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
--- <exception caught here> ---
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1055, in infoCallback
    return wrapped(connection, where, ret)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1157, in _identityVerifyingInfoCallback
    transport = connection.get_app_data()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
exceptions.AttributeError: 'NoneType' object has no attribute '_app_data'

From callback <function infoCallback at 0x7fbc17d997d0>:
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 702, in wrapper
    callback(Connection._reverse_mapping[ssl], where, return_code)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py", line 1059, in infoCallback
    connection.get_app_data().failVerification(f)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 1224, in get_app_data
    return self._app_data
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/OpenSSL/SSL.py", line 838, in __getattr__
    return getattr(self._socket, name)
AttributeError: 'NoneType' object has no attribute '_app_data'
2015-11-04 00:59:56 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/37549_2015_10_21_09_37_41_677.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:02 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12829_2015_10_21_04_03_59_467.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:03 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13439_2015_09_08_08_43_19_643.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:05 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13441_2015_09_08_08_43_21_857.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:11 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12842_2015_10_21_05_06_41_497.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:12 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13451_2015_09_08_08_43_34_290.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:14 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11859_2015_09_08_07_58_04_060.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:15 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/14803_2015_09_08_09_17_26_837.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:16 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12810_2015_10_21_04_04_01_650.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:23 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13435_2015_09_08_08_43_12_920.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:23 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12819_2015_11_03_06_41_25_033.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:32 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11860_2015_09_08_07_58_06_120.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:33 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/14796_2015_10_21_04_09_21_320.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:37 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/12841_2015_10_21_05_06_44_380.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:38 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13450_2015_09_08_08_43_31_967.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:38 [scrapy] INFO: Crawled 1754 pages (at 93 pages/min), scraped 1374 items (at 63 items/min)
2015-11-04 01:00:51 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/11858_2015_09_08_07_58_02_017.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:55 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/46089_2015_11_03_04_14_34_307.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:59 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/13434_2015_09_08_08_43_10_907.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:00:59 [scrapy] ERROR: Spider error processing <GET https://az768132.vo.msecnd.net/documents/16298_2015_09_08_10_25_36_107.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:01:42 [scrapy] INFO: Crawled 1795 pages (at 41 pages/min), scraped 1419 items (at 45 items/min)
2015-11-04 01:01:43 [scrapy] ERROR: Spider error processing <GET https://hgipublicwebsite.blob.core.windows.net/documents/11863_2015_11_03_03_32_41_267.gzip.pdf> (referer: https://www.henderson.com/row)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:02:35 [scrapy] INFO: Crawled 1844 pages (at 49 pages/min), scraped 1460 items (at 41 items/min)
2015-11-04 01:03:50 [scrapy] INFO: Crawled 1891 pages (at 47 pages/min), scraped 1520 items (at 60 items/min)
2015-11-04 01:04:37 [scrapy] INFO: Crawled 1930 pages (at 39 pages/min), scraped 1551 items (at 31 items/min)
2015-11-04 01:05:49 [scrapy] INFO: Crawled 1968 pages (at 38 pages/min), scraped 1598 items (at 47 items/min)
2015-11-04 01:06:44 [scrapy] INFO: Crawled 2004 pages (at 36 pages/min), scraped 1636 items (at 38 items/min)
2015-11-04 01:07:09 [scrapy] ERROR: Error downloading <GET https://az768132.vo.msecnd.net/documents/38143_2015_09_09_05_39_39_107.gzip.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://az768132.vo.msecnd.net/documents/38143_2015_09_09_05_39_39_107.gzip.pdf took longer than 180.0 seconds..
2015-11-04 01:07:41 [scrapy] INFO: Crawled 2062 pages (at 58 pages/min), scraped 1681 items (at 45 items/min)
2015-11-04 01:08:38 [scrapy] INFO: Crawled 2096 pages (at 34 pages/min), scraped 1722 items (at 41 items/min)
2015-11-04 01:09:55 [scrapy] INFO: Crawled 2144 pages (at 48 pages/min), scraped 1780 items (at 58 items/min)
2015-11-04 01:10:41 [scrapy] INFO: Crawled 2192 pages (at 48 pages/min), scraped 1812 items (at 32 items/min)
2015-11-04 01:11:50 [scrapy] INFO: Crawled 2222 pages (at 30 pages/min), scraped 1852 items (at 40 items/min)
2015-11-04 01:12:50 [scrapy] INFO: Crawled 2272 pages (at 50 pages/min), scraped 1894 items (at 42 items/min)
2015-11-04 01:13:59 [scrapy] INFO: Crawled 2325 pages (at 53 pages/min), scraped 1946 items (at 52 items/min)
2015-11-04 01:14:49 [scrapy] INFO: Crawled 2365 pages (at 40 pages/min), scraped 1988 items (at 42 items/min)
2015-11-04 01:15:50 [scrapy] INFO: Crawled 2425 pages (at 60 pages/min), scraped 2039 items (at 51 items/min)
2015-11-04 01:16:44 [scrapy] INFO: Crawled 2446 pages (at 21 pages/min), scraped 2085 items (at 46 items/min)
2015-11-04 01:17:35 [scrapy] INFO: Crawled 2481 pages (at 35 pages/min), scraped 2115 items (at 30 items/min)
2015-11-04 01:18:46 [scrapy] INFO: Crawled 2524 pages (at 43 pages/min), scraped 2149 items (at 34 items/min)
2015-11-04 01:19:54 [scrapy] INFO: Crawled 2545 pages (at 21 pages/min), scraped 2178 items (at 29 items/min)
2015-11-04 01:20:35 [scrapy] INFO: Crawled 2560 pages (at 15 pages/min), scraped 2193 items (at 15 items/min)
2015-11-04 01:21:37 [scrapy] INFO: Crawled 2579 pages (at 19 pages/min), scraped 2215 items (at 22 items/min)
2015-11-04 01:22:36 [scrapy] INFO: Crawled 2652 pages (at 73 pages/min), scraped 2254 items (at 39 items/min)
2015-11-04 01:22:41 [scrapy] ERROR: Spider error processing <GET http://finsurgency.com/wp-content/uploads/Trends-in-Payments-Innovation.pdf> (referer: http://www.baincapitalventures.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 01:23:40 [scrapy] INFO: Crawled 2711 pages (at 59 pages/min), scraped 2301 items (at 47 items/min)
2015-11-04 01:24:35 [scrapy] INFO: Crawled 2750 pages (at 39 pages/min), scraped 2344 items (at 43 items/min)
2015-11-04 01:25:38 [scrapy] INFO: Crawled 2850 pages (at 100 pages/min), scraped 2412 items (at 68 items/min)
2015-11-04 01:25:48 [scrapy] ERROR: Error downloading <GET http://wp.me/p5hvhT-6Ydu>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:26:34 [scrapy] INFO: Crawled 2914 pages (at 64 pages/min), scraped 2483 items (at 71 items/min)
2015-11-04 01:27:39 [scrapy] INFO: Crawled 3007 pages (at 93 pages/min), scraped 2554 items (at 71 items/min)
2015-11-04 01:28:38 [scrapy] INFO: Crawled 3098 pages (at 91 pages/min), scraped 2657 items (at 103 items/min)
2015-11-04 01:29:41 [scrapy] INFO: Crawled 3230 pages (at 132 pages/min), scraped 2773 items (at 116 items/min)
2015-11-04 01:30:16 [scrapy] ERROR: Error downloading <GET https://pando.com/2015/04/07/at-the-womens-venture-capital-conference-in-boston-the-shadow-of-ellen-pao-looms-large/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:30:24 [scrapy] ERROR: Error downloading <GET https://twitter.com/CNBCTV18News/status/647368718658371584>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:30:36 [scrapy] INFO: Crawled 3339 pages (at 109 pages/min), scraped 2891 items (at 118 items/min)
2015-11-04 01:30:58 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=XQLX_1-17-04-pr_protection_one_maj_owner.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:01 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=RPXQ_Cinemark_Quadrangle_pr_1_5_05.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:01 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=YLPS_ntelos_pr.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:02 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=IJRT_1-17-04-2-pr_protection_one_closing_final.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:02 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=GKJC_P1_Press_release_final.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:03 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=QGIC_Huber+and+Steiner+to+Serve+as+Co-Presidents+of+Quadrangle+Group+-+2+23+09.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:07 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=QXGW_pr_2_9_2006.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:07 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=XTNR_pr_9_25_07.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:08 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=BYYG_pr_GET_11_20_07.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:08 [scrapy] ERROR: Error downloading <GET https://subscribe.ft.com/barrier/logic?location=http%3A%2F%2Fwww.ft.com%2Fcms%2Fs%2F0%2F5cfcbcbc-e692-11e4-afb7-00144feab7de.html%3Fsiteedition%3Duk&referer=http%3A%2F%2Fwww.baincapitalventures.com%2Fnews%2F&classification=conditional_standard>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 01:31:08 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=XSON_TVI_QCP_PR_Final.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:08 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=DDIJ_Press+Release+Final+4+15+10.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:09 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=SEGI_Cinemark+Announcement+Press+Release.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:10 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=BCEU_PONE+Press+Release+4.26.10.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:11 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=BJQA_Windstream+press+release_pretty.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:12 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=RDQU_NTELOS+Press+Release_2010+12+08.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:17 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=RQSQ_Dice+Press+Release_2010+12+09.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:20 [scrapy] ERROR: Spider error processing <GET http://www.vancestreetcapital.com/Docs/Klune_SaleSFBJ.pdf> (referer: http://www.vancestreetcapital.com/Portfolio.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:31:33 [scrapy] INFO: Crawled 3424 pages (at 85 pages/min), scraped 2977 items (at 86 items/min)
2015-11-04 01:32:40 [scrapy] INFO: Crawled 3446 pages (at 22 pages/min), scraped 2995 items (at 18 items/min)
2015-11-04 01:33:40 [scrapy] INFO: Crawled 3447 pages (at 1 pages/min), scraped 3003 items (at 8 items/min)
2015-11-04 01:34:35 [scrapy] INFO: Crawled 3471 pages (at 24 pages/min), scraped 3023 items (at 20 items/min)
2015-11-04 01:35:41 [scrapy] INFO: Crawled 3501 pages (at 30 pages/min), scraped 3039 items (at 16 items/min)
2015-11-04 01:37:45 [scrapy] INFO: Crawled 3515 pages (at 14 pages/min), scraped 3058 items (at 19 items/min)
2015-11-04 01:38:36 [scrapy] INFO: Crawled 3539 pages (at 24 pages/min), scraped 3095 items (at 37 items/min)
2015-11-04 01:39:35 [scrapy] INFO: Crawled 3592 pages (at 53 pages/min), scraped 3139 items (at 44 items/min)
2015-11-04 01:40:45 [scrapy] INFO: Crawled 3710 pages (at 118 pages/min), scraped 3244 items (at 105 items/min)
2015-11-04 01:41:11 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=RUDL_West_press.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:11 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=PXCK_NTELOS+Press+Release_2013+12+21.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:11 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=MQMP_Lumos+Press+Release_2013+11+14.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:11 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=EFLG_Ono+Press+Release_2014+03+17.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:15 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=XXHP_Get+AS+Press+Release_2014+09+15.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:15 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=PJDB_West+Secondary+Offering+Press+Release_2015+03+12.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:16 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=MUHE_Lumos+Secondary+Offering+Press+Release_2015+03+11.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:34 [scrapy] ERROR: Spider error processing <GET http://www.quadranglegroup.com/ViewDocument.aspx?f=CRFJ_Cequel+News+Release_2012+07+18.pdf> (referer: http://www.quadranglegroup.com/PressReleases/Default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:41:35 [scrapy] INFO: Crawled 3769 pages (at 59 pages/min), scraped 3297 items (at 53 items/min)
2015-11-04 01:42:37 [scrapy] INFO: Crawled 3860 pages (at 91 pages/min), scraped 3386 items (at 89 items/min)
2015-11-04 01:43:33 [scrapy] ERROR: Spider error processing <GET http://www.kennedywilson.com/kennedy-wilson-secures-planning-permission-capital-dock-development-dublin-ireland/print> (referer: http://www.kennedywilson.com/kennedy-wilson-secures-planning-permission-capital-dock-development-dublin-ireland)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:43:34 [scrapy] ERROR: Spider error processing <GET http://www.kennedywilson.com/kennedy-wilson-announce-third-quarter-2015-earnings/print> (referer: http://www.kennedywilson.com/kennedy-wilson-announce-third-quarter-2015-earnings)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:43:34 [scrapy] INFO: Crawled 3924 pages (at 64 pages/min), scraped 3469 items (at 83 items/min)
2015-11-04 01:44:33 [scrapy] INFO: Crawled 4008 pages (at 84 pages/min), scraped 3548 items (at 79 items/min)
2015-11-04 01:45:47 [scrapy] INFO: Crawled 4129 pages (at 121 pages/min), scraped 3665 items (at 117 items/min)
2015-11-04 01:46:39 [scrapy] INFO: Crawled 4203 pages (at 74 pages/min), scraped 3731 items (at 66 items/min)
2015-11-04 01:47:35 [scrapy] INFO: Crawled 4352 pages (at 149 pages/min), scraped 3871 items (at 140 items/min)
2015-11-04 01:48:45 [scrapy] INFO: Crawled 4500 pages (at 148 pages/min), scraped 4026 items (at 155 items/min)
2015-11-04 01:49:44 [scrapy] INFO: Crawled 4539 pages (at 39 pages/min), scraped 4082 items (at 56 items/min)
2015-11-04 01:49:53 [scrapy] ERROR: Error downloading <GET https://www.southerncrossgroup.com/scg/html/general/index.php?accion=login>: Connection was refused by other side: 111: Connection refused.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET https://portal.tscp.com/>: An error occurred while connecting: 113: No route to host.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.pgamlp.com>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.careers.weissasset.com>: DNS lookup failed: address 'www.careers.weissasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.mad>: DNS lookup failed: address 'www.mad' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.pro>: DNS lookup failed: address 'www.pro' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.tit>: DNS lookup failed: address 'www.tit' not found: [Errno -2] Name or service not known.
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.famainvestimentos.com/novoUsuario.php>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.famainvestimentos.com/foreignInvestors.php>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:50:04 [scrapy] ERROR: Error downloading <GET http://www.famainvestimentos.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 01:50:18 [scrapy] ERROR: Error downloading <GET http://www.polunin.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 01:50:50 [scrapy] INFO: Crawled 4654 pages (at 115 pages/min), scraped 4183 items (at 101 items/min)
2015-11-04 01:51:33 [scrapy] INFO: Crawled 4686 pages (at 32 pages/min), scraped 4225 items (at 42 items/min)
2015-11-04 01:52:33 [scrapy] INFO: Crawled 4686 pages (at 0 pages/min), scraped 4225 items (at 0 items/min)
2015-11-04 01:53:33 [scrapy] INFO: Crawled 4686 pages (at 0 pages/min), scraped 4225 items (at 0 items/min)
2015-11-04 01:53:44 [scrapy] ERROR: Error downloading <GET http://www.oldmutualus.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 01:53:44 [scrapy] ERROR: Error downloading <GET http://www.cornwallcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 01:54:33 [scrapy] INFO: Crawled 4686 pages (at 0 pages/min), scraped 4225 items (at 0 items/min)
2015-11-04 01:55:17 [scrapy] ERROR: Error downloading <GET http://www.chinaamc.com>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.chinaamc.com took longer than 180.0 seconds..
2015-11-04 01:55:17 [scrapy] INFO: Closing spider (finished)
2015-11-04 01:55:17 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 286,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 9,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 30,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 6,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 22,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 213,
 'downloader/request_bytes': 2412642,
 'downloader/request_count': 5815,
 'downloader/request_method_count/GET': 5815,
 'downloader/response_bytes': 160346970,
 'downloader/response_count': 5529,
 'downloader/response_status_count/200': 4608,
 'downloader/response_status_count/301': 367,
 'downloader/response_status_count/302': 463,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/400': 12,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 17,
 'downloader/response_status_count/404': 19,
 'downloader/response_status_count/416': 5,
 'downloader/response_status_count/429': 30,
 'downloader/response_status_count/503': 1,
 'downloader/response_status_count/999': 5,
 'dupefilter/filtered': 36620,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 1, 55, 17, 370093),
 'item_scraped_count': 4225,
 'log_count/CRITICAL': 2,
 'log_count/ERROR': 333,
 'log_count/INFO': 90,
 'offsite/domains': 962,
 'offsite/filtered': 6747,
 'request_depth_max': 2,
 'response_received_count': 4686,
 'scheduler/dequeued': 5815,
 'scheduler/dequeued/memory': 5815,
 'scheduler/enqueued': 5815,
 'scheduler/enqueued/memory': 5815,
 'spider_exceptions/AttributeError': 300,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 0, 30, 33, 853367)}
2015-11-04 01:55:17 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 01:56:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 01:56:19 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 01:56:19 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 01:56:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 01:56:19 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 01:56:19 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 01:56:19 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 01:56:19 [scrapy] INFO: Spider opened
2015-11-04 01:56:19 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 01:56:20 [scrapy] ERROR: Error downloading <GET http://www.key>: DNS lookup failed: address 'www.key' not found: [Errno -2] Name or service not known.
2015-11-04 01:56:20 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 01:57:36 [scrapy] INFO: Crawled 210 pages (at 210 pages/min), scraped 101 items (at 101 items/min)
2015-11-04 01:58:22 [scrapy] INFO: Crawled 298 pages (at 88 pages/min), scraped 178 items (at 77 items/min)
2015-11-04 01:58:23 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/bfdbd276-6717-4e6c-b9b6-3badac22d7c1.pdf> (referer: http://shareholders.fortress.com/file.aspx?FID=30491733&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:59:22 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001171648.PDF?Y=&O=PDF&D=&fid=1001171648&T=&iid=4147324> (referer: http://careers.fortress.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 01:59:22 [scrapy] INFO: Crawled 398 pages (at 100 pages/min), scraped 271 items (at 93 items/min)
2015-11-04 01:59:38 [scrapy] ERROR: Error downloading <GET https://www.maninvestments.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 02:00:22 [scrapy] INFO: Crawled 502 pages (at 104 pages/min), scraped 367 items (at 96 items/min)
2015-11-04 02:01:34 [scrapy] ERROR: Spider error processing <GET https://www.corsaircap.com/documents/649471> (referer: https://www.corsaircap.com/privacypolicy)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 02:01:34 [scrapy] INFO: Crawled 581 pages (at 79 pages/min), scraped 469 items (at 102 items/min)
2015-11-04 02:02:30 [scrapy] INFO: Crawled 672 pages (at 91 pages/min), scraped 537 items (at 68 items/min)
2015-11-04 02:03:03 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/53626ecf-bc92-480e-b76d-ca0ed4e5caab.pdf> (referer: http://shareholders.fortress.com/file.aspx?FID=31438202&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:03:24 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/18e085f0-5f31-4988-baf8-1a892c844d4a.pdf> (referer: http://shareholders.fortress.com/file.aspx?FID=30022959&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:03:24 [scrapy] INFO: Crawled 753 pages (at 81 pages/min), scraped 602 items (at 65 items/min)
2015-11-04 02:04:20 [scrapy] INFO: Crawled 833 pages (at 80 pages/min), scraped 670 items (at 68 items/min)
2015-11-04 02:05:24 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=8a32faec-492e-e211-b01a-b499baba33c8&title=13.01.2012%20-%20Aviva%20Investors%20and%20Tiberius%20launch%20Global%20Commodity%20Plus%20Fund> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:05:25 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=9b27d232-49ef-e011-aa94-b499baba33c8&title=04.09.2007%20-%20Tiberius%20continues%20to%20expand%20its%20team%20and%20focus%20on%20its%20core%20business> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:05:25 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=3cedcd56-4aef-e011-aa94-b499baba33c8&title=10.08.2006%20-%20Two%20commodities%20experts%20chart%20a%20new%20course> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:05:36 [scrapy] INFO: Crawled 891 pages (at 58 pages/min), scraped 742 items (at 72 items/min)
2015-11-04 02:05:36 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=2c5991a5-4aef-e011-aa94-b499baba33c8&title=17.07.2006%20-%20Dr.%20Joachim%20Berlenbach%20and%20Team%20to%20join%20Tiberius> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:05:37 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=7c37f107-4bef-e011-aa94-b499baba33c8&title=22.05.2006%20-%20Tiberius%20Asset%20Management%20PLC%20expands> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:05:37 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=8c8fbe6a-49ef-e011-aa94-b499baba33c8&title=05.10.2006%20-%20Tiberius%20gets%20fixed%20income%20expert%20Dr.%20Bernd%20Fr%c3%bch%20and%20Team%20on%20board> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:05:37 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=7c5027e6-4aef-e011-aa94-b499baba33c8&title=07.07.2006%20-%20Tiberius%20Asset%20Management%20AG%20reinforces%20Team> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:10 [scrapy] ERROR: Spider error processing <GET http://www.jhpartners.com/overview/consumer-expertise> (referer: http://www.jhpartners.com/overview)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 99, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 02:06:12 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=0c871907-49ef-e011-aa94-b499baba33c8&title=18.02.2008%20-%20BNP%20Paribas%20partners%20with%20Tiberius%20to%20develop%20actively%20managed%20commodity%20structured%20pro> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:20 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=7cfe4b08-47ef-e011-aa94-b499baba33c8&title=16.08.2011%20-%20Tiberius%20Press%20Release%20IoF%20Summer%20Academy> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:33 [scrapy] INFO: Crawled 947 pages (at 56 pages/min), scraped 778 items (at 36 items/min)
2015-11-04 02:06:35 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=fcfa5e54-48ef-e011-aa94-b499baba33c8&title=14.10.2008%20-%20For%20turbulent%20times%20-%20Tiberius%20top%20Performance%20fixed-income%20funds> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:36 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=4cc53425-46ef-e011-aa94-b499baba33c8&title=01.07.2006%20Gewinne%20mit%20Rohstoffen%20(Die%20Bank)> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:47 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=0c88bc95-48ef-e011-aa94-b499baba33c8&title=26.03.2008%20-%20Tiberius%20launches%20an%20Absolute%20Return%20Commodity%20Fund> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:47 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=7c642bd5-48ef-e011-aa94-b499baba33c8&title=07.03.2008%20-%20European%20Business%20School%20and%20Tiberius%20Group%20found%20Competence%20Center%20for%20Commodities> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:48 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=1c6392b2-47ef-e011-aa94-b499baba33c8&title=13.12.2010%20-%20Tiberius%20Press%20Release%20Capital%20Markets%20Outlook%202011> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:06:49 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=acc61621-48ef-e011-aa94-b499baba33c8&title=12.01.2010%20-%20Tiberius%20press%20release%20Capital%20Markets%20Outlook%202010> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:07:38 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=ac043816-44ef-e011-aa94-b499baba33c8&title=15.05.2008%20Market%20Analysis%20Energies%20-%20Crude%20Oil> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:07:47 [scrapy] INFO: Crawled 1058 pages (at 111 pages/min), scraped 850 items (at 72 items/min)
2015-11-04 02:07:49 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=1ced427d-47ef-e011-aa94-b499baba33c8&title=04.03.2011%20-%20Tiberius%20Press%20Release%20New%20Employees> (referer: http://www.tiberiusgroup.com/en/press-and-media/press-releases)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:08:11 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=2d0437c3-45ef-e011-aa94-b499baba33c8&title=19.09.2007%20Legende%20der%20Goldhausse%20ohne%20Ende%20(goldseiten)> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:08:56 [scrapy] INFO: Crawled 1122 pages (at 64 pages/min), scraped 917 items (at 67 items/min)
2015-11-04 02:08:57 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=bca3bb5f-45ef-e011-aa94-b499baba33c8&title=02.11.2007%20Gold%20im%20Spannungsfeld%20zwischen%20Asset%20Deflation%20und%20Dollarschw%c3%a4che%20(Internationale%20Edelmet> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:08:58 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=1b971ade-42ef-e011-aa94-b499baba33c8&title=29.09.2009%20Market%20Outlook%20Copper%20(Thomas%20Benedix/%20Tiberius)> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:09:16 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=ccd4dfbf-43ef-e011-aa94-b499baba33c8&title=20.05.2008%20Market%20Analysis%20Precious%20Metals%20-%20Gold> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:09:31 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=6c3c2627-43ef-e011-aa94-b499baba33c8&title=01.05.2009%20Subjektive%20Einsch%c3%a4tzung%20eines%20Investments%20in%20Rohstoffen%20unter%20ethischen%20Gesichtspunkten%20(> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:09:32 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=4c77cc8e-42ef-e011-aa94-b499baba33c8&title=06.11.2009%20Das%20Ende%20der%20traditionellen%20Geldpolitik%20(Markus%20Mezger/%20Goldmesse%20M%c3%bcnchen)> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:09:33 [scrapy] INFO: Crawled 1168 pages (at 46 pages/min), scraped 962 items (at 45 items/min)
2015-11-04 02:10:07 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=8c662543-42ef-e011-aa94-b499baba33c8&title=31.08.2010%20Markteinsch%c3%a4tzung%20Gold%20(Markus%20Mezger/%20Tiberius)> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:10:20 [scrapy] INFO: Crawled 1220 pages (at 52 pages/min), scraped 1005 items (at 43 items/min)
2015-11-04 02:10:31 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=7cd15bb2-41ef-e011-aa94-b499baba33c8&title=23.09.2010%20Mi(e)nenspiel%20(Markus%20Mezger/%20Tiberius)> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:10:45 [scrapy] ERROR: Spider error processing <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=4ef027d5-b4f8-e011-ae6b-b499baba33c8&title=01.10.2011%20Gold%20-%20Das%20Ende%20eines%20Megatrends%20(Markus%20Mezger/%20Tiberius)> (referer: http://www.tiberiusgroup.com/en/press-and-media/trade-journal-articles)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:11:30 [scrapy] INFO: Crawled 1287 pages (at 67 pages/min), scraped 1088 items (at 83 items/min)
2015-11-04 02:12:33 [scrapy] ERROR: Spider error processing <GET http://www.arespublicfunds.com/files/6014/2230/7115/Advantages_of_a_Dynamic_Credit_Strategy.pdf> (referer: http://www.arespublicfunds.com/funds/ardc/overview/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 02:12:35 [scrapy] INFO: Crawled 1394 pages (at 107 pages/min), scraped 1162 items (at 74 items/min)
2015-11-04 02:29:13 [scrapy] INFO: Crawled 1432 pages (at 38 pages/min), scraped 1196 items (at 34 items/min)
2015-11-04 02:29:37 [scrapy] INFO: Crawled 1432 pages (at 0 pages/min), scraped 1232 items (at 36 items/min)
2015-11-04 02:46:19 [scrapy] INFO: Crawled 1499 pages (at 67 pages/min), scraped 1272 items (at 40 items/min)
2015-11-04 02:46:24 [scrapy] ERROR: Error downloading <GET http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=7c26f73f-41ef-e011-aa94-b499baba33c8&title=13.01.2011%20International%20Finance%20&%20Commodity%20Markets%20(Thomas%20Benedix/%20Universit%c3%a4t%20Ulm)>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.tiberiusgroup.com/DownloadExternalDocument.aspx?id=7c26f73f-41ef-e011-aa94-b499baba33c8&title=13.01.2011%20International%20Finance%20&%20Commodity%20Markets%20(Thomas%20Benedix/%20Universit%c3%a4t%20Ulm) took longer than 180.0 seconds..
2015-11-04 04:24:22 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/page.asp?id=3'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Brynwood Partners is the lower middle market firm of choice for corporate divestitures. An operationally-focused fund Brynwood Partners understands the dynamics of corporate divestitures. Brynwood Partners has a successful track record of conducting fast and efficient due diligence with minimal disruption negotiating and entering into short-term transition services and supply agreements and closing transactions quickly.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 267, in _send_cmd
    return self._socket.recv()
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 228, in recv_plain
    raise errors.InterfaceError(errno=2013)
InterfaceError: 2013: Lost connection to MySQL server during query
2015-11-04 04:24:22 [scrapy] INFO: Crawled 1516 pages (at 17 pages/min), scraped 1290 items (at 18 items/min)
2015-11-04 04:24:23 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?bid=1&id=6'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. Hartong III is the Chairman & CEO of Brynwood Partners three most recent funds (Brynwood V VI and VII) and also chairs the firms Executive Committee. Mr. Hartong III joined Brynwood in 2004 as a Managing Partner after the divestiture of Lincoln Snacks Company a Brynwood III portfolio company where he was the President & CEO from 1998 until 2004. Prior to joining Lincoln Snacks Mr. Hartong III was Vice President of Marketing for Activision (Nasdaq: ATVI) a well-known videogame developer. Mr. Hartong III also held various sales and marketing roles at Baskin Robbins USA Co. and Nestl USA Inc. prior to joining Activision. Mr. Hartong III currently serves as Chairman of the following active Brynwood Partners investments: Back to Nature Foods Company LLC Harvest Hill Beverage Company Josephs Gourmet Pasta Company and Pearson Candy Company. Mr. Hartong III holds a B.A. from Lafayette College and an M.B.A. from Harvard Business School.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 04:24:23 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/default.asp'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Established in 1984 Brynwood Partners is a control-oriented operationally-focused lower middle market buyout fund that acquires consumer products companies. The managing partners of Brynwood Partners have extensive operating experience as CEOs and CFOs of both public and private companies which allows the firm to acquire both performing and underperforming businesses and corporate orphan brands with or without management teams. Brynwood Partners will only target investment opportunities where it can leverage the operational expertise of its managing partners to create significant shareholder value. With over $725 million of assets under management Brynwood Partners is actively seeking new investments for its latest fund Brynwood Partners VII L.P. with $420 million of committed capital.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 04:24:24 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?bid=2&id=6'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. MacTaggart is President & COO of Brynwood Partners three most recent funds (Brynwood V VI and VII) and is a member of the firms Executive Committee. Mr. MacTaggart joined Brynwood Partners in 1996 as a Vice President in Brynwood III. Prior to joining the Firm Mr. MacTaggart spent six years at Merrill Lynch & Co. in both the Mergers and Acquisitions and the Corporate Finance departments. Mr. MacTaggart currently serves on the Board of Directors of the following Brynwood Partners investments: Back to Nature Foods Company LLC G&T Conveyor Company Harvest Hill Beverage Company High Ridge Brands Co. Josephs Gourmet Pasta Company Lightlife Foods Inc. Newhall Laboratories Inc. and Pearson Candy Company. Mr. MacTaggart holds a B.S. in Accounting from Boston College and an M.B.A. from The Fuqua School of Business at Duke University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 04:24:24 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?id=6'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. Hartong III is the Chairman & CEO of Brynwood Partners three most recent funds (Brynwood V VI and VII) and also chairs the firms Executive Committee. Mr. Hartong III joined Brynwood in 2004 as a Managing Partner after the divestiture of Lincoln Snacks Company a Brynwood III portfolio company where he was the President & CEO from 1998 until 2004. Prior to joining Lincoln Snacks Mr. Hartong III was Vice President of Marketing for Activision (Nasdaq: ATVI) a well-known videogame developer. Mr. Hartong III also held various sales and marketing roles at Baskin Robbins USA Co. and Nestl USA Inc. prior to joining Activision. Mr. Hartong III currently serves as Chairman of the following active Brynwood Partners investments: Back to Nature Foods Company LLC Harvest Hill Beverage Company Josephs Gourmet Pasta Company and Pearson Candy Company. Mr. Hartong III holds a B.A. from Lafayette College and an M.B.A. from Harvard Business School.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:04 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/page.asp?id=2'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Underperforming Companies at the Time of Acquisition']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:05 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/sitemap.asp?p=1'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:05 [scrapy] INFO: Crawled 1516 pages (at 0 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:51:06 [scrapy] ERROR: Error processing {'pagetitle': [u'Sridhar Narayan Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/sridhar-narayan/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Sridhar Narayan joined GEF in 2012 as a Principal with GEF Advisors India; he provides overall guidance for GEFs investment activities in India and South Asia. He brings 16 years of experience investing in India with expertise evaluating opportunities in solar power waste management and water treatment. Prior to GEF Mr. Narayan worked for JRE Partners in Mumbai where he led minority growth capital investments in a variety of rapidly advancing sectors within India. Prior to JRE Partners he served as Vice President of Direct Investment for American International Group (AIG) a Mumbai-based company as well. While there he invested both the proprietary capital of AIG and third-party private equity funds managed by AIG. He began his career with Zurich Asset Management India/ITC Threadneedle AMC India as the Head of Fixed Income responsible for the portfolio management trading fixed income research and the monthly communications to investors. Mr. Narayan has a Post Graduate Diploma in Management from the Indian Institute of Management Bangalore and a Bachelor of Technology in Mechanical Engineering from the Indian Institute of Technology BHU (IITBHU).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:07 [scrapy] ERROR: Error processing {'pagetitle': [u'Benjamin Sessions Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/benjamin-sessions/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Benjamin Sessions has been working with GEF since 1998 and is a member of the firms investment committee for Latin America. Prior to joining GEF Mr. Sessions worked in the Financial Institutions Group at Lehman Brothers structuring debt and equity offerings as well as M&A transactions for banks insurance companies and finance companies. Previously he was a consultant with Ernst & Young where he advised the government of Kazakhstan on its privatization program first in the establishment of a countrywide public education program in support of privatization and then directing the transport privatization effort in the southern part of the country. Mr. Sessions a CFA Charterholder has a BA cum laude in Soviet and East European Studies from Tufts University and an MA with distinction in International Economics from the Johns Hopkins School for Advanced International Studies (SAIS).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:08 [scrapy] ERROR: Error processing {'pagetitle': [u'Alipt Sharma Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/alipt-sharma-2/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mr. Sharma joined GEF in 2010 as an employee of GEF Advisors India bringing extensive experience in Indian private equity transactions. He is responsible for sourcing structuring and managing investments in South Asia. Prior to joining GEF Mr. Sharma was a member of the investment team at AMP Capital Investors (AMP) in India where he was responsible for identifying analyzing and managing Indian infrastructure opportunities. Prior to joining AMP Mr. Sharma was an Associate Vice President with Ambit Corporate Finance a leading Indian investment bank where he focused on acquisitions and private equity transactions while assisting in the expansion of its capital markets practice. Mr. Sharma also spent seven years with Arthur Andersen and Ernst & Young leading teams on consulting assignments for companies in the power and telecommunications sectors. He is proficient in English and Hindi. Mr. Sharma has a BA in Economics from Shri Ram College of Commerce Delhi University and an MBA from the Indian School of Business. He is also a Chartered Accountant.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:08 [scrapy] ERROR: Error processing {'pagetitle': [u'Gabriel Siqueira Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/gabriel-siqueira/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Gabriel Siqueira joined GEF in 2014. He is based in the So Paulo office and is part of the team managing the firms investment program in Latin America. Prior to joining GEF Mr. Siqueira was part of the Corporate Finance team of Banco Santander Brasil in the Mergers & Acquisitions and Equity Capital Markets groups. He has worked in several transactions in the utilities consumer and infrastructure sectors. He joined Santander in 2010 through its Risk Management Trainee Program. Mr. Siqueira a native Brazilian has a BS in Computer Engineering from the Instituto Tecnolgico de Aeronutica (ITA).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:10 [scrapy] ERROR: Error processing {'pagetitle': [u'Mario Mafud Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/mario-mafud/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mario Mafud joined GEF in 2014. He is based in the So Paulo office and is a member of the team leading GEFs Latin America investment program. Prior to joining GEF he was part of the corporate finance team at Voga a mergers and acquisitions boutique acquired by Banco Indusval & Partners in 2013 and later joined the team responsible for structuring the Investment Banking franchise at BI&P a portfolio company of Warburg Pincus. Mr. Mafud holds a bachelors degree in Production Engineering from Universidade de So Paulo (So Carlos SP) and has credits in finance from Universidad Politcnica de Madrid (Spain).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:10 [scrapy] ERROR: Error processing {'pagetitle': [u'Derek Beaty Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/derek-beaty/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Derek Beaty joined GEF in 2008. He has investment sourcing deal execution and portfolio monitoring responsibilities across each of the target sectors in emerging market countries for GEF. Prior to joining GEF Mr. Beaty evaluated investments and monitored portfolio companies for CIVC Partners a Chicago-based private equity firm. Prior to CIVC Mr. Beaty worked in M&A advisory for the investment banking divisions of both BMO Capital and Arthur Andersen. Mr. Beaty has BS in Accounting from the University of Illinois at Urbana-Champaign and an MBA from Northwestern Universitys Kellogg School of Management. Mr. Beaty is a Certified Public Accountant (inactive). He has working knowledge of Spanish and Portuguese.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:11 [scrapy] ERROR: Error processing {'pagetitle': [u'Anibal Wadih Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/anibal-wadih/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Anibal Wadih joined GEF in 2014 as a Managing Director based in So Paulo Brazil and is a member of the Latin America investment team. He joins GEF from Canepa Management where he was a partner responsible for direct investments and business development throughout Latin America. Previously Mr. Wadih was a Managing Director with Macquarie Capital where he led a private capital merchant banking team investing in Latin America with a focus on the infrastructure and agribusiness sectors. While at Macquarie he participated in a wide range of private equity transactions as well as the development of private equity and mezzanine fund platforms for the region where he served as an investment committee member. Prior to Macquarie he was a senior member of the M&A group with Deutsche Bank in Latin America focusing on financial sponsors. Mr. Wadih holds a BS in Electrical Engineering (BSEE) from Universidad Simon Bolivar (Venezuela) a Masters in Finance from IESA (Venezuela) and an MBA from the Stern School of Business at New York University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:12 [scrapy] ERROR: Error processing {'pagetitle': [u'Aditya Arora Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/aditya-arora/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Aditya Arora joined GEF in 2015 as an employee of GEF Advisors India and is responsible for sourcing executing and managing investments in South Asia. He offers extensive experience in Indian private equity transactions including portfolio management. Prior to GEF Mr. Arora was an Investment Manager with Navis Capital a South/Southeast Asia-focused private equity firm with $5.0 billion in assets under management where he was responsible for sourcing structuring managing and exiting investments. Prior to Navis Mr. Arora was a member of the investment banking team of JM Financial one of Indias leading investment banks executing transactions in the consumer healthcare and infrastructure sectors. Mr. Arora holds an MBA from Indian School of Business Hyderabad and received his Bachelor of Commerce from the University of Calcutta. He is also a Chartered Accountant and is proficient in English Hindi and Bengali.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:14 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=161'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:15 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=141'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:16 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=81'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:16 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=101'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:17 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=121'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:18 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=61'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:19 [scrapy] ERROR: Error processing {'pagetitle': [u'Scott MacLeod Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/scott-macleod/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Scott MacLeod joined GEF in 1998 and has over 40 years of experience in international finance and investment focused on establishing acquiring restructuring privatizing and expanding private-sector companies and projects in emerging markets. He is a Board Member of GEF and serves as Vice Chairman of the investment committees. Prior to joining GEF Mr. MacLeod served for 20 years in various capacities at the International Finance Corporation (IFC). He has worked in more than 30 countries in Eastern Europe Asia Africa and Latin America with corporate executives and senior-level government officials on the execution of private-sector transactions. Mr. MacLeod has a BA from Yale University and an MBA from Columbia University Graduate School of Business.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:20 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=21'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:21 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?Type=AL&iid=4147324&mode=1&start=41'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:22 [scrapy] ERROR: Error processing {'pagetitle': [u'George McPherson Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/george-mcpherson/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['George McPherson joined GEF in 2004 and has primary responsibility for GEFs business development and fund formation activities. Prior to joining GEF Mr. McPherson worked with a number of private equity firms advising them on fund formation and raising capital. From 1988 through 2000 he held a variety of engineering business development and management positions with the Hewlett-Packard Company. Mr. McPherson has a BS in Electrical Engineering from the State University of New York at Buffalo and an MBA in Finance from George Washington University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:23 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress to Announce Third Quarter 2014 Results and Dividend on October 30 2014'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=25566036&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) plans to announce its financial results and dividend for the third quarter 2014 prior to the opening of the New York Stock Exchange on Thursday October 30 2014. A copy of the press release will be posted to the Investor Relations section of Fortresss website www.fortress.com. In addition management will host a conference call on October 30 2014 at 10:00 A.M. Eastern Time. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Third Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Thursday November 6 2014 by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 18383849. Fortress Investment Group LLC is a leading highly diversified global investment firm with approximately $63.8 billion in assets under management as of June 30 2014. Founded in 1998 Fortress manages assets on behalf of approximately 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE:FIG). For more information please visitwww.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:25 [scrapy] ERROR: Error processing {'pagetitle': [u'Region Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/category/portfolio/region-portfolio/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Concord is one of the largest providers of industrial water solutions in India with products that address waste water treatment re-cycling and re-use and modular desalination applications worldwide. The Company is a pioneer in the development of membrane-based filtration solutions used successfully by over 300 customers in a variety of industries.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:25 [scrapy] INFO: Crawled 1531 pages (at 15 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:51:26 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Issues \u201cPrivate Equity and Permanent Capital Overview\u201d'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=25858426&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) has issued a presentation Fortress Private Equity & Permanent Capital Overview summarizing the history performance and outlook for the companys private equity business and permanent capital vehicles. The presentation is available in the Public Shareholders section of Fortresss website at www.fortress.com. Fortress Investment Group LLC is a leading highly diversified global investment firm with $66.0 billion in assets under management as of September 30 2014. Founded in 1998 Fortress manages assets on behalf of over 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:26 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001178746.PDF?Y=&O=PDF&D=&fid=1001178746&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073748403&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:51:27 [scrapy] ERROR: Error processing {'pagetitle': [u'Logan Circle Partners Announces Launch of Registered Mutual Funds'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=27428985&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['PHILADELPHIA--(BUSINESS WIRE)-- Logan Circle Partners L.P. (Logan Circle) today announced the launch of the Logan Circle Partners Funds* a family of mutual funds that will broaden investor access to investment strategies previously offered exclusively to the firms institutional clients. The Logan Circle Partners Core Plus Fund (Tickers: LPCIX LPCYX) the fund familys first offering was launched on December 31 2014 and is accepting new investors. Logan Circle is in the process of launching a High Yield Fund and has filed a registration statement for this fund with the Securities and Exchange Commission. As the traditional investment arm within Fortress Investment Group (NYSE:FIG) Logan Circle is focused on asset management for institutional clients. Utilizing the firms foundation of credit research and security and sector selection these recently established investment vehicles will provide investors an alternative means to access Logan Circles investment strategies. Accordingly the Logan Circle Partners Funds will be available to investors through Institutional (I-Class) and Retirement (R-Class) share classes. The investment philosophy and process for each of the funds will be consistent with Logan Circles existing separately managed account strategies. The Logan Circle Partners Funds will complement the firms existing long/short credit mutual fund: the Fortress Long/Short Credit Fund** for which Logan Circle acts as investment manager. This fund is considered a liquid alternative strategy and is available to institutional and individual investors (Tickers: LPLIX LPLRX LPLAX LPLCX). Additionally Logan Circle is the adviser to an SEI Trust Company-sponsored Collective Investment Trust vehicle offering the Core Plus Collective Trust and Emerging Markets Debt Collective Trust to serve pension profit sharing and governmental plans. For additional information regarding the Logan Circles commingled investment vehicles please visit www.logancirclepartners.com. * Logan Circle Partners Funds have been established as a series of the Advisors Inner Circle Fund III. Advisors Inner Circle is a turnkey bundled solution providing an independent board of trustees compliance program and operational infrastructure for mutual fund offerings. ** The Fortress Long/Short Credit Fund has been established as a series of the Northern Lights Fund Trust. Logan Circle Partners is a traditional asset management company focused on the institutional market with $31 billion in assets under management as of September 30 2014. Founded in 2007 Logan Circle was acquired by Fortress Investment Group in April 2010 as its traditional asset management arm. Fortress is a leading global investment firm that manages assets on behalf of over 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). Carefully consider the Logan Circle Partners Funds investment objectives risk factors charges and expenses before investing. This and additional information can be found in the funds prospectus which may be obtained by visiting www.logancirclepartners.com. Logan Circle Partners Funds are distributed by SEI Investments Distribution Co. 1 Freedom Valley Dr. Oaks PA 19456 which is not affiliated with Fortress Investment Group Logan Circle Partners or Northern Lights Distributors LLC. Read the prospectus carefully before investing. A registration statement relating to the securities of the Logan Circle Partners High Yield Fund has been filed with the Securities and Exchange Commission but has not yet become effective. These securities may not be sold nor may offers to buy be accepted prior to the time the registration statement becomes effective. This communication shall not constitute an offer to sell or the solicitation of an offer to buy nor shall there be any sale of these securities in any State in which such offer solicitation or sale would be unlawful prior to registration or qualification under the securities laws of any such state. Carefully consider the Fortress Long/Short Credit Fund investment objectives risk factors charges and expenses before investing. This and additional information can be found in the funds prospectus which may be obtained by calling 855-477-8100. The Fortress Long/Short Credit Fund is distributed by Northern Lights Distributors LLC member FINRA/SIPC at 17605 Wright Street Omaha NE 68130. Logan Circle Partners Fortress Investment Group SEI Investments Distribution Co. and Northern Lights Distributors LLC are not affiliated. Read the prospectus carefully before investing. All investing is subject to risk including the possible loss of principal. The Logan Circle Partners Master Collective Investment Trust has been established for the collective investment of assets of participating tax qualified pension and profit sharing plans and related trusts and governmental plans as more fully described in the Declaration of Trust. As bank collective trusts the Logan Circle Partners Master Collective Investment Trust is exempt from registration as an investment company. The Logan Circle Partners Master Collective Investment Trust is managed by SEI Trust Company the trustee based on the investment advice of Logan Circle Partners the investment adviser to the trust. SEI Trust Company is a subsidiary of SEI Investments Company (NASDAQ:SEIC) a leading global provider of investment operations solutions investment processing and investment management. The Logan Circle Partners Master Collective Investment Trust is for investment by Eligible Plans (as that term is defined in the Declaration of Trust for the Trust) only (generally retirement plans that are tax-qualified under Section 401(a) of the Internal Revenue Code and governmental plans established pursuant to Section 414(d) of the Internal Revenue Code). The information is provided solely for use by a fiduciary or service provider of such plans and cannot be used or relied upon by any other person or entity.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:28 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress to Release Fourth Quarter & Full Year 2014 Results and Announce Fourth Quarter 2014 Dividend on February 26 2015'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=27480461&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) plans to announce its fourth quarter and full year 2014 financial results and its fourth quarter 2014 dividend prior to the opening of the New York Stock Exchange on Thursday February 26 2015. A copy of the press release will be posted to the Investor Relations section of Fortresss website www.fortress.com. In addition management will host a conference call on February 26 2015 at 10:00 A.M. Eastern Time. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Fourth Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Thursday March 5 2015 by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 82551687. Fortress Investment Group LLC is a leading highly diversified global investment firm with $66.0 billion in assets under management as of September 30 2014. Founded in 1998 Fortress manages assets on behalf of over 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:43 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Third Quarter 2014 Results and Announces Dividend of $0.08 per Share'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=25854925&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) (Fortress or the Company) today reported its third quarter 2014 financial results. Fortress is on track to deliver another strong full year of financial results said Fortress Chief Executive Officer Randy Nardone. At $0.72 per share we recorded our highest first three quarters of pre-tax distributable earnings since 2007. AUM closed the quarter at an all-time high of $66 billion reflecting robust investor demand for Fortress and Logan Circle strategies and our permanent capital vehicles. Strong investment performance has continued to contribute to very substantial embedded value both in our funds and on our balance sheet. Given these strengths we are confident that we will have another strong full year with catalysts in place for considerable upside in 2015 and the years ahead. Fortresss business model is highly diversified and management believes that this positions the Company to capitalize on opportunities for investing capital formation and harvesting profits that can occur at different points in any cycle for our individual businesses. Fortresss business model generates stable and predictable management fees which is a function of the majority of Fortresss alternative AUM residing in long-term investment structures. Fortresss alternative investment businesses also generate variable incentive income based on performance and this incentive income can contribute meaningfully to financial results. Balance sheet investments represent a third component of Fortresss business model and the Company has built substantial value in these investments which are made in Fortress funds alongside the funds limited partners. The table below summarizes Fortresss operating results for the quarter and year-to-date period ended September 30 2014. The consolidated GAAP statement of operations and balance sheet are presented on pages 13-14 of this press release. Fortress recorded GAAP net income of $10 million or $0.02 per diluted Class A share for the third quarter of 2014 compared to GAAP net income of $101 million or $0.12 per diluted Class A share for the third quarter of 2013. Our diluted earnings per share for all periods presented includes the income tax effects to net income (loss) attributable to Class A shareholders from the assumed conversion of Fortress Operating Group units and fully vested restricted partnership units to Class A shares. In 2014 Fortress formed a new liquid hedge fund and a new private equity fund that Fortress determined qualify as variable interest entities of which Fortress is the primary beneficiary and therefore consolidates those funds. Consequently Fortresss financial statements include the assets liabilities related operations and cash flows of these funds and other consolidated entities (collectively the Investment Company). In our consolidated financial statements Fortresss results and the results of the consolidated funds are disclosed under the Investment Manager caption and the consolidated funds related amounts are further detailed under the Investment Company caption. Fortress also consolidates New Media Investment Group Inc.s (New Media or the Media Business) (NYSE: NEWM) financial position and results of operations. Although New Medias operating results impact GAAP net income it does not have a material impact on the net income attributable to Fortresss Class A shareholders Class A basic and diluted earnings per share or total Fortress shareholders equity as substantially all of the operating results of New Media are attributable to non-controlling interests. In our consolidated financial statements New Medias results are disclosed under the Non-Investment Manager caption. The year-over-year decrease in Fortresss third quarter 2014 GAAP net income was primarily driven by a $214 million increase in GAAP expenses and a $58 million decrease in Other Income partially offset by a $172 million increase in GAAP revenues. The increase in GAAP expenses was primarily attributable to $163 million of expenses associated with the Media Business as a result of the consolidation of New Media beginning in February 2014 as well as a $41 million increase in Investment Manager expenses related to compensation and benefits. The decrease in Other Income was primarily related to $26 million of losses in the fair value of options and common stock in our publicly traded permanent capital vehicles in the third quarter. The increase in GAAP revenues was primarily attributable to $165 million of revenues from the Media Business as a result of the consolidation of New Media. This section provides information about each of Fortresss businesses: (i) Credit Hedge Funds and Credit Private Equity (PE) Funds (ii) Private Equity Funds and Permanent Capital Vehicles (iii) Liquid Hedge Funds and (iv) Logan Circle. In the third quarter of 2014 Fortress reorganized its segments by: (i) reclassifying its investments in and resulting pre-tax DE from the Fortress Funds which were previously presented under the principal investments segment to each of the other segments that the investment relates to and (ii) reclassifying one of its private equity funds WWTAI from its Private Equity Funds segment to its Permanent Capital Vehicles segment. Prior period amounts have been reclassified to reflect this segment reorganization. Fortress uses DE as the primary metric to manage its businesses and gauge the Companys performance and it uses DE exclusively to report segment results. All DE figures are presented on a pre-tax basis. Consolidated segment results are non-GAAP information and are not presented as a substitute for Fortresss GAAP results. Fortress urges you to read Non-GAAP Information below. Pre-tax DE was $55 million in the third quarter of 2014 down from $65 million in the third quarter of 2013. This decrease was primarily due to higher segment expenses and lower net investment income partially offset by higher management fees and incentive income. Management fees were $149 million in the third quarter of 2014 up from $136 million in the third quarter of 2013 primarily due to higher management fees from the Credit Hedge Funds Logan Circle Liquid Hedge Funds and Permanent Capital Vehicles partially offset by lower management fees from the Private Equity Funds. Incentive income recorded in the third quarter of 2014 totaled $88 million up from $42 million recorded in the third quarter of 2013 primarily due to higher incentive income from the Credit PE Funds and Permanent Capital Vehicles and a reduction in the reversal of accrued incentive income from the Liquid Hedge Funds in third quarter of 2013 partially offset by lower incentive income from the Credit Hedge Funds and Private Equity Funds. Additionally Fortress had $1.0 billion in gross undistributed unrecognized incentive income based on investment valuations as of September 30 2014. This includes $949 million from our funds and private permanent capital vehicle and $64 million from options in our publicly traded permanent capital vehicles. Net investment income (loss) totaled $(12) million in the third quarter of 2014 compared to $12 million in the third quarter of 2013. The decrease in net investment income was primarily due to losses and impairments related to investments held on our balance sheet. The Companys segment revenues and distributable earnings will fluctuate materially depending upon the performance of its funds and the realization events within its Private Equity businesses as well as other factors. Accordingly the revenues and distributable earnings in any particular period should not be expected to be indicative of future results. As of September 30 2014 AUM totaled $66.0 billion up from $63.8 billion as of June 30 2014. During the third quarter Fortress recorded $2.2 billion of net client inflows for Logan Circle had a $0.9 billion increase in invested capital raised $0.7 billion of capital that was directly added to AUM and had $0.4 billion of market-driven valuation gains. These increases to AUM were partially offset by (i) $1.2 billion of capital distributions to investors (ii) $0.6 billion of Liquid Hedge Fund redemptions and (iii) $0.2 billion of payments to Credit Hedge Fund investors from redeeming capital accounts. As of September 30 2014 the Credit Funds Private Equity Funds and private permanent capital vehicle had approximately $4.8 billion $2.1 billion and $0.6 billion of uncalled capital respectively that will become AUM if deployed/called. Uncalled capital or dry powder capital committed to the funds but not invested and generating management fees includes $2.0 billion that is only available for follow-on investments management fees and other fund expenses. Notably approximately 78% of alternative AUM was in funds with long-term investment structures as of September 30 2014 which provides for a stable predictable base of management fees. Below is a discussion of third quarter 2014 segment results and business highlights. The Credit business which includes our Credit PE Funds and Credit Hedge Funds generated pre-tax DE of $36 million in the third quarter of 2014 compared to $40 million in the third quarter of 2013. The year-over-year decline in DE was primarily driven by higher expenses and lower net investment income partially offset by higher incentive income and management fees. The Credit PE Funds generated pre-tax DE of $18 million in the quarter up from $9 million in the third quarter of 2013 as increased realization activity resulted in $41 million of incentive income recorded in the quarter. Positive Credit PE Fund performance in the last twelve months also contributed to a $232 million year-over-year net change in gross unrecognized Credit PE incentive income which totaled $844 million as of September 30 2014. At quarter end the Credit PE Funds had $10.2 billion of incentive eligible NAV above performance thresholds as all flagship Credit Opportunities and Japan Real Estate Funds were valued above their preferred thresholds and eligible to generate incentive income. The Credit Hedge Funds generated pre-tax DE of $18 million in the quarter down from $31 million in the third quarter of 2013 primarily due to lower incentive income and lower net investment income. DBSO LP Fortresss flagship credit hedge fund had net returns of 1.8% in the third quarter and 7.7% for the nine month period ending September 30 2014. At quarter end the Credit Hedge Funds had $5.4 billion of incentive eligible NAV above performance thresholds and eligible to generate additional incentive income. The Credit Hedge Funds raised $165 million of third-party capital in the quarter primarily for the DBSO funds of which $155 million immediately contributed to AUM. The Credit Hedge Funds have raised $947 million of third-party capital year-to-date through September 30 2014. The Private Equity business which includes Private Equity Funds and Permanent Capital Vehicles recorded pre-tax DE of $25 million in the third quarter of 2014 down from $37 million in the third quarter of 2013 primarily due to higher expenses and lower net investment income partially offset by higher Permanent Capital Vehicle incentive income. The Private Equity funds generated $18 million of pre-tax DE in the quarter down from $30 million in the third quarter of 2013 primarily due to higher operating expenses and lower net investment income. During the quarter the Private Equity business raised $101 million of capital for the Italian NPL Opportunities Fund bringing total third-party commitments to approximately $830 million. Private Equity Fund valuations increased 3.2% in the quarter primarily due to appreciation of Springleaf Holdings Inc. (NYSE: LEAF) a publicly traded portfolio company investment held within Fund V and certain privately held portfolio company investments. The Permanent Capital Vehicles generated $7 million of pre-tax DE in the quarter flat compared to the third quarter of 2013 as increased incentive income and management fees were offset by higher expenses. During the quarter the Permanent Capital Vehicles raised $673 million of capital including $361 million for WWTAI $198 million for NCT and $115 million for NEWM. Year-to-date through September 30 2014 the Permanent Capital Vehicles have raised $1.1 billion of capital. WWTAI which has been reclassified from the Private Equity Fund segment to the Permanent Capital Vehicle segment has filed IPO documents with the SEC. Fortresss ability to complete an IPO of WWTAI is subject to certain conditions including but not limited to the SEC declaring the registration statement relating to the IPO effective and approval of an application to list WWTAIs common stock on the NYSE. There can be no assurance these conditions will be satisfied. In June 2014 NCT announced plans to spin off all of its senior housing assets into a new publicly traded real estate investment trust New Senior Investment Group Inc. (New Senior). New Senior will be externally managed by an affiliate of Fortress and will primarily target senior housing related investments. The spin-off has been approved by NCTs Board of Directors and is expected to be completed with the distribution of shares of common stock of New Senior (NYSE: SNR) on or about November 6 2014 to shareholders of NCT. The Liquid Hedge Funds recorded a pre-tax DE loss of $4 million in the third quarter of 2014 compared to a pre-tax DE loss of $9 million in the third quarter of 2013. The year-over-year improvement in pre-tax DE was primarily due to a $26 million reversal of accrued incentive income in the third quarter of 2013. Net returns for the quarter ended September 30 2014 for the Fortress Macro Funds Fortress Asia Macro Funds Fortress Partners Funds and Fortress Convex Asia Funds were 1.1% 1.5% (1.4)% and (0.5)% respectively. Net returns year-to-date through October 24 2014 for the Fortress Macro Funds Fortress Asia Macro Funds and Fortress Convex Asia Funds were (9.3)% (7.3)% and (4.5)% respectively.* Liquid Hedge Funds ended the quarter with $7.5 billion of AUM up 9% from the third quarter of 2013 primarily due to $2.4 billion of capital raised in the last twelve months partially offset by $1.6 billion of redemptions in the last twelve months. Subsequent to quarter end the Liquid Hedge Funds raised approximately $168 million of additional capital which will be added to AUM in the fourth quarter of 2014. As of September 30 2014 there were $603 million Liquid Hedge Fund redemption notices outstanding $296 million of which will be paid primarily within one quarter. Logan Circle our traditional asset management business recorded a pre-tax DE loss of $2 million in the quarter flat compared to the third quarter of 2013 as increased management fees were offset by higher operating expenses and lower net investment income. Logan Circle ended the quarter with $31.1 billion in AUM a 7% increase compared to the prior quarter and a 32% increase compared to the previous year. The year-over-year increase in AUM was primarily due to net client inflows of $5.9 billion and market-driven valuation gains of $1.6 billion. Since Fortresss acquisition of Logan Circle in April 2010 Logan Circles AUM has grown at a CAGR of 25%. Notably 14 of 16 of Logan Circles fixed income strategies outperformed their respective benchmarks year-to-date through September 30 2014. Since inception 15 of 16 Logan Circle fixed income strategies have outperformed their respective benchmarks and as of September 30 2014 eight were ranked in the top quartile of performance for their competitor universe. As of September 30 2014 Fortress (excluding New Media and Investment Company consolidated VIEs) had cash and cash equivalents of $332 million and debt obligations of $75 million. As of September 30 2014 Fortress had $1.1 billion of investments in Fortress funds and $0.1 billion of investments in options in Permanent Capital Vehicles. As of September 30 2014 Fortress had a total of $149 million of outstanding commitments to its funds. In addition at quarter end the NAV of Fortresss investments in its own funds exceeded its segment cost basis by $572 million representing net unrealized gains that have not yet been recognized for segment reporting purposes. Fortresss Board of Directors declared a third quarter 2014 cash dividend of $0.08 per dividend paying share. The dividend is payable on November 17 2014 to Class A shareholders of record as of the close of business on November 12 2014. The declaration and payment of any dividends are at the sole discretion of the Board of Directors which may decide to change its dividend policy at any time. Please see below for information on the U.S. federal income tax implications of the dividend. DE is a primary metric used by management to measure Fortresss operating performance. Consistent with GAAP DE is the sole measure that management uses to manage and thus report on Fortresss segments namely: Private Equity Permanent Capital Vehicles Credit Hedge Funds Credit PE Funds Liquid Hedge Funds and Logan Circle.DE differs from GAAP net income in a number of material ways. For a detailed description of the calculation of pre-tax DE and fund management DE see Exhibit 3 to this release and note 11 to the financial statements included in the Companys most recent quarterly report on Form 10-Q. Fortress aggregates its segment results to report consolidated segment results as shown in the table under Summary Financial Results and in the Total column of the table under Consolidated Segment Results (Non-GAAP). The consolidated segment results are non-GAAP financial information. Management believes that consolidated segment results provide a meaningful basis for comparison among present and future periods. However consolidated segment results should not be considered a substitute for Fortresss consolidated GAAP results. The exhibits to this release contain reconciliations of the components of Fortresss consolidated segment results to the comparable GAAP measures and Fortress urges you to review these exhibits. Fortress also uses weighted average dividend paying shares and units outstanding (used to calculate pre-tax DE per dividend paying share) and net cash and investments. The exhibits to this release contain reconciliations of these measures to the comparable GAAP measures and Fortress urges you to review these exhibits. Management will host a conference call today Thursday October 30 2014 at 10:00 A.M. Eastern Time. A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Third Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Thursday November 6 2014 by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 18383849. Fortress Investment Group LLC (NYSE:FIG) is a leading highly diversified global investment management firm with $66.0 billion in assets under management as of September 30 2014. Fortress applies its deep experience and specialized expertise across a range of investment strategies - private equity credit liquid hedge funds and traditional asset management - on behalf of over 1600 institutional clients and private investors worldwide. For more information regarding Fortress Investment Group LLC or to be added to its e-mail distribution list please visit www.fortress.com. Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding Fortresss sources of management fees incentive income and investment income (loss) estimated fund performance and the amount and source of expected capital commitments.These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements and any such differences could cause the Companys actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysQuarterly Report on Form 10-Q which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. This announcement is intended to be a qualified notice as provided in the Internal Revenue Code (the Code) and the Regulations thereunder. For U.S. federal income tax purposes the dividend declared in October 2014 will be treated as a partnership distribution. The per share distribution components are as follows: Distributable earnings is Fortresss supplemental measure of operating performance used by management in analyzing segment and overall results. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income in accordance with GAAP and it is not necessarily indicative of liquidity or cash available to fund the Companys operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 11 to the financial statements included in the Companys Quarterly Report on Form 10-Q for the quarter ended September 30 2014. Growing distributable earnings is a key component to the Companys business strategy and distributable earnings is the supplemental measure used by management to evaluate the economic profitability of each of the Companys businesses and total operations. Therefore Fortress believes that it provides useful information to investors in evaluating its operating performance. Fortresss definition of distributable earnings is not based on any definition contained in its amended and restated operating agreement. Fund management DE is equal to pre-tax distributable earnings excluding our direct investment-related results. It is comprised of Segment Revenues net of Segment Expenses and Principal Performance Payments. Fund management DE and its components are used by management to analyze and measure the performance of our investment management business on a stand-alone basis. Fortress defines segment operating margin to be equal to fund management DE divided by segment revenues. The Company believes that it is useful to provide investors with the opportunity to review our investment management business using the same metrics. Fund management DE and its components are subject to the same limitations as pre-tax distributable earnings as described above. Dividend paying shares and units represents the number of shares and units outstanding at the end of the period which were entitled to receive dividends or related distributions. The Company believes it is useful for investors in computing the aggregate amount of cash required to make a current per share distribution of a given amount per share. It excludes certain potentially dilutive equity instruments primarily non-dividend paying restricted Class A share units and therefore is limited in its usefulness in computing per share amounts. Accordingly dividend paying shares and units should be considered only as a supplement and not an alternative to GAAP basic and diluted shares outstanding. The Companys calculation of dividend paying shares and units may be different from the calculation used by other companies and therefore comparability may be limited. Net cash and investments represents cash and cash equivalents plus investments less debt outstanding. The Company believes that net cash and investments is a useful supplemental measure because it provides investors with information regarding the Companys net investment assets. Net cash and investments excludes certain assets (investments in options due from affiliates deferred tax asset other assets) and liabilities (due to affiliates accrued compensation and benefits deferred incentive income and other liabilities) its utility as a measure of financial position is limited. Accordingly net cash and investments should be considered only as a supplement and not an alternative to GAAP book value as a measure of the Companys financial position. The Companys calculation of net cash and investments may be different from the calculation used by other companies and therefore comparability may be limited.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:44 [scrapy] ERROR: Error processing {'pagetitle': [u'About Fortress - Leadership - Board of Directors - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://www.fortress.com/AboutFortress/Leadership/Board.aspx?id=12'],
 'siteurl': ['fortress.com'],
 'text': ['George W. Wellde Jr. has been a member of the Board of Directors of Fortress Investment Group LLC since August 2009. Mr. Wellde served as Vice Chairman of the Securities Division at Goldman Sachs & Co. from 2005 until his retirement in 2008. Prior to that he was head of North America Sales for the Fixed Income Currency and Commodities division. Mr. Wellde joined Goldman Sachs in 1979 became a partner in 1992. In addition he was branch manager of the Goldman Sachs Tokyo office and head of its Fixed Income Division from 1994 to 1999. Prior to joining Goldman Sachs he worked for the Federal Reserve Board of Governors in Washington from 1976 to 1979. Mr. Wellde is currently on the Board of Trustees of George Washington University and is a member of the Investment Audit and Finance and Executive committees. Additionally he is a member of the Investment Committee of the University of Richmond where he previously served on the Board from 2000 to 2010 and as the Chair of the Board from 2006 to 2010. He currently serves on the Board of the Partnership for Public Service in Washington D.C. and is a member of the Council on Foreign Relations. Mr Wellde also served on the Board of the Gavi Alliance from 2001 to 2014 on the Execcutive Governance and Investment committees. Mr. Wellde holds a B.S. from the University of Richmond and an M.B.A. from George Washington University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:45 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Announces Acquisition of the Rihga Royal Hotel Kyoto'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=27666977&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['Iconic Addition to Portfolio of Over 70 Hotels Acquired in Past Five Years NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) today announced an agreement with the Royal Hotel Ltd. (Royal Hotel) regarding the acquisition by a Fortress-managed fund of the Rihga Royal Hotel Kyoto one of the regions most prestigious hotels. Originally built in 1969 the hotel is a landmark in Kyoto with a deep tradition and convenient access to Kyoto Station. Working with Royal Hotel on a long-term basis the Fortress fund (together with other Fortress affiliates Fortress) plans to invest a significant amount of capital following the acquisition to enhance the current facilities and rooms to better meet the standards of a modern hotel while retaining the historic feel and traditional elements that have enabled the Rihga Royal Hotel Kyoto to develop a loyal and distinguished clientele. Fortress expects the acquisition to close in the first quarter of 2015. In addition to the planned capital expenditures Fortress was selected by Royal Hotel due to its deep experience in the Japanese hospitality market having acquired over 70 hotels in Japan over the past five years. Fortress has renovated or developed over 30 of these hotels including the recently completed development of the Hotel MyStays Haneda and the Hotel MyStays Kanazawa and a substantial nearly completed renovation of the Sheraton Grande Tokyo Bay Hotel next to Tokyo Disneyland. Fortresss investments in Japan have been made through the firms Japan Opportunity Funds whose investments benefit from management by a 34 person Tokyo-based real estate team headed by CIO Thomas Pulley. Fortress is the sponsor of Invincible Investment Corporation (Invincible) a publicly-traded REIT (TSE: 8963) that is an active acquirer of hotels in Japan with 23 acquisitions over the past year (including 22 hotels from Fortress-managed funds). Invincible currently has an approximate JPY135 billion market capitalization and based on share price appreciation is the top performing Japan-based REIT over the past two years. Fortress believes in the importance of the REIT market as a long-term holder of assets that can help channel the vast personal savings in Japan toward real estate investments. Fortress Investment Group LLC is a leading highly diversified global investment firm with $66.0 billion in assets under management as of September 30 2014. Founded in 1998 Fortress manages assets on behalf of over 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:45 [scrapy] ERROR: Error processing {'pagetitle': [u'About Fortress - Leadership - Board of Directors - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://www.fortress.com/AboutFortress/Leadership/Board.aspx?id=40'],
 'siteurl': ['fortress.com'],
 'text': ['Michael G. Rantz has been a member of the Board of Directors since July 2015. Mr. Rantz was responsible for Global Government Bond Trading at Goldman Sachs & Co. in London from 1995 to until his retirement in 1999. During this time Mr. Rantz was the Founding Chairman of Brokertec Global LLC an electronic inter-dealer broker in the government bond markets. Prior to that Mr. Rantz was responsible for the Goldman Sachs & Co. United States Government Primary Dealership based out of New York. Mr. Rantz joined Goldman Sachs in 1983 and became a Partner in 1992. In addition he was responsible for Municipal Bond Sales Trading and Underwriting from 1991 to 1993. Mr. Rantz currently serves on the Investment Committees of Castilleja School and Avenidas a non-profit dedicated to enriching the lives of older adults both located in Palo Alto California. He previously served on the Cornell University College of Arts and Sciences Advisory Council from 2000 to 2008 and the Cornell University Council from 2002 to 2008. Mr. Rantz was a member of the Board of Trustees of the International Center for the Disabled in New York City where he was a member of the Finance and Audit Committees from 2003 to 2008 the Board of Trustees of the Phillips Brooks School in Menlo Park California where he was Chairman of the Finance and Investment Committees and a member of the Executive Audit and Development Committees from 2007 to 2012 and the Board of Trustees of the Castilleja School in Palo Alto California where he was the Chairman of the Investment Committee and a member of the Audit and Executive Committees from 2008 to 2015. Mr. Rantz holds a B.A. from Cornell University and an M.B.A. from the University of Chicago.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:45 [scrapy] ERROR: Error processing {'pagetitle': [u'About Fortress - Leadership - Board of Directors - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://www.fortress.com/AboutFortress/Leadership/Board.aspx?id=7'],
 'siteurl': ['fortress.com'],
 'text': ['Douglas L. Jacobs has been a member of the Board of Directors of Fortress Investment Group LLC since February 2007. Mr. Jacobs is a director of Doral Financial Corporation a financial services company where he is Chairman of the Risk Policy Committee and member of the Audit and Dividend Committees. From November 2004 to mid-2008 Mr. Jacobs was also a director of ACA Capital Holdings Inc. a financial guaranty company where he was Chairman of the Audit Committee and a member of the Compensation Committee and Risk Management Committees. Mr. Jacobs was a director and Chairman of the Audit Committee for Global Signal Inc. from February 2004 until January 2007. Mr. Jacobs has also been a director of Hanover Capital Mortgage Holdings Inc from 2003 until 2007. From 1988 to 2003 Mr. Jacobs was an Executive Vice President and Treasurer at FleetBoston Financial Group managing the companys funding securitization capital and asset and liability management activities in addition to its securities derivatives and mortgage loan portfolios. Prior to joining FleetBoston Mr. Jacobs was active in a variety of positions at Citicorp over seventeen years culminating in his role as Division Executive of the Mortgage Finance Group. Mr. Jacobs holds a B.A. from Amherst College and an M.B.A. from the Wharton School of Business at the University of Pennsylvania.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:46 [scrapy] ERROR: Error processing {'pagetitle': [u'About Fortress - Leadership - Board of Directors - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://www.fortress.com/AboutFortress/Leadership/Board.aspx?id=35'],
 'siteurl': ['fortress.com'],
 'text': ['David B. Barry has been a member of the Board of Directors of Fortress Investment Group LLC since January 2011. Mr. Barry is President of Ironstate Development Company a privately held real estate development and management company based in Hoboken New Jersey. Mr. Barry is a Trustee of Beat the Streets Wrestling Inc. a Board Member of New Jersey Apartment Association Team Leader for USA Wrestling for 2012 Olympics and a Member of the Board of Governors for the National Wrestling Hall of Fame. Mr. Barry received his B.A. from Columbia University and a J.D. from Georgetown University Law Center.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:47 [scrapy] ERROR: Error processing {'pagetitle': [u'Statement from Fortress Co-Chairman Wes Edens on Springleaf\u2019s Announced Agreement to Acquire OneMain Financial'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=28149375&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (Fortress) issued the following statement from Fortress Co-Chairman Wes Edens on Springleaf Holdings Inc.s (Springleaf) announced agreement to acquire OneMain Financial (OneMain): As a significant shareholder of Springleaf we believe this is a compelling transaction both strategically and financially. It brings together two companies with similar cultures and exceptional management teams singularly focused on meeting the everyday financing needs of an enormous population of working Americans. This combination will create a premier consumer finance company serving approximately 2.5 million customers with $15 billion in assets 2000 branches and a leading digital presence in the market. Funds managed by Fortress own approximately 73 million shares of Springleaf. The investment dates to August 2010 when Fortress agreed to purchase an 80% stake in American General Finance for approximately $125 million. Springleaf completed an initial public offering on October 15 2013 and trades on the New York Stock Exchange (NYSE: LEAF). Fortress has retained its ownership stake in the company. Fortress Investment Group LLC is a leading highly diversified global investment firm with $67.5 billion in assets under management as of December 31 2014. Founded in 1998 Fortress manages assets on behalf of over 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE:FIG). For additional information please visit www.fortress.com. Certain statements in this press release may constitute "forward-looking statements" within the meaning of the Private Securities Litigation Reform Act of 1995. These statements are based on the current expectations and beliefs of management and are subject to a number of trends and uncertainties that could cause actual results to differ materially from those described in the forward-looking statements many of which are beyond our control. The consummation of the proposed acquisition of OneMain Financial is subject to customary closing conditions and regulatory approvals some of which are beyond our control. Accordingly no assurance can be given that the acquisition will be completed on the contemplated terms or at all and you should not place undue reliance on any forward-looking statements contained in this press release. Statements preceded by followed by or that otherwise include the words anticipate appears believe foresee intend should expect estimate project plan may could will are likely and similar expressions are intended to identify forwardlooking statements. These statements involve predictions of our future financial condition performance plans and strategies and are thus dependent on a number of factors including without limitation assumptions and data that may be imprecise or incorrect. Specific factors that may impact performance or other predictions of future actions include but are not limited to: various risks relating to the proposed acquisition including in respect of the satisfaction of closing conditions to the acquisition; unanticipated difficulties financing the purchase price; unanticipated expenditures relating to the acquisition; uncertainties as to the timing of the acquisition; litigation relating to the acquisition; the impact of the acquisition on each companys relationships with employees and third parties; and the inability to obtain or delays in obtaining cost savings and synergies from the acquisition. In addition new risks and uncertainties emerge from time to time and it is not possible for Fortress to predict or assess the impact of every factor that may cause its actual results to differ from those expressed or implied in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release and you should not regard any forward-looking statement as a representation by Fortress or any other person that the future plans estimates or expectations currently contemplated by Fortress will be achieved. You should consider any forward-looking statements included in this release in light of the risks and other important factors described in Fortress s filings with the Securities and Exchange Commission particularly those identified in the sections entitled "Risk Factors" and "Managements Discussion and Analysis of Financial Condition and Results of Operations" in Fortress s most recent Quarterly Report on Form 10-Q and Annual Report on Form 10-K which are available on Fortress s website (www.fortress.com). Fortress can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. Fortress expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Fortresss expectations with regard thereto or any change in events conditions or circumstances on which any statement is based.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:51:48 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Announces Release of 2014 K-1 Tax Packages'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=28613667&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (Fortress) today announced that its 2014 Schedule K-1 tax packages are now available online. Fortress shareholders may access their Schedule K-1 tax package at www.taxpackagesupport.com/fortress or by visiting the Public Shareholders section of Fortresss website at www.fortress.com. The mailing of 2014 Schedule K-1 tax packages is expected to commence today. For additional information shareholders may call Fortress Tax Package Support toll free at (866) 526-0125. Fortress Investment Group LLC is a leading highly diversified global investment firm with $67.5 billion in assets under management as of December 31 2014. Founded in 1998 Fortress manages assets on behalf of over 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:01 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Fourth Quarter and Year End 2014 Results and Announces Dividend of $0.38 per Share'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=28000390&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE: FIG) (Fortress or the Company) today reported its fourth quarter and year end 2014 financial results. Record earnings in the fourth quarter capped off a terrific year for Fortress said Fortress Chief Executive Officer Randy Nardone. We closed 2014 with AUM at an all-time high of $67.5 billion driven by $12 billion in aggregate of new capital raised for our alternative funds equity raised for our permanent capital vehicles and net client inflows for Logan Circle Partners. Strong investment performance and increased realization activity contributed to full-year pre-tax DE of $0.99 per share a 13% increase over the prior year and our highest full year of pre-tax DE since 2007. With these outstanding results we are pleased to announce a $0.38 per share fourth quarter dividend bringing full year dividends to $0.80 per share or 100% of our after-tax DE. With strong investor demand for our strategies major positive developments in some of our largest portfolio investments and tremendous embedded value both in our funds and on our balance sheet we have great visibility and optimism about the value we can create and distribute in the coming years. Fortresss business model is highly diversified and management believes that this positions the Company to capitalize on opportunities for investing capital formation and harvesting profits that can occur at different points in any cycle for our individual businesses. Fortresss business model generates stable and predictable management fees which is a function of the majority of Fortresss alternative AUM residing in long-term investment structures. Fortresss alternative investment businesses also generate variable incentive income based on performance and this incentive income can contribute meaningfully to financial results. Balance sheet investments represent a third component of Fortresss business model and the Company has built substantial value in these investments which are made in Fortress funds alongside the funds limited partners. The table below summarizes Fortresss operating results for the quarter and full year period ended December 31 2014. The consolidated GAAP statement of operations and balance sheet are presented on pages 13-14 of this press release. In addition to its investment management activity in 2014 Fortress consolidated certain funds and investment vehicles. Fortresss financial statements include the assets liabilities related operations and cash flows of these consolidated entities (collectively the Investment Company). Fortresss results and the results of the consolidated entities are disclosed under the Investment Manager caption and the consolidated entities related amounts are further detailed under the Investment Company caption in our consolidated financial statements. Fortress also consolidated the financial position and results of operations of New Media Investment Group Inc. (New Media) (NYSE: NEWM) beginning on February 14 2014 and New Senior Investment Group Inc. (New Senior) (NYSE: SNR) beginning on November 7 2014. Although New Seniors and New Medias operating results impact GAAP net income they do not have a material impact on the net income attributable to Fortresss Class A shareholders Class A basic and diluted earnings per share or total Fortress shareholders equity as substantially all of the operating results of New Senior and New Media are attributable to non-controlling interests. New Seniors and New Medias results are disclosed under the Non-Investment Manager caption in our consolidated financial statements. Fortress recorded GAAP net income of $148 million or $0.24 per diluted Class A share for the fourth quarter of 2014 compared to GAAP net income of $318 million or $0.49 per diluted Class A share for the fourth quarter of 2013. Our diluted earnings per share for all periods presented includes the income tax effects to net income (loss) attributable to Class A shareholders from the assumed conversion of Fortress Operating Group units and fully vested restricted partnership units to Class A shares. The year-over-year decrease in Fortresss fourth quarter 2014 GAAP net income was primarily driven by a $286 million increase in GAAP expenses partially offset by a $105 million increase in GAAP revenues. The increase in GAAP expenses was primarily attributable to $219 million of expenses associated with the Non-Investment Manager as a result of the consolidation of New Senior and New Media as well as a $67 million increase in Investment Manager expenses primarily related to compensation and benefits. The increase in GAAP revenues was primarily attributable to $229 million of revenues from the Non-Investment Manager as a result of the consolidation of New Senior and New Media partially offset by a $124 million decrease in Investment Manager revenues primarily related to lower incentive income. This section provides information about each of Fortresss businesses: (i) Credit Hedge Funds and Credit Private Equity (PE) Funds (ii) Private Equity Funds and Permanent Capital Vehicles (iii) Liquid Hedge Funds and (iv) Logan Circle. Fortress uses DE as the primary metric to manage its businesses and gauge the Companys performance and it uses DE exclusively to report segment results. All DE figures are presented on a pre-tax basis. Consolidated segment results are non-GAAP information and are not presented as a substitute for Fortresss GAAP results. Fortress urges you to read Non-GAAP Information below. Pre-tax DE was $123 million in the fourth quarter of 2014 up from $121 million in the fourth quarter of 2013. This increase was primarily due to higher incentive income and management fees partially offset by higher segment expenses. Incentive income recorded in the fourth quarter of 2014 totaled $191 million up from $136 million recorded in the fourth quarter of 2013 primarily due to higher incentive income from the Credit PE Funds and Permanent Capital Vehicles partially offset by lower incentive income from the Liquid Hedge Funds Credit Hedge Funds and Private Equity Funds. Management fees were $153 million in the fourth quarter of 2014 up from $143 million in the fourth quarter of 2013 primarily due to higher management fees from the Permanent Capital Vehicles Credit Hedge Funds Liquid Hedge Funds and Logan Circle partially offset by lower management fees from the Private Equity Funds. Additionally Fortress had $999 million in gross undistributed unrecognized incentive income based on investment valuations as of December 31 2014. This includes $929 million from our funds $64 million from options in our publicly traded permanent capital vehicles and $6 million from our private permanent capital vehicle. The Companys segment revenues and distributable earnings will fluctuate materially depending upon the performance of its funds and the realization events within its Private Equity businesses as well as other factors. Accordingly the revenues and distributable earnings in any particular period should not be expected to be indicative of future results. As of December 31 2014 AUM totaled $67.5 billion up from $66.0 billion as of September 30 2014. During the fourth quarter Fortress had a $1.2 billion increase in invested capital recorded $1.0 billion of net client inflows for Logan Circle raised $0.9 billion of capital that was directly added to AUM and had $0.5 billion of market-driven valuation gains. These increases to AUM were partially offset by (i) $1.2 billion of capital distributions to investors (ii) $0.5 billion of fund reset date adjustments (iii) $0.3 billion of Liquid Hedge Fund redemptions and (iv) $0.1 billion of payments to Credit Hedge Fund investors from redeeming capital accounts. As of December 31 2014 the Credit Funds Private Equity Funds and private permanent capital vehicle had $5.4 billion $2.0 billion and $0.4 billion of uncalled capital respectively that will become AUM if deployed/called. Uncalled capital or dry powder capital committed to the funds but not invested and generating management fees includes $2.3 billion that is only available for follow-on investments management fees and other fund expenses. Notably approximately 77% of alternative AUM was in funds with long-term investment structures as of December 31 2014 which provides for a stable predictable base of management fees. Below is a discussion of fourth quarter and full year 2014 segment results and business highlights. The Credit business which includes our Credit PE Funds and Credit Hedge Funds generated pre-tax DE of $81 million in the fourth quarter of 2014 compared to $49 million in the fourth quarter of 2013. The year-over-year increase in DE was primarily driven by higher Credit PE incentive income partially offset by lower Credit Hedge Fund incentive income and higher Credit PE profit-sharing expenses. The Credit PE Funds generated pre-tax DE of $68 million in the quarter up from $13 million in the fourth quarter of 2013 as increased realization activity resulted in $134 million of incentive income recorded in the quarter. Full year 2014 Credit PE incentive income totaled a record $254 million a 112% increase compared to 2013. In addition to this substantial level of incentive income recognized in 2014 positive Credit PE Fund performance in 2014 resulted in a $143 million or 21% year-over-year increase in gross unrecognized Credit PE incentive income which totaled $824 million at year end. The Credit PE Funds raised $1.2 billion of capital in 2014 approximately $900 million of which that was raised in the fourth quarter. Fundraising activity in the quarter included a first close for Fortress Japan Opportunity Fund (FJOF) III of approximately $800 million. FJOF III is a successor fund to FJOF II which closed in December 2012 with approximately $1.6 billion of commitments. Fortress anticipates a total capital raise of approximately $5.0 billion the targeted cap for FCO IV and related accounts. FCO IV is the successor fund to FCO III and its related accounts which closed in September 2011 with approximately $4.5 billion in commitments. The Credit Hedge Funds generated pre-tax DE of $13 million in the quarter down from $36 million in the fourth quarter of 2013 primarily due to lower incentive income. DBSO LP Fortresss flagship credit hedge fund had net returns of 2.1% in the fourth quarter and 9.9% in 2014. Over the past five years through December 31 2014 DBSO LP has generated annualized net returns of 16.4%. The Credit Hedge Funds raised $995 million of capital in 2014 primarily for the DBSO funds nearly 80% more than the amount raised in 2013. The Private Equity business which includes Private Equity Funds and Permanent Capital Vehicles recorded pre-tax DE of $34 million in the fourth quarter of 2014 down from $39 million in the fourth quarter of 2013 primarily due to lower Private Equity Fund investment income and incentive income partially offset by higher Permanent Capital Vehicle incentive income and management fees. The Private Equity funds generated $19 million of pre-tax DE in the quarter down from $29 million in the fourth quarter of 2013 primarily due to lower net investment income and incentive income. Private Equity Fund valuations declined 1.5% in the quarter primarily due to depreciation of Nationstar Mortgage Holdings Inc. (NYSE: NSM) and certain privately held portfolio company investments partially offset by appreciation of Springleaf Holdings Inc. (NYSE: LEAF). The Permanent Capital Vehicles generated $15 million of pre-tax DE in the quarter up from $10 million in the fourth quarter of 2013 primarily due to higher incentive income and increased management fees. The Permanent Capital Vehicles raised $1.1 billion of capital in 2014 of which $0.6 billion was raised for WWTAI and $0.5 billion was raised across Newcastle New Residential and New Media. Subsequent to year end New Media raised an additional $151 million of capital. In November 2014 Newcastle completed the spin-off of all of its senior housing assets into a new publicly traded real estate investment trust New Senior Investment Group Inc. New Senior is externally managed by an affiliate of Fortress and primarily focuses on senior housing related investments. Fortress consolidated New Senior beginning in November 2014. In February 2015 New Residential and HLSS announced a definitive agreement under which New Residential will acquire all of the outstanding shares of HLSS for a total cash purchase price of $18.25 per share or approximately $1.3 billion. The acquisition has been approved by the Board of Directors of each company and is expected to close in the second quarter of 2015 subject to HLSS shareholder approval and other customary closing conditions. The Liquid Hedge Funds recorded pre-tax DE of $10 million in the fourth quarter of 2014 down from $36 million in the fourth quarter of 2013. The year-over-year decline in pre-tax DE was primarily due to lower incentive income partially offset by higher investment income and management fees. Full year 2014 net returns for the Fortress Macro Funds Fortress Asia Macro Funds Fortress Partners Funds and Fortress Convex Asia Funds were (1.6)% (1.2)% (0.1)% and (4.9)% respectively. Net returns year-to-date through February 20 2015 for the Fortress Macro Funds Fortress Convex Asia Funds and Fortress Centaurus Global Funds were (5.8)% 0.3% and 3.0% respectively.* Liquid Hedge Funds ended the quarter with $8.1 billion of AUM up 10% from the fourth quarter of 2013 primarily due to $2.8 billion of capital raised in the last twelve months partially offset by $1.8 billion of redemptions. As of December 31 2014 there were $926 million of outstanding Liquid Hedge Fund redemption notices to be paid primarily in the first quarter of 2015 including $388 million related to the Fortress Macro Funds $318 million related to the Fortress Partners Funds and $189 million related to the Fortress Asia Macro Funds. In January 2015 we completed the previously announced transition of management of the Fortress Asia Macro Funds to Graticule Asset Management Asia L.P. ("Graticule Asset Management") a new autonomous asset management business on Fortresss affiliated manager platform in which Fortress has a non-controlling interest. Fortress also receives additional fees for providing infrastructure services (technology back office and other services) to Graticule Asset Management. As of December 31 2014 the Fortress Asia Macro Funds and related managed accounts had $3.5 billion of AUM. Logan Circle our traditional asset management business recorded breakeven pre-tax DE in the quarter compared to a pre-tax DE loss of $3 million in the fourth quarter of 2013 primarily due to higher management fees and lower operating expenses. Logan Circle ended the year with $32.3 billion in AUM a 4% increase compared to the prior quarter and a 27% increase compared to the previous year. The year-over-year increase in AUM was primarily due to net client inflows of $5.4 billion and market-driven valuation gains of $1.5 billion. Since Fortresss acquisition of Logan Circle in April 2010 AUM has grown at a CAGR of 25%. Over a five year period through December 31 2014 14 of Logan Circles 16 fixed income strategies outperformed their respective benchmarks. Since inception 15 of 16 Logan Circle fixed income strategies have outperformed their respective benchmarks and seven were ranked in the top quartile of performance for their competitor universe. Subsequent to year end we announced the launch of the Logan Circle Partners Funds a family of mutual funds that will broaden investor access to investment strategies previously offered exclusively to Logan Circles institutional clients. As of December 31 2014 Fortress (excluding Non-Investment Manager and Investment Company) had cash and cash equivalents of $391 million and debt obligations of $75 million. As of December 31 2014 Fortress had $1.2 billion of investments in Fortress funds and options in Permanent Capital Vehicles. As of December 31 2014 Fortress had a total of $147 million of outstanding commitments to its funds. In addition the NAV of Fortresss investments in its own funds exceeded its segment cost basis by $554 million at year-end representing net unrealized gains that have not yet been recognized for segment reporting purposes. Fortresss Board of Directors declared a fourth quarter 2014 cash dividend of $0.38 per dividend paying share comprised of a base quarterly cash dividend of $0.08 per dividend paying share and a special cash dividend of $0.30 per dividend paying share. The dividend is payable on March 17 2015 to Class A shareholders of record as of the close of business on March 12 2015. The declaration and payment of any dividends are at the sole discretion of the Board of Directors which may decide to change its dividend policy at any time. Please see below for information on the U.S. federal income tax implications of the dividend. DE is a primary metric used by management to measure Fortresss operating performance.Consistent with GAAP DE is the sole measure that management uses to manage and thus report on Fortresss segments namely: Private Equity Permanent Capital Vehicles Credit Hedge Funds Credit PE Funds Liquid Hedge Funds and Logan Circle.DE differs from GAAP net income in a number of material ways. For a detailed description of the calculation of pre-tax DE and fund management DE see Exhibit 3 to this release and note 11 to the financial statements included in the Companys most recent annual report on Form 10-K or most recent quarterly report on Form 10-Q. Fortress aggregates its segment results to report consolidated segment results as shown in the table under Summary Financial Results and in the Total column of the table under Consolidated Segment Results (Non-GAAP). The consolidated segment results are non-GAAP financial information. Management believes that consolidated segment results provide a meaningful basis for comparison among present and future periods. However consolidated segment results should not be considered a substitute for Fortresss consolidated GAAP results. The exhibits to this release contain reconciliations of the components of Fortresss consolidated segment results to the comparable GAAP measures and Fortress urges you to review these exhibits. Fortress also uses weighted average dividend paying shares and units outstanding (used to calculate pre-tax DE per dividend paying share) and net cash and investments. The exhibits to this release contain reconciliations of these measures to the comparable GAAP measures and Fortress urges you to review these exhibits. Management will host a conference call today Thursday February 26th at 10:00 A.M. Eastern Time. A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Fourth Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Thursday March 5 2015 by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 82551687. Fortress Investment Group LLC (NYSE: FIG) is a leading highly diversified global investment management firm with $67.5 billion in assets under management as of December 31 2014. Fortress applies its deep experience and specialized expertise across a range of investment strategies - private equity credit liquid hedge funds and traditional asset management - on behalf of over 1600 institutional clients and private investors worldwide. For more information regarding Fortress Investment Group LLC or to be added to its e-mail distribution list please visit www.fortress.com. Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding Fortresss sources of management fees incentive income and investment income (loss) estimated fund performance and the amount and source of expected capital commitments.These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements and any such differences could cause the Companys actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysAnnual Report on Form 10-K which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. This announcement is intended to be a qualified notice as provided in the Internal Revenue Code (the Code) and the Regulations thereunder. For U.S. federal income tax purposes the dividend declared in February 2015 will be treated as a partnership distribution. The per share distribution components are as follows: Distributable earnings is Fortresss supplemental measure of operating performance used by management in analyzing segment and overall results. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income in accordance with GAAP and it is not necessarily indicative of liquidity or cash available to fund the Companys operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 11 to the financial statements included in the Companys Annual Report on Form 10-K for the year ended December 31 2014. Growing distributable earnings is a key component to the Companys business strategy and distributable earnings is the supplemental measure used by management to evaluate the economic profitability of each of the Companys businesses and total operations. Therefore Fortress believes that it provides useful information to investors in evaluating its operating performance. Fortresss definition of distributable earnings is not based on any definition contained in its amended and restated operating agreement. Fund management DE is equal to pre-tax distributable earnings excluding our direct investment-related results. It is comprised of Segment Revenues net of Segment Expenses and Principal Performance Payments. Fund management DE and its components are used by management to analyze and measure the performance of our investment management business on a stand-alone basis. Fortress defines segment operating margin to be equal to fund management DE divided by segment revenues. The Company believes that it is useful to provide investors with the opportunity to review our investment management business using the same metrics. Fund management DE and its components are subject to the same limitations as pre-tax distributable earnings as described above. Dividend paying shares and units represents the number of shares and units outstanding at the end of the period which were entitled to receive dividends or related distributions. The Company believes it is useful for investors in computing the aggregate amount of cash required to make a current per share distribution of a given amount per share. It excludes certain potentially dilutive equity instruments primarily non-dividend paying restricted Class A share units and therefore is limited in its usefulness in computing per share amounts. Accordingly dividend paying shares and units should be considered only as a supplement and not an alternative to GAAP basic and diluted shares outstanding. The Companys calculation of dividend paying shares and units may be different from the calculation used by other companies and therefore comparability may be limited. Net cash and investments represents cash and cash equivalents plus investments less debt outstanding. The Company believes that net cash and investments is a useful supplemental measure because it provides investors with information regarding the Companys net investment assets. Net cash and investments excludes certain assets (investments in options due from affiliates deferred tax asset other assets) and liabilities (due to affiliates accrued compensation and benefits deferred incentive income and other liabilities) its utility as a measure of financial position is limited. Accordingly net cash and investments should be considered only as a supplement and not an alternative to GAAP book value as a measure of the Companys financial position. The Companys calculation of net cash and investments may be different from the calculation used by other companies and therefore comparability may be limited.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:02 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress to Release First Quarter Results and Announce Dividend on May 7 2015'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=29099897&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) plans to announce its financial results and dividend for the first quarter 2015 prior to the opening of the New York Stock Exchange on Thursday May 7 2015. A copy of the press release will be posted to the Investor Relations section of Fortresss website www.fortress.com. In addition management will host a conference call on May 7 2015 at 9:00 A.M. Eastern Time. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress First Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available after the live call by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 27940522. Fortress Investment Group LLC is a leading highly diversified global investment firm with $67.5 billion in assets under management as of December 31 2014. Founded in 1998 Fortress manages assets on behalf of over 1600 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE:FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:03 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Investor Relations'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/marketing.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Michael Considine joined MidOcean Partners in 2008 and is responsible for Marketing and Investor Relations for MidOcean as well as for business development. Prior to joining MidOcean he was a Managing Director at StoneWater Capital where he headed Marketing and Investor Relations and was involved in business development and expanding distribution channels for the firm. Earlier Mr. Considine oversaw investment banking at Hamilton Securities Group a boutique firm and previously he led the investment banking group at Auerbach Pollak & Richardson. He holds a BA in History magna cum laude from Columbia University and an MBA in Finance from the Columbia University Graduate School of Business.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:16 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports First Quarter 2015 Results and Announces Dividend of $0.08 per Share'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=29441118&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) (Fortress or the Company) today reported its first quarter 2015 financial results. We had a very active start to the year highlighted by the launch of our largest Credit PE fund in our history our largest PE fund crossing its preferred return threshold and a number of important strategic transactions across our permanent capital vehicles said Fortress Chief Executive Officer Randy Nardone. With $1.2 billion of gross embedded incentive income not yet recognized in earnings and nearly $11 billion of dry powder for us to put to work we see great prospects for growth and value creation in the quarters and years ahead. Fortresss business model is highly diversified and management believes that this positions the Company to capitalize on opportunities for investing capital formation and harvesting profits that can occur at different points in any cycle for our individual businesses. Fortresss business model generates stable and predictable management fees which is a function of the majority of Fortresss alternative AUM residing in long-term investment structures. Fortresss alternative investment businesses also generate variable incentive income based on performance and this incentive income can contribute meaningfully to financial results. Balance sheet investments represent a third component of Fortresss business model and the Company has built substantial value in these investments which are made in Fortress funds alongside the funds limited partners. The table below summarizes Fortresss operating results for the quarter ended March 31 2015. The consolidated GAAP statement of operations and balance sheet are presented on pages 12-13 of this press release. Fortress recorded GAAP net income of $87 million or $0.15 per diluted Class A share for the first quarter of 2015 compared to GAAP net income of $9 million or $0.01 per diluted Class A share for the first quarter of 2014. Our diluted earnings per share for all periods presented includes the income tax effects to net income (loss) attributable to Class A shareholders from the assumed conversion of Fortress Operating Group units and fully vested restricted partnership units to Class A shares. The year-over-year increase in Fortresss first quarter 2015 GAAP net income was primarily driven by a $198 million increase in other income partially offset by a $98 million increase in expenses. The increase in other income and expenses was primarily related to the transfer of our interest in Graticule Asset Management (Graticule). In January 2015 the Fortress Asia Macro Funds and related managed accounts transitioned to Graticule an autonomous asset management business on Fortresss affiliated manager platform (Affiliated Managers). Fortress recorded the results of this transaction at fair value which for the first quarter of 2015 resulted in a non-cash gain of $134 million recorded in other income a non-cash expense of $101 million recorded in expenses related to the portion of Fortresss interest that was transferred to a former senior employee and $33 million from our resulting retained interest recorded as an equity method investment. Excluding the effects of the transfer described above GAAP net income increased year-over-year primarily due to increases in the fair value of options and common stock held in our publicly traded permanent capital vehicles and increases in earnings from our equity method investees primarily with respect to our investments in Fund V and Affiliated Managers partially offset by lower incentive income generated by our funds. In February 2015 the Financial Accounting Standards Board issued amendments to its accounting standards for consolidation of certain entities and Fortress elected to early adopt such guidance for all periods presented in its consolidated financial statements. As a result of such adoption certain entities that were previously consolidated by Fortress are no longer consolidated subsidiaries including New Media Investment Group Inc. (NYSE: NEWM) and New Senior Investment Group Inc. (NYSE: SNR) which were both initially consolidated in 2014. This section provides information about each of Fortresss businesses: (i) Credit Hedge Funds and Credit PE Funds (ii) Private Equity Funds and Permanent Capital Vehicles (iii) Liquid Hedge Funds and (iv) Logan Circle. Fortress uses DE as the primary metric to manage its businesses and gauge the Companys performance and it uses DE exclusively to report segment results. All DE figures are presented on a pre-tax basis. Consolidated segment results are non-GAAP information and are not presented as a substitute for Fortresss GAAP results. Fortress urges you to read Non-GAAP Information below. Pre-tax DE was $55 million in the first quarter of 2015 down from $97 million in the first quarter of 2014. This decrease was primarily due to lower incentive income and management fees partially offset by lower profit sharing compensation expenses. Management fees were $139 million in the first quarter of 2015 down from $147 million in the first quarter of 2014 primarily due to lower management fees from the Liquid Hedge Funds and Private Equity Funds partially offset by higher management fees from the Permanent Capital Vehicles Credit Hedge Funds Credit PE Funds and Logan Circle. Incentive income recorded in the first quarter of 2015 totaled $51 million down from $104 million recorded in the first quarter of 2014 primarily due to lower incentive income from the Credit PE Funds and Credit Hedge Funds. Earnings from Affiliated Managers totaled $9 million in the first quarter of 2015 related to our interests in Graticule. Additionally Fortress had $1.2 billion in gross undistributed unrecognized incentive income based on investment valuations as of March 31 2015. This includes $1.1 billion from our funds $100 million from options in our publicly traded permanent capital vehicles and $8 million from our private permanent capital vehicle. The Companys segment revenues and distributable earnings will fluctuate materially depending upon the performance of its funds and the realization events within its Private Equity businesses as well as other factors. Accordingly the revenues and distributable earnings in any particular period should not be expected to be indicative of future results. As of March 31 2015 AUM totaled $69.9 billion up from $67.5 billion as of December 31 2014. During the quarter Fortress had $1.4 billion of market-driven valuation gains a $0.7 billion increase in invested capital recorded $0.6 billion of net client inflows for Logan Circle raised $0.6 billion of capital and equity that was directly added to AUM and had a net $0.5 billion increase in AUM of Affiliated Managers. These increases to AUM were partially offset by (i) $0.7 billion of Liquid Hedge Fund redemptions (ii) $0.5 billion of capital distributions to investors (iii) $0.1 billion of crystallized incentive income and (iv) $0.1 billion of Credit Hedge Fund redemptions and payments to Credit Hedge Fund investors from redeeming capital accounts. As of March 31 2015 the Credit Funds Private Equity Funds and private permanent capital vehicle had $8.6 billion $1.8 billion and $0.4 billion of uncalled capital respectively that will become AUM if deployed/called. Uncalled capital or dry powder capital committed to the funds but not invested and generating management fees includes $3.1 billion that is only available for follow-on investments management fees and other fund expenses. Notably approximately 78% of alternative AUM was in funds with long-term investment structures as of March 31 2015 which provides for a stable predictable base of management fees. Below is a discussion of first quarter 2015 segment results and business highlights. The Credit business which includes our Credit PE Funds and Credit Hedge Funds generated pre-tax DE of $29 million in the first quarter of 2015 compared to $56 million in the first quarter of 2014. The year-over-year decrease in DE was primarily driven by lower incentive income partially offset by higher management fees and lower profit sharing expenses. The Credit Hedge Funds generated pre-tax DE of $22 million in the quarter essentially flat compared to the first quarter of 2014. DBSO LP Fortresss flagship credit hedge fund had net returns of 2.2% in the first quarter following net returns of 10.0% in 2014. The Credit Hedge Funds raised $175 million of capital in the quarter primarily for the DBSO funds. The Credit PE Funds generated pre-tax DE of $7 million in the quarter down from $33 million in the first quarter of 2014 primarily due to lower realization activity and incentive income in the quarter. Gross unrecognized Credit PE incentive income totaled $848 million at quarter end up $126 million from March 31 2014 despite $212 million of gross recognized Credit PE incentive income over the last twelve months. The Credit PE Funds raised $4.9 billion of capital in the quarter including $4.7 billion for FCO IV and $0.2 billion for Fortress Real Estate Opportunities Fund II. Fortress anticipates a total capital raise of approximately $5.0 billion the targeted cap for FCO IV and related accounts. FCO IV is the successor fund to FCO III and its related accounts which closed in September 2011 with approximately $4.5 billion in commitments. The Private Equity business recorded pre-tax DE of $19 million in the first quarter of 2015 including $15 million for the Private Equity Funds and $4 million for the Permanent Capital Vehicles. Pre-tax DE declined from $34 million in the first quarter of 2014 primarily due to lower Private Equity Fund management fees and investment income partially offset by higher Permanent Capital Vehicle management fees. Private Equity Fund valuations increased 4.8% in the quarter primarily due to appreciation of Springleaf Holdings Inc. (NYSE: LEAF) partially offset by depreciation of Nationstar Mortgage Holdings Inc. (NYSE: NSM) and certain privately held portfolio company investments. Fund V appreciated 22.5% during the first quarter of 2015 primarily due to a 43.1% increase in Springleafs share price. This NAV appreciation combined with approximately $80 million in capital distributions to LPs in the quarter resulted in Fund V crossing its preferred return threshold and beginning its 60/40 catch-up accrual phase of gross incentive income for Fortress as the GP. As of March 31 2015 Fund V had $188 million of gross undistributed incentive income not yet recognized in DE. The Permanent Capital Vehicles had $108 million of gross undistributed unrecognized incentive income at quarter end including $100 million related to in-the-money options we hold in our publicly traded permanent capital vehicles. The Permanent Capital Vehicles have raised $1.4 billion of capital year-to-date through April 2015 including $150 million raised by New Media in January and approximately $340 million and $860 million raised in April by Eurocastle Investment Limited and New Residential respectively. A portion of the capital raised by New Residential was used to help fund the acquisition of HLSS in April 2015. The Liquid Hedge Funds recorded pre-tax DE of $9 million in the first quarter of 2015 flat compared to the first quarter of 2014 as lower management fees were offset by earnings from Affiliated Managers and higher investment income. Earnings from Affiliated Managers totaled $9 million in the first quarter of 2015 related to our interests in Graticule. First quarter 2015 net returns for the Fortress Macro Funds Fortress Convex Asia Funds Fortress Centaurus Global Funds and Fortress Partners Funds were (4.7)% (0.6)% 3.9% and 1.2% respectively. Net returns year-to-date through April 30 2015 for the Fortress Macro Funds Fortress Convex Asia Funds and Fortress Centaurus Global Funds were (8.0)% (1.1)% and 4.2% respectively.* In January 2015 we completed the previously announced transition of management of the Fortress Asia Macro Funds and related managed accounts to Graticule a new autonomous asset management business on Fortresss affiliated manager platform. Fortress has retained a perpetual minority interest in Graticule amounting to 30% of earnings during 2015 and receives additional fees for providing various infrastructure services. Liquid Hedge Funds ended the quarter with $7.8 billion of AUM including $4.0 billion related to Affiliated Managers. Liquid Hedge Fund AUM declined slightly during the quarter primarily due to $0.7 billion of redemptions partially offset by a net $0.5 billion increase in the AUM of Affiliated Managers and $0.1 billion of capital raised. Subsequent to quarter end the Liquid Hedge Funds raised approximately $0.1 billion of additional capital which will be added to AUM in the second quarter of 2015. At quarter end there were $396 million of Liquid Hedge Fund redemption notices outstanding including $385 million to be paid primarily in the second quarter of 2015. Logan Circle our traditional asset management business recorded a pre-tax DE loss of $1 million in the quarter compared to a pre-tax DE loss of $2 million in the first quarter of 2014 primarily due to higher management fees partially offset by higher operating expenses. Logan Circle ended the quarter with $33.4 billion in AUM a 3% increase compared to the prior quarter and a 26% increase compared to the previous year. The year-over-year increase in AUM was primarily due to net client inflows of $5.5 billion and market-driven valuation gains of $1.3 billion. Over a five-year period through March 31 2015 14 of Logan Circles 16 fixed income strategies outperformed their respective benchmarks. Since inception 15 of 16 Logan Circle fixed income strategies have outperformed their respective benchmarks and six were ranked in the top quartile of performance for their competitor universe. As of March 31 2015 Fortress had cash and cash equivalents of $145 million and debt obligations of $75 million. As of March 31 2015 Fortress had $1.3 billion of investments in Fortress funds and options in publicly traded permanent capital vehicles. As of March 31 2015 Fortress had a total of $173 million of outstanding commitments to its funds. In addition the NAV of Fortresss investments in its own funds exceeded its segment cost basis by $576 million at quarter end representing net unrealized gains that have not yet been recognized for segment reporting purposes. Fortresss Board of Directors declared a first quarter 2015 cash dividend of $0.08 per dividend paying share. The dividend is payable on May 21 2015 to Class A shareholders of record as of the close of business on May 18 2015. The declaration and payment of any dividends are at the sole discretion of the Board of Directors which may decide to change its dividend policy at any time. Please see below for information on the U.S. federal income tax implications of the dividend. DE is a primary metric used by management to measure Fortresss operating performance.Consistent with GAAP DE is the sole measure that management uses to manage and thus report on Fortresss segments namely: Private Equity Permanent Capital Vehicles Credit Hedge Funds Credit PE Funds Liquid Hedge Funds and Logan Circle.DE differs from GAAP net income in a number of material ways. For a detailed description of the calculation of pre-tax DE and fund management DE see Exhibit 3 to this release and note 10 to the financial statements included in the Companys most recent quarterly report on Form 10-Q. Fortress aggregates its segment results to report consolidated segment results as shown in the table under Summary Financial Results and in the Total column of the table under Consolidated Segment Results (Non-GAAP). The consolidated segment results are non-GAAP financial information. Management believes that consolidated segment results provide a meaningful basis for comparison among present and future periods. However consolidated segment results should not be considered a substitute for Fortresss consolidated GAAP results. The exhibits to this release contain reconciliations of the components of Fortresss consolidated segment results to the comparable GAAP measures and Fortress urges you to review these exhibits. Fortress also uses weighted average dividend paying shares and units outstanding (used to calculate pre-tax DE per dividend paying share) and net cash and investments. The exhibits to this release contain reconciliations of these measures to the comparable GAAP measures and Fortress urges you to review these exhibits. Management will host a conference call today Thursday May 7th at 9:00 A.M. Eastern Time. A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress First Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 27940522. Fortress Investment Group LLC (NYSE: FIG) is a leading highly diversified global investment management firm with $69.9 billion in assets under management as of March 31 2015. Fortress applies its deep experience and specialized expertise across a range of investment strategies - private equity credit liquid hedge funds and traditional asset management - on behalf of approximately 1700 institutional clients and private investors worldwide. For more information regarding Fortress Investment Group LLC or to be added to its e-mail distribution list please visit www.fortress.com. Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding Fortresss sources of management fees incentive income and investment income (loss) estimated fund performance and the amount and source of expected capital commitments.These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements and any such differences could cause the Companys actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysQuarterly Report on Form 10-Q which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. This announcement is intended to be a qualified notice as provided in the Internal Revenue Code (the Code) and the Regulations thereunder. For U.S. federal income tax purposes the dividend declared in May 2015 will be treated as a partnership distribution. The per share distribution components are as follows: Distributable earnings is Fortresss supplemental measure of operating performance used by management in analyzing segment and overall results. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income in accordance with GAAP and it is not necessarily indicative of liquidity or cash available to fund the Companys operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 10 to the financial statements included in the Companys Quarterly Report on Form 10-Q for the quarter ended March 31 2015. Growing distributable earnings is a key component to the Companys business strategy and distributable earnings is the supplemental measure used by management to evaluate the economic profitability of each of the Companys businesses and total operations. Therefore Fortress believes that it provides useful information to investors in evaluating its operating performance. Fortresss definition of distributable earnings is not based on any definition contained in its amended and restated operating agreement. Fund management DE is equal to pre-tax distributable earnings excluding our direct investment-related results. It is comprised of Pre-tax Distributable Earnings net of Investment Loss (Income) and Interest Expense. Fund management DE and its components are used by management to analyze and measure the performance of our investment management business on a stand-alone basis. Fortress defines segment operating margin to be equal to fund management DE divided by segment revenues. The Company believes that it is useful to provide investors with the opportunity to review our investment management business using the same metrics. Fund management DE and its components are subject to the same limitations as pre-tax distributable earnings as described above. Dividend paying shares and units represents the number of shares and units outstanding at the end of the period which were entitled to receive dividends or related distributions. The Company believes it is useful for investors in computing the aggregate amount of cash required to make a current per share distribution of a given amount per share. It excludes certain potentially dilutive equity instruments primarily non-dividend paying restricted Class A share units and therefore is limited in its usefulness in computing per share amounts. Accordingly dividend paying shares and units should be considered only as a supplement and not an alternative to GAAP basic and diluted shares outstanding. The Companys calculation of dividend paying shares and units may be different from the calculation used by other companies and therefore comparability may be limited. Net cash and investments represents cash and cash equivalents plus investments less debt outstanding. The Company believes that net cash and investments is a useful supplemental measure because it provides investors with information regarding the Companys net investment assets. Net cash and investments excludes certain assets (investments in options due from affiliates deferred tax asset other assets) and liabilities (due to affiliates accrued compensation and benefits deferred incentive income and other liabilities) and its utility as a measure of financial position is limited. Accordingly net cash and investments should be considered only as a supplement and not an alternative to GAAP book value as a measure of the Companys financial position. The Companys calculation of net cash and investments may be different from the calculation used by other companies and therefore comparability may be limited.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:17 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Contact'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/contact.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:17 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Compliance'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/compliance.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Candice Richards joined MidOcean Partners as a Compliance Officer in 2013. Previously Mrs. Richards was a Compliance Officer at Gresham Investment Management LLC a financial investment advisory firm. Prior to this Mrs. Richards was a Compliance Officer at Oppenheimer & Co. Inc. Mrs. Richards received her JD from University of Miami School of Law and her BA from New York University. She is admitted to the New York State and State of Connecticut Bar.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:18 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Finance and Operations'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/finance.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['King C. Chong is the Director of Tax at MidOcean Partners and is responsible for the tax compliance planning and reporting functions for MidOceans private equity and credit hedge funds. Prior to joining MidOcean Mr. Chong was a Managing Director at Fund Tax Services where he was involved in advising alternative investment funds on tax planning and compliance. Previously Mr. Chong was a Senior Vice President at The Blackstone Group responsible for the tax compliance planning and reporting functions for the private equity energy and the tactical opportunities funds. Mr. Chong was also a Director of Tax at Mourant Director of Tax at Bisys Vice President at Goldman Sachs and a Senior Tax Consultant at Deloitte. Mr. Chong received his BS in Accounting from New York University and his MBA from St. Johns University. Mr. Chong is a Certified Public Accountant in New York.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:19 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Investment Professionals'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/investment.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Mr. Galanek is a Principal and senior trader responsible for identifying and executing portfolio investments. Mr. Galanek has over 15 years of experience in the fixed income markets as a research analyst and trader; first as part of an Institutional Investor ranked high yield technology media and telecom research team at Bear Stearns and Merrill Lynch and later on the buy side as a PM and trader at Tricadia Capital (credit multi-strategy firm) and Pinebank Asset Management (long/short credit manager). Prior to joining MidOcean Partners Mr. Galanek was the head trader and co-portfolio manager at Pinebank Asset Management a long/short credit fund. From 2010-2013 he was responsible for levered loan/high yield/distressed credit trading idea generation risk management and was an Investment Committee member. Prior to working at Pinebank Mr. Galanek was the second portfolio manager/trader hired at Tricadia with the mandate of trading the whole capital structure from levered loans down to special situation equities. Prior to Tricadia Ed was a senior credit analyst at Libertas Partners a broker-dealer and hedge fund. Mr. Galanek holds a BA in Economics from Columbia University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:20 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Career Opportunities'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/careers.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['For employment consideration please e-mail to hr@midoceanpartners.com']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:20 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Investment Approach'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/approach.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['MidOcean Credit Partners objective is to deliver superior returns through its complementary investment strategies focused on the credit markets. Investments are thoroughly researched and driven by fundamental analysis combined with identification of relative value event-driven and arbitrage opportunities. The Firm utilizes its extensive research capital markets and trading expertise as well as the overall MidOcean knowledge of companies strong relationships and the insights of leading industry executives who are affiliated with MidOcean to exploit these investment opportunities.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:21 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Credit Partners :: Trade Support and Risk Anlaytics'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/partners/middleOffice.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Mr. Sullivan joined the Firm in February of 2013 as a consultant and in February 2014 became the Chief Administrative Officer and a Managing Director at MidOcean Credit Partners. He is also a member of the Pricing Committee. Prior to joining MidOcean Credit Partners Mr. Sullivan was a Managing Director with Deutsche Bank and a predecessor bank Bankers Trust from 1980 until November of 2012. Mr. Sullivan held various positions of increasing responsibility over his thirty-two years at Deutsche Bank and Bankers Trust including Global Head for Loan Sales Trading and Capital Markets Head of Leveraged Finance Asia and last serving as Group Head for Asset Based Lending. He was a member of the Capital Commitments Committee from 2002 to 2012 and a member of the Equity Investment Committee from 2008 to 2012. Mr. Sullivan has an MBA in Finance from St. Johns University and a BS in Accounting from Fordham University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:21 [scrapy] INFO: Crawled 1551 pages (at 20 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:52:22 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001175236.PDF?Y=&O=PDF&D=&fid=1001175236&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073748403&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:22 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001179955.PDF?Y=&O=PDF&D=&fid=1001179955&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073748403&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:23 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001177693.PDF?Y=&O=PDF&D=&fid=1001177693&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073748403&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:24 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001184226.PDF?Y=&O=PDF&D=&fid=1001184226&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073748403&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:24 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001171635.PDF?Y=&O=PDF&D=&fid=1001171635&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:24 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001171634.PDF?Y=&O=PDF&D=&fid=1001171634&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:25 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Our Team - Marketing Team'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/team/marketing.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Michael Considine joined MidOcean Partners in 2008 and is responsible for Marketing and Investor Relations for MidOcean as well as for business development. Prior to joining MidOcean he was a Managing Director at StoneWater Capital where he headed Marketing and Investor Relations and was involved in business development and expanding distribution channels for the firm. Earlier Mr. Considine oversaw investment banking at Hamilton Securities Group a boutique firm and previously he led the investment banking group at Auerbach Pollak & Richardson. He holds a BA in History magna cum laude from Columbia University and an MBA in Finance from the Columbia University Graduate School of Business.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:26 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Our Team - Executive Board'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/team/board.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Jodi Kahn is the Executive Vice President of iVillage the essential digital brand for women which serves as the digital anchor of NBC Universal Women & Lifestyle Entertainment Networks Group. Under her leadership iVillage began a site wide redesign in September 2009 culminating in September 2010 with a year-long relaunch of all eight verticals including Entertainment Pregnancy & Parenting Food Beauty & Style Health Home & Garden just to name a few. As the only digital womens brand nestled within one of the largest media and entertainment companies Kahn is responsible for the strategic business operations of the company while also overseeing iVillage UK Astrology.com and GardenWeb. Kahn reports to Lauren Zalaznick President NBC Universal Women and Lifestyle Entertainment Networks. Since her ascension in Fall 2008 Kahn has been instrumental in the overall success of the brand that now serves over 30 million unique visitors according to comScore which has also experienced tremendous growth across many facets of business including trafic advertising sales and editorial. Kahn is a true digital business strategist with a strong foundation building online consumer products. Prior to iViilage Kahn was President Global Digital Media for Readers Digest Association Inc. There she is credited with building digital businesses across the company overseeing content product development and monetization for Readers Digest Associations family of 14 online brands including allrecipes.com and rachaelraymag.com. Kahn developed multiplatform strategies oversaw key site launches led global digital expansion and oversaw the creation of an extensive digital content repository to grow audiences revenue and profits. Before joining RDA Kahn led Time for Kids the in-school magazine from Time Inc. that reaches 4.4 million students weekly. There she developed new business sources which contributed lo double-digit profit growth and subscriber increases and reshaped the ad sponsorship model resulting in record growth for the brand. Previous to her tenure at Time for Kids Kahn held a VP role at Time Inc. Interactive where she led the integration of Time Inc. web properties and AOL. In addition Kahn has held Marketing positions at Time and Sonv/Columbia House handling both strategy and execution. She began her career as a media planner at Wunderman Worldwide part of Young & Rubicam. Kahn resides in New York City with her husband and two children.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:27 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Our Team - Compliance Team'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/team/compliance.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Candice Richards joined MidOcean Partners as a Compliance Officer in 2013. Previously Mrs. Richards was a Compliance Officer at Gresham Investment Management LLC a financial investment advisory firm. Prior to this Mrs. Richards was a Compliance Officer at Oppenheimer & Co. Inc. Mrs. Richards received her JD from University of Miami School of Law and her BA from New York University. She is admitted to the New York State and State of Connecticut Bar.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:27 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Our Team - Management Affiliates'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/team/affiliates.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['Mr. Caulk is a seasoned CEO with extensive experience building businesses and integrating acquisitions in a Private Equity environment. From 1999 through 2005 Bob was Chairman and CEO of United Industries a $1+ Billion manufacturer and marketer of consumer lawn garden and pet products. Its brands included Cutter Spectracide Vigoro Tetra and others. During his tenure the company grew in revenue from $270 Million to over $1 Billion through an aggressive new product development program and a series of successful industry-consolidating acquisitions. United Industries was owned by private-equity sponsor Thomas H. Lee and was sold in 2005 to Rayovac Corporation. Prior to joining United Bob was President of Clopay Building Products the market-leading manufacturer of residential and commercial garage doors. Immediately before Clopay Bob was the North American President of Johnson Outdoors the parent company for a number of market-leading recreational products businesses including Scubapro Old Town Mitchell Minn-Kota and others. Earlier in his career he held finance marketing and corporate development positions at S.C.Johnson (consumer products) and DuPont (chemicals). Bob currently serves on the Board of two MidOcean portfolio companies. He is Executive Chairman of Hunter Fan Inc. the market leader in branded ceiling fans and on the board of Waterpik Inc. the largest manufacturer of water flossing devices and showerheads. In addition to the MidOcean portfolio companies Bob is also a member of the Board of Directors of Menard Inc. a multi-billion privately owned retailer; Maritz Inc. a $1 billion privately owned travel loyalty and marketing services company and several other corporate and non-profit boards. He previously wasChairman of the Board of Directors of Bushnell Outdoor Products a MidOcean Partners portfolio company and a global marketer of optics and other consumer products for outdoor recreational use. Key brands are Bushnell Bolle Serengeti and Hoppes. Bushnell was sold in November 2013. Bob has a B.A. in History from the University of Delaware and an MBA from the Harvard Business School. He and his family reside in St. Louis.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:28 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: News - Press'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/news.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:29 [scrapy] ERROR: Error processing {'pagetitle': [u'Shareholder Information'],
 'pageurl': ['http://shareholders.fortress.com/CorporateProfile.aspx?iid=4147324&print=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:29 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045759.PDF?Y=&O=PDF&D=&fid=1500045759&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:29 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045758.PDF?Y=&O=PDF&D=&fid=1500045758&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:29 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045757.PDF?Y=&O=PDF&D=&fid=1500045757&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:30 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045753.PDF?Y=&O=PDF&D=&fid=1500045753&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:30 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045754.PDF?Y=&O=PDF&D=&fid=1500045754&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:30 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045756.PDF?Y=&O=PDF&D=&fid=1500045756&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:31 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=3'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT - January 5 2011 - Brynwood Partners V L.P. announced today that it has sold its investment in Metro Door Inc. to an undisclosed NYSE listed provider of specialized business services. The terms and conditions of the transaction were not disclosed. Metro Door headquartered in Great River NY is a leading provider of facility maintenance services to national retail and restaurant accounts in addition to designing and manufacturing retail security enclosures. Brynwood V acquired Metro Door in 2004 and over the past six years the company has experienced significant growth through new customer acquisitions and the introduction of facility services. Metro Door currently employs 130 people and its management team will remain with the company after the closing. In announcing the sale of Metro Door Robert Sperry Chairman Metro Door and Managing Partner Brynwood V said "We are grateful to Metro Doors management team and all its hard working employees who despite a challenging retail environment over the past few years have successfully grown the business by focusing on quality execution new services and productivity improvements. We are pleased to place Metro Door into the hands of this highly regarded strategic buyer and wish them success with this fine company." Robert W. Baird & Company served as the investment banking advisor to Metro Door. About Brynwood Partners L.P. Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity (c) service businesses and (d) specialty retailing. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.BrynwoodPartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:31 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=4'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT - January 4 2011 - Brynwood Partners VI L.P. announced today that its newly-formed portfolio company High Ridge Brands Co. has acquired the Zest brand from The Procter & Gamble Company. High Ridge Brands purchased the rights to the Zest brand in the United States Canada and the Caribbean markets. The company will be based in Stamford CT and Brynwood VI has majority ownership of the company. The transaction closed today and the terms and conditions of the transaction were not disclosed. Zest is an iconic brand that was launched in 1952 by Procter & Gamble and is well known for its bar soap and body wash products. The Zest brand is a household name and is recognized for its unique fragrances great lather and its famous "Zestfully Clean" advertising campaign. Zests products are widely distributed in the acquired markets and remain very popular among consumers. "We are delighted to announce the acquisition of the Zest brand" said Dario Margve non-executive Chairman High Ridge Brands and Managing Partner Brynwood VI. "We believe that this will be a great new platform investment for our firm and look forward to not only bringing renewed attention and energy to the great Zest brand but to also adding complementary brands to the company in the future." Brynwood Partners L.P. has a strong track record in the personal care space. Margve was CEO of J.B. Williams Company Inc. a Brynwood Partners II L.P. portfolio company from 1992 to 2002. J.B. Williams was known for its brands Aqua Velva Cepacol Brylcreem and Lectric Shave. J.B. Williams was divested to Combe Incorporated generating a significant return on invested capital for Brynwood II. "We are excited to be investing in the personal care space again as this has proven to be a very good area for our firm" said Hendrik J. Hartong III Senior Managing Partner Brynwood Partners. "We are pleased to have Dario Margve overseeing this investment for Brynwood VI and have high confidence that he and the management team will put the Zest brand on a growth track again." About Brynwood Partners L.P. Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity (c) service businesses and (d) specialty retailing. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:31 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=5'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['STAMFORD CT - January 3 2011 - DeMets Candy Company announced today that it has completed the acquisition of the TrueNorth nut snack brand from Frito-Lay North America ("FLNA" - a division of Pepsico Inc.). DeMets will merge TrueNorth into its existing portfolio of confectionary products that includes: DeMets Turtles DeMets Flipz DeMets Treasures and DeMets Stixx. DeMets is owned by the Greenwich CT-based private equity fund Brynwood Partners V L.P. "DeMets is pleased to welcome the TrueNorth products to its family of brands" said David D. Clarke President and CEO of DeMets. "These high quality all natural products will provide DeMets with many new avenues for growth and innovation both in market segments that TrueNorth presently competes in as well as in new ones." "We are very pleased to have completed our first transaction with Pepsico" said Hendrik J. Hartong III Chairman DeMets and Senior Managing Partner Brynwood Partners. "We remain very active investors in the food sector and we are excited to add the TrueNorth brand to our successful investment in DeMets." About DeMets Candy Company Formed in 2007 and majority owned by Brynwood Partners V L.P. DeMets is a leading manufacturer and marketer of chocolate confectionary products including DeMets Turtles DeMets Flipz DeMets Treasures and DeMets Stixx. The company is headquartered in Stamford CT and operates company-owned manufacturing facilities in Big Flats NY and Mohnton PA. For more information on DeMets please visit www.demetscandy.com. About Brynwood Partners L.P. Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity (c) service businesses and (d) specialty retailing. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:32 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Our Team - Overview'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/team/team_overview.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['The MidOcean Partners team is a group of seasoned investment Professionals who are supported by an Executive Board and a team of Management Affiliates. MidOceans strong and active Executive Board is comprised of senior executives who have experience running companies within our targeted industries and is available as a resource to our portfolio companies providing insights and resources for managers. MidOceans Management Affiliates are accomplished operating managers who provide guidance to investment opportunities. They provide MidOcean Partners portfolio companies detailed knowledge of operational matters as well as insight into competitive dynamics and are active in all phases of the pre-acquisition evaluation and strategic planning processes. MidOcean Partners Management Affiliates also often assume active roles in portfolio companies.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:32 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045755.PDF?Y=&O=PDF&D=&fid=1500045755&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:52:33 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=29'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT December 2 2011 Brynwood Partners VI L.P. announced today that it acquired Santa Clarita California-based Newhall Laboratories through its wholly-owned subsidiary Golden Sun Holdings Inc. Terms of the transaction which closed yesterday were not disclosed. Newhall Laboratories is a leading marketer of widely recognized personal care brands including la bella Monkey Brains and GroWorks. Newhall Laboratories was acquired by Brynwood Partners latest fund Brynwood VI and will operate as an independent platform company. This acquisition represents Brynwood VIs third acquisition in the personal care sector. We are delighted to announce the acquisition of Newhall Laboratories said Dario Margve Managing Partner Brynwood VI. We believe that this will be a great new platform investment for our firm and look forward to not only bringing renewed attention and energy to la bella Monkey Brains and GroWorks but also to adding complementary brands to the company in the future. Brynwood Partners has a strong track record of making acquisitions in the personal care sector with investments such as J.B. Williams Company Inc. a leading marketer of the Aqua Velva Lectric Shave Brylcreem Cepacol and Williams Mug Soap brands and most recently High Ridge Brands Co. Brynwood VI created High Ridge Brands through two separate transactions one with The Procter & Gamble Company and one with Unilever. High Ridge Brands acquired the Zest personal cleansing brand from Proctor & Gamble in January 2011 and the Alberto VO5 and Rave hair care brands from Unilever in August 2011. Based in Stamford CT High Ridge Brands has grown to approximately $200 million in sales in a nine month period. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:33 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=31'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Newhall Laboratories Acquires the L.A. Looks Dep Sport Zero Frizz Thicker Fuller Hair Pure & Natural and Soft & Dri Brands from Henkel GREENWICH CT April 3 2012 Brynwood Partners VI L.P. announced today that its portfolio company Golden Sun Inc. (d/b/a Newhall Laboratories) has acquired the rights to the L.A. Looks Dep Sport Zero Frizz and Thicker Fuller Hair portfolio of hair care brands the Soft & Dri female deodorant brand and the Pure & Natural liquid hand soap and body wash brand from The Dial Corporation a subsidiary of the Henkel group. Based in Santa Clarita CA Golden Sun is wholly-owned by Brynwood VI. Terms and conditions of the transaction which closed on April 1st were not disclosed. In a simultaneous transaction Brynwood VI portfolio company High Ridge Brands Co. acquired the Coast personal cleansing brand from Henkel. We are delighted to announce the acquisition of this great portfolio of brands said Dario Margve Chairman of Golden Sun. We believe that in particular the L.A. Looks and Dep Sport businesses will greatly complement our la bella hair care brand. The remaining acquired brands have loyal consumer followings and will greatly benefit from the renewed attention that our company will provide to the sales and marketing efforts. Brynwood Partners has developed a unique niche of being the lower middle market firm of choice for corporate divestitures. In its 28-year history Brynwood Partners has acquired 30 corporate brands from 13 different corporate sellers including the Henkel acquisitions. Since January 2011 the firm has successfully completed the acquisitions of 14 corporate brands in five separate transactions. On behalf of Brynwood Partners and Golden Sun I would like to thank the Henkel team for its partnership in closing the acquisition of these iconic personal care brands said Henk Hartong III Senior Managing Partner of Brynwood Partners. Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. Golden Sun (d/b/a Newhall Laboratories) was acquired by Brynwood VI in December 2011. The company markets the la bella GroWorks and Monkey Brains hair care brands. Golden Sun is headquartered in Santa Clarita CA. For more information on Golden Sun please visit www.labellabeauty.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:34 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Industry Focus - Consumer'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/focus/consumer.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['The MidOcean Partners Consumer team targets consumer product and services companies that have unique consumer propositions that are not being fully exploited. We look for companies that have strong brands and dominant market positions for their core products. By partnering with seasoned industry executives and tapping into our international network MidOcean Partners is able to identify the most promising businesses in its target areas.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:34 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=26'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Brynwood Partners VI L.P. Acquires the Kretschmer Wheat Germ Brand and Manufacturing Facility from The Quaker Oats Company GREENWICH CT November 30 2011 Brynwood Partners VI L.P. announced today that its newly-formed portfolio company Sun Country Foods Inc. has acquired the Kretschmer wheat germ brand and manufacturing facility from The Quaker Oats Company. Sun Country Foods purchased the rights to the Kretschmer brand in the U.S. Canada and Puerto Rico. Additionally Sun Country Foods acquired the Sun Country trademark. Terms and conditions of the transaction which closed yesterday were not disclosed. Sun Country Foods will be headquartered in Boston Massachusetts with its manufacturing facility to continue to be located in Manhattan Kansas. The company intends to retain existing employees to support plant productions. Kretschmer an iconic brand that was founded over 70 years ago is the largest wheat germ brand in the U.S. As part of this transaction Brynwood VI announced the appointments of Roy Lubetkin as President and CEO and Michael Morin as CFO of the company. Prior to joining Sun Country Foods Mr. Lubetkin served as CEO of Backyard Farms LLC and prior to that held executive and marketing positions at Kraft Foods Inc. Mr. Morin previously served as CFO of Richelieu Foods Inc. a successful Brynwood Partners V L.P. portfolio company that was divested in 2010. We are pleased to announce the acquisition of the Kretschmer wheat germ brand and its manufacturing facility from Quaker Oats said Henk Hartong III Chairman Sun Country Foods and Senior Managing Partner Brynwood VI. The Sun Country acquisition marks Brynwood Partners 20th corporate divestiture acquisition from eleven different corporate sellers. Kretschmer is a well-known brand with excellent nutrition and wellness attributes. We intend to grow the Kretschmer brand organically and will seek strategic add-on acquisitions to grow the Sun Country Foods platform. Brynwood Partners has a strong investment track record in the food space. Brynwood V holds an investment in DeMets Candy Company a leading U.S.-based chocolate confectionery company manufacturing and marketing the Turtles Flipz and Treasures brands. In 2010 DeMets acquired the TrueNorth nut snack brand from Frito-Lay North America a division of PepsiCo Inc. Brynwood VI owns the Balance Bar Company which was acquired from Kraft Foods Inc. Balance Bar markets the Balance Bar line of energy / nutrition bars. Most recently Brynwood VI acquired Pearson Candy Company a leading manufacturer and marketer of widely recognized confectionery brands including Pearsons Salted Nut Roll Pearsons Mint Patties Pearsons Nut Goodies and Pearsons Bun. In 2010 Brynwood V divested Richelieu Foods which was a leader in the private label frozen pizza and pourables food categories. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:35 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Portfolio Holdings'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/portfolio/holdings.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:36 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Portfolio - Industrial Services'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/portfolio/industrial.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:36 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Portfolio - Business & Media Services'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/portfolio/business.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:37 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Our Team - Investment Professionals'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/team/professionals.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': [' Following his time at Chrysler Mr. Miller then joined the James D. Wolfensohn Inc. a New York investment bank as a senior partner form 1992-1993. During this time Mr. Miller also served as an advisor to Olympia & York a privately held real estate firm as it restructured over $20 billion of debts. Robert S. (Steve) Miller joined as Chairman of MidOcean Partners in 2009. Mr. Miller who is widely recognized for his broad operational experience served as Chairman of insurance conglomerate AIG form 2010 to 2015. He recently served as Chief Executive Officer at Hawker Beechcraft Inc. (2012-2013) and as Chairman and CEO of auto parts giant Delphi Corporation (2005-2009). He has also held top leadership posts at many companies including Federal Mogul Bethlehem Steel Waste Management and Morrison Knudson. Mr. Miller began his career in 1968 at Ford Motor Company and after more than a decade there joined Chrysler Corporation where he worked for 13 years eventually serving as Vice Chairman. While at Chrysler corporate staffs financial services international automotive operations and non-automotive operations all reported to him. Mr. Miller led the financial negotiations with 400 bank lenders and the Federal government which resulted in the Loan Guarantee Act bailout package in 1980 that saved Chrysler.Following his time at Chrysler Mr. Miller then joined the James D. Wolfensohn Inc. a New York investment bank as a senior partner form 1992-1993. During this time Mr. Miller also served as an advisor to Olympia & York a privately held real estate firm as it restructured over $20 billion of debts.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:38 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Industry Focus - Industrial Services'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/focus/industrial_services.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['The MidOcean Partners Industrial Services team targets investment opportunities in niche sub-sectors that are typically less cyclical and asset intensive than traditional heavy manufacturing operators. MidOcean Partners has also focused on identifying service or logistic companies that will benefit from changes in the supply chain without assuming the cyclical risk of a traditional industrials company. We believe that we are positioned to identify the most promising businesses in our target areas.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:38 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=2'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT - August 22 2011 - Brynwood Partners VI L.P. announced today that it acquired St. Paul Minnesota-based Pearson Candy Company. Terms and conditions of the transaction which closed today were not disclosed. Pearson previously owned by Larry Hassler and founded over 100 years ago is a leading manufacturer and marketer of widely recognized confectionery brands including Pearsons Salted Nut Roll Pearsons Mint Patties Pearsons Nut Goodies and Pearsons Bun. As part of this transaction Brynwood VI announced the appointment of Michael Keller as President and CEO of the company. Keller will assume the responsibilities of Hassler who will remain with the company in a consulting capacity during a transition period. Prior to joining Pearson Keller served as the Chief Marketing Officer of International Dairy Queen. "We are pleased to announce the acquisition of the Pearson Candy Company and we are excited to attract an executive of Michael Kellers caliber to this opportunity" said Henk Hartong III Chairman Pearson Candy Company and Senior Managing Partner Brynwood VI. "Pearson is an iconic confectionery business and is a terrific addition to Brynwood VIs portfolio. We expect Pearson to become a great platform for additional acquisitions in the candy sector. We are grateful to have had the chance to work with Larry Hassler and look forward to continuing to grow the great company that he has built." "Having owned and operated Pearson since 1985 I am confident that I am leaving the company in good hands with Brynwood VI" said Larry Hassler. "Given Brynwood Partners experience in the food space I am confident that Brynwood VI will have great success in enhancing the value of the organization." Minneapolis-based investment banking firm Prestwick Partners represented Hassler on the sale of the company. Brynwood Partners has a strong track record in the food space. Brynwood Partners V L.P. holds an investment in DeMets Candy Company a leading U.S.-based chocolate confectionery company manufacturing the Turtles Flipz and Treasures brands. Most recently DeMets acquired the TrueNorth nut-cluster snack brand from PepsiCo Inc. In addition Brynwood VI owns the Balance Bar Company which was acquired from Kraft Foods Inc. Brynwood V recently divested its investment in Richelieu Foods Inc. a leading U.S.-based manufacturer of private label frozen pizza and salad dressings which grew sales from approximately $100 million to over $250 million under Brynwood Vs ownership. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity (c) service businesses and (d) specialty retailing. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:39 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Portfolio - Consumer'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/portfolio/leisure.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:39 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=1'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Brynwood Partners VI L.P. to Acquire the U.S. and Puerto Rican Rights to the Alberto VO5 Brand and the Global Rights to the Rave Brand from Unilever GREENWICH CT - August 24 2011 - Brynwood Partners VI L.P. announced today that its portfolio company High Ridge Brands Co. owner of the North American Zest personal cleansing franchise has signed a definitive purchase agreement with Unilever to acquire from Unilever the Alberto VO5 brand and marketing rights in the U.S. and Puerto Rico; as well as the rights to the Rave brand and marketing rights worldwide. Unilever has retained the rights to the Alberto VO5 brand outside the U.S. and Puerto Rico. High Ridge Brands is based in Stamford CT and Brynwood VI is its majority owner. The transaction which is subject to approval by the U.S. Department of Justice is expected to close by the end of August. Terms and conditions of the transaction will not be disclosed. Alberto VO5 is an iconic brand that was launched in 1955 by the Alberto-Culver Company and is well known for its hair care products including shampoos conditioners hair treatments and styling aids. Alberto VO5 has been a top 15 hair care brand for decades and enjoys very high consumer awareness. Originally founded in 1978 Rave is an enduring brand with strong consumer loyalty for its line of hair spray products. Rave is a household name and is recognized for styling products that provide long-lasting hold. The products of both brands are widely distributed in the U.S. and Puerto Rico. "We are delighted to announce the signing of an agreement to acquire the Alberto VO5 and Rave brands" said James Daniels President & CEO High Ridge Brands. "We believe that these businesses will greatly complement the Zest personal cleansing brand currently marketed by High Ridge Brands and we look forward to bringing renewed attention and energy to both the Alberto VO5 and Rave brands." "We are excited to make another acquisition in the personal care space and to add to our very successful Zest business. In an eight month period we will have created a company with approximately $200 million in sales and look forward to growing the business through renewed marketing product innovation and selected add-on acquisitions" said Dario Margve Managing Partner Brynwood Partners and Chairman of High Ridge Brands. Brynwood Partners has developed a unique niche of being the lower middle market firm of choice for corporate divestitures. The Alberto VO5 and Rave transaction follows Brynwood Partners acquisitions of the Zest brand from the Procter & Gamble Company the TrueNorth brand from PepsiCo Inc. and the Balance Bar Company from Kraft Foods Inc. Brynwood Partners also acquired several confections brands from Nestl USA between 2004 and 2007. "I would like to thank the Unilever team for their partnership in the signing of the definitive purchase agreement for the acquisition of the Alberto VO5 and Rave brands" said Henk Hartong III Senior Managing Partner Brynwood Partners. "We are very proud that our list of corporate divestiture relationships will now include Unilever." Over the course of its 27-year history Brynwood Partners has purchased a total of 16 brands from eight corporations in the consumer segment. About Brynwood Partners Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) businesses services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About High Ridge Brands Founded in January 2011 High Ridge Brands was established to purchase the Zest personal cleansing brand from the Procter & Gamble Company and to serve as a platform for further acquisitions in the personal care consumer segment. High Ridge Brands is headquartered in Stamford CT. For more information on High Ridge Brands please visit www.highridgebrands.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:40 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Industry Focus - Business & Media Services'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/focus/business_services.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['The MidOcean Partners Business and Media Services team targets companies that are uniquely positioned to benefit from changing landscapes and shifting business needs. We look for companies that offer differentiated and valued solutions to the many challenges facing traditional businesses across a broad range of industries. Within its Business and Media Services vertical focus sub sectors include information services outsourced services environmental services educational services media and content and marketing services. MidOcean Partners is able to identify promising businesses in this target area by partnering with seasoned industry executives and leveraging our extensive network.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:40 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=36'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT November 26 2012 Brynwood Partners VI L.P. announced today that it has sold its portfolio company Balance Bar Company to NBTY Inc. a global leader in vitamins supplements and active nutrition products and a portfolio company of The Carlyle Group. Balance Bar founded in 1992 and acquired by Brynwood VI from Kraft Foods Inc. in 2009 has been an innovator in the nutrition/energy bar category. The company which is headquartered in Valhalla NY is one of the leading brands in the high growth $1 billion nutrition/energy bar category. Balance Bar enjoys a strong and loyal consumer following and has some of the most recognized bars in the industry. Balance Bar products offer a unique combination of great taste and lasting energy through balanced nutrition. We are delighted to announce the divestiture of Balance Bar which was our first investment in our most recent fund Brynwood VI said Hendrik J. Hartong III Chairman Balance Bar and Senior Managing Partner Brynwood Partners. We are grateful to all of the Balance Bar employees for their hard work and dedication to the business. We wish NBTY continued success with this great brand. On behalf of Brynwood Partners I would like to thank the Carlyle and NBTY teams for their efforts in completing this transaction. We have enjoyed the relationship said Ian B. MacTaggart Managing Partner Brynwood Partners. Houlihan Lokey Capital Inc. served as the investment banking advisor to Balance Bar. About Brynwood Partners: Founded in 1984 and headquartered in Greenwich CT Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About Balance Bar Company: Founded in 1992 and headquartered in Valhalla NY Balance Bar one of Americas original nutrition/energy bars inspires consumers to lead active well-rounded and balanced lives by providing great-tasting healthy and convenient nutrition for lasting energy. Balance Bars are available in four unique product lines (Original Gold Bare and Minis) and all the Companys flavors are based on the 40/30/30 nutrition model (40% of calories from healthy carbohydrates 30% from quality protein and 30% from dietary fat). For more information on Balance Bar please visit www.balance.com. About NBTY Inc.: NBTY Inc. is the leading vertically integrated manufacturer marketer distributor and retailer of high-quality vitamins nutritional supplements and related products in the United States with operations worldwide. NBTY currently markets over 25000 SKUs under many brands including Natures Bounty Sundown American HealthEster-C Solgar MET-Rx Osteo Bi-Flex SISU Rexall Pure Protein Body Fortress Puritans Pride Vitamin World Holland & Barrett GNC (UK) Physiologics and De Tuinen. About The Carlyle Group: The Carlyle Group is a global alternative asset manager with $157 billion of assets under management in 101 active funds and 64 fund of fund vehicles as of September 30 2012. Carlyle invests across four segments Corporate Private Equity Real Assets Global Market Strategies and Fund of Funds Solutions in Africa Asia Australia Europe the Middle East North America and South America. Carlyle has developed expertise in various industries including: aerospace defense & government services consumer & retail energy financial services healthcare industrial technology & business services telecommunications & media and transportation. The Carlyle Group employs more than 1300 people in 32 offices across six continents.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:41 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=37'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT December 11 2012 Brynwood Partners VI L.P. announced that it has sold its investment in Sun Country Foods Inc. to Continental Mills Inc. The transaction closed yesterday and the terms and conditions were not disclosed. Sun Country Foods headquartered in Norwood Massachusetts with a manufacturing facility in Manhattan Kansas manufactures and markets the iconic Kretschmer Wheat Germ brand. Founded in 1936 by Charles Kretschmer the brand has a 90% market share in the U.S. wheat germ category and is offered in two flavors: Original Toasted and Honey Crunch. Sun Country Foods was formed by Brynwood VI in 2011 to acquire the Kretschmer Wheat Germ brand from The Quaker Oats Company. We are delighted to announce the divestiture of Sun Country Foods Brynwood VIs second portfolio company sale in the last month said Hendrik J. Hartong III Chairman Sun Country Foods and Senior Managing Partner Brynwood Partners. We are grateful to all of the Sun Country Foods employees for their hard work and dedication to the business. We wish Continental Mills much success with this great brand. We are very happy with the outcome of this transaction and on behalf of Brynwood Partners I would like to thank the Continental Mills team as it has been a pleasure working with them said Ian B. MacTaggart Managing Partner Brynwood Partners. Sun Country Foods was not represented by an investment banker on this transaction. About Brynwood Partners: Founded in 1984 and headquartered in Greenwich Connecticut Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About Sun Country Foods Inc.: Sun Country Foods Inc. headquartered in Norwood Massachusetts was formed by Brynwood Partners VI L.P. in November 2011 to acquire the Kretschmer Wheat Germ brand and its Manhattan Kansas manufacturing facility from The Quaker Oats Company. For more information on Sun Country Foods please visit www.suncountryfoodsinc.com. About Continental Mills Inc.: Continental Mills Inc. is a privately held manufacturer and marketer of prepared flour mixes and related products located in Tukwila Washington. The company has strong branded products sold primarily under its Krusteaz trademark and several licensed brands. The companys products are sold through retail foodservice and club store channels throughout the U.S. For more information on Continental Mills please visit www.continentalmills.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:41 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=38'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT May 1 2013 Brynwood Partners VI L.P. announced today that its portfolio company Pearson Candy Company has acquired the Bit-O-Honey confectionery brand from Nestl USA Inc. The acquisition marks the fifth brand Brynwood Partners has acquired from Nestl. Terms and conditions of the transaction which closed today were not disclosed. Bit-O-Honey is an iconic national brand with over a 90-year history with U.S. consumers. Bit-O-Honey is available in a variety of formats and is widely distributed in the U.S. through leading retailers in the food drug mass and dollar channels. We are excited to announce the acquisition of the Bit-O-Honey brand said Michael Keller President and CEO of Pearsons. Mr. Keller continued We look forward to adding the brand into Pearsons which is known for its Pearsons Salted Nut Roll Pearsons Mint Patties Pearsons Nut Goodies and Pearsons Bun. We plan to bring renewed focus to the Bit-O-Honey brand by increasing its availability to consumers and by offering customers new and innovative Bit-O-Honey products. The acquisition of Bit-O-Honey will complement our strong Pearsons business. While management continues to focus on product innovation and expanding the companys geographic reach the Bit-O-Honey brand has strong national awareness. We feel this acquisition will be a conduit to expand the reach of all of Pearsons brands throughout the U.S. said Henk Hartong III Senior Managing Partner of Brynwood VI and Chairman of Pearsons. Mr. Hartong III continued On behalf of Brynwood Partners and Pearsons I would like to thank the Nestl team for its partnership in planning the sale and transition of the business. Brynwood Partners has developed a unique niche in the corporate carve out sector. In its 29-year history Brynwood Partners has acquired 37 corporate brands from 14 different corporate sellers including the Bit-O-Honey acquisition. Since January 2011 the firm has successfully completed the acquisition of 23 corporate brands in 10 separate transactions. About Brynwood Partners: Founded in 1984 and headquartered in Greenwich CT Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the consumer sector where it can leverage the operational expertise of its managing partners to create shareholder value. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About Pearson Candy Company: Headquartered in St. Paul MN Pearson Candy Company was acquired by Brynwood VI in August 2011. Pearsons which has operated for more than 90 years manufactures and markets the Pearsons Salted Nut Roll Pearsons Mint Patties Pearsons Nut Goodies and Pearsons Bun. For more information on Pearsons please visit www.pearsonscandy.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:42 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress to Release Second Quarter Results and Announce Dividend on July 30 2015'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=30227493&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) plans to announce its financial results and dividend for the second quarter 2015 prior to the opening of the New York Stock Exchange on Thursday July 30 2015. A copy of the press release will be posted to the Investor Relations section of Fortresss website www.fortress.com. In addition management will host a conference call on July 30 2015 at 10:00 A.M. Eastern Time. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Second Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available after the live call by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 79002443. Fortress Investment Group LLC is a leading highly diversified global investment firm with $69.9 billion in assets under management as of March 31 2015. Founded in 1998 Fortress manages assets on behalf of approximately 1700 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE:FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:44 [scrapy] ERROR: Error processing {'pagetitle': [u'Shareholder Information'],
 'pageurl': ['http://shareholders.fortress.com/calendar.aspx?iid=4147324&print=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['Fortress Presentation and Transcript for the Goldman Sachs US Financial Services Conference']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:45 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=39'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT May 7 2013 Brynwood Partners V L.P. announced today that its portfolio company DeMets Candy Company has sold the TrueNorth brand and selected assets to B&G Foods Inc. Terms and conditions of the transaction which closed yesterday were not disclosed. TrueNorth is a leader in the better-for-you snack category. TrueNorth is a nationally distributed premium snack nut brand that DeMets acquired from Frito-Lay North America a division of PepsiCo Inc. in 2010. Under its ownership DeMets introduced new TrueNorth flavor varieties to leading customers in the club mass and food retail channels. The divestiture of the TrueNorth brand allows DeMets to focus on its core confectionary products including Turtles chocolate-caramel nut clusters Flipz chocolate covered pretzels and Treasures filled chocolates said Peter Wilson Vice Chairman of DeMets. We are pleased to announce the divestiture of TrueNorth said Hendrik J. Hartong III Chairman DeMets and Senior Managing Partner Brynwood Partners. We wish B&GFoods much success with this terrific brand. It has been a pleasure working with the B&GFoods team and we are excited to have completed our first transaction with them. Houlihan Lokey Capital Inc. served as the investment banking advisor to DeMets. About Brynwood Partners: Founded in 1984 and headquartered in Greenwich CT Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the consumer sector where it can leverage the operational expertise of its managing partners to create shareholder value. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About DeMets Candy Company: Formed in 2007 and majority owned by Brynwood Partners V L.P. DeMets is a leading manufacturer and marketer of chocolate confectionary products including Turtles Flipz and Treasures. Headquartered in Stamford CT the company owns manufacturing facilities in Big Flats NY and Mohnton PA. For more information on DeMets please visit www.demetscandy.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:45 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=30'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT April 3 2012 Brynwood Partners VI L.P. announced today that its portfolio company High Ridge Brands Co. has acquired the global rights to the Coast personal cleansing brand from The Dial Corporation a subsidiary of the Henkel group. Based in Stamford CT High Ridge Brands is majority-owned by Brynwood VI. Terms and conditions of the transaction which closed on March 31st will not be disclosed. In a simultaneous transaction Brynwood VI portfolio company Golden Sun Inc. (d/b/a Newhall Laboratories) acquired the rights to the L.A. Looks Dep Sport Zero Frizz and Thicker Fuller Hair portfolio of hair care brands the Soft & Dri female deodorant brand and the Pure & Natural liquid hand soap and body wash brand from Henkel. Originally launched in 1975 Coast is an iconic personal cleansing brand and known as The Eye Opener for awakening users through an invigorating and refreshing early morning shower experience. Coast is available in both bar soaps (Pacific Force and Arctic Boost) and body washes (Pacific Force and Urban Fuel). Coast products are widely distributed in the U.S. through leading retailers in the food drug mass and dollar channels. We are delighted to announce the acquisition of the Coast brand said James Daniels President and CEO of High Ridge Brands. Mr. Daniels continued the Coast brand has a very loyal consumer following and we look forward to bringing renewed attention and energy to Coast. We are excited to make another acquisition in the personal care space and to add to our very successful High Ridge Brands business. In just over a year we have created a leading personal care company with well-known consumer brands including Zest Alberto VO5 Rave and now Coast. We will continue to manage our investment by growing all of these brands through renewed marketing and product innovation said Dario Margve Managing Partner of Brynwood VI and Chairman of High Ridge Brands. Brynwood Partners has developed a unique niche of being the lower middle market firm of choice for corporate divestitures. In its 28-year history Brynwood Partners has acquired 30 corporate brands from 13 different corporate sellers including the Henkel acquisitions. Since January 2011 the firm has successfully completed the acquisitions of 14 corporate brands in five separate transactions. On behalf of Brynwood Partners and High Ridge Brands I would like to thank the Henkel team for its partnership in closing the acquisition of the Coast brand said Henk Hartong III Senior Managing Partner of Brynwood Partners. Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. High Ridge Brands was formed by Brynwood VI in January 2011 to purchase the North American Zest personal cleansing brand from the The Procter & Gamble Company and to serve as a platform for further acquisitions in the personal care consumer segment. In August 2011 High Ridge Brands acquired the U.S. and Puerto Rico brand and marketing rights to the Alberto VO5 brand and the worldwide marketing and brand rights to the Rave brand. High Ridge Brands is headquartered in Stamford CT. For more information on High Ridge Brands please visit www.highridgebrands.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:46 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=32'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['High Ridge Brands To Acquire the White Rain Brand from Sun Products GREENWICH CT and WILTON CT June 26 2012 Brynwood Partners VI L.P. and The Sun Products Corporation jointly announced today that Brynwood VIs portfolio company High Ridge Brands Co. has signed a definitive agreement to acquire the rights to the White Rain personal care brand from Sun Products. Additionally as part of the transaction High Ridge Brands is acquiring the Adorn The Dry Look Mink and Toni brands. Terms and conditions of the transaction which is scheduled to close on July 2nd will not be disclosed. White Rain an iconic brand with a long history with U.S. consumers was first introduced by Gillette in 1952. The brand was originally advertised with the tagline your lovely hair is the essence of romance so keep it sunshine bright with White Rain. Today White Rain products provide hydration and moisturizing benefits to its many loyal users. White Rain offers a diverse product portfolio that includes shampoos conditioners body washes and hair styling products for the entire family. Products are widely distributed in the U.S. through leading retailers in the food drug mass and dollar channels. We are excited to announce the acquisition of the White Rain brand said James Daniels President and CEO of High Ridge Brands. Mr. Daniels continued We look forward to integrating these brands into our portfolio of iconic brands which include Zest Coast Alberto VO5 and Rave. We will revitalize the White Rain brand with a renewed focus on providing consumers with new and innovative value-oriented products. The White Rain brand has deep heritage in the personal care business and we have worked to protect and build upon that heritage by growing the business behind a broader product line-up and improved packaging said Jeffrey P. Ansell Sun Products Chairman President and CEO. However our personal care products are not strategically aligned with Sun Products core portfolio of iconic laundry and household care products. We are pleased to transition ownership to a company where these brands represent a much better strategic fit. Id like to thank Sun Products employees who have nurtured the White Rain brand and High Ridge Brands for working closely together to facilitate a smooth transition. The acquisition of White Rain will add to our very successful High Ridge Brands business. We are proud to have created a company with over $300 million of sales in a short period of time. While management continues to focus its attention on growing the company organically we are fortunate to be adding another very attractive strategic acquisition said Dario Margve Managing Partner of Brynwood VI and Chairman of High Ridge Brands. Brynwood Partners has developed a unique niche of being the lower middle market firm of choice for corporate divestitures. In its 28-year history Brynwood Partners has acquired 35 corporate brands from 14 different corporate sellers including the Sun Products acquisition. Since January 2011 the firm has successfully completed the acquisitions of 19 corporate brands in six separate transactions. On behalf of Brynwood Partners and High Ridge Brands I would like to thank the Sun Products team for its partnership in planning the sale and transition of the business said Henk Hartong III Senior Managing Partner of Brynwood Partners. About Brynwood Partners: Founded in 1984 and headquartered in Greenwich Connecticut Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About High Ridge Brands Co.: High Ridge Brands Co. headquartered in Stamford Connecticut was formed by Brynwood VI in January 2011 to purchase the North American Zest personal cleansing brand from the The Procter & Gamble Company and to serve as a platform for further acquisitions in the personal care consumer segment. In August 2011 High Ridge Brands acquired the U.S. and Puerto Rican brand and marketing rights to the Alberto VO5 brand and the worldwide marketing and brand rights to the Rave brand from Unilever. In March 2012 High Ridge Brands acquired the global rights to the Coast personal cleansing brand from Henkel. For more information on High Ridge Brands please visit www.highridgebrands.com. About The Sun Products Corporation: The Sun Products Corporation headquartered in Wilton Connecticut is a leading North American manufacturer of laundry and household care products. With annual net sales of $2 billion the companys portfolio of products are sold under well-known brands that include all Snuggle Wisk Sun Surf and Sunlight. In addition Sun Products is the manufacturing partner for many retailer brand high quality laundry and household care products in North America. For more information visit www.sunproductscorp.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:46 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=40'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT September 16 2013 Brynwood Partners VI L.P. announced today that its newly-formed portfolio company Lightlife Foods Inc. has acquired Lightlife one of ConAgra Foods smaller brands with product lines that include vegetarian-based burgers hot dogs and other meatless frozen and refrigerated items. The acquisition includes the Lightlife manufacturing operation in Turners Falls MA. Terms and conditions of the transaction which closed today were not disclosed. Lightlife Foods will be headquartered in the Boston MA area and its manufacturing facility will continue to be located in Turners Falls MA. The new company looks forward to continuing to utilize the facility to support the manufacturing needs of the business. Lightlife a healthy eating brand that was founded more than 34 years ago is a leader in the meat alternative category. As part of this transaction Brynwood VI announced the appointments of Roy Lubetkin as President and CEO and Michael Morin as CFO of the company. Before joining Lightlife Foods Mr. Lubetkin served as CEO of Sun Country Foods Inc. a successful Brynwood VI portfolio company that was divested in 2012 and prior to that served as CEO of Backyard Farms LLC. Mr. Morin most recently served as CFO of Sun Country Foods and prior to that held the position of CFO of Richelieu Foods Inc. a successful Brynwood Partners V L.P. portfolio company that was divested in 2010. We are pleased to announce the acquisition of Lightlife and its manufacturing facility from ConAgra Foods said Henk Hartong III Senior Managing Partner Brynwood VI. Lightlife is a well-known brand with excellent nutritional and wellness attributes. Lightlife has a very loyal consumer following and customer base. We look forward to investing in Lightlife Foods and extending the companys refrigerated and frozen meatless offerings. Brynwood Partners has now completed 38 brand acquisitions from 15 different corporate sellers. The Lightlife acquisition is Brynwood Partners first transaction with ConAgra Foods. Brynwood Partners has a strong investment track record in the food space with active investments including: Prior investments by Brynwood Partners in the food space include Sun Country Foods Balance Bar Company Richelieu Foods Lincoln Snacks Company and Signature Snacks Company. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in North American based lower middle market companies in the consumer sector. Brynwood Partners currently manages more than $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About ConAgra Foods: ConAgra Foods Inc. (NYSE: CAG) is one of North Americas largest packaged food companies with branded and private branded food found in 99 percent of Americas households as well as a strong commercial foods business serving restaurants and foodservice operations globally. Consumers can find recognized brands such as Banquet Chef Boyardee Egg Beaters Healthy Choice Hebrew National Hunts Marie Callenders Orville Redenbachers PAM Peter Pan Reddi-wip Slim Jim Snack Pack and many other ConAgra Foods brands along with food sold by ConAgra Foods under private brand labels in grocery convenience mass merchandise club and drug stores. Additionally ConAgra Foods supplies frozen potato and sweet potato products as well as other vegetable spice bakery and grain products to commercial and foodservice customers. ConAgra Foods operates ReadySetEat.com an interactive recipe website that provides consumers with easy dinner recipes and more. For more information please visit www.conagrafoods.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:47 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=41'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Brynwood Partners Completes First and Final Close of Brynwood Partners VII L.P. Greenwich CT September 25 2013 Brynwood Partners an operationally-focused private equity fund that makes control investments in lower middle market companies in the consumer sector is pleased to announce that it has completed the first and final close of its latest fund Brynwood Partners VII L.P. with $400 million of committed capital. We are both pleased and proud to announce the first and final close of Brynwood VII with $400 million of committed capital the largest fund in our 29 year history said Hendrik J. Hartong III Senior Managing Partner. We are fortunate to have been able to reach our hard cap target with one close. The high level of interest in our Fund from the investment community validates our belief that returns in the lower middle market are primarily driven by operational improvements at the portfolio company level. Brynwood Partners is actively pursuing new platform investments for Brynwood VII and continues to maximize returns for its remaining investments in prior funds. We received strong support from our existing limited partner base and appreciate their continued commitment to Brynwood Partners. We will continue to evaluate add-on acquisition opportunities for our five portfolio companies in Brynwood Partners VI L.P and will be seeking new platform investment opportunities for Brynwood VII said Ian B. MacTaggart Managing Partner. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the consumer sector. Brynwood Partners is currently managing over $900 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:47 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=42'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Brynwood Partners V L.P. to Divest DeMets Candy Company to the Owner of the Godiva Chocolate Brand GREENWICH CT December 27 2013 Brynwood Partners V L.P. announced today that it has signed an agreement to sell its investment in DeMets Candy Company for $221 million to Yildiz Holdings A.S. owner of the Godiva chocolate brand. The transaction is expected to close in January 2014. DeMets Candy headquartered in Stamford CT is a leading U.S. manufacturer and marketer of premium priced chocolate confection products under the Turtles Treasures and Flipz brands. Since its formation in 2007 by Brynwood V DeMets Candy has significantly expanded its sales production and profitability. All of the DeMets brands were acquired by Brynwood Partners from Nestl USA in separate transactions. DeMets Candy employs approximately 200 people and operates two manufacturing facilities in the U.S. During Brynwood Vs ownership the company built one of these plants bringing approximately 150 jobs to Big Flats NY while significantly investing in and upgrading the other. After the closing Peter Wilson the companys CEO will be joining Brynwood Partners VII L.P. Brynwood Partners most recently raised fund. The rest of DeMets Candys management team will remain with the company. We are delighted to announce the divestiture of DeMets Candy said Hendrik J. Hartong III Chairman DeMets Candy and Senior Managing Partner Brynwood Partners. This investment highlights Brynwood Partners unique operational capabilities in the private equity sector. We originally formed DeMets Candy in 2007 to acquire the Turtles brand in the U.S. from Nestl USA and combined it with the Flipz brand which we had acquired from Nestl USA in 2004. After forming DeMets Candy we quickly hired a management team with whom we have worked collaboratively to create significant shareholder value. We are grateful to DeMets Candys management team and all of the hard working employees in the manufacturing plants for their tireless efforts under our ownership. We wish Yildiz success with this outstanding company. Houlihan Lokey Capital Inc. served as the investment advisor to DeMets Candy. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in North American based lower middle market companies in the consumer sector. Brynwood Partners currently manages more than $900 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About Yildiz Holding A.S.: Yildiz Holding A.S. is a leading global food and beverages company with sales in over 80 countries. Through its subsidiaries including Ulker Biskuvi and Godiva Yildiz Holding A.S. produces a range of products including biscuits chocolate and chocolate-covered products margarine and edible oils beverages dairy products culinary products and baby food. Founded in 1944 Yildiz Holding A.S. is based in Istanbul Turkey and employs approximately 40000 people globally. Yildiz Holding has a total of 56 factories of which 47 are dispersed in five regions nationwide and nine plants operate in nine countries outside of Turkey. For more information on Yildiz Holding A.S. please visit www.yildizholding.com.tr.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:48 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=43'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Back to Nature Foods Company LLC Acquires the SnackWells Business from Mondelz Global LLC GREENWICH CT December 30 2013 Back to Nature Foods Company LLC a portfolio company of Brynwood Partners VI L.P. announced today that it has acquired the SnackWells cookies and snacks business from Mondelz Global LLC. Terms and conditions of the transaction were not disclosed. Brynwood VI and Mondelz Global LLC formed the Back to Nature joint venture in 2012 through an innovative structure whereby Brynwood VI acquired operating control while Mondelz Global LLC retained a minority position in the new company. Back to Nature was one of the original natural food brands in the United States. Since the acquisition Brynwood VI has effected a smooth transition of the business accelerated new product introductions and increased marketing focus. SnackWells was originally launched by Nabisco Inc. in 1992 as a line of reduced-fat and fat-free cookies and crackers. The brand offers its loyal consumers a healthy snacking alternative with great tasting products all with reduced-fat content. SnackWells offers products including Crme Sandwich Cookies Devils Food Cakes Fudge Pretzels Yogurt Pretzels and Popcorn products in portion control sizes. SnackWells products are widely distributed in the United States through leading retailers. We look forward to owning the SnackWells brand and integrating it into our Back to Nature organization said Vincent Fantegrossi President and CEO of Back to Nature. Its healthy attributes loyal consumer base and strong distribution make SnackWells a very complementary product line offering to our Back to Nature products. SnackWells is an iconic brand and we are excited about the potential that this acquisition brings to our company. SnackWells significantly increases our scale in the important cookie and cracker categories. We are thrilled to announce the SnackWells transaction and are honored that Mondelz Global LLC has the confidence in Brynwood Partners to add another brand to this unique joint venture arrangement said Henk Hartong Senior Managing Partner of Brynwood VI and Non-Executive Chairman of Back to Nature. Mr. Hartong continued We are excited to continue to work with Mondelz Global LLC in our collective efforts to maximize the value of Back to Nature which will be enhanced by the addition of SnackWells. Brynwood Partners has developed a unique niche of being the lower middle market firm of choice for corporate divestitures. In its 29-year history Brynwood Partners has now acquired 39 corporate brands from 15 different corporate sellers including the SnackWells acquisition. The SnackWells transaction marks Brynwood Partners second transaction with Mondelz Global LLC in addition to two others with Kraft Foods Inc. the predecessor company of Mondelz Global LLCs parent Mondelz International Inc. Brynwood Partners has a well-established track record of reviving smaller brands or businesses that are divested from large corporations. Some of Brynwood Partners investments both current and prior include: Brynwood Partners has a strong investment track record in the food space. Brynwood V holds an investment in DeMets Candy Company a leading United States-based chocolate confectionery company manufacturing and marketing the Turtles Flipz and Treasures brands. Besides Back to Nature Brynwood VI owns Pearson Candy Company a leading manufacturer and marketer of widely recognized confectionery brands including Pearsons Salted Nut Roll Pearsons Mint Patties Pearsons Nut Goodies Pearsons Bun and Bit-O-Honey along with Lightlife Foods Inc. a manufacturer and marketer of soy-based vegetarian burgers hot dogs tempeh and other frozen and refrigerated items in the meat alternative category. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in North American based lower middle market companies in the consumer sector. Brynwood Partners currently manages more than $900 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About Back to Nature: The Back to Nature mission is to create great-tasting foods made from simple ingredients. The Back to Nature brand is committed to providing high quality options for people who want foods they can feel good about eating because they have no artificial flavors or preservatives. Back to Nature offers a wide range of products including crackers and cookies nuts and trail mixes granolas and juices. To learn more about Back to Nature please visit www.backtonaturefoods.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:49 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress to Release Third Quarter Results and Announce Dividend on October 29 2015'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=31365144&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) plans to announce its financial results and dividend for the third quarter 2015 prior to the opening of the New York Stock Exchange on Thursday October 29 2015. A copy of the press release will be posted to the Investor Relations section of Fortresss website www.fortress.com. In addition management will host a conference call on October 29 2015 at 10:00 A.M. Eastern Time. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Third Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available after the live call by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 57023583. Fortress Investment Group LLC is a leading highly diversified global investment firm with $72.0 billion in assets under management as of June 30 2015. Founded in 1998 Fortress manages assets on behalf of approximately 1700 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:49 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Private Equity - Firm Overview'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/private-equity/investmentStrategy.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['MidOcean Partners employs a sector-focused investment strategy coupled with a relationship-oriented management methodology. MidOcean Partners identifies opportunities in sectors chosen based on macro- and micro-trends. This sector specialization enables MidOceans investment professionals to create unique investment themes identify and develop relationships with talented operators and management teams and originate compelling investment opportunities. MidOcean Partners adds value to each portfolio company by providing input into long-term strategic direction operational insight organic growth opportunities and strategic acquisition ideas. As part of its sector-focused strategy MidOcean Partners has developed a Management Affiliate approach to leverage established relationships with industry executives who are involved in all stages of deal generation execution and operation. This Management Affiliate methodology provides an exceptional base of operational expertise and support to the MidOcean Partners investment team and their portfolio companies ensuring that management teams have the necessary tools and knowledge to refine strategies and expand operations.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:50 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Private Equity - Firm Overview'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/private-equity/overview.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['MidOcean Partners is a private investment firm that specializes in middle market investments in the US. MidOcean targets control investments with minimum equity contribution of $25 million. The Firms seasoned investment team is supported by an experienced Executive Board and its Management Affiliates allowing MidOcean to be an active investor in three target sectors: Business and Media Services Consumer and Industrial Services. MidOcean Partners is dedicated to driving strategic operational and financial growth in portfolio companies and has a strong history of proven investment success through varying market conditions. MidOcean invests with both management expertise and insight into economic cycles aligning with management teams and drawing upon a vast network of resources to deliver significant results for its partners.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:50 [scrapy] ERROR: Error processing {'pagetitle': [u'MidOcean Partners :: Industry Focus - Target Sectors'],
 'pageurl': ['http://www.midoceanpartners.com/cgi-bin/focus/sectors.pl'],
 'siteurl': ['midoceanpartners.com'],
 'text': ['MidOcean Partners targets three sectors Business and Media Services Consumer and Industrial Services. MidOcean believes this industry specialization gives it several competitive advantages: Our teams spend significant time developing a deep understanding of each industry MidOcean Partners enables it to deploy capital with the knowledge and insight of a strategic investor. As part of our sector-focused strategy MidOcean Partners has developed an Executive Board and a Management Affiliate methodology that aligns with our industry focus. These top operating executives are made available as a resource and sounding board for our portfolio companies to help them implement best practices and achieve growth targets. Our sector knowledge and international approach inform our strategic decisions as we work with portfolio company management teams to expand into new markets.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:51 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=33'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Brynwood Partners VI L.P. To Acquire a Controlling Stake in the Back to Nature Brand Through a Joint Venture with Kraft Foods GREENWICH CT August 22 2012 Brynwood Partners VI L.P. announced today that it has signed a definitive agreement to acquire a controlling stake in the Back to Nature brand food business through a joint venture partnership with Kraft Foods. Kraft Foods will continue to hold a substantial minority stake in the business and will have board representation in the joint venture that is to be formed. Terms and conditions of the transaction which is scheduled to close in October will not be disclosed. Founded in 1960 Back to Nature is one of the original natural food brands in the United States. The joint venture will initially offer Back to Nature products in four categories: (i) crackers (ii) cookies (iii) trail mixes & nuts and (iv) granola while management of the Back to Nature macaroni and cheese business will remain with Kraft Foods. An iconic natural and organic brand Back to Nature products are widely distributed in the United States through leading retailers in the natural food drug mass and club channels. As part of the transaction Brynwood VI will appoint Vincent Fantegrossi as President and CEO of the new company. Mr. Fantegrossi previously served as President and CEO of Richelieu Foods Inc. a Brynwood Partners V L.P. investment that returned over six times total invested capital and generated a gross internal rate of return of over 40% for the fund. Under Brynwood Vs ownership Richelieu Foods sales grew from $100 million to over $250 million. We are excited to announce the Back to Nature transaction and are honored that Kraft Foods selected Brynwood VI as its joint venture partner said Henk Hartong III Senior Managing Partner of Brynwood VI. Mr. Hartong III continued We look forward to growing the Back to Nature brand and helping it to achieve even greater success in both the natural and mainstream channels through select new product and new category introductions while staying true to the mission of giving people truly flavorful wholesome foods made with simple ingredients. We are excited to work with Kraft in this unique joint venture between a private equity firm and a consumer products company formed to maximize the value of a smaller brand in its portfolio. Brynwood Partners has developed a unique niche of being the lower middle market firm of choice for corporate divestitures. In its 28-year history Brynwood Partners has acquired 36 corporate brands from 14 different corporate sellers including the Back to Nature acquisition. Since January 2011 the firm has completed the acquisitions of 20 corporate brands in seven separate transactions. The Back to Nature transaction marks Brynwood Partners third transaction with Kraft Foods. Brynwood Partners has a well-established track record of reviving smaller brands or businesses that are divested from large corporations. Some of Brynwood Partners investments both current and prior include: In each of these transactions Brynwood Partners acquired the entire business. In the case of Back to Nature Brynwood VI will acquire operating control but will partner with Kraft Foods to maximize the value of the new company. Brynwood VI will have the opportunity to apply its operating discipline in partnership with a leading consumer products company. This deal has a unique structure where a large consumer company like Kraft Foods has identified a way to partner with a specialist like Brynwood Partners to increase growth of a non-core asset while maintaining a significant financial interest continued Hartong III. We think that this is a very forward-looking concept and we are delighted to be Krafts partner. Brynwood Partners has a strong investment track record in the food space. Brynwood V holds an investment in DeMets Candy Company a leading U.S.-based chocolate confectionery company manufacturing and marketing the Turtles Flipz Treasures and TrueNorth brands. Brynwood VI owns the Balance Bar Company a marketer of energy and nutrition bars and Pearson Candy Company a leading manufacturer and marketer of widely recognized confectionery brands including Pearsons Salted Nut Roll Pearsons Mint Patties Pearsons Nut Goodies and Pearsons Bun. Additionally in November 2011 Brynwood VI formed Sun Country Foods Inc. to acquire the Kretschmer wheat germ brand and manufacturing facility. About Brynwood Partners: Founded in 1984 and headquartered in Greenwich Connecticut Brynwood Partners is an operationally-focused private equity fund that makes control investments in lower middle market companies. Brynwood Partners targets companies operating in the following sectors: (a) consumer products (b) light manufacturing with low capital intensity and (c) business services. Brynwood Partners is currently managing over $500 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About Back to Nature: The Back to Nature mission is to create great-tasting foods made from simple ingredients. The brand is committed to providing high quality options for people who want foods they can feel good about eating because they have no artificial flavors or preservatives. Back to Nature offers a wide range of products including crackers and cookies nuts and trail mixes granola and macaroni and cheese. To learn more about Back to Nature visit www.backtonaturefoods.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:51 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Raises $1.1 billion for Latest Opportunistic Japan Real Estate Fund'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=31719295&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['Third Credit Successor Fund to Close at its Cap in 2015 NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) announced today the successful close of Fortress Japan Opportunity Fund III (FJOF III) at the Funds cap. Investor subscriptions in either USD or JPY totaled approximately $1.1 billion. FJOF III is a successor fund to FJOF II and FJOF which closed at their respective caps in December 2012 and June 2010. As of September 30 2015 FJOF and FJOF II (Yen) had generated annualized inception-to-date net IRRs of 32.3% and 23.5% respectively. FJOF III had invested over 17 billion of capital across 10 closed transactions through September 30 2015. The fund is expected to be fully invested over the next 24 months. Fortresss Japan Opportunity funds primarily focus on opportunistic investments in distressed real estate-related debt and other assets in Japan. The funds seek to capitalize on dynamics related to significant deleveraging by financial institutions and near-term debt maturities: supply demand gaps limited credit availability price distortions volatility and sales of non-core or distressed real estate-related assets. Since 2009 the FJOF funds have invested over 210 billion of capital in opportunistic real estate debt and related assets in Japan. With the completion of this fundraise Fortresss Credit business has now closed three successor funds in 2015 at their targeted caps: Fortress Credit Opportunities Fund IV Fortress Real Estate Opportunities Fund II and FJOF III. In aggregate these three successor Credit funds have raised over $7 billion of new capital. Fortress Investment Group LLC is a leading highly diversified global investment firm with $74.3 billion in assets under management as of September 30 2015. Founded in 1998 Fortress manages assets on behalf of approximately 1800 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:52 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=44'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['GREENWICH CT January 6 2014 Brynwood Partners VII L.P. announced today that its newly-formed portfolio company JPC Acquisition Co. has acquired Josephs Pasta Company from Nestl Prepared Foods Company. Nestl acquired Josephs in 2006. Terms and conditions of the transaction were not disclosed. The Josephs business headquartered in Haverhill MA has a leading position as a manufacturer and marketer of premium frozen stuffed pasta for the foodservice segment. Josephs incorporates time-honored recipes and methodologies to make a wide range of superior pasta and sauces products that inspire innovation and customer loyalty. Its artisan products include stuffed cannelloni stuffed ravioli stuffed tortellini gnocchi and manicotti. Josephs premium products are differentiated from its competitors offerings as the company only uses the highest quality pasta and freshest ingredients. The business employs over 300 individuals. We are pleased to announce the acquisition of Josephs from Nestl said Henk Hartong Senior Managing Partner Brynwood Partners. Josephs manufactures delicious and unique products in its state-of-the-art manufacturing facility. We are excited about the opportunity to accelerate innovation and explore new distribution channels. We look forward to investing in the business and working with Josephs loyal and talented employees to grow and enhance the business. I would like to thank the Nestl team for its partnership and assistance in transitioning the business. Brynwood has an established track record in the frozen food space. One of Brynwood Partners most successful investments was in Richelieu Foods Inc. a leading private label manufacturer of frozen pizza and salad dressings. Richelieu Foods was a portfolio company of Brynwood Partners V L.P. and was divested in 2010. We had a great deal of success with Richelieu Foods in the frozen food space in both the retail and foodservice channels Hartong continued. We hope to be able to utilize some of that experience in the Josephs investment. This acquisition marks the first for Brynwood VII Brynwood Partners largest and most recently raised fund with $400 million of committed capital. Including Josephs Brynwood Partners has now completed five acquisitions from Nestl with this being the second in 2013. Earlier in the year Brynwood Partners VI L.P.s portfolio company Pearson Candy Company acquired the iconic Bit-O-Honey brand from Nestl. Brynwood Partners has developed a unique niche of being the lower middle market firm of choice for corporate divestitures. The Firm has a well-established track record of reviving smaller brands or businesses that are divested from large corporations. In its 29-year history Brynwood Partners has now acquired 40 corporate brands from 15 different corporate sellers. Brynwood Partners has a strong investment track record in the food space with active investments including: Prior investments by Brynwood Partners in the food space include Sun Country Foods Inc. Balance Bar Company Lincoln Snacks Company and Signature Snacks Company. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in North American based lower middle market companies in the consumer sector. Brynwood Partners currently manages more than $900 million of private equity capital for its limited partners who include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:52 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=45'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Brynwood Partners VII L.P. Acquires the Juicy Juice Brand from Nestl USA Inc. GREENWICH CT July 2 2014 Brynwood Partners VII L.P. announced today that its newly formed portfolio company Harvest Hill Beverage Company has acquired the Juicy Juice business from Nestl USA. Terms and conditions of the transaction will not be disclosed. Juicy Juice an iconic 37-year-old brand enjoys national distribution through leading retailers in the food mass and club sectors as well as in the foodservice channel. Juicy Juice is widely recognized as one of the pioneer brands in the 100 percent juice category. We are delighted to announce the acquisition of the Juicy Juice business from Nestl USA said Henk Hartong III Senior Managing Partner of Brynwood Partners. Mr. Hartong continued We look forward to bringing renewed focus and attention to this great brand that has and continues to hold a special place with families over many generations. By further leveraging the brands nutritional and wellness attributes we plan to continue providing our loyal customers with its high quality great tasting products in innovative packaging formats. We thank Nestl USA for the opportunity to work with them on this transaction and look forward to a smooth transition of the business." The company which expects to announce its management team in the coming weeks will be based in Stamford CT. Brynwood Partners has completed six acquisitions from Nestl USA with this acquisition being the third in the past 14 months. Last year two transactions were completed with Nestl USA: Brynwood Partners VII L.P. acquired Josephs Gourmet Pasta Company and Pearson Candy Company a Brynwood Partners VI L.P. portfolio company acquired the Bit-O-Honey brand. The Juicy Juice acquisition marks the Firms largest buy side transaction since it was founded in 1984. Brynwood Partners has a well-established track record of acquiring and reviving brands or businesses that are divested from large corporations. In its 30-year history Brynwood Partners has acquired 41 corporate brands from 15 different corporate sellers. Brynwood Partners has a strong investment track record in the food space with active investments including: Prior investments by Brynwood Partners in the food sector include DeMets Candy Company Richelieu Foods Inc. Sun Country Foods Inc. Balance Bar Company Lincoln Snacks Company and Signature Snacks Company. About Brynwood Partners: Founded in 1984 Brynwood Partners is an operationally-focused private equity fund that makes control investments in North American based lower middle market companies in the consumer sector. Brynwood Partners currently manages more than $900 million of private equity capital for its limited partners which include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:53 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=46'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Harvest Hill Beverage Company owner of the Juicy Juice brand to Acquire American Beverage Corporation from Wessanen GREENWICH CT March 9 2015 Brynwood Partners VII L.P. announced today that its majority-owned portfolio company Harvest Hill Beverage Company has signed a definitive purchase agreement to acquire American Beverage Corporation from Wessanen for approximately $55 million. Based in Stamford CT Harvest Hill was formed in June 2014 to acquire the Juicy Juice brand from Nestl USA Inc. The ABC transaction is expected to close during the first half of 2015 and is subject to regulatory approval. Founded in 1960 ABC is a diversified beverage company that is a leading manufacturer and marketer of fruit drinks ready-to-drink alcoholic drinks and non-alcoholic cocktail mixers. The companys brands include the iconic and highly recognizable Little Hug Fruit Barrels Big Hug and Guzzler in addition to Dailys (alcoholic cocktail drinks and mixers). ABCs products are nationally distributed through the grocery drug mass warehouse club and liquor store channels. Additionally Dailys products are sold through foodservice distributors to bars and restaurants. The company which has approximately 337 employees is headquartered in Rye Brook NY and manufactures its products out of a company-owned state-of-the-art manufacturing facility in Verona PA and a leased facility in Phoenix AZ. We are pleased to announce the acquisition of American Beverage Company said Henk Hartong Chairman and CEO of Brynwood Partners and Chairman of Harvest Hill. The Little Hug brand provides Harvest Hill with an entry into the sizeable juice drink category. Mr. Hartong continued The addition of Little Hug to our retail juice portfolio enables Harvest Hill to participate in all segments of the juice aisle strengthening our position with our retail partners. We are also excited about the companys Verona PA and Phoenix AZ manufacturing facilities as they will provide Harvest Hill with additional production flexibility. The business was founded 55 years ago in Verona PA as Daily Orange Juice Company Inc. and many of its original manufacturing capabilities remain in place today. We look forward to working with ABCs loyal employee base as we add the companys brands into Harvest Hill said Sal DePrima President and CEO of Harvest Hill. This acquisition makes Harvest Hill one of the largest independently owned juice companies in the U.S. and we plan to build on the momentum that the Little Hug and Dailys business have in the marketplace. On behalf of Brynwood Partners and Harvest Hill I would like to express my sincere gratitude to the Wessanen and ABC teams for working with us on this pending transaction said Ian MacTaggart President and COO of Brynwood Partners. This marks Harvest Hills first strategic add-on acquisition since we established the company only eight months ago. A significant investment for our firm Harvest Hills acquisition of ABC highlights Brynwood Partners dedication and commitment to this investment platform as we continue to build upon a leading U.S. beverage franchise. Brynwood Partners has developed a unique niche in the corporate divestiture segment. In its 31-year history Brynwood Partners including the ABC acquisition will have acquired 45 brands from 16 different corporate sellers. This acquisition represents the third investment by Brynwood Partners VII L.P. the firms most recently raised fund. About Brynwood Partners: Brynwood Partners founded in 1984 and based in Greenwich CT is an operationally-focused private equity firm that makes control investments in North American-based lower middle market companies in the consumer sector. Brynwood Partners currently manages more than $725 million of private equity capital for its limited partners which include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About Harvest Hill Beverage Company: Harvest Hill Beverage Company based in Stamford CT was formed by Brynwood Partners VII L.P. in June 2014 to acquire the iconic Juicy Juice brand from Nestl USA Inc. Juicy Juice is the largest 100% juice brand in the U.S. focused on the kids segment. Harvest Hill currently markets Juicy Juice products in single-serve and multi-serve formats to the retail and foodservice channels. The company intends to grow both organically and through strategic add-on acquisitions. For more information on Harvest Hill please visit www.harvesthill.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:54 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2006&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:54 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2007&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/news.asp?id=9&pid=47'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['High Ridge Brands Co. Acquires Continental Fragrances Ltd. From SG Holdings Acquisition Inc. GREENWICH CT October 1 2015 High Ridge Brands Co. a portfolio company of Brynwood Partners VI L.P. announced today that it has acquired Continental Fragrances Ltd. owner of the iconic Salon Grafix hair styling brand from SG Holdings Acquisition Inc. Terms and conditions of the transaction were not disclosed. Since 1998 Continental Fragrances has been marketing the classic Salon Grafix white can brand of premium salon-quality hair styling products to consumers throughout North America. Today Continental Fragrances offers a diverse hair styling portfolio of brands that in addition to Salon Grafix includes High Beams temporary spray-on hair colors and Healthy Hair Nutrition natural hair care and styling solutions among others. The companys products are widely distributed through leading retailers in the food drug mass and beauty channels. We are excited to own Salon Grafix and the other Continental Fragrances brands said James Daniels President and CEO of High Ridge Brands Co. We look forward to integrating these brands into our organization and are excited about the potential that this acquisition brings to our company. Salon Grafixs premium positioning loyal consumer base and strong distribution makes it very complementary to High Ridge Brands current hair styling portfolio which includes the iconic Alberto VO5 White Rain Rave and L.A. Looks brands. We are thrilled to announce the acquisition of Continental Fragrances into our very successful High Ridge Brands business said Dario Margve Chairman of High Ridge Brands Co. and a Managing Director of Brynwood Partners VI L.P. The addition of the Continental Fragrances brands will enable High Ridge Brands to participate in all segments of the hair styling category strengthening our position with our retail partners. Mr. Margve continued While management continues to focus on growing the company organically we are fortunate to be adding another very attractive strategic acquisition. About Brynwood Partners: Brynwood Partners founded in 1984 and based in Greenwich Connecticut is an operationally-focused private equity firm that makes control investments in North American-based lower middle market companies in the consumer sector. Brynwood Partners currently manages over $700 million of private equity capital for its limited partners which include U.S. and international pension funds fund-of-funds endowments high net worth family investment offices and financial institutions. For more information on Brynwood Partners please visit www.brynwoodpartners.com. About High Ridge Brands Co.: High Ridge Brands Co. headquartered in Stamford Connecticut was formed by Brynwood Partners VI L.P. in January 2011 to purchase the North American Zest personal cleansing brand from The Procter & Gamble Company and to serve as a platform for further acquisitions in the personal care consumer segment. In August 2011 High Ridge Brands Co. acquired the U.S. and Puerto Rican brand and marketing rights to the Alberto VO5 brand and the worldwide marketing and brand rights to the Rave brand from Unilever. In March 2012 High Ridge Brands Co. acquired the global rights to the Coast personal cleansing brand from Henkel. In June 2012 the worldwide rights to the White Rain brand was purchased from The Sun Products Corporation. In May 2015 High Ridge Brands Co. acquired the global rights to the brands of La Bella L.A. Looks Soft & Dri Dep Thicker Fuller Hair Zero Frizz Pure & Natural Adorn Mink and The Dry Look from Newhall Laboratories Inc. For more information on High Ridge Brands Co. please visit www.highridgebrands.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:56 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2008&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:57 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2009&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:58 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2010&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:59 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2011&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:52:59 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2013&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:00 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2014&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:01 [scrapy] ERROR: Error processing {'pagetitle': [u'News - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&Year=2015&mode=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:02 [scrapy] ERROR: Error processing {'pagetitle': [u'John Earhart Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/john-earhart/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mr. Earhart co-founded GEF in 1990 serving as a founding shareholder and since then as the Chairman of the Board. Mr. Earhart primarily works on GEFs forestry investment activities. With extensive experience in international environmental policy and natural resource management Mr. Earhart assumes primary oversight of GEFs environmental analyses compliance and outreach processes. As a professional forester Mr. Earhart has served as technical advisor and helped to structure private equity investments project finance and international grants for a number of major forest management projects undertaken in the tropical Americas. Mr. Earhart is a member of several boards of directors. Prior to starting GEF Mr. Earhart served as senior fellow to World Wildlife Fund and The Conservation Foundation and as associate director of the Peace Corps in Paraguay. Mr. Earhart has a BA from California State University Sacramento and an MF from the Yale School of Forestry and Environmental Studies.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:03 [scrapy] ERROR: Error processing {'pagetitle': [u'Investment Committee Members Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/team/investment-committee-members/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['The most important factor in GEFs long-term success is our people. Each of our investment teams consists of professionals with a deep knowledge of the global markets in which they operate and in-depth sector expertise drawing on decades of operational and investment management experience. The investment team is supported by a skilled corporate team research staff and efficient administrative personnel who ensure the funds smooth day-to-day operations.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:04 [scrapy] ERROR: Error processing {'pagetitle': [u'Reports Archive Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/media-room/reports/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['AUGUST 2008 The Electricity Economy: New Opportunities from the Transformation of the Electric Power Sector']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:04 [scrapy] ERROR: Error processing {'pagetitle': [u' Fore Research & Management LP - Retrieve Password'],
 'pageurl': ['http://www.foreresearch.com/RetrievePassword.aspx'],
 'siteurl': ['foreresearch.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:04 [scrapy] ERROR: Error processing {'pagetitle': [u' Fore Research & Management LP'],
 'pageurl': ['http://www.foreresearch.com/Default.aspx'],
 'siteurl': ['foreresearch.com'],
 'text': ['Fore Research & Management is a New York-based investment advisor founded in 2003 by Matthew Li. We focus primarily on credit long/short investments with an emphasis on event driven opportunities. Our culture is firmly rooted in bottom-up fundamental research of individual credits including companies sovereigns and structured products framed within the context of the global macroeconomic environment. We prize risk discipline and seek to employ liquid instruments while being mindful of ever-changing market technicals. We have been a registered investment advisor with the U.S. Securities & Exchange Commission since February 2004. If youd like to contact us directly you can reach us at 212.984.3800. To access our research select the Research tab below. To access our client-related documents select Investors.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:05 [scrapy] ERROR: Error processing {'pagetitle': [u'Articles Archive Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/media-room/articles/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['JULY 22 2015 Jeff Leonard featured in Bloomberg Breif. (Scroll to Q&A section.) Bloomberg Brief July 22 2015 APRIL 25 2014 Alliance for Workplace Excellence GEF to Receive the EcoLeadership Award for the 6th Straight Year NOVEMBER/DECEMBER 2012 Washington Monthly How We Could Blow the Energy Boom By Jeffery Leonard MAY 31 2012 Alliance for Workplace Excellence GEF Receives the AWE EcoLeadership Award for the Fourth Straight Year SEPTEMBER 2011 EMPEA: Emerging Markets Private Equity Association Case Study: NEOgs (Brazil) and the impact of private equity in emerging markets JULY 2011 Rainforest Alliance Monte Alto: Steward of Patagonia receives FSC certification for management of its forestlands sawmill and processing facilities. MARCHAPRIL 2009 Harvard Magazine Turning Green into Gold: A private-equity firm invests in alternatives to oil around the globe OCTOBER 2008 Washington Monthly The Plug-in Revolution: A grand plan for Americas energy woes By Jeffrey Leonard']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:06 [scrapy] ERROR: Error processing {'pagetitle': [u'Videos & Podcasts Archive Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/media-room/videos/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['NOVEMBER 14 2012 PODCAST The Takeaway with John Hockenberry: Jeffrey Leonard and American Energy in the Age of the Superstorm']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:06 [scrapy] ERROR: Error processing {'pagetitle': [u'Press Releases Archive Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/media-room/press-releases/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:07 [scrapy] ERROR: Error processing {'pagetitle': [u'Shareholder Information'],
 'pageurl': ['http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1&print=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:08 [scrapy] ERROR: Error processing {'pagetitle': [u'Shareholder Information'],
 'pageurl': ['http://shareholders.fortress.com/stockinfo.aspx?KeyFndg=177083&iid=4147324&print=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['Notify me with an end-of-day stock quote. For the definition of a financial field select the field name link.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:09 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-fund-vii/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Fund VII (U.S.) L.P. and Lone Star Fund VII (Bermuda) L.P. (collectively Lone Star Fund VII) formed in December 2009 held their final closings effective July 2011 with $4.6 billion in combined capital commitments. Over its 39 month investment period Lone Star Fund VII invested substantially all of its equity capital in 32 investments comprised of 23491 assets with an aggregate purchase price of approximately $11.9 billion. Transactions consummated by Lone Star Fund VII included investments in the U.S. Western Europe and Japan in loans and securities including single family residential and corporate debt products and investments in financially oriented and real estate-rich operating companies. Lone Star Fund VII invested primarily in the U.S. residential mortgage market. Utilizing a dedicated team of professionals based in New York Lone Star Fund VII acquired in excess of $21.3 billion (face value) of mortgages and mortgage-related securities. Lone Star Fund VII also acquired opportunities abroad including European mortgage portfolios and non-performing loans and other operating companies in Japan.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:09 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-fund-vi/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Fund VI (U.S.) L.P. and Lone Star Fund VI (Bermuda) L.P. (collectively Lone Star Fund VI) formed in December 2007 held their final closings in July 2008with $7.5 billion in combined capital commitments. Over its 24 monthinvestment period Lone Star Fund VI invested substantially all of its equity capital in 28 investments comprised of 79210 assets with an aggregate purchase price of approximately $20.0 billion. Transactions consummated by Lone Star Fund VI included investments in loans and securities including single family residential commercial real estate and consumer debt products and financially oriented operating companies. Lone Star Fund VI invested primarily in the U.S. residential mortgage market as the scale of the distressed opportunity reached unprecedented levels following the failure of several prominent U.S. investment banks. Utilizing a dedicated team of professionals based in New York Lone Star Fund VI acquired in excess of $55 billion (face value) of mortgages and mortgage-related securities. The collapse of the U.S. residential market also led to opportunities abroad that included Lone Star Fund VIs acquisition of a large German financial institution.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:09 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-real-estate-fund-ii/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Real Estate Fund II (U.S.) L.P. and Lone Star Real Estate Fund II (Bermuda) L.P. (collectively Lone Star Real Estate Fund II) formed in December 2009 held their final closings in May 2011 with $5.5 billion in combined capital commitments.Over its 46 month investment period Lone Star Real Estate Fund II invested substantially all of its equity capital in 31 investments comprised of 2936 assets with an aggregate purchase price of approximately $14.4 billion. Transactions consummated by Lone Star Real Estate Fund IIincluded investments in commercial real estate debt and equity products in the U.S. Western Europe and Japan. Lone Star Real Estate Fund II invested the majority of its equity capital in the Americas and Europe (over 50% and 40% respectively) and the remainder in Japan. Investments primarily consisted of commercial real estate loan portfolios direct real estate CMBS and a commercial real estate company.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:10 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-real-estate-fund-iii/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Real Estate Fund III (U.S.) L.P. and Lone Star Real Estate Fund III (Bermuda) L.P. (collectively Lone Star Real Estate Fund III) formed in October 2013 held their final closings in October 2013 with approximately $7.0 billion in combined capital commitments. Over its 19 month investment period Lone Star Real Estate Fund III invested substantially all of its equity capital in 20 investments comprised of 933 assets with an aggregate purchase price of approximately $19.9 billion. Transactions consummated by Lone Star Real Estate Fund III included investments in commercial real estate debt and equity products in the U.S. Europe and Japan. Lone Star Real Estate Fund IIIs investment activity was focused on opportunities in Europe with approximately 90% of equity capital invested in commercial real estate transactions in Europe. Investments primarily consisted of commercial real estate loan portfolios direct real estate and commercial real estate companies across the United Kingdom France Spain Portugal Ireland the Netherlands and certain countries in Eastern Europe.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:10 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-fund-viii/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Fund VIII (U.S.) L.P. and Lone Star Fund VIII (Bermuda) L.P. (collectively Lone Star Fund VIII) formed in March 2013 held their final closings in May 2013 with $5.1 billion in combined capital commitments. Over its13 month investment period Lone Star Fund VIIIinvested substantially all of its equity capital in 21 investments comprised of47416 assets with an aggregate purchase price of approximately $17.2 billion. Transactions consummated by Lone Star Fund VIII included investments in the Americas Western Europe and Japan in loans and securities including single family residential debt and corporate debt products as well as investments in financially oriented and other operating companies. Lone Star Fund VIII invested a substantial amount of its equity capital in U.S. residential debt including the acquisition of both mortgages and mortgage-related securities from U.S. and European financial institutions. Additional investments in the Americas included corporate loans and operating companies. Acquisitions outside of the Americas included corporate and residential loans in Europe and corporate loans and an operating company in Japan.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:11 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisor'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-residential-mortgage-fund-i/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Residential Mortgage Fund I (U.S.) L.P. and Lone Star Residential Mortgage Fund I Holdings (Bermuda) L.P. (collectively Lone Star Residential Mortgage Fund I) formed in December 2014 held their first and final closings in December 2014 with $1.3 billion in combined capital commitments. Transactions targeted by Lone Star Residential Mortgage Fund I include investments in newly originated performing U.S. single family residential mortgage loans and related investments.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:11 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-real-estate-fund-iv/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Real Estate Fund IV (U.S.) L.P. and Lone Star Real Estate Fund IV (Bermuda) L.P. (collectively Lone Star Real Estate Fund IV) formed in April 2015 held their first and final closings in April 2015 with $5.8 billion in combined capital commitments. Transactions targeted by Lone Star Real Estate Fund IV include investments in commercial real estate debt and equity products in the Americas Europe and Japan.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:11 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/funds/lone-star-fund-ix/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Fund IX (U.S.) L.P. andLone Star Fund IX (Bermuda) L.P. each formed in April 2014and Lone Star Fund IX Parallel (Bermuda) L.P. formed inJune 2014(collectively Lone Star Fund IX) held their final closings in July 2014 with $7.2 billion in combined capital commitments. Transactions targeted by Lone Star Fund IX include investments in the Americas Western Europe and Japan in financial and other investment assets including single family residential debt and corporate and consumer debt products as well as investments in financially oriented and other operating companies.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:11 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001173855.PDF?Y=&O=PDF&D=&fid=1001173855&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:12 [scrapy] ERROR: Error processing {'pagetitle': [u' Sitemap : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/site-map/'],
 'siteurl': ['sciensam.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:12 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045752.PDF?Y=&O=PDF&D=&fid=1500045752&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:13 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - Current Assets Under Management'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/current-assets-under-management/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:14 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - Aggregate Purchase Price of Assets'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/aggregate-purchase-price-of-assets/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:14 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - About Lone Star'],
 'pageurl': ['http://www.hudson-advisors.com/about-lone-star/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Lone Star Funds (Lone Star) is a leading private equity firm that invests globally in real estate equity credit and other financial assets. Since the establishment of its first fund in 1995 Lone Star has organized fifteen private equity funds (the Funds) with aggregate capital commitments totaling approximately $60 billion. The Funds are structured as closed-end private-equity limited partnerships the limited partners of which include corporate and public pension funds sovereign wealth funds university endowments foundations fund of funds and high net worth individuals.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:14 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - Experienced Management'],
 'pageurl': ['http://www.hudson-advisors.com/about-us/management/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Hudson has managed over 880000 assets with an aggregate purchase price of approximately $148 billion (including acquisition financing and co-investors) across Asia greater North America and Europe which necessitates a focused proactive and hands-on approach to maximize investor returns. Senior management and key employees of Hudson possess substantial asset management experience in both the public and private investment sectors. In addition they have significant skill at managing and restructuring underperforming assets. These talented individuals with backgrounds in capital markets real estate development lending and loan workouts work together to bring creative solutions to the asset management process.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:15 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - About Hudson'],
 'pageurl': ['http://www.hudson-advisors.com/about-us/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Hudson is a globally integrated asset management company that performs due diligence and analysis asset management and other support services for Lone Star and to the assets acquired by the Funds. Formed in 1995 Hudson Advisors L.P. (formerly known as Brazos Advisors LLC) is headquartered in Dallas Texas is an investment adviser registered with the U.S. Securities and Exchange Commission and has subsidiary offices in New York Montreal San Juan London Frankfurt Luxembourg Dublin Madrid Paris Amsterdam and Tokyo. Hudson collectively employs approximately 850 professionals. Hudson has significant expertise in loan servicing and asset management providing a full range of services that include due diligence and valuation financial services and reporting income and property tax compliance currency and interest rate hedging risk management information technology development and systems support. Certain of the Hudson subsidiaries are rated primary and/or special servicer by Fitch Ratings and Standard & Poors Rating Services. Hudson has direct experience in servicing performing and sub-performing loan portfolios workouts of loan portfolios corporate restructurings/turnarounds rehabilitation and repositioning of real estate assets across a variety of property types land and real estate development and servicing a variety of debt instruments. Since the inception of Brazos Fund Hudson has provided due diligence and analysis asset management and other support services to over 880000 assets with an aggregate purchase price of approximately $148 billion (including acquisition financing and co-investors). Hudson maintains strategic oversight of specialty management firms that are owned by a Fund or Funds to service certain assets requiring specific management expertise. Read more about Specialty Management Firms.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:15 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Hudson is a globally integrated full-service asset management company providing due diligence and analysis asset management and global support services for Lone Star Funds a leading private equity firm.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:15 [scrapy] ERROR: Error processing {'pagetitle': [u' Privacy : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/privacy/'],
 'siteurl': ['sciensam.com'],
 'text': ['Protecting your privacy is important to Sciens which is why we wanted you to know:']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:16 [scrapy] ERROR: Error processing {'pagetitle': [u' Eiffel Credit Opportunities Launches On Sciens Managed Account Platform : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/2013/12/eiffel-credit-opportunities-launches-on-sciens-managed-account-platform/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 5 December 2013 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announces that Eiffel Credit Opportunities a long-short sector focused European credit fund will be replicated in a new cell on the Sciens Managed Account Platform. Please click on the link below to view the press release PRESS RELEASE Eiffel credit launches on Sciens MAP']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:16 [scrapy] ERROR: Error processing {'pagetitle': [u' PVE Capital Latest Credit Strategy To Launch On The Sciens Managed Account Platform : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/2014/10/pve-capital-latest-credit-strategy-to-launch-on-the-sciens-managed-account-platform/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 1 October 2014 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announced that London-based PVE Capital has launched its Special Situation Credit Strategy on the Sciens Managed Account Platform. Please click on the link below to view the press release PRESS RELEASE PVE Capital launches on Sciens MAP']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:17 [scrapy] ERROR: Error processing {'pagetitle': [u' Sciens and PVE announce Italian NPL Fund : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/2015/04/sciens-and-pve-announce-italian-npl-fund/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 13 April 2015 Sciens Alternative Investments and PVE Capital today jointly announced the launch of the PVE European Distressed Fund I a closed ended fund that has invested in a portfolio of Italian Non-Performing Loans (NPLs) with a gross book value of 408m primarily composed of secured loans backed by Italian real estate. The fund closed earlier this year after reaching full capacity. Please click on the link below to view the press release PRESS RELEASE Sciens and PVE announce Italian NPL Fund']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:17 [scrapy] ERROR: Error processing {'pagetitle': [u' Return or position-based value at risk? : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/2014/09/return-or-position-based-value-at-risk/'],
 'siteurl': ['sciensam.com'],
 'text': ['The emergence of managed account platforms for investing in hedge funds has given the investor access to a range of new risk measures. This study published in the International Journal of Portfolio Analysis and Management (2014 Vol. 1) analyses the accuracy of various value at risk (VaR) methodologies in the context of hedge fund investing. In our sample of data we found that position-based VaR provides the best risk assessment measure for short horizons (up to five days). For longer term horizons (five days or more) position-based VaR can sometimes be inaccurate. Our interpretation is that despite being based on actual underlying positions position-based VaR is not designed to capture the dynamic of hedge fund portfolios. Over longer term horizons VaR calculated using historic returns still provides a rough but reliable risk indicator for the investor. Our study also highlights the importance of regular monitoring of any risk models accuracy in its specific context. Available from the publishers website: http://www.inderscience.com/offer.php?id=64380']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:17 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500064360.PDF?Y=&O=PDF&D=&FID=1500064360&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:18 [scrapy] ERROR: Error processing {'pagetitle': [u' Steamboat Capital Partners Launches on The Sciens Managed Account Platform : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/2015/07/steamboat-capital-partners-launches-on-the-sciens-managed-account-platform/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 06 July 2015 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single and multi-strategy funds of hedge funds and managed account services today announced that New York based Steamboat Capital Partners has launched its event driven long short strategy on the Sciens Managed Account Platform. Please click on the link below to view the press release PRESS RELEASE Steamboat Capital Partners launches on Sciens MAP']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:18 [scrapy] ERROR: Error processing {'pagetitle': [u' Product Enquiry : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/product-enquiry/'],
 'siteurl': ['sciensam.com'],
 'text': ['Please fill out and submit this form for enquiries.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:19 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001191620.PDF?Y=&O=PDF&D=&FID=1001191620&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:19 [scrapy] ERROR: Error processing {'pagetitle': [u'Information Request - Investor Kit by Mail - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/inforequest.aspx?iid=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['Please enter the verification text shown below This form submits information via e-mail which is inherently insecure. Please do not include ANY personal information that you do not wish to be shared with others.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500046848.PDF?Y=&O=PDF&D=&fid=1500046848&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:20 [scrapy] ERROR: Error processing {'pagetitle': [u'Shareholder Information'],
 'pageurl': ['http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324&print=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': [' Please click on the links below to save to your computer Annual Report 10-Q 10-K Latest Earnings Results Proxy Statement To download full investor kit please click here . To receive printed material by mail please click here .']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:20 [scrapy] INFO: Crawled 1686 pages (at 135 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:53:21 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/contact.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['Chicago 30 South Wacker Drive Suite 2750 Chicago IL 60606 View Map / Directions Questions? inquiries@laramargroup.com Denver 7900 E Union Ave Suite 500 Denver CO 80237 View Map / Directions See our communities web site www.laramargroup.com']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:21 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/executive_team.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Mr. Elowe founded The Laramar Group and co-founded Elkor Realty Corporation and Elkor Properties. Mr. Elowe also co-founded Elkor Commercial Properties which invested primarily in hotels and office buildings. Mr. Elowe is Chairman of the Investment Committee and is responsible for the companys overall strategic planning investments asset management and investor relations. Mr. Elowe has built a successful 20-year real estate investment track record having invested approximately $500 million of equity in over 100 assets in transactions totaling nearly $2 billion. In addition Mr. Elowe is on the Board of Directors of the National Multi-Housing Council (NMHC) is a member of the Urban Land Institute (ULI) and is a member of Young Presidents Organization (YPO). Mr. Elowe has a B.S. degree from the University of Illinois and an M.D. degree from Northwestern University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:21 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_studies.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:22 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/portfolio.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['To view our portfolio please click on the alphabetical category to the left.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:22 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/news.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Laramar Group Acquires Oceanview Waterfront Multifamily Property in Boca Raton Read the Story Laramar Group Opens California Office With Largest Bay Area Apartment Acquisition of 2008 Read the Story Laramar Group Acquires 543-Unit Property in Suburban Seattle Read the Story Laramar Group Concludes First Year of Multi-Family Value Fund - Acquisition of Houston and Washington DC Properties Read the Story Laramar Group Acquires Snell Isle Apartments in Pinellas County FL Read the Story']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:23 [scrapy] ERROR: Error processing {'pagetitle': [u'Shareholder Information'],
 'pageurl': ['http://shareholders.fortress.com/GenPage.aspx?GKP=208605&IID=4147324&print=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['Fortress Investment Group LLC (NYSE:FIG) is a publicly traded partnership. We are generally not subject to federal or state income tax. Instead each partner is required to report an allocable share of our items of income gain loss deduction or tax credit in the partners income tax return. 2014 Tax Information for FIG Unitholders Tax Support: (866) 526-0125 Please note that to access your Tax Package Support information you will be leaving fortress.com and entering a website of a third party provided only as an informational source. This site is not intended to supplement or to substitute for independent tax or legal advice. Any federal tax advice contained in this site (including in any attachments or in any other source accessed through links provided on this site) is not intended to be used and cannot be used to avoid penalties under the Internal Revenue Code or to promote market or recommend to another party any tax related matters addressed herein. You are urged to consult with your own legal and tax advisors for advice based on your particular circumstances prior to taking any action related to any of the information provided herein.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:23 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001194890.PDF?Y=&O=PDF&D=&FID=1001194890&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:23 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500068964.PDF?Y=&O=PDF&D=&FID=1500068964&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:24 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/company.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['Laramar with corporate headquarters in Chicago Illinois and property management headquarters in Denver Colorado is a fully integrated real estate investment and management company with 12 regional and operating offices and over 500 employees across the United States. Since 1989 Laramar and its predecessor have invested in transactions throughout the United States representing total value in excess of $2.6 billion.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:24 [scrapy] ERROR: Error processing {'pagetitle': [u' Contact : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/contact/'],
 'siteurl': ['sciensam.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:25 [scrapy] ERROR: Error processing {'pagetitle': [u'Doug May Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/doug-may/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Doug May is an Operating Partner at GEF and works closely with the investment teams contributing his management and sales expertise to the firms portfolio companies. Mr. May was the President and CEO of Unirac one of GEFs most successful portfolio companies. Under Mr. Mays leadership from 20072011 Unirac grew rapidly gaining significant market share and leading the industry with innovative products. Mr. May placed a strong focus on Uniracs customers employees and distribution channels allowing Unirac to become an industry leader. Mr. May has over 30 years of experience domestically and internationally in high tech and renewable energy. Prior to Unirac Mr. May was the VP Worldwide Sales Service and Marketing at Xantrex a leader in the renewable energy consumer electronics and power products markets. Before Xantrex Mr. May held senior positions at Fujitsu and Autodesk. A driven results-oriented executive with strong sales and marketing experience Mr. May has a unique perspective on channels of distribution web based tools and digital applications. Mr. May has a BS in Business Administration from the University of Southern California.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:25 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500068088.PDF?Y=&O=PDF&D=&FID=1500068088&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:26 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001194328.PDF?Y=&O=PDF&D=&FID=1001194328&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:27 [scrapy] ERROR: Error processing {'pagetitle': [u'Financial Reports & Filings - Quarterly and Annual Results - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/GenPage.aspx?GKP=1073750304&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:28 [scrapy] ERROR: Error processing {'pagetitle': [u'Public Shareholders - IR Overview - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/corporateprofile.aspx?iid=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:28 [scrapy] ERROR: Error processing {'pagetitle': [u'Del Mar Asset Management'],
 'pageurl': ['http://www.delmarasset.com/about'],
 'siteurl': ['delmarasset.com'],
 'text': ['Founded in 2005 Del Mar Asset Management LP is a global investment management firm providing sophisticated investors with alternative investment solutions across equity fixed income and derivative asset classes. Our client base comprises institutional money managers insurance companies high net worth advisors pension funds and family offices. Del Mar is registered with the US Securities & Exchange Commission as an investment advisor under the Investment Advisers Act of 1940 (1).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:28 [scrapy] ERROR: Error processing {'pagetitle': [u'Del Mar Asset Management'],
 'pageurl': ['http://www.delmarasset.com/contact-us'],
 'siteurl': ['delmarasset.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:28 [scrapy] ERROR: Error processing {'pagetitle': [u'Del Mar Asset Management'],
 'pageurl': ['http://www.delmarasset.com/home'],
 'siteurl': ['delmarasset.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:28 [scrapy] ERROR: Error processing {'pagetitle': [u'Del Mar Asset Management'],
 'pageurl': ['http://www.delmarasset.com/'],
 'siteurl': ['delmarasset.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:29 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/index.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['The Laramar Groups mission is to provide exceptional apartment communities while delivering extraordinary returns to our investors.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:29 [scrapy] ERROR: Error processing {'pagetitle': [u' Sciens Offices : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/offices/'],
 'siteurl': ['sciensam.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:30 [scrapy] ERROR: Error processing {'pagetitle': [u' Investment Services : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/investment-services/'],
 'siteurl': ['sciensam.com'],
 'text': ['Sciens Alternative Investments (SAI) offers a range of best-in-class products & services the cornerstone of which is our independent Managed Account Platform (MAP). Our MAP provides clients with the requisite degree of control protection and transparency that is increasingly being demanded by clients looking to invest in alternative assets. In addition to the protected cell company structure Sciens MAP also furnishes clients with the ability to decompose and disaggregate the risk for each individual strategy and also across multiple funds according to analyse their aggregate exposure. It does this via SMART our recently launched state-of-the-art risk analytics engine. Both within its MAP offering and also as a broader mission statement SAI seeks to identify where viable to enfranchise and then to deliver to market a diversified array of high quality and largely uncorrelated investment strategies. For a select few Funds SAI will also look at raising money for direct investment into the strategy alongside our asset raising efforts for the Managed Account cell. In addition to its MAP offering SAI also offers a premier Portfolio Advisory service which has been serving and successfully fulfilling the needs of Alternatives Investors for the past 11 years. SAI is also home to a substantial FoHF business within which Sciens seeks to offer to clients a select number of robust & risk-weighted aggregate investment exposures. Sciens is fully committed in all its endeavours to serving the best interests of our clients by identifying and delivering to market the best-of-the-best Alternative Investment Strategies and with full transparency at the heart of everything we do and all that we offer.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:30 [scrapy] ERROR: Error processing {'pagetitle': [u' Fund of Hedge Funds : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/hedge-funds/fund-of-funds/'],
 'siteurl': ['sciensam.com'],
 'text': ['Since 1994 Sciens has been investing in alternative strategies and managing portfolios of hedge funds. We have developed a consistent and disciplined investment process combining fundamental top-down strategic asset allocation and in-depth quantitative and qualitative bottom-up manager selection. Sciens believes in strong risk management at all stages of the portfolio construction. We have developed proprietary state-of-the-art portfolio and risk management and monitoring tools. Understanding investors needs for liquidity transparency and an optimum balance between return and risk our innovative investment solutions comprise funds of funds and funds of managed accounts with a multi-strategy or thematic approach. Our robust institutional approach to portfolio management has been consistently recognised since 2003 by Fitch who credited us with a High Standards rating in August 2014. Our successful track record is regularly acknowledged by our peers with several funds shortlisted by InvestHedge for their annual global awards.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:30 [scrapy] ERROR: Error processing {'pagetitle': [u' Managed Account Platform : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/hedge-funds/managed-account-platform/'],
 'siteurl': ['sciensam.com'],
 'text': ['Sciens provides flexible and integrated hedge fund managed account solutions to investors and hedge fund managers. Sciens managed account platform is independent from any prime broker administrator trading counterparty or large financial institution thus mitigating the risk of any conflict of interest. Our open architecture allows cooperation with numerous counterparties when establishing managed accounts. Sciens teams with investors and hedge fund managers to on-board new managed accounts. We focus on providing clients with all the benefits of hedge fund managed account investing: separate ownership transparency liquidity and risk management. We focus on institutional standards. Safeguarding of assets is key and constant monitoring of risk limits is essential. To support our approach we have developed a proprietary risk monitoring system that provides state-of-the-art and flexible risk analytics and reporting. S.M.A.R.T. is a unique interactive look-through engine that allows the investor to perform in-depth analysis of: It is designed to enable clients to leverage our proprietary portfolio management analysis technology for the evaluation of their respective portfolios and aggregated fund exposures. S.M.A.R.T. complements the standard risk reports and data which are sent to investors on a daily basis. Please click on the link below to access S.M.A.R.T. S.M.A.R.T.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:31 [scrapy] ERROR: Error processing {'pagetitle': [u' Advisory Solutions : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/advisory-services-2/'],
 'siteurl': ['sciensam.com'],
 'text': ['As financial markets become more challenging and the number of alternative strategies and managers increases Sciens understands the need of investors for tailored investment solutions and services. Sciens advisory team combines the firms hedge fund research capabilities its experience in liquid and illiquid investment strategies and its institutional portfolio management processes to develop customised investment solutions. We focus on investor requirements such as risk and return targets investment restrictions including liquidity constraints and exposure to defined market factors. Sciens works closely with investors at all stages of the portfolio construction and implementation process with regular reporting on manager selection performance asset allocation optimisation and the general investment environment. Leveraging on its long term experience in alternative asset class investment Sciens offers a range of services to help investors take informed alternative investment decisions. Our performance analytics and risk management infrastructure provides flexible reporting on multi-asset multi-strategy multi-provider portfolios with full customisation to investors requirements.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:31 [scrapy] ERROR: Error processing {'pagetitle': [u' Sciens Alternative Investments : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/hedge-funds/'],
 'siteurl': ['sciensam.com'],
 'text': ['At Sciens we are committed to fully understanding our clients needs and challenges before offering them the most suitable bespoke investment solutions. This is accomplished by offering tailored solutions that:']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:32 [scrapy] ERROR: Error processing {'pagetitle': [u'Current Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/category/portfolio/current/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Concord is one of the largest providers of industrial water solutions in India with products that address waste water treatment re-cycling and re-use and modular desalination applications worldwide. The Company is a pioneer in the development of membrane-based filtration solutions used successfully by over 300 customers in a variety of industries.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:33 [scrapy] ERROR: Error processing {'pagetitle': [u'Justin Heyman Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/justin-heyman/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mr. Heyman joined GEF in 2011 and works across GEF sectors with the firms prospective and existing U.S. investments. Prior to joining GEF Mr. Heyman was a Senior Associate at Oliver Wyman a global management consulting firm where he was responsible for advising a variety of companies on strategy and operations issues. While at Oliver Wyman he had significant experience in the private equity energy and clean technology industries. Previously at Investor Growth Capital Mr. Heyman identified and evaluated clean technology investment opportunities. Mr. Heyman has a BS in Economics from The Wharton School at the University of Pennsylvania and an MBA from Columbia Business School.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:34 [scrapy] ERROR: Error processing {'pagetitle': [u'John Fox Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/john-fox/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['John Fox is an Operating Partner at GEF and works closely with the investment teams contributing his expertise to the firms portfolio companies. Previously Mr. Fox was part of the firms U.S. investment team bringing with him over 30 years of operational and investment experience in the energy and utility industries. Prior to joining GEF Mr. Fox was a Senior Managing Director at Perseus L.L.C. leading the firms private equity transactions in the energy sector for 14 years. Previously he was Chief Operating Officer for Ontario Power one of North Americas largest electric utilities where he was responsible for generation transmission and distribution. From 1992 to 1993 Mr. Fox served as Executive Vice President of the Canyon Group a Los Angeles-based marketing and management consultancy. He also held various management positions with Pacific Gas and Electric Company (PG&E) in San Francisco from 1981 to 1992 drawing from an early foundation of work in the utility manufacturing and construction industries. Mr. Fox holds a BS in Civil Engineering from the University of Toronto and an MBA from McMaster University in Hamilton Ontario.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:35 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Drawbridge Special Opportunities Named \u201cBest Credit Fund\u201d at HFMWeek\u2019s 2015 US Performance Awards'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=31616972&IID=4147324&printable=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- For a third consecutive year funds managed by Fortress Investment Group LLC (NYSE:FIG) were recognized at HFMWeeks US Hedge Fund Performance Awards. Fortresss Drawbridge Special Opportunities Funds (DBSO) received top honors as the industrys Best Credit Fund with over $1 billion in assets under management. In both 2013 and 2014 DBSO won HFMWeeks Single Manager Long-Term Performance Overall award for five-year performance. Winners were chosen by senior-level industry representatives who evaluated a combination of quantitative and qualitative information including annual performance as well as volatility risk management and track record. With $6 billion under management at June 30 2015 DBSO is focused on making highly diversified investments globally in undervalued and distressed assets including loans assets and corporate securities. The firms flagship Drawbridge Special Opportunities Fund LP recorded inception-to-date and 5-year annualized net returns of 11.1% and 14.3% respectively through August 2015. Fortress Investment Group LLC is a leading highly diversified global investment firm with $72.0 billion in assets under management as of June 30 2015. Founded in 1998 Fortress manages assets on behalf of approximately 1700 institutional clients and private investors worldwide across a range of private equity credit liquid hedge funds and traditional asset management strategies. Fortress is publicly traded on the New York Stock Exchange (NYSE: FIG). For additional information please visit www.fortress.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:35 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500070799.PDF?Y=&O=PDF&D=&FID=1500070799&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:36 [scrapy] ERROR: Error processing {'pagetitle': [u'Lorenzo Bared Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/lorenzo-bared/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Lorenzo Bared joined GEF in 2012. He works across GEF sectors with the firms existing and prospective investments in the United States. Prior to joining GEF Mr. Bared was part of the business development and finance groups at UniversalPegasus International a global oil and gas engineering company where he was responsible for providing analysis and strategy recommendations to senior management. While at UniversalPegasus Mr. Bared had significant exposure to the utilities sector. Mr. Bared has a BA in International Studies and American History from the University of North Carolina at Chapel Hill and a MS in Global Finance and Business at Georgetown University where he was a recipient of the Karl F. Landegger International Business Diplomacy honors certificate.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:37 [scrapy] ERROR: Error processing {'pagetitle': [u'Lisa Schule Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/lisa-schule/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Lisa Schule joined GEF in 2005 and leads GEFs investment activities in the U.S. She is responsible for technology and service related investments particularly those involving energy technologies utilizing over twelve years of experience in the energy industry. Prior to joining GEF Ms. Schule was a Vice President at Perseus L.L.C. a private equity investment firm where she specialized in energy technology investments. She was responsible for all aspects of the direct investment process including sourcing opportunities negotiating and structuring transactions and developing strategic plans and exit opportunities for portfolio companies. Ms. Schule served on the boards of directors of several Perseus portfolio companies and focused extensively on portfolio company operations. Prior to Perseus Ms. Schule worked at PECO Energy a Philadelphia-based utility where she was a member of the management team for PECO Energys wholesale power marketing division. Ms. Schule has BA magna cum laude from Amherst College and a JD from Georgetown University School of Law.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:37 [scrapy] ERROR: Error processing {'pagetitle': [u' Real Assets : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/real-assets/'],
 'siteurl': ['sciensam.com'],
 'text': ['In 2007 Sciens acquired 50% of Apollo Aviation Holdings Ltd. a specialist manager in the aviation sector. As of 2013 Apollo is a leading manager trader and operating lessor of commercial aircraft assets. Apollo is headquartered in Miami and manages a fleet of over 100 aircraft primarily Boeing and Airbus. Aviation investment products include: Please click on the image below to navigate to the website In 2013 Sciens formed GSMI Management Co. Ltd. an asset management company with Golden Union Shipping Co. S.A. a leading operator owner and manager of dry bulk assets. Golden Union is based in Piraeus Greece and manages a fleet of over 50 dry bulk vessels. Shipping related products include: Sciens has been investing in coloured diamonds since forming Sciens Diamond Management BV an asset management company in 2010 with AMMA Asset Management BV. Diamonds in red green pink and blue intense or vivid colours are assets of exceptional rarity and value. The principals of AMMA have had an active presence in trading coloured diamonds for over 20 years and have pioneered the concept of creating investment products to invest in this asset class. Coloured Diamond investment products include: Sciens conducts its real estate investment activities through Diolkos Real Estate Management S.A. in Europe and Sciens Real Estate Management LLC in the United States. Sciens invests in various types of real estate projects including residential commercial retail and industrial.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:38 [scrapy] ERROR: Error processing {'pagetitle': [u'Exited Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/category/portfolio/exited/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mozwood is a 25-year concession approved in 2009 which combined with the Pemba Sun concession consists of 160000 hectares of Eastern Miombo Woodlands in Mozambique. Together the concessions have in excess of 200000 m3 commercialized hardwood volumes and approximately 280000 m3 of species that can be commercialized.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:39 [scrapy] ERROR: Error processing {'pagetitle': [u' Login Page'],
 'pageurl': ['https://reports.sciensam.com/cp/login.aspx?sm=2'],
 'siteurl': ['reports.sciensam.com'],
 'text': [' To obtain a password please register by To obtain a password please register by clicking here This section of the document portal requires a password and the information herein is intended solely for the recipient of the password. Neither the materials nor the password may be transmitted (in any form) to any other person without the consent of Sciens.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:40 [scrapy] ERROR: Error processing {'pagetitle': [u' Login Page'],
 'pageurl': ['https://reports.sciensam.com/cp/login.aspx?sm=1'],
 'siteurl': ['reports.sciensam.com'],
 'text': [' Access to the Sciens Information Portal is password protected. To obtain a password please register by To obtain a password please register by clicking here This section of the document portal requires a password and the information herein is intended solely for the recipient of the password. Neither the materials nor the password may be transmitted (in any form) to any other person without the consent of Sciens.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:41 [scrapy] ERROR: Error processing {'pagetitle': [u'Alexandre Alvim Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/alexandre-alvim/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Alexandre Alvim joined GEF in 2015 as a Managing Director based in So Paulo Brazil. He is a member of the Latin America investment team. Mr. Alvim has over 15 years of experience as an investor board member and executive with various private equity-backed portfolio companies and for the past eight years his work has focused on the environmental services and renewable energy sectors. Prior to GEF Mr. Alvim was Executive Director of Energy and Business Development with Estre Ambiental Brazils largest environmental services company where he led its waste-to-energy business. Prior to Estre Ambiental he co-founded and served as Managing Partner of Greentech Capital an investment and advisory firm in the clean tech and renewable energy sectors. Mr. Alvim holds a BS in Electrical Engineering from Universidade Estadual de Campinas (UNICAMP) and an MBA from the Kellogg School of Management at Northwestern University. He has also completed the CEAG course at Fundao Getlio Vargas.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:41 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500070798.PDF?Y=&O=PDF&D=&FID=1500070798&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:41 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/news_archive.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:42 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/portfolio.php?section=3'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['To view our portfolio please click on the alphabetical category to the left.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:42 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/portfolio.php?section=2'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['To view our portfolio please click on the alphabetical category to the left.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:42 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/portfolio.php?section=1'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['To view our portfolio please click on the alphabetical category to the left.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:43 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=9'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Add much-needed institutional amenities to a large property in order to compete in the market Expand property amenities through construction of pool leasing center and commercial center Renovate all units (60% were vacant at acquisition) and upgrade kitchen and baths for greater market appeal Implement tax-exempt financing strategy in three separate rounds to enhance market position and improve capital structure']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:43 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=12'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Acquire a high-quality property in an off-market transaction sourced directly from a motivated seller Capitalize on a large well located asset in a strong apartment market Execute a strategic renovation of unit interiors including upgrades to kitchens and bathrooms and the addition of washer/dryers Implement Premium Pricing rental structure to capitalize on unit location unit amenities and views Upgrade curb appeal with a redesigned leasing and amenity center and building exterior improvements']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:44 [scrapy] ERROR: Error processing {'pagetitle': [u' Expertise : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/private-equity/expertise/'],
 'siteurl': ['sciensam.com'],
 'text': ['Sciens is an active private equity investor that partners with management teams to implement corporate strategies and establish successful enduring companies. We engage in the formulation and execution of product corporate and financial strategies as well as in management selection and development. Sciens focuses on industries experiencing strategic or technological change where the implementation of these strategies can lead to changes in industry structure and market share. Sciens has developed a turnaround expertise and focus on stressed and distressed investing with proprietary asset recovery capabilities in corporate real estate transportation and liquid and illiquid hedge fund assets. Our primary objective is to invest in distressed asset transactions where we or our affiliates and co-investors can play a leading role in unlocking hidden value. Distressed transactions customarily have a medium to long term horizon and duration.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:44 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/asset_management.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['Laramar focuses on each line item of a propertys income statement to identify value creation opportunities. The following illustrates some of the ways in which Laramar implements this approach: Active Revenue Management Laramars revenue management approach empowers on-site and regional leaders to adjust rents. These decisions are based on unit availability premiums and location. Capitalize on Preference Pricing Laramar believes that "if theres a preference theres a premium." When acquiring a new property Laramar completes a premium pricing analysis and accordingly adjusts market pricing for each apartment home to reflect any special attributes or amenities. Strategic Lease Expiration Management Laramar has developed a proprietary Lease Expiration Management program. LEM establishes lease lengths to match supply and demand characteristics for specific unit types in any given month. Mitigate Credit Loss Every prospective resident is quickly screened through a third-party company via the Internet. Consequently Laramar is able to approve (or deny) residents within minutes and execute a lease immediately. Creatively Find Additional Income Laramar negotiates revenue share agreements with various service providers such as cable phone internet laundry and vending. New concepts are tested and if successful are implemented throughout the Laramar portfolio. Seek Reimbursement of Utilities Upon acquisition Laramar investigates the viability of utility expense recovery programs and implements where appropriate. Customer Service Approach to Repairs and Maintenance Laramar strives to complete resident service requests within 24 hours. If circumstances prevent completion residents are informed of the expected timing of the repair. Utilize Economies of Scale for Purchasing Laramar has developed a cost effective national purchasing program. Laramar has also developed an inventory management program that analyzes its purchasing patterns by property region and Company. Actively Pursue Real Estate Tax Reductions Laramar actively manages its Real Estate tax expense by working with local and regional real estate tax appeal firms. Efficient and Measurable Marketing Efforts Laramar constantly tracks all prospective resident calls to determine which marketing sources are most productive. This strategy enables the Company to focus on property-level marketing resources to maximize cost effectiveness.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:44 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500047355.PDF?Y=&O=PDF&D=&FID=1500047355&T=&IID=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:44 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001175174.PDF?Y=&O=PDF&D=&fid=1001175174&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:44 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001177665.PDF?Y=&O=PDF&D=&fid=1001177665&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:45 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/investment_strategy.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['The Laramar Group has an established strategy of pursuing value-add investments in multi-family real estate assets on a national basis. Specifically Laramar seeks to generate superior risk-adjusted returns by investing in discounted assets management turnaround opportunities complex financial/operational situations and assets in need of strategic renovation or rehabilitation. Laramar and its predecessor have demonstrated the ability to generate attractive returns for their investors through a disciplined approach to acquiring managing and strategically selling multi-family assets. Laramars exclusive focus extensive expertise and standing relationships in the multi-family sector provide an advantage through their superior market knowledge deal flow and execution. Pursue Discounted Assets Laramar focuses on sourcing and acquiring properties at a discount to market value or replacement cost. Laramar pursues market transactions primarily through an extensive network of relationships. Seek Management Turnarounds Ownership of multi-family properties is fragmented ranging from small individual entrepreneurs who own a single property to large publicly-held REITs that own hundreds of properties. Given the diversity of ownership Laramar has historically found and continues to find assets that are mismanaged. Strategically Renovate/Rehabilitate Laramar identifies properties that are lacking amenities have poor curb appeal or suffer from deferred maintenance. Laramar seeks to drive NOI growth through its disciplined approach to capital investment. Solve Complex Financial and Operational Situations Laramar targets transactions characterized as complex due to size capital structure physical conditions community dynamics or tax considerations. Such complex situations although difficult to categorize are typically multi-disciplined and multi-phased in execution.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:45 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=5'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Acquire a high-quality property from a motivated seller in a premier multi-family core market of San Francisco Navigate multiple city agencies to complete the transaction and renovation Significant construction defects existed which had negatively impacted occupancy and reputation Complete substantial repairs to units and exterior including replacement of all windows replacement of drywall in 75% of the units and redesign of all common areas Repair parking structure (1200 spaces) to allow for resident parking Reduce costs through utility savings systems and negotiations with maintenance union']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:46 [scrapy] ERROR: Error processing {'pagetitle': [u' Private Equity : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/private-equity/'],
 'siteurl': ['sciensam.com'],
 'text': ['Sciens has been active in private equity since 1994 and has invested in over 50 direct private equity transactions. Sciens long history of private equity investment provides in-depth knowledge and a network of contacts that have been a catalyst in the growth and increase in value of portfolio companies. Sciens is an opportunistic investor and focuses on middle market buyouts growth buyouts and distressed and turnaround opportunities in North America and Europe. Sciens is an active investor generally seeking to take majority control or lead investor positions with significant representation on the board of directors and all board committees. Sciens invests in a variety of industries and has historically focused on investments in defence and security solar energy financial services information technology luxury goods and the manufacturing sectors.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:46 [scrapy] ERROR: Error processing {'pagetitle': [u' Asset Classes : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/'],
 'siteurl': ['sciensam.com'],
 'text': ['Dedicated teams of specialist professionals manage each product and strategy. Sciens utilises its expertise in different alternative strategies to offer integrated and tailored solutions to institutional and private investors.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:46 [scrapy] ERROR: Error processing {'pagetitle': [u' History of the Company : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/history/'],
 'siteurl': ['sciensam.com'],
 'text': ['Founded in 1988 Sciens has been successfully managing alternative assets since 1994. Originally focusing on private equity venture capital and turnaround investments over time Sciens added real estate and funds of hedge funds. In 2007 Sciens entered aviation asset management through a joint venture with Apollo Aviation Group. In 2008 Sciens acquired Atlas Capital a well established London based funds of hedge funds business that had been investing in hedge funds since 1994.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:47 [scrapy] ERROR: Error processing {'pagetitle': [u' Our Philosophy : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/company/why-sciens/'],
 'siteurl': ['sciensam.com'],
 'text': ['We believe that Investment opportunities exist in all market environments. Sciens is a fully independent investment manager with majority ownership by its management. Through its private equity and hedge fund capabilities Sciens has an overview of all markets and is therefore able to identify and capture emerging opportunities by investing early in the cycle. Sciens can also create innovative investment solutions combining private equity and hedge funds. Sciens provides investment solutions through customized portfolios and through Sciens Funds that allow: Sciens has a lengthy track record in all alternative investment classes since 1994. Since inception Sciens has offered institutional investors high net worth individuals and family offices a variety of investment product solutions including private equity venture capital distressed aviation and real estate opportunities and funds of hedge funds. Sciens has developed its own proprietary technology and has the infrastructure to enable an effective decision making process controlled implementation and robust risk management for alternative portfolios managed accounts and traditional assets. These technologies allow us to develop state-of-the-art and flexible risk reporting tailored to our investors needs.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:47 [scrapy] ERROR: Error processing {'pagetitle': [u' Sciens : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/news/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 06 July 2015 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single and multi-strategy funds of hedge funds and managed account services today announced that New York based Steamboat Capital Partners has launched its event driven long short strategy on the Sciens Managed Account Platform. Please [] LONDON NEW YORK 13 April 2015 Sciens Alternative Investments and PVE Capital today jointly announced the launch of the PVE European Distressed Fund I a closed ended fund that has invested in a portfolio of Italian Non-Performing Loans (NPLs) with a gross book value of 408m primarily composed of secured loans backed by Italian [] LONDON NEW YORK 1 October 2014 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announced that London-based PVE Capital has launched its Special Situation Credit Strategy on the Sciens Managed Account Platform. Please click on the link [] The emergence of managed account platforms for investing in hedge funds has given the investor access to a range of new risk measures. This study published in the International Journal of Portfolio Analysis and Management (2014 Vol. 1) analyses the accuracy of various value at risk (VaR) methodologies in the context of hedge fund investing. []']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:48 [scrapy] ERROR: Error processing {'pagetitle': [u' Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/'],
 'siteurl': ['sciensam.com'],
 'text': ['Sciens Capital Management is an independent alternative investment management firm. We focus on providing bespoke and integrated investment solutions to institutional and private clients globally. Covering a large spectrum of alternative strategies Sciens offers investment solutions throughout market and economic cycles. Sciens staff consists of 118 professionals covering both liquid and illiquid investment strategies. We have affiliated entities regulated in multiple jurisdictions and major financial centres: SEC (U.S.A.) FCA (U.K.) GFSC (Guernsey) CIMA (Cayman Islands).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:48 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=4'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Improve the propertys amenities and curb-appeal to enhance its position in the marketplace Capitalize on the bayside location with a significant renovation to the clubhouse and amenity center including a "resort-like" pool deck and beach features Repair deferred maintenance including major roof reconstruction and upgrades of kitchens and bathrooms']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:49 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=3'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:49 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=8'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Acquire a well-located property at a significant discount due to the complexities associated with purchasing from the U.S. government Install new management to improve market reputation and performance Create an institutional quality asset by improving units and apartment community center Improve building and mechanical systems for higher efficiency and functionality Build institutional grade amenity center on excess land purchased with the property which at the time was a novel approach to the Long Island apartment market']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:49 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=11'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Implement a strategic and thorough renovation including unit interior improvements exterior faade and roof repairs and landscaping upgrades Enhance the propertys appeal by adding creative amenities upgrading existing amenities and creating a new leasing and amenity center Investigate the opportunity to add desirable boat slips to the property']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:50 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=7'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Improve on-site management and implement proprietary programs and procedures such as revenue management and lease expiration management Upgrade units with new kitchens baths carpets as well as electrical infrastructure and HVAC Improve amenities and property programs with added resident services resulting in greater market appeal']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:50 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500058431.PDF?Y=&O=PDF&D=&fid=1500058431&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:51 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/case_study.php?id=6'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Opportunity Implement Premium Pricing Rental structure to capitalize on unit location and views']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:51 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/executive_team.php?obj_id=21'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Scott joined the Laramar Group in 2004 as the Accounting Manager for the third party fee management portfolio. Scott spent the first two years of his tenure forming the accounting team for this portfolio and developing policies and procedures to enable Laramar to provide back office support for a wide variety of fee management clients. Following this Scott became the Controller for the Property Accounting division at Laramar. In this role Scott managed the entire Property Accounting staff and monitored the production of all fee managed property financial statements as well as all Laramar property financial statements. Last year Scott was promoted to Controller of the Laramar Group. This new position has expanded Scotts duties to include the supervision of both the Property Accounting department as well as the Laramar Group Corporate Accounting department. Prior to Laramar Scott spent several years at ABC Supply one of the worlds largest building products distributors. At ABC Supply Scott had several positions including Internal Audit and Accounting Manager. Scott has a B.S degree in Finance with an emphasis in Accounting from the University of Northern Iowa.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:51 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/investment_process.php'],
 'siteurl': ['laramarinvestor.com'],
 'text': ['Laramar has established an investment process that emphasizes detailed due diligence and senior management involvement in every investment decision. Laramar incorporates teamwork control risk management oversight and efficient information management. The following are key steps of Laramars investment process: Creative Transaction Sourcing Laramars market and submarket knowledge as well as its extensive management presence in multiple locations enables it to identify off-market opportunities. The Company utilizes its broad-based management team to access local investments and provide property specific information to evaluate potential acquisitions. Execute Thorough Due Diligence The Laramar staff conducts direct due diligence on all acquisitions which includes in-depth evaluation of market conditions financial analysis loan underwriting and capital/physical improvement review. A select group of Laramar professionals from various departments and properties executes due diligence on all potential acquisitions. Detailed Market Analysis Laramar utilizes its extensive national network to conduct research in established and target markets. This analysis includes supply and demand trends employment patterns demographic growth trends industry competition and annual rent growth. Construction and Physical Analysis Laramars construction department analyzes all physical issues and develops an improvement plan which outlines the scope of work for all assets.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:52 [scrapy] ERROR: Error processing {'pagetitle': [u' Password Reset'],
 'pageurl': ['https://reports.sciensam.com/cp/PasswordReset.aspx?sm=1'],
 'siteurl': ['reports.sciensam.com'],
 'text': ['The services described on this web site may not be available in all jurisdictions or to all persons. For further detail please see our Terms of Use. All rights reserved. 2015 Sciens Capital Management LLC']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:52 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/executive_team.php?obj_id=20'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Mr. Slad joined The Laramar Group in 2004 and is responsible for asset management portfolio analysis financial reporting and investor relations. Mr. Slad works closely with all team members including accounting acquisitions construction operations and property management to help oversee a $1 billion multifamily real estate portfolio. Prior to joining Laramar Mr. Slad worked at Imperial Realty Co. in Chicago where his responsibilities included asset management financial analysis and project coordination. Mr. Slad has a B.A. degree and a Masters of Architecture degree from the University of Illinois. Mr. Slad also has an M.B.A. with a concentration in finance from the University of Illinois.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:53 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/executive_team.php?obj_id=11'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Mr. Neuman joined The Laramar Group in 1998 and is responsible for property acquisitions in Laramars Midwest Southeast and East Regions. At Laramar Mr. Neuman has overseen the acquisition of properties with a total capitalized value of approximately $1 billion. Additionally he has played a pivotal role in all of Laramars affordable housing activities including more than $150 million of tax-exempt bond financed transactions. Mr. Neuman has developed strong relationships with local owners and brokers in many markets allowing him to identify properties for acquisition that are not widely marketed. Prior to joining Laramar Mr. Neuman was an Asset Manager with M. Myers Properties a private multi-family owner with approximately 6000 units located throughout the Midwest. Mr. Neuman oversaw operations for this portfolio and managed the physical rehabilitation and repositioning of distressed assets acquired through foreclosure. Trained as an architect Mr. Neuman worked for Holabird & Root Architects prior to working at M. Myers Properties. Holabird & Root is one of Chicagos preeminent architecture firms and Mr. Neumans main focus was design programming and space planning of a new headquarters for a national financial services company. Mr. Neuman has a B.A. degree and a Masters of Architecture degree from the University of Michigan. Mr. Neuman also has an M.B.A. with a concentration in finance from the University of Chicago.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:53 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/executive_team.php?obj_id=7'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Mr. Boyack joined The Laramar Group in 2002 and is responsible for asset management portfolio performance financial reporting and investor relations. Prior to becoming Vice President of Asset Management Mr. Boyack served for four years as Regional Manager for Laramars Northern California Region where he was responsible for the oversight of over $200 million of multifamily and commercial real estate. Prior to Laramar Mr. Boyack was a General Manager with Draper and Kramer where his responsibilities included the preparation of financial and market analyses annual operating budgets management plans and oversight of the day-to-day operation of a Class A luxury high-rise development on the Chicago lakefront. Prior to Draper and Kramer Mr. Boyack was a Property Manager with RMK Management Company where he was responsible for a real estate portfolio consisting of high-rise apartments garden-style apartments and retail located in Chicago and its suburbs. Mr. Boyack has 14 years of experience in the management renovation and repositioning of multifamily and commercial real estate. In addition Mr. Boyack is on the Board of Directors of the Institute of Real Estate Management (IREM) and is a Certified Property Manager (CPM). Mr. Boyack has a B.S. in Economics with a double emphasis in math and psychology from the University of Iowa.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:53 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/executive_team.php?obj_id=22'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Marc joined The Laramar Group in August of 2010. The Laramar Group owns and manages approximately 35000 units in over 20 markets throughout the U.S. As Chief Financial Officer Marc is a member of the executive committee and has overall responsibility for the financial operations of Laramar. Marc joined Laramar after several years as CFO and Head of Asset Management for BayNorth Capital LLC a real estate private equity firm in Boston. Prior to moving to Boston he spent six years in London where he was Chief Investment Officer for DigiPlex a subsidiary of The Carlyle Group Chief Investment Officer for Access Storage Space and Senior Vice President for Security Capital European Realty. In the US he was a Vice President at ProLogis Trust and a Senior Vice President/Finance Manager for Trammell Crow Company. Marc is a CPA a Licensed Real Estate Broker and a member of ULI PREA and NAIOPs Sustainability Forum. He received a B.S. degree in Business Administration/Accounting from California State University Fresno and graduated from the Advanced Management Development Program in Real Estate from the Harvard University Graduate School of Design.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?id=7'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. Conley has been affiliated with Brynwood since 2012 when he became Chief Financial Officer of Newhall Laboratories. Prior to joining Newhall Laboratories Mr. Conley worked in financial management positions for various consumer products companies including Pepsi and Bear Naked (acquired by Kellogg). Mr. Conley started his career in public accounting.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?bid=14&id=7'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. Conley has been affiliated with Brynwood since 2012 when he became Chief Financial Officer of Newhall Laboratories. Prior to joining Newhall Laboratories Mr. Conley worked in financial management positions for various consumer products companies including Pepsi and Bear Naked (acquired by Kellogg). Mr. Conley started his career in public accounting.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?bid=4&id=7'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. DiCarlo joined Brynwood in January 2006. Prior to joining Brynwood Mr. DiCarlo was Executive Vice President and Chief Financial Officer of Village Voice Media LLC. Prior to his eight years at Village Voice Media he spent seven years with Harmon Publishing as Vice President of Finance/Production and eight years with The McGraw-Hill Companies in various financial and operational roles. Mr. DiCarlo holds a B.S. in Finance and an M.B.A. from the University of Connecticut.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?bid=5&id=7'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. Eagle joined Brynwood in 2011. Prior to joining the firm Mr. Eagle worked in private equity and investment banking at Emigrant Capital G.C. Andersen Partners and Stifel Nicolaus. Mr. Eagle holds a B.S. in Commerce with concentrations in Finance and Accounting from the University of Virginia and an M.B.A. from the Wharton School of the University of Pennsylvania.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?bid=6&id=7'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. Einav joined Brynwood in 2011. Prior to joining the firm Mr. Einav spent 11 years in public accounting with EisnerAmper LLP where he specialized in auditing private equity funds. Mr. Einav holds a B.B.A. in Accounting and an MBA from the Zicklin School of Business at Baruch College.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Brynwood Partners - Private Equity Lower Middle Market Consumer Products'],
 'pageurl': ['http://www.brynwoodpartners.com/team.asp?bid=7&id=7'],
 'siteurl': ['brynwoodpartners.com'],
 'text': ['Mr. Hartnett has been affiliated with Brynwood since 1993 when he became Chief Financial Officer of J.B. Williams Company Inc. a Brynwood II portfolio company. Prior to joining J.B. Williams Company Inc. Mr. Hartnett spent four years as Director of Finance & Accounting for the Clorox Company and more than 15 years in various financial management assignments with Nestl USA Inc. Mr. Hartnett holds a B.S. in Accounting from the University of Dayton and an M.B.A. from Iona College.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:56 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001174691.PDF?Y=&O=PDF&D=&fid=1001174691&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:56 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001176140.PDF?Y=&O=PDF&D=&fid=1001176140&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:56 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500051305.PDF?Y=&O=PDF&D=&fid=1500051305&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:57 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001179945.PDF?Y=&O=PDF&D=&fid=1001179945&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:58 [scrapy] ERROR: Error processing {'pagetitle': [u'The Laramar Group \u2014 People. Properties. Potential.'],
 'pageurl': ['http://www.laramarinvestor.com/executive_team.php?obj_id=2'],
 'siteurl': ['laramarinvestor.com'],
 'text': [' Mr. Elowe founded The Laramar Group and co-founded Elkor Realty Corporation and Elkor Properties. Mr. Elowe also co-founded Elkor Commercial Properties which invested primarily in hotels and office buildings. Mr. Elowe is Chairman of the Investment Committee and is responsible for the companys overall strategic planning investments asset management and investor relations. Mr. Elowe has built a successful 20-year real estate investment track record having invested approximately $500 million of equity in over 100 assets in transactions totaling nearly $2 billion. In addition Mr. Elowe is on the Board of Directors of the National Multi-Housing Council (NMHC) is a member of the Urban Land Institute (ULI) and is a member of Young Presidents Organization (YPO). Mr. Elowe has a B.S. degree from the University of Illinois and an M.D. degree from Northwestern University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:58 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500053986.PDF?Y=&O=PDF&D=&fid=1500053986&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:53:59 [scrapy] ERROR: Error processing {'pagetitle': [u' Sciens launches S.M.A.R.T. : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/sciens-launches-smart/'],
 'siteurl': ['sciensam.com'],
 'text': ['Click here to register for the event. Sciens Capital invites you to a breakfast seminar on Thursday 18th April 2013 in Geneva to celebrate the launch of S.M.A.R.T. our new Managed Account Platform portfolio analysis portal. S.M.A.R.T. is a sophisticated tool custom built by Sciens to augment its Managed Account Platform services. It brings the Hedge Fund world analytical capabilities previously confined to the domain of traditional long only asset management. In addition to showcasing this new cutting edge technology the seminar will also explore broader advances in Managed Account Platform portfolio analytics and the dawn of a new era in Managed Account Platform service levels. We will also look at four years after the Global Financial Crisis how effectively-designed Managed Accounts solve many problems that managers and allocators previously faced. Dr Siokos is responsible for the general management of the fund of funds and the managed account platform businesses based in London Guernsey and Zurich. He is a member of Sciens Investment Committee and reports to John Rigas Chairman and Chief Executive of the Group. Dr Siokos formerly spent 13 years serving as managing director within the equities division of Citigroup in London where he was global head of alternative execution sales (program trading algorithmic trading Direct Market Access) global head of pre and post trade analytics global head of portfolio trading strategies as well as European head of the companys pension funds and insurance companies structured solutions for equities. During that period he and his teams were top ranked in most of the major global and European investment quantitative research surveys. Subsequently he was head of investment management for Piraeus Bank in Greece where he was Chairman of Piraeus Asset Management and Piraeus Equity Advisors the private equity and venture capital arms of the Piraeus Group both of which he initiated and implemented. He was also Chairman of Piraeus Wealth Management a joint venture between the Piraeus Bank and BNP Paribas Wealth Management. Dr Siokos holds an electrical and computer engineering diploma from the University of Patras Greece as well as a masters degree in the same field and a Ph.D. in operations research (with the focus on financial engineering) from the University of Massachusetts. Roberto Botero is Director of Global Advisory advising institutional clients on portfolio construction and manager selection. He works closely with the research and risk management teams to regularly monitor the individual managers and advisory mandates performance. Advisory mandates include allocations to hedge funds through traditional vehicles and managed accounts as well as private equity investments. Before joining Sciens in 2006 Mr Botero was an associate consultant at Aon Consulting in London where he advised corporate pension schemes on actuarial liabilities. Mr Botero holds a Diploma in Actuarial Techniques a Certificate in Finance and Investment and a Certificate in Derivatives from the Institute of Actuaries. He also holds a Master of Advanced Studies in Applied Mathematics and Theoretical Physics from the University of Cambridge in England and is a physicist from the University of Los Andes in Bogota Colombia where he was also a lecturer. Since October 2007 Steve Brosnan has been Commercial Director and Head of Risk for the Cumulus Funds. From 1998 to 2007 Dr Brosnan directed the energy trading function within British Nuclear Fuels plc where he set up and managed the subsidiary that traded the power produced by the UKs first-generation (Magnox) nuclear power plants a hydro-electric plant and a gas-fired plant. He led the companys successful project to design and implement the systems needed for the New Electricity Trading Arrangements which were introduced into the UK in 2001. He left the business in 2007 following the outsourcing of trading to British Energy in response to the reduced work-load when only two Magnox stations remained in operation. Dr Brosnan chaired the Investment Committee of the Magnox Group of the Electricity Supply Pension Scheme which had 1.6 billion under management. Under his leadership a number of changes to asset allocation were introduced which greatly reduced the risks faced by the Group. Prior to 1998 Dr Brosnan held a number of senior posts associated with nuclear fuel supply and nuclear liabilities management within Magnox Electric plc and its predecessors Nuclear Electric plc and CEGB. He has a DPhil in Physics from the University of Oxford and has an MBA from Henley Management College/Brunel University. In December 2008 after ten years at Citigroup Henric Tamm left the banking industry to establish R Capital with the vision of providing unbiased investment advices to Ultra High Net Worth Individuals families trusts and foundations. Henric joined Citibank Private Bank in Geneva. In early 1999 Henric transferred to the advisory group in Geneva with special focus on fixed income. In 2003 Henric transferred to London and started to trade actively in financial futures options & commodities in addition to the conventional equities & bonds. In 2006 Henric joined the Middle East team as a Private Banker. Since establishing R Capital Henric has worked on various client assignments with a focus on investment asset allocation and portfolio construction and monitoring as well as wealth structuring. Paul has eight years experience in sales and distribution of alternative investments. Prior to joining Mulvaney Capital Management Paul was a Director at Optio Advisors Ltd and responsible for the selection and distribution of single manager hedge funds and fund of hedge funds of leading alternative investment managers. In addition Paul worked as an Associate at Redi & Partners investment advisors to Lyxor Asset Management and also at Unifortune Investment Management as a Marketing Manager covering distribution for a range of funds. Paul has built a strong global investor network including family offices private banks IFAs insurance companies fund of funds with a focus on Europe Switzerland and the Middle East. A French and British National Paul is a graduate of the European Business School London. Sciens Managed Account Platform Sciens Managed Account Platform is an investment vehicle that can be used to replicate alternative investment strategies e.g. hedge fund investments. As a managed account provider Sciens is dedicated to managing operational risk and has developed a proprietary risk monitoring system that provides state-of-the-art flexible risk analytics and reporting. Sciens has complete control over the managed account platform and provides investors with useful transparency. Liquidity of the managed account corresponds to the best liquidity the underlying positions can provide. Sciens is able to tailor a managed account to investors needs including the provision of bespoke reporting. Click here to register for the event. If you have any further questions about the event please contact Josh Pickford on: +44 20 7078 0600 or email us at: seminar@sciensam.com']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:53:59 [scrapy] ERROR: Error processing {'pagetitle': [u' Terms and Conditions : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/register/terms-and-conditions/'],
 'siteurl': ['sciensam.com'],
 'text': ['Purpose of the Site: The material set out on this Site is for the purpose of providing information about Sciens Capital Management LLC Sciens Fund of Funds Management Ltd Sciens Group Fund Services Limited and Sciens Management LLC and their related companies (collectively Sciens) and about alternative investment strategies. It is not and should not be construed as a solicitation offer or recommendation to acquire or dispose of any investment or to engage in any investment business. Responsibility for Information: The information on this Site has been obtained from both internal and external sources. While Sciens makes every effort to ensure that the information or opinions contained on this Site are accurate reliable and complete it makes no representation that this is in fact the case. The information is subject to change without notice. The information contained on this Site does not constitute investment legal tax or other advice and should not be construed as such. It should not be relied upon in making an investment or other decision. Sciens reserves the right to change the terms conditions and notices under which this Site is offered.Limitations of Liability: Sciens will not be liable for any loss or damage of any kind howsoever arising including (without limitation) any direct special indirect or consequential damages arising out of or in connection with the access (or inability to access) or use of this Site. No warranty: The Site including information and materials contained in the Site text graphics software links and other items are provided as is and as available. Sciens does not warrant the accuracy adequacy completeness timeliness or availability of the Site and expressly disclaims liability for errors or omissions in the web site. There is no warranty of merchantability no warranty of fitness for a particular purpose no warranty of non-infringement no warranty of any kind implied express or statutory in conjunction with this Site. Copyright: The information on this Site is the copyright of Sciens and no information may be reproduced in any form other than for the purpose of making copies for their own use by permitted users of this Site without the prior written permission of Sciens. All rights are reserved. This Site and all accompanying screens information materials user documentation user interfaces images arrangements of information related software and other proprietary property of Sciens or its licensors accessible via the Site is and shall remain the exclusive property of Sciens and its licensors as the case may be. The name SCIENS is a registered trademark of Sciens Capital Management LLC. Unauthorized Access: This Site is not absolutely protected against unauthorized third parties. You acknowledge that any information provided through the Internet may be potentially accessed by unauthorized third parties. Although Sciens will make reasonable efforts to protect the privacy of users of this Site no guarantee can be made that unauthorized third parties will not access the information contained on the Site. You acknowledge that Sciens is not responsible for notifying you that unauthorized third parties have gained such access or that any data has been otherwise compromised during transmission across computer networks or telecommunications facilities including but not limited to the Internet. Passwords and Security: You are responsible for the confidentiality and use of your password. Your password is an important means of protection for you. You agree to contact us immediately if you believe that an unauthorized person has obtained access to your password. Sciens has the right to deactivate a password at any time. The issuance of a password is solely within Sciens discretion. Links: Sciens makes no representations whatsoever about the opinions of any third party appearing on a linked website neither regularly monitors nor has control over the contents of such websites and does not endorse and disclaims all responsibility for the content of such statements or websites. Privacy Policy: Sciens will not disclose Investors Non-Public Personal Information or that of any former clients to third parties other than (i) Affiliates and/or (ii) other third party firms that assist Sciens in providing advisory services and/or effecting client transactions (such as brokers fund administrators accounting support firms and compliance/operational support service providers) or (iii) except as may be required by law or regulation. Additionally Sciens disposal of Non-Public Information shall be done in a secure manner. A copy of our Privacy Policy can be found by clicking on Privacy at the bottom of any page on this Site. Performance: PAST PERFORMANCE IS NOT NECESSARILY INDICATIVE OF FUTURE RETURNS. Any information you receive from the Site does not necessarily reflect the most up to date or current information available on the fund product or service provided by Sciens. Availability: AS REQUIRED BY THE U.S. SECURITIES AND EXCHANGE COMMISSION THE PASSWORD-PROTECTED AREA OF THIS SITE MAY ONLY BE ACCESSED BY EXISTING CLIENTS OF SCIENS AND PROSPECTIVE CLIENTS OF SCIENS WHO AMONG OTHER REQUIREMENTS ARE QUALIFIED AS ACCREDITED INVESTORS AND WHO GENERALLY ARE SOPHISTICATED IN FINANCIAL MATTERS SUCH THAT THEY ARE CAPABLE OF EVALUATING THE MERITS AND RISKS OF PROSPECTIVE INVESTMENTS. Offerings: THE INFORMATION ON THIS SITE IS NOT AN OFFER TO SELL OR SOLICITATION OF AN OFFER TO BUY AN INTEREST IN ANY INVESTMENT FUND OR FOR THE PROVISION OF ANY INVESTMENT MANAGEMENT OR ADVISORY SERVICES. ANY SUCH OFFER OR SOLICITATION WILL BE MADE ONLY BY MEANS OF A PRIVATE OFFERING MEMORANDUM RELATING TO A PARTICULAR FUND OR INVESTMENT MANAGEMENT CONTRACT AND ONLY IN THOSE JURISDICTIONS WHERE PERMITTED BY LAW. Sciens offers investment opportunities through both domestic funds and offshore funds. Offshore offerings are made in accordance with Regulation S under the Securities Act of 1933 and Sciens may concurrently make offerings to U.S. persons (generally tax-exempt persons) in accordance with Regulation D under the Securities Act of 1933. To ensure that an offshore offering is not targeted at the general U.S. audience and to ensure that an offshore offering is not used as a general solicitation or advertisement for the U.S. offering in violation of Regulation D you are required to provide Sciens with certain information regarding your residency and investor accreditation. Upon providing Sciens with this information you will be eligible to receive a password which will enable to you view applicable sections within the Site. Further By agreeing to these Terms of Use and clicking Proceed below you will be representing and warranting to Sciens that you are not breaching and that you are not causing Sciens to breach any of those restrictions by your accessing this Site. Sciens will rely on your representations and warranties. The information on this Site has been approved for publication in the UK by Sciens Capital Limited (SCL) which is authorised and regulated by the Financial Services Authority to conduct investment business in the UK. Availability: The funds referred to on this Site are not registered with or authorized by the Financial Services Authority and as such they are all unregulated collective investment schemes (Unregulated Schemes). Section 238 of the Financial Services and Markets Act 2000 and the Financial Services and Markets Act 2000 (Promotion of Collective Investment Schemes) (Exemptions) Order 2001 (SI 2001/1060) as amended prohibits the promotion of Unregulated Schemes to persons in the United Kingdom (UK Investors) except to the following types of persons (Eligible Investors):- a. Persons authorized to conduct investment business in the UK and investment firms from other Member States of the European Union authorized to establish a branch or provide services in the UK; b. A government local authority or public authority; c. Any body corporate which has a called up share capital or net assets of: (i) not less than 500000 where the body corporate has more than 20 members or is a subsidiary of such a parent; (ii) not less than 5 million in the case of any other body corporate; d. Any unincorporated association or partnership which has net assets of not less than 5 million; e. The trustee of a high value trust (a trust whose aggregate value of cash and investments forming part of the Trusts assets before deducting liabilities is 10 million or more or has been 10 million or more at any time during the last twelve months); f. Any person whilst acting in the capacity of director officer or employee of the entities described in (a) to (e) above; g. Investment journalists (i.e. persons who receive a communication in the course of a business which involves the dissemination through a publication of information concerning investments); or h. Existing customers of any of the funds. A UK Investor seeking to purchase interests in Sciens funds from affiliates of Sciens will have to demonstrate that they fall into one of the categories a to f or h above. By agreeing to these Terms of Use and clicking Proceed below you will be representing and warranting to Sciens that you are not breaching and that you are not causing Sciens to breach any of those restrictions by your accessing this website. Sciens will rely on your representations and warranties. Further to ensure that only Eligible Investors have access to this Site UK Investors are required to provide Sciens with certain information regarding your residency and investor accreditation. Upon providing Sciens with this information you will be eligible to receive a password which will enable to you view applicable sections within the Site. Availability: The information provided on this Site is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject Sciens to any registration requirement within such jurisdiction or country. To use this Site you must be a resident of or located in a jurisdiction where the distribution publication or provision of the information and content of this site may be lawfully provided or offered. By agreeing to these Terms of Use and clicking Proceed below you will be representing and warranting to Sciens that you are not breaching and that you are not causing Sciens to breach any of those restrictions by your accessing this Site. Sciens will rely on your representations and warranties. To ensure that only suitably qualified investors have access to this Site you are required to provide Sciens with certain information regarding your residency and investor accreditation. Upon providing Sciens with this information you will be eligible to receive a password which will enable to you view applicable sections within the Site.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:00 [scrapy] ERROR: Error processing {'pagetitle': [u' Real Assets : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/news/realassets/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 13 April 2015 Sciens Alternative Investments and PVE Capital today jointly announced the launch of the PVE European Distressed Fund I a closed ended fund that has invested in a portfolio of Italian Non-Performing Loans (NPLs) with a gross book value of 408m primarily composed of secured loans backed by Italian []']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:00 [scrapy] ERROR: Error processing {'pagetitle': [u' Managed Accounts Platform : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/news/managedaccountsplatform/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 13 April 2015 Sciens Alternative Investments and PVE Capital today jointly announced the launch of the PVE European Distressed Fund I a closed ended fund that has invested in a portfolio of Italian Non-Performing Loans (NPLs) with a gross book value of 408m primarily composed of secured loans backed by Italian [] LONDON NEW YORK 1 October 2014 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announced that London-based PVE Capital has launched its Special Situation Credit Strategy on the Sciens Managed Account Platform. Please click on the link [] The emergence of managed account platforms for investing in hedge funds has given the investor access to a range of new risk measures. This study published in the International Journal of Portfolio Analysis and Management (2014 Vol. 1) analyses the accuracy of various value at risk (VaR) methodologies in the context of hedge fund investing. [] LONDON NEW YORK 5 December 2013 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announces that Eiffel Credit Opportunities a long-short sector focused European credit fund will be replicated in a new cell on the Sciens Managed []']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:00 [scrapy] ERROR: Error processing {'pagetitle': [u' Funds of Funds : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/news/fohf/'],
 'siteurl': ['sciensam.com'],
 'text': ['The emergence of managed account platforms for investing in hedge funds has given the investor access to a range of new risk measures. This study published in the International Journal of Portfolio Analysis and Management (2014 Vol. 1) analyses the accuracy of various value at risk (VaR) methodologies in the context of hedge fund investing. []']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:01 [scrapy] ERROR: Error processing {'pagetitle': [u' Company : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/company/'],
 'siteurl': ['sciensam.com'],
 'text': ['Sciens Capital Management is a global independent alternative investment management firm. We currently manage and advise on approximately $5.7 billion of assets as of June 2011. Sciens staff consists of 118 professionals covering both liquid and illiquid investment strategies. We have affiliated entities regulated in multiple jurisdictions and major financial centres: SEC (U.S.A.) FSA (U.K.) GFSC (Guernsey) CIMA (Cayman Islands).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:01 [scrapy] ERROR: Error processing {'pagetitle': [u' Registration : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/register/'],
 'siteurl': ['sciensam.com'],
 'text': ['The information contained in this Investor Qualification Questionnaire and Agreement is being furnished by you in connection with determining whether you will be granted access to certain password-protected areas of the web site of Sciens Capital Management LLC. All fields must be completed']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:02 [scrapy] ERROR: Error processing {'pagetitle': [u'Jim Heyes Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/jim-heyes/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mr. Heyes joined GEF in 2007 and focuses on GEFs timber investment activities. He is responsible for environmental social and governance matters in GEFs forestry portfolio as well as portfolio company monitoring investment strategy and value creation. Prior to joining GEF Mr. Heyes worked for the New England Forestry Foundation where he founded and directed a community-based forestry and economic development initiative known as North Quabbin Woods. He also worked for two years at Heyes Forest Products his familys sawmill in western Massachusetts. Mr. Heyes has a BA in Geology and Environmental Studies from Williams College and a Master of Public Affairs at Princeton Universitys Woodrow Wilson School with a focus on environment and international development. He studied community-based forestry in the Philippines as a Fulbright Scholar in 1997 and is a CFA Charterholder.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:03 [scrapy] ERROR: Error processing {'pagetitle': [u' Login Page'],
 'pageurl': ['https://reports.sciensam.com/CP/login.aspx?ReturnUrl=%2fCP%2f'],
 'siteurl': ['reports.sciensam.com'],
 'text': [' Access to the Sciens Information Portal is password protected. To obtain a password please register by To obtain a password please register by clicking here This section of the document portal requires a password and the information herein is intended solely for the recipient of the password. Neither the materials nor the password may be transmitted (in any form) to any other person without the consent of Sciens.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:03 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - Specialty Management Firms'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/specialty-management-firms/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['In the course of making investments certain of the Funds have acquired or created certain specialty management companies to service various assets requiring specific management expertise a selection of which is detailed below. While these specialty companies manage the day-to-day activities of these investments Hudson maintains strategic oversight including managing the business plans for the investments and providing these organizations with a capital-focused discipline.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:04 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/terms-of-use/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['USER AGREEMENT Welcome to the Hudson Advisors L.P. ("Hudson") web site located at http://www.Hudson-Advisors.com. This User Agreement (this "Agreement") sets forth the terms under which you may use the Website (this "Site"). The terms "you" and "your" as used in this Agreement refer to all individuals and entities accessing this Site for any reason. The terms Lone Star or Lone Star Funds as used in this Agreement refer to Lone Star Global Acquisitions Ltd. and its subsidiaries. The term "the Funds" as used in this Agreement refer to Brazos Fund L.P. Lone Star Opportunity Fund L.P. Lone Star Fund II (U.S.) L.P. Lone Star Fund II (Bermuda) L.P. Lone Star Fund III (U.S.) L.P. Lone Star Fund III (Bermuda) L.P. Lone Star Fund IV (U.S.) L.P. Lone Star Fund IV (Bermuda) L.P. Lone Star Fund V (U.S.) L.P. Lone Star Fund V (Bermuda) L.P. Lone Star Fund VI (U.S.) L.P. Lone Star Fund VI (Bermuda) L.P. Lone Star Real Estate Fund (U.S.) L.P. Lone Star Real Estate Fund (Bermuda) L.P. Lone Star Fund VII (U.S.) L.P. Lone Star Fund VII (Bermuda) L.P. Lone Star Real Estate Fund II (U.S.) L.P. Lone Star Real Estate Fund II (Bermuda) L.P. Lone Star Fund VIII (U.S.) L.P. Lone Star Fund VIII (Bermuda) L.P. Lone Star Real Estate Fund III (U.S.) L.P. Lone Star Real Estate Fund III (Bermuda) L.P. Lone Star Fund IX (U.S.) L.P. Lone Star Fund IX (Bermuda) L.P. Lone Star Fund IX Parallel (Bermuda) L.P. Lone Star Residential Mortgage Fund I (U.S.) L.P. Lone Star Residential Mortgage Fund I Holdings (Bermuda) L.P. Lone Star Real Estate Fund IV (U.S.) L.P. and Lone Star Real Estate Fund IV (Bermuda) L.P. and any subsequent funds formed by the sponsors of the foregoing. BY ACCESSING THIS SITE YOU ACKNOWLEDGE THAT YOU HAVE READ AGREE TO BE BOUND BY AND UNDERSTAND THIS AGREEMENT AND THAT YOU WILL COMPLY WITH ALL APPLICABLE LAWS AND REGULATIONS INCLUDING UNITED STATES COPYRIGHT AND TRADEMARK LAWS. If you have any questions about your obligations under this Agreement email Inquiries@hudson-advisors.com. 1. Limited License and Site Use. Subject to the terms of this Agreement Hudson grants you a limited revocable nonexclusive nontransferable license to view store bookmark download and print the pages within this Site that you are authorized to access for your personal informational and noncommercial use. This Site may only be used by registered users for lawful purposes. Except as otherwise stated in this Agreement you may not: (a) Modify copy distribute transmit post display perform reproduce publish broadcast license create derivative works from transfer sell or exploit any reports data information content or other materials on generated by or obtained from this Site (collectively "Materials"); (b) Use any automated means to access the Site or collect any information from the Site including without limitation robots spiders or scripts (this means among other activities that you agree not to engage in the practices of "screen scraping" "database scraping" or any other activity with the purpose of obtaining information from this Site); (c) Frame the Site place pop-up windows over its pages or otherwise affect the display of its pages; (d) Engage in any conduct that could damage disable or overburden (i) this Site or (ii) any systems networks servers or accounts related to this Site including without limitation using devices or software that provide repeated automated access to this Site; (e) Probe scan or test the vulnerability of any Materials services systems networks servers or accounts related to this Site or attempt to gain unauthorized access to Materials systems networks servers or accounts connected or associated with this Site through hacking password or data mining or any other means of circumventing any access-limiting user authentication or security device of any Materials systems networks servers or accounts related to this Site. 2. Passwords. Certain parts of this Site are protected by passwords and require a login. You agree not to disclose or share your password with any third party or use your password for any unauthorized purpose. You are responsible for maintaining the confidentiality of your information and password. You shall be responsible for all uses of your registration whether or not authorized by you. You agree to immediately notify Hudson of any unauthorized use of your registration or password. 3. Indemnity. You agree to indemnify and hold Hudson and its subsidiaries affiliates officers agents partners and employees harmless from any claim or demand including reasonable attorneys fees made by any third party due to or arising out of your use of the Site your connection to the Site your violation of this Agreement or your violation of any rights of another. 4. Intellectual Property. You relinquish all ownership rights in any ideas or suggestions that you submit to Hudson through this Site. You acknowledge that Hudson has full rights to use and implement any such ideas and suggestions. All Materials on this Site whether separate or compiled including but not limited to text graphics logos buttons images downloads files and code as well as all copyright patent trademark trade dress and other rights therein are owned or licensed by Hudson and are protected by United States and international intellectual property laws. 5. Termination. Upon any breach by you of this Agreement Hudson may pursue in its sole discretion all of its legal remedies including but not limited to termination of your registration and your ability to access this Site. Hudson may also in its sole discretion and at any time discontinue providing the Site or any part thereof with or without notice. You agree that any termination of your access to the Site may be effected without prior notice and you acknowledge and agree that Hudson may immediately deactivate or delete your account and all related information and material in your account and/or bar any further access to such information or to the Site. Further you agree that Hudson is not liable to you or any third-party for any termination of your access to the Site. 6. Links. Hudson may provide or third parties may provide links to other World Wide Web sites or resources that are beyond Hudsons control. Hudson makes no representations as to the quality suitability functionality or legality of any sites to which links may be provided and you hereby waive any claim you might have against Hudson with respect to such sites. Hudson IS NOT RESPONSIBLE FOR THE CONTENT ON THE INTERNET OR WORLD WIDE WEB PAGES THAT ARE CONTAINED OUTSIDE THE SITE. If you decide to access linked third party Web sites you do so at your own risk. 7. No Warranty. HUDSON DOES NOT WARRANT THAT THE SITE WILL OPERATE ERROR-FREE OR THAT THE SITE OR ITS SERVER ARE FREE OF COMPUTER VIRUSES OR OTHER HARMFUL MECHANISMS. THE SITE AND THE INFORMATION AND MATERIAL HEREIN ARE PROVIDED "AS IS" AND HUDSON EXPRESSLY DISCLAIMS ALL WARRANTIES OR CONDITIONS OF ANY KIND (EXPRESS IMPLIED OR STATUTORY) INCLUDING WITHOUT LIMITATION THE IMPLIED WARRANTIES OF TITLE NON-INFRINGEMENT MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. Without limiting the foregoing Hudson does not promise or warrant to you that any aspect of the Site will work properly or will be available continuously. Some states do not allow the disclaimer of implied warranties so the foregoing disclaimer may not apply to you. This warranty gives you specific legal rights and you may also have other legal rights which vary from state to state. 8. Limit on Liability. IN NO EVENT SHALL HUDSON BE LIABLE FOR ANY LOST PROFITS OR SPECIAL INCIDENTAL OR CONSEQUENTIAL DAMAGES (HOWEVER ARISING INCLUDING NEGLIGENCE) ARISING OUT OF OR IN CONNECTION WITH THE SITE OR THIS AGREEMENT. FURTHER IN NO EVENT SHALL HUDSON BE LIABLE TO YOU IN AN AMOUNT GREATER THAN $100. Some states do not allow the foregoing limitations of liability so they may not apply to you. 9. Governing Law and Arbitration. This Agreement is governed in all respects by the laws of the State of Delaware. Any controversy or claim arising out of or relating to this Agreement or the Site will be settled by binding arbitration in accordance with the commercial arbitration rules of the American Arbitration Association. Any such controversy or claim shall be arbitrated on an individual basis and shall not be consolidated in any arbitration with any claim or controversy of any other party. The arbitration shall be conducted in the State of Delaware and judgment on the arbitration award may be entered into any court having jurisdiction thereof. The award of the arbitrator shall be final and binding upon the parties without appeal or review except as permitted by Delaware law. Notwithstanding the foregoing either party may seek any interim or preliminary relief from a court of competent jurisdiction in the State of Delaware as necessary to protect the partys rights or property pending the completion of arbitration. By using this Site you consent and submit to the exclusive jurisdiction and venue of the state and federal courts located in Delaware. 10. Entire Agreement. This Agreement constitutes the entire agreement and supersedes the provisions of any other agreements or understandings (oral or written) between the parties with respect to your use of this Site. 11. Modifications to Terms of Agreement. Hudson reserves the right to revise amend or modify the terms of this Agreement at any time and in any manner at its sole discretion. Please check these terms periodically for changes. Notice of any revision amendment or modification of the terms will be posted in this section of the Site and any such revisions amendment or modifications will be effective upon the posting of such notice. Continued use of the Site by you constitutes your binding acceptance of such revisions amendments and modifications. IF YOU DO NOT AGREE TO THE TERMS OF THIS AGREEMENT PLEASE DO NOT ACCESS OR USE THE SITE. 12. General. You shall comply with all laws and regulations applicable to your access and use of the Site. If any portion of this Agreement is deemed unenforceable that portion shall be enforced to the maximum extent possible and the remaining portions of the Agreement shall be given full effect. Hudsons failure to act in a particular circumstance does not waive the ability to act with respect to that circumstance or similar circumstances. Hudson shall be excused for any failure to perform to the extent that its performance is prevented by any reason outside of its control. No agency partnership joint venture employment or franchise relationship is intended or created by this Agreement. Hudson may change remove or require registration or payment to continue use of any aspect of the Site at any time without further notice to you.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:04 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001183295.PDF?Y=&O=PDF&D=&fid=1001183295&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:04 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001186503.PDF?Y=&O=PDF&D=&FID=1001186503&T=&IID=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:05 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001187903.PDF?Y=&O=PDF&D=&fid=1001187903&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:05 [scrapy] ERROR: Error processing {'pagetitle': [u' Selected Study Cases : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/selected-study-cases/'],
 'siteurl': ['sciensam.com'],
 'text': ['The emergence of managed account platforms for investing in hedge funds has given the investor access to a range of new risk measures. This study analyses the accuracy of various Value at Risk (VaR) methodologies in the context of hedge fund investing. Please click on the link below to view the research paper Return or Position based Value at Risk.pdf']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:06 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Investing.aspx'],
 'siteurl': ['onex.com'],
 'text': ['We are with extensive expertise in carve-outs of subsidiaries and mission-critical supply divisions from multinational corporations cost reductions and operational restructurings and platforms for add-on acquisitions. We are with a focus on competitive excellence and financial performance. Beginning with the establishment of ONCAP L.P. in 1999 Onex evolved its business model from direct investment to that of co-investor through private equity real estateand credit funds that we manage. Onex has had more than 30 years of successful investing and has pursued an active ownership style that has generated a 28% gross IRR on realized substantially realized and publicly traded investments. That outstanding record of value creation is a direct reflection of the investment philosophy that drives our team of approximately80investmentprofessionals:Beginning with the establishment of ONCAP L.P. in 1999 Onex evolved its business model from direct investment to that of co-investor through private equity real estateand credit funds that we manage.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:07 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Onex_at_Glance.aspx'],
 'siteurl': ['onex.com'],
 'text': ['How We Are Invested']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:07 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Private_Equity.aspx'],
 'siteurl': ['onex.com'],
 'text': [' Throughout our history we have developed a successful approach to investing. In private equity we pursue businesses with world-class capabilities and strong free cash flow characteristics where we have identified an opportunity in partnership with company management to effect change and build market leaders. As an active owner we are focused on executionrather than macro-economic or industry trends. Specifically we focus on (i) carve-outs of subsidiaries and mission-critical supply divisions from multinational corporations; (ii) cost reduction and operational restructurings; and (iii) platforms for add-on acquisitions. We have historically been conservative with the use of financial leverage which has served Onex and its businesses well through many cycles. We typically acquire a control position which allows us to drive important strategic decisions and effect change at our businessess. Onex does not get involved in the daily operating decisions of the businesses. Today Onex private equity investment activities are conducted exclusively through its Onex Partners and ONCAP fund families. Onex participates in all these funds as the largest limited partner and through its ownership of the funds general partners and managers. Onex Partners ONCAP Direct Investments For more than three decades Onex has employed an active approach to building industry-leading businesses. Onex manages its own capital and that of investors from around the world including public and private pension funds sovereign wealth funds banks and insurance companies. The Company has generated a gross multiple of capital invested of3.0 times from its core private equity activities since inception on realized substantially realized and publicly traded investments.Throughout our history we have developed a successful approach to investing. In private equity we pursue businesses with world-class capabilities and strong free cash flow characteristics where we have identified an opportunity in partnership with company management to effect change and build market leaders. As an active owner we are focused on executionrather than macro-economic or industry trends. Specifically we focus on (i) carve-outs of subsidiaries and mission-critical supply divisions from multinational corporations; (ii) cost reduction and operational restructurings; and (iii) platforms for add-on acquisitions.We have historically been conservative with the use of financial leverage which has served Onex and its businesses well through many cycles.We typically acquire a control position which allows us to drive important strategic decisions and effect change at our businessess. Onex does not get involved in the daily operating decisions of the businesses.Today Onex private equity investment activities are conducted exclusively through its Onex Partners and ONCAP fund families. Onex participates in all these funds as the largest limited partner and through its ownership of the funds general partners and managers.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:08 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Onex_Partners.aspx'],
 'siteurl': ['onex.com'],
 'text': ['How We Are Invested']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:09 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/ONCAP_Investing.aspx'],
 'siteurl': ['onex.com'],
 'text': ['How We Are Invested']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:10 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Direct_Investments.aspx'],
 'siteurl': ['onex.com'],
 'text': [' (Revenues: $5.6 billion (2014);approximately 26000employees) Onex completed 144 acquisitions having an aggregate transaction value of C$18 billion prior to the formation of the Onex Partners and ONCAP families of funds. Onex currently holds two direct investments. Celestica Inc. (Revenues: $5.6 billion (2014);approximately 26000employees) Sitel Worldwide (Revenues: $1.4 billion (2014);approximately 60000employees)']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:11 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Onex_Credit_Partners_Investing.aspx'],
 'siteurl': ['onex.com'],
 'text': ['Onex Credit Partners LLC ("Onex Credit") is the exclusive credit investing arm of Onex focused on credit-oriented investment strategies such as event-driven long/short long only and market dislocation strategies. Key areas of focus include senior secured loans high yield bonds and stressed / distressed corporate debt. The firm invests capital for bank insurance company foundation fund of hedge fund and high net worth investors and its products include separate accounts and closed-end funds. Onex Creditis an SEC Registered Investment Advisor and registered in the Canadian province of Ontario as a Portfolio Manager in the provinces of Ontario Quebec and Newfoundland and Labrador as an Investment Fund Manager and in all the provinces and territories of Canada as an Exempt Market Dealer. TheOnex Creditteam hasover 20 full-timeprofessionals with an average of19 years of experience that has generated attractive absolute and risk-adjusted returns that compare favorably with returns and volatility of benchmark indices including the Debt Opportunity Strategys over 14-year track record. As ofJuly31 2015 the firm managedapproximately $6.2 billion through several debt strategies. Each of these has a differentiated return/risk objective and portfolio construction as well as a focus on more actively traded positions through market cycles. These strategies are: Opportunistic and event-driven focus on senior debt of companies that exhibit stress or are distressed investing both long and short without the use of leverage. Closed-end Canadian retail fund listed on the TSX that follows a similar strategy to the Debt Opportunity Strategy. Focused primarily on performing first-lien loans with moderate exposure to event driven situations and utilizing modest leverage to enhance returns. Closed-end Canadian retail fund listed on the TSX focused primarily on first-lien floating rate senior secured loans of non-investment grade North American issuers utilizing modest leverage. Focused solely on performing first-lien loans with no exposure to stress/distressed situations and without the use of leverage. Leveraged structured vehicle that holds a widely diversified collateral asset portfolio that is funded through the issuance of long-term debt in a series of rated tranches of secured notes and equity.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:11 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Onex_Credit_Partners_Team.aspx'],
 'siteurl': ['onex.com'],
 'text': ['The Onex Credit team led by Michael Gelblat hasover 20 full-time professionals and is based in Englewood Cliffs New Jersey.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:12 [scrapy] ERROR: Error processing {'pagetitle': [u'Kevin Tidwell Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/kevin-tidwell/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mr. Tidwell joined GEF in 2004 and focuses on GEFs timber investment activities in Africa. He has expertise in identifying opportunities to improve silviculture residue management and value-chain efficiencies. His responsibilities include deal sourcing monitoring and value creation related to integrated sustainable forestry projects. Mr. Tidwell plays an active role as a director of Monte Alto Forestal in Chile and Cape Pine Investment Holdings in South Africa. Prior to joining GEF Mr. Tidwell was a Watson Fellow in Africa and Latin America examining eco-tourism strategies. Mr. Tidwell has a BA from Rice University in History and Environmental Engineering an MBA from the Yale School of Management and a Master of Environmental Science from the Yale School of Forestry and Environmental Science.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:13 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - Benefits of Dedicated Asset Management'],
 'pageurl': ['http://www.hudson-advisors.com/managed-assets/benefits-of-dedicated-asset-management/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Dedicated Relationship: Through its relationship with Hudson Lone Star receives dedicated focus and attention. Lone Star also benefits from Hudsons extensive knowledge of global market pricing underwriting and servicing. Market Information: Critical proprietary market information can be maintained primarily within the Lone Star and Hudson organizations limiting the amount of sensitive market data that is available to third parties. Scale / Cost Control: Hudsons large scale and experience allow for cost effective asset management services. Alignment of Interests: Through an employee Co-Investment Program Lone Star and Hudson provide incentives to key employees and consultants to ensure an alignment of interests between Lone Star Hudson and the Funds.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:13 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/safe-harbor/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Hudson Advisors L.P. (Hudson Advisors) respects your concerns about privacy. Hudson Advisors has certified that for the Services discussed below we abide by the Safe Harbor privacy principles as set forth by the U.S. Department of Commerce regarding the collection storage use transfer and other processing of certain Personal Data (as defined below) transferred from the European Economic Area (EEA) or Switzerland to the United States. This Policy outlines our general policy and practices for implementing the Safe Harbor privacy principles for the relevant Personal Data. For purposes of this policy: Client means any entity for which Hudson Advisors provides Services. Data Subject means any natural person who is located in the EEA or Switzerland whose Personal Data Hudson Advisors obtains in connection with the Services (but excludes any individual acting in his or her capacity as a current former or prospective employee contractor or consultant of any subsidiary or affiliate of Hudson Advisors). Personal Data means any information that (i) is transferred to Hudson Advisors in the U.S. from the EEA or Switzerland (ii) is recorded in any form (iii) relates to an identified or identifiable Data Subject and (iv) can be linked to that Data Subject. Counter Parties means third-parties including without limitation banks and other financial institutions that offer to sell debt and/or equity products such as performing sub-performing and non-performing loans collateral that has been foreclosed on by such third parties and related derivatives and guarantees. Services means Hudson Advisors pre- and post-acquisition services including due diligence analysis administering client investments regulatory compliance and other services provided to our Clients with respect to their investment activities in the EEA or Switzerland. Hudson Advisors Safe Harbor certification can be found at https://safeharbor.export.gov/list.aspx. For more information about the Safe Harbor principles please visit http://www.export.gov/safeharbor. Hudson Advisors practices regarding the collection storage use transfer and other processing of Personal Data comply as appropriate with the Safe Harbor principles of notice choice onward transfer access security data integrity and enforcement and oversight. How Hudson Advisors Obtains and Uses Personal Data In the course of performing the Services for our Clients Hudson Advisors obtains and processes certain Personal Data about Data Subjects. Hudson Advisors may obtain the Personal Data from Clients Counter Parties or Data Subjects. The Personal Data that Hudson Advisors may obtain includes information that might be contained in materials provided by Clients Counter Parties and Data Subjects including without limitation Data Subjects names addresses assets liabilities income employer bank account information family member information net worth and dates of birth. In connection with the Services Hudson Advisors uses the Personal Data for the purposes of (i) conducting due diligence (ii) administering investments made by our Clients and (iii) complying with certain legal and regulatory obligations including anti-money laundering fraud prevention and audit requirements and other reporting obligations. In connection with the Services Hudson Advisors acts as a service provider to its Clients and pursuant to their instructions. Hudson Advisors provides information in this Safe Harbor Privacy Policy regarding our Personal Data practices including the purposes for which we collect and use Personal Data. Because Hudson Advisors acts as a service provider for its Clients in connection with the Services for which Hudson Advisors has certified to the Safe Harbor Framework Hudson Advisors Clients or Counter Parties as appropriate are responsible for providing appropriate notice to Data Subjects whose Personal Data is transferred to the U.S. and for obtaining any requisite consent. As a service provider for its Clients Hudson Advisors obtains Personal Data about Data Subjects with whom Hudson Advisors does not have a direct relationship. Consequently Hudson Advisors Clients or Counter Parties as appropriate are responsible for providing the relevant Data Subjects with certain choices with respect to the Clients or Counter Parties use or disclosure of the Data Subjects Personal Data. Hudson Advisors may share Personal Data with third parties as indicated in the Choice section above. Except as permitted or required by applicable law we require third parties to whom we disclose Personal Data and who are not subject to the European Union Data Protection Directive 95/46 or an adequacy finding to either (i) subscribe to the relevant Safe Harbor principles or (ii) contractually agree to provide at least the same level of protection for Personal Data as is required by the relevant Safe Harbor principles. As a service provider for its Clients Hudson Advisors obtains Personal Data about Data Subjects with whom Hudson Advisors does not have a direct relationship. Consequently Hudson Advisors Clients or Counter Parties as appropriate are responsible for providing the relevant Data Subjects with access to their Personal Data and the right to correct amend or delete the information where it is inaccurate. Data Subjects should direct their questions to the appropriate Client or Counter Party as appropriate. When a Data Subject is unable to contact or does not obtain a response from the appropriate Client or Counter Party Hudson Advisors will provide reasonable assistance in forwarding the Data Subjects request to the appropriate Client or Counter Party. Hudson Advisors takes reasonable precautions to protect Personal Data from loss misuse and unauthorized access disclosure alteration and destruction. Hudson Advisors takes reasonable steps to ensure that the Personal Data we process is (i) relevant for the purposes for which it is to be used (ii) reliable for its intended use and (iii) accurate complete and current. As a service provider for its Clients Hudson Advisors obtains Personal Data about Data Subjects with whom Hudson Advisors does not have a direct relationship. Consequently Hudson Advisors depends on the relevant Clients and Counter Parties to update and correct Personal Data to the extent necessary to serve the purposes for which the Personal Data was collected or subsequently authorized by the relevant Data Subjects. Clients and Counter Parties may contact Hudson Advisors as indicated below to request that Hudson Advisors update or correct Personal Data Hudson Advisors obtains on their behalf as appropriate. Hudson Advisors has established procedures for periodically verifying implementation of and compliance with the Safe Harbor principles. We conduct an annual self-review of our Personal Data practices to verify that the attestations and assertions we make about our privacy practices are true and that our privacy practices have been implemented as represented. As a service provider for its Clients Hudson Advisors obtains Personal Data about Data Subjects with whom Hudson Advisors does not have a direct relationship. Consequently Data Subjects should submit complaints concerning the processing of their Personal Data to the relevant Client or Counter Party as appropriate in accordance with the relevant Clients or Counter Partys dispute resolution process. We will participate in this process at the request of the Client Counter Party or Data Subject. We will take steps to remedy issues arising out of our failure to comply with the Safe Harbor principles. Please contact us as indicated below to address complaints regarding Hudson Advisors Personal Data practices. If a Data Subjects complaint cannot be resolved through the appropriate Clients or Counter Partys dispute resolution process or our internal processes we will cooperate with JAMS in accordance with the JAMS Safe Harbor program which is described on the JAMS website at http://www.jamsadr.com/safeharbor/. JAMS or the Data Subject also may refer the matter to the U.S. Federal Trade Commission which has Safe Harbor enforcement jurisdiction over Hudson Advisors. For questions or concerns about this Safe Harbor Privacy Policy please send an email to safeharborcommunications@hudson-advisors.com or contact our Deputy Chief Compliance Officer at:']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:13 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001191717.PDF?Y=&O=PDF&D=&fid=1001191717&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:14 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - History'],
 'pageurl': ['http://www.hudson-advisors.com/history/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Hudsons origins trace back to a joint venture between a third party investment group and the Federal Deposit Insurance Corporation in 1993 called Brazos Partners L.P. (Brazos Partners) in which nearly 1300 bad bank assets that were impaired as a result of the U.S. savings and loan crisis of the early 1990s were acquired and resolved. As Chairman and CEO of the general partner of Brazos Partners John Grayken led the operation. After the majority of the assets held by Brazos Partners had been liquidated Mr. Grayken began organizing institutional capital to continue pursuing investment opportunities on a larger scale. Brazos Fund L.P. (Brazos Fund) closed in 1995 with approximately $250 million of capital commitments and subsequently targeted investments primarily in debt and real estate in North America. Brazos Advisors LLC was established in 1995 to carry out the day-to-day management and servicing of the assets acquired by Brazos Fund. Mr. Grayken next organized Lone Star Opportunity Fund L.P. (Lone Star Opportunity Fund) which closed in November 1996 with $396 million of capital commitments. Brazos Advisors LLC which was renamed Hudson Advisors L.P. provided asset management and other support services to Lone Star Opportunity Fund. Since the establishment of the first Fund in 1995 Hudson has provided asset management and day-to-day support services for investments made by the Funds. Read more about Managed Assets.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:14 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - Contact Info'],
 'pageurl': ['http://www.hudson-advisors.com/contact-us/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Please see Careers for available job opening information.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:15 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors - Global Offices'],
 'pageurl': ['http://www.hudson-advisors.com/contact-us/global-offices/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Hudsons management expertise was initially developed in the United States. In supporting Lone Stars investment efforts in Asia greater North America and Europe Hudson established an international presence and today has offices and subsidiaries located in each of these regions. Hudson is recognized as a leading global service provider having successfully adapted and applied its core asset management strategies techniques and proprietary systems to legal tax and business guidelines throughout the world.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:15 [scrapy] ERROR: Error processing {'pagetitle': [u' Hudson Advisors'],
 'pageurl': ['http://www.hudson-advisors.com/privacy-policy/'],
 'siteurl': ['hudson-advisors.com'],
 'text': ['Hudson (as such term is defined in the User Agreement related to the Site) is committed to protecting any personal information that you may provide to us. The purpose of this Privacy Notice is to inform you of how we treat information about you that we may receive from the web site located at www.Hudson-Advisors.com (the "Site"). This Privacy Notice only applies to information collected on the Site. 1. What information does Hudson collect? Generally you can visit the Site without telling us who you are or revealing any information about yourself. However to access certain reports and confidential information you will be required to login using your confidential password and registration information. Once you have logged in you are no longer anonymous to us. Additionally we automatically receive and record information from our servers and from your browser including your IP address the time and information about the page you requested. Our web servers may also seek (as many web sites do) to place a "cookie" (a small data file) on your computers hard drive which allows the server to recognize the computer when it visits again. This allows us to track statistical information about navigation to and throughout certain areas of the Site. Cookies are not used to obtain your name or any personal data. 2. How does Hudson use the information? Hudson uses the password and login information to restrict unauthorized access to confidential information. We may also use non-personally identifying information to better understand our users and their needs. 3. Will Hudson share my personal information? No Hudson will not share your personal information with any person or entity outside of Hudson without first receiving your personal consent or unless Hudson is compelled by a binding legal order to share it. 4. What are my options? If you do not want to provide your password and login information you can choose not to access the confidential information available on the Site. You can still view some of the content offered by our site without signing in. We also give users the choice to opt not to provide certain personal information when registering. 5. How will I know if the Privacy Notice is changed? Any modifications to our privacy practices will be reflected first in this Privacy Notice. If there is a material change in our privacy practices we will indicate on our site that our privacy practices have changed and provide a link to the new privacy statement. If we are going to be using the information collected from users in a manner materially different from that stated at the time of collection we will send affected users written notice of the change. 6. How can I update my personally identifiable information? You can correct or change your personal information by sending us an email detailing the requested change or correction at Inquiries@hudson-advisors.com. 7. Privacy of children The Site is not directed to children under the age of 13. We operate our site in compliance with the Childrens Online Privacy Protection Act and do not permit registration by and will not knowingly collect or use personally identifiable information from anyone under 13 years of age. Questions regarding this Privacy Notice or regarding any other aspects of the Site should be sent via e-mail to Inquiries@hudson-advisors.com.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:16 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500062924.PDF?Y=&O=PDF&D=&fid=1500062924&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:16 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/384b3b26-5eb8-47f3-ae73-68b5754a100f.pdf> (referer: http://shareholders.fortress.com/CorporateProfile.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:17 [scrapy] ERROR: Error processing {'pagetitle': [u'Katie Vasilescu Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/katie-vasilescu/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Katie Vasilescu is responsible for the administrative management of each of GEFs funds. At the firm level she develops human resource management policy and programs that contribute to the acquisition retention motivation and development of employees. In addition to her role as CAO Ms. Vasilescu is GEFs Compliance Officer monitoring the compliance program and reporting structure for the firm. Before rejoining GEF in 2002 Ms. Vasilescu volunteered in Romania as a consultant advising new companies on how to more efficiently manage their offices. Prior to her work abroad she was the Office Manager at GEF from 1999 until 2001. Ms. Vasilescu has a BA with honors in Anthropology from the University of Maryland.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:18 [scrapy] ERROR: Error processing {'pagetitle': [u'Stuart Barkoff Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/stuart-barkoff/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Stuart Barkoff joined GEF in 2008. Since 2011 he has served as GEFs General Counsel and is also a member of GEFs compliance team. As General Counsel he is responsible for all legal matters and aspects of GEFs business and investment activities including through the oversight of or participation in fund and deal structuring due diligence investments and exits. Prior to joining GEF Stuart was an attorney with Arnold & Porter LLP an international law firm based in Washington D.C. where he worked principally with private equity and venture capital funds and their portfolio companies on a wide variety of acquisitions and investments. At Arnold and Porter Mr. Barkoff advised GEF as legal counsel on many of GEFs U.S. investments. Mr. Barkoff has an AB in English from Vassar College as well as a JD and an MBA from Emory University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:19 [scrapy] ERROR: Error processing {'pagetitle': [u'Wendell W. Robinson Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/wendell-w-robinson/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Wendell W. Robinson joined GEF in 1994 and brings over 40 years of experience in domestic and international financial investment and company management to his service on the investment committees of GEF. Before joining GEF Mr. Robinson was a senior investment professional for Rockefeller & Company where he oversaw the investment and successful sale of more than $250 million in private equity and venture capital positions in the United States Southeast Asia and Europe. In 1992 Mr. Robinson expanded the firms global private investment programs to include an additional $140 million of investments in Latin America Asia Europe and the United States. Mr. Robinson has been an active member of investment advisory boards and investment committees for private investment funds and partnerships in Argentina Brazil China Spain and France as well as the U.S. Mr. Robinson a CFA Charterholder has a BBA and an MA in Economics from Case Western Reserve University.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:19 [scrapy] ERROR: Error processing {'pagetitle': [u'H. Jeffrey Leonard Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/h-jeffrey-leonard/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Chief Executive Officer and Founding Partner Investment Committee Member for Emerging Markets United States Asia Africa and Forestry Dr. Leonard is the President and Chief Executive Officer of GEF. With extensive experience in private equity economic policy project finance and environmental issues Dr. Leonard maintains primary oversight of GEFs investment activities and is Chairman of GEFs investment committees. Dr. Leonard is a board member of the Emerging Markets Private Equity Association (EMPEA) a global organization that promotes private equity and venture capital investment in emerging markets; the National Council for Science and the Environment (NCSE); and the New America Foundation which focuses on critical global and domestic policy challenges. Dr. Leonard is Chairman of the Board of The Washington Monthly and Chairman of the Board of CityYear (Washington D.C.). He is a founding board member and Chairman Emeritus of the Board of Beacon House Community Ministry a not-for-profit organization dedicated to improving the lives of children and their families in northeast Washington D.C. He has previously served as co-chairman of the Clean Technology Venture Network. In 200607 Dr. Leonard served as Co-Chairman of the energy transition team of Maryland Governor Martin OMalley and from 1992 through 1998 Dr. Leonard was a member of the Hydrogen Technical Advisory Panel to the U.S. Secretary of Energy. He also served as a member of the Advisory Board of the U.S.Brazil Biofuels Partnership. Dr. Leonard has a PhD from Princeton an MS Econ from London School of Economics and an AB magna cum laude from Harvard College. He is the author of five books and numerous technical articles relating to global environmental issues international trade energy production and technology development.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:20 [scrapy] ERROR: Error processing {'pagetitle': [u'Steve Guffey Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/steve-guffey/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Steve Guffey joined GEF in 2006 and is responsible for overseeing the fund and corporate accounting functions. Prior to joining GEF Mr. Guffey was the Assistant Controller at The Carlyle Group where he was responsible for corporate financial reporting and accounting systems. During his seven years at Carlyle Mr. Guffey was also in charge of various corporate and managerial financial reporting as well as budgeting forecasting and planning. Mr. Guffey has a BBA in Accounting from James Madison University and is a Certified Public Accountant.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:21 [scrapy] ERROR: Error processing {'pagetitle': [u'Nicholas Morriss Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/nicholas-morriss/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Mr. Morriss joined GEF in 2011 and provides strategic guidance on fund formation and structuring. Mr. Morriss has over 30 years of experience in private equity corporate finance and privatization in both developed and emerging economies. Prior to joining GEF Mr. Morriss was co-founder and Managing Partner of EMAlternatives a private equity funds investment group focused on emerging markets. Before that he was Partner in charge of Mergers & Acquisitions/Privatization Advisory at Coopers and Lybrand (currently PricewaterhouseCoopers) initially in the U.K. and from 1992 in the U.S. Prior to Coopers and Lybrand he was an Assistant Director in the Corporate Finance division of Barclays de Zoete Wedd in London. He spent a number of years with Price Waterhouse as an audit and corporate finance manager in London and Cape Town South Africa. Mr. Morriss has a BA in Economics from the University of York (England).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:21 [scrapy] INFO: Crawled 1815 pages (at 129 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:54:22 [scrapy] ERROR: Error processing {'pagetitle': [u' Password Reset'],
 'pageurl': ['https://reports.sciensam.com/cp/PasswordReset.aspx?sm=2'],
 'siteurl': ['reports.sciensam.com'],
 'text': ['The services described on this web site may not be available in all jurisdictions or to all persons. For further detail please see our Terms of Use. All rights reserved. 2015 Sciens Capital Management LLC']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:22 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001184022.PDF?Y=&O=PDF&D=&FID=1001184022&T=&IID=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:23 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500062764.PDF?Y=&O=PDF&D=&fid=1500062764&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:23 [scrapy] ERROR: Error processing {'pagetitle': [u' Capital Introduction : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/asset-classes/hedge-funds/managed-account-platform/capital-introduction/'],
 'siteurl': ['sciensam.com'],
 'text': ['With more than fifteen years of presence in the alternative investment industry Sciens has developed an extensive network of institutional and private investors in all regions. We offer capital introduction services to top quartile and/or emerging hedge fund managers on our platform. Our in-house and affiliated sales and marketing team partners with managers to raise awareness and interest in their trading programme. Sciens managed account platforms team facilitates on-boarding of prospective clients by providing them with due diligence legal and operational support. We deliver state-of-the-art performance and risk reporting in a flexible format to all investors.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:24 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001198186.PDF?Y=&O=PDF&D=&fid=1001198186&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 06:54:25 [scrapy] ERROR: Error processing {'pagetitle': [u'Bhairavi Tripathi Global Environment Fund'],
 'pageurl': ['http://www.globalenvironmentfund.com/bhairavi-tripathi/'],
 'siteurl': ['globalenvironmentfund.com'],
 'text': ['Bhairavi Tripathi joined GEF in 2002. She is responsible for the corporate and fund financial reporting and overseeing tax compliance for all entities. Ms. Tripathi has a BS in Commerce with a concentration in Accounting from the University of Virginia and an MS in Taxation from American University. She is a Certified Public Accountant.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:25 [scrapy] ERROR: Error processing {'pagetitle': [u'ELCO Management Company LLC. Investment Solutions'],
 'pageurl': ['http://www.elcomanagement.com/'],
 'siteurl': ['elcomanagement.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:27 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Investment / Portfolio Impact: The U.S. Current Account and Vanishing Global Liquidity'],
 'pageurl': ['http://www.blogcastalia.com/investment-portfolio-impact-the-u-s-current-account-and-vanishing-global-liquidity/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:28 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Integrated Markets / Without Water Lower Agricultural Yields Fewer Ships Required'],
 'pageurl': ['http://www.blogcastalia.com/integrated-markets-without-water-lower-agricultural-yields-fewer-ships-required/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:29 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog The Looming Threat Of Global Water Scarcity'],
 'pageurl': ['http://www.blogcastalia.com/the-looming-threat-of-global-water-scarcity/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:30 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Chinese Smog: Impact on the Dry Bulk Market'],
 'pageurl': ['http://www.blogcastalia.com/chinese-smog-impact-on-the-dry-bulk-market/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:32 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Commodity Implosion \u2013 July 2015'],
 'pageurl': ['http://www.blogcastalia.com/commodity-implosion-july-2015/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:32 [scrapy] ERROR: Error processing {'pagetitle': [u' Terms & Conditions : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/disclaimer/'],
 'siteurl': ['sciensam.com'],
 'text': ['Purpose of the Site: The material set out on this Site is for the purpose of providing information about Sciens Capital Management LLC Sciens Fund of Funds Management Ltd Sciens Group Fund Services Limited and Sciens Management LLC and their related companies (collectively Sciens) and about alternative investment strategies. It is not and should not be construed as a solicitation offer or recommendation to acquire or dispose of any investment or to engage in any investment business. Responsibility for Information: The information on this Site has been obtained from both internal and external sources. While Sciens makes every effort to ensure that the information or opinions contained on this Site are accurate reliable and complete it makes no representation that this is in fact the case. The information is subject to change without notice. The information contained on this Site does not constitute investment legal tax or other advice and should not be construed as such. It should not be relied upon in making an investment or other decision. Sciens reserves the right to change the terms conditions and notices under which this Site is offered.Limitations of Liability: Sciens will not be liable for any loss or damage of any kind howsoever arising including (without limitation) any direct special indirect or consequential damages arising out of or in connection with the access (or inability to access) or use of this Site. No warranty: The Site including information and materials contained in the Site text graphics software links and other items are provided as is and as available. Sciens does not warrant the accuracy adequacy completeness timeliness or availability of the Site and expressly disclaims liability for errors or omissions in the web site. There is no warranty of merchantability no warranty of fitness for a particular purpose no warranty of non-infringement no warranty of any kind implied express or statutory in conjunction with this Site. Copyright: The information on this Site is the copyright of Sciens and no information may be reproduced in any form other than for the purpose of making copies for their own use by permitted users of this Site without the prior written permission of Sciens. All rights are reserved. This Site and all accompanying screens information materials user documentation user interfaces images arrangements of information related software and other proprietary property of Sciens or its licensors accessible via the Site is and shall remain the exclusive property of Sciens and its licensors as the case may be. The name SCIENS is a registered trademark of Sciens Capital Management LLC. Unauthorized Access: This Site is not absolutely protected against unauthorized third parties. You acknowledge that any information provided through the Internet may be potentially accessed by unauthorized third parties. Although Sciens will make reasonable efforts to protect the privacy of users of this Site no guarantee can be made that unauthorized third parties will not access the information contained on the Site. You acknowledge that Sciens is not responsible for notifying you that unauthorized third parties have gained such access or that any data has been otherwise compromised during transmission across computer networks or telecommunications facilities including but not limited to the Internet. Passwords and Security: You are responsible for the confidentiality and use of your password. Your password is an important means of protection for you. You agree to contact us immediately if you believe that an unauthorized person has obtained access to your password. Sciens has the right to deactivate a password at any time. The issuance of a password is solely within Sciens discretion. Links: Sciens makes no representations whatsoever about the opinions of any third party appearing on a linked website neither regularly monitors nor has control over the contents of such websites and does not endorse and disclaims all responsibility for the content of such statements or websites. Privacy Policy: Sciens will not disclose Investors Non-Public Personal Information or that of any former clients to third parties other than (i) Affiliates and/or (ii) other third party firms that assist Sciens in providing advisory services and/or effecting client transactions (such as brokers fund administrators accounting support firms and compliance/operational support service providers) or (iii) except as may be required by law or regulation. Additionally Sciens disposal of Non-Public Information shall be done in a secure manner. A copy of our Privacy Policy can be found by clicking on Privacy at the bottom of any page on this Site. Performance: PAST PERFORMANCE IS NOT NECESSARILY INDICATIVE OF FUTURE RETURNS. Any information you receive from the Site does not necessarily reflect the most up to date or current information available on the fund product or service provided by Sciens. Availability: AS REQUIRED BY THE U.S. SECURITIES AND EXCHANGE COMMISSION THE PASSWORD-PROTECTED AREA OF THIS SITE MAY ONLY BE ACCESSED BY EXISTING CLIENTS OF SCIENS AND PROSPECTIVE CLIENTS OF SCIENS WHO AMONG OTHER REQUIREMENTS ARE QUALIFIED AS ACCREDITED INVESTORS AND WHO GENERALLY ARE SOPHISTICATED IN FINANCIAL MATTERS SUCH THAT THEY ARE CAPABLE OF EVALUATING THE MERITS AND RISKS OF PROSPECTIVE INVESTMENTS. Offerings: THE INFORMATION ON THIS SITE IS NOT AN OFFER TO SELL OR SOLICITATION OF AN OFFER TO BUY AN INTEREST IN ANY INVESTMENT FUND OR FOR THE PROVISION OF ANY INVESTMENT MANAGEMENT OR ADVISORY SERVICES. ANY SUCH OFFER OR SOLICITATION WILL BE MADE ONLY BY MEANS OF A PRIVATE OFFERING MEMORANDUM RELATING TO A PARTICULAR FUND OR INVESTMENT MANAGEMENT CONTRACT AND ONLY IN THOSE JURISDICTIONS WHERE PERMITTED BY LAW. Sciens offers investment opportunities through both domestic funds and offshore funds. Offshore offerings are made in accordance with Regulation S under the Securities Act of 1933 and Sciens may concurrently make offerings to U.S. persons (generally tax-exempt persons) in accordance with Regulation D under the Securities Act of 1933. To ensure that an offshore offering is not targeted at the general U.S. audience and to ensure that an offshore offering is not used as a general solicitation or advertisement for the U.S. offering in violation of Regulation D you are required to provide Sciens with certain information regarding your residency and investor accreditation. Upon providing Sciens with this information you will be eligible to receive a password which will enable to you view applicable sections within the Site. Further By agreeing to these Terms of Use and clicking Proceed below you will be representing and warranting to Sciens that you are not breaching and that you are not causing Sciens to breach any of those restrictions by your accessing this Site. Sciens will rely on your representations and warranties. The information on this Site has been approved for publication in the UK by Sciens Capital Limited (SCL) which is authorised and regulated by the Financial Services Authority to conduct investment business in the UK. Availability: The funds referred to on this Site are not registered with or authorized by the Financial Services Authority and as such they are all unregulated collective investment schemes (Unregulated Schemes). Section 238 of the Financial Services and Markets Act 2000 and the Financial Services and Markets Act 2000 (Promotion of Collective Investment Schemes) (Exemptions) Order 2001 (SI 2001/1060) as amended prohibits the promotion of Unregulated Schemes to persons in the United Kingdom (UK Investors) except to the following types of persons (Eligible Investors):- a. Persons authorized to conduct investment business in the UK and investment firms from other Member States of the European Union authorized to establish a branch or provide services in the UK; b. A government local authority or public authority; c. Any body corporate which has a called up share capital or net assets of: (i) not less than 500000 where the body corporate has more than 20 members or is a subsidiary of such a parent; (ii) not less than 5 million in the case of any other body corporate; d. Any unincorporated association or partnership which has net assets of not less than 5 million; e. The trustee of a high value trust (a trust whose aggregate value of cash and investments forming part of the Trusts assets before deducting liabilities is 10 million or more or has been 10 million or more at any time during the last twelve months); f. Any person whilst acting in the capacity of director officer or employee of the entities described in (a) to (e) above; g. Investment journalists (i.e. persons who receive a communication in the course of a business which involves the dissemination through a publication of information concerning investments); or h. Existing customers of any of the funds. A UK Investor seeking to purchase interests in Sciens funds from affiliates of Sciens will have to demonstrate that they fall into one of the categories a to f or h above. By agreeing to these Terms of Use and clicking Proceed below you will be representing and warranting to Sciens that you are not breaching and that you are not causing Sciens to breach any of those restrictions by your accessing this website. Sciens will rely on your representations and warranties. Further to ensure that only Eligible Investors have access to this Site UK Investors are required to provide Sciens with certain information regarding your residency and investor accreditation. Upon providing Sciens with this information you will be eligible to receive a password which will enable to you view applicable sections within the Site. Availability: The information provided on this Site is not intended for distribution to or use by any person or entity in any jurisdiction or country where such distribution or use would be contrary to law or regulation or which would subject Sciens to any registration requirement within such jurisdiction or country. To use this Site you must be a resident of or located in a jurisdiction where the distribution publication or provision of the information and content of this site may be lawfully provided or offered. By agreeing to these Terms of Use and clicking Proceed below you will be representing and warranting to Sciens that you are not breaching and that you are not causing Sciens to breach any of those restrictions by your accessing this Site. Sciens will rely on your representations and warranties. To ensure that only suitably qualified investors have access to this Site you are required to provide Sciens with certain information regarding your residency and investor accreditation. Upon providing Sciens with this information you will be eligible to receive a password which will enable to you view applicable sections within the Site.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:33 [scrapy] ERROR: Error processing {'pagetitle': [u' WIP : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/wip/'],
 'siteurl': ['sciensam.com'],
 'text': ['Please fill out and submit this form for enquiries.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:33 [scrapy] ERROR: Error processing {'pagetitle': [u' U.S. Persons \u2013 Institutional Investor : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/u-s-persons-institutional-investor/'],
 'siteurl': ['sciensam.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:33 [scrapy] ERROR: Error processing {'pagetitle': [u' sciens : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/sciens/'],
 'siteurl': ['sciensam.com'],
 'text': ['Sciens Capital Management is an independent alternative investment management firm.We focus on providing bespoke and integrated investment solutions to institutional and private clients globally.Covering a large spectrum of alternative strategies Sciens offers investment solutions throughout market and economic cycles.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:34 [scrapy] ERROR: Error processing {'pagetitle': [u' U.S. Persons \u2013 Individual Investor : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/u-s-persons-individual-investor/'],
 'siteurl': ['sciensam.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:34 [scrapy] ERROR: Error processing {'pagetitle': [u' Qualifying Investor : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/qualifying-investor/'],
 'siteurl': ['sciensam.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:35 [scrapy] ERROR: Error processing {'pageurl': ['http://www.satoricapital.com/who_we_are/ceo_partners'],
 'siteurl': ['satoricapital.com'],
 'text': ['Sustainability or conscious capitalism is about operating a business with the recognition and appreciation that a companys ecosystem is critical to its success. This ecosystem is the interconnected web of relationships and stakeholders that allows the business to thrive. We think that paying attention to and appreciating these relationships and the people that make it possible is the best way to do business. And this approach is important enough to us that it is part of the written strategy outlined in agreements with our investors. Thank you for visiting the Satori Capital website. This policy describes how we gather and use information from our website visitors. Your privacy is important to us. The only personally identifiable information we record is information collected through user-submitted forms. This information is used solely by Satori Capital for internal business purposes such as identifying and contacting appropriate investment candidates and is not shared with any third parties. Satori Capital is committed to continuously improving the web site experience for our visitors. Therefore we collect aggregate data about our visitors such as page views visitor frequency Internet browser usage and length of time spent on the web site. This information is analyzed internally to understand the effectiveness of our site. This information does not identify individual users. In no event shall Satori or its related persons be liable to any party for any claims liabilities losses costs or damages including but not limited to any direct indirect punitive special incidental or consequential damages arising out of or in connection with any access use (or inability to use) or distribution of the site or any information or materials obtained through use of the site. this is true even if Satori has been advised of the possibility of such damages or losses. The entrepreneurs journey of building his or her business from $20 million to $200 million of revenue is arguably one of the most challenging. Achieving this new level of growth and scale requires a shift in mindset decision-making and organizational structure along with infrastructure improvements enhanced reporting and a clearly defined strategy. The day-to-day realities of being on the leadership team of a rapidly growing business will shift from working primarily in the business to working primarily on the business. Our unique structure and aligned investor base enables us to hold investments beyond typical private equity time horizons. This allows our portfolio companies to operate without the pressure or need to make poor long-term decisions to satisfy their investors short-term investment horizons. Satori Capital was founded by CEOs our capital comes from CEOs and we partner with CEOs to help them grow their business. Our team has a long and successful track record as private equity investors and as founders and CEOs of both private and public companies. We believe that a mindset of sustainability or conscious capitalism is a foundational component of a thriving business and a force-multiplier on value creation. Sustainable companies operate with a long-term perspective have a strong sense of mission and purpose and are willing to make short-term sacrifices for the long-term success of the company and its ecosystem of stakeholders. Companies with sustainable business practices are better managed more innovative less risky and better positioned to create more enduring value over the long-term. A capital partner should help create value and owners should be able to sell without selling out. Success is about more than money and investment is about more than capital. Theyre built on meaningful relationships inspired by vision and best when enjoyed as a journey. How a company behaves can create more enduring value than what it produces or provides alone. Over the long-term profit is a function of value created for stakeholders a measure of what was given not what was taken. Sustainable companies make better investments. While the day-to-day operations of our portfolio companies are led by talented executives in whom we place a high degree of trust we welcome the opportunity to play a significant and collaborative role in areas such as strategic planning leadership development corporate governance capital allocation strategic relationships and strategic acquisitions. We build close and collaborative relationships with the management teams of our portfolio companies. These leaders are among our most important partners and our intent is to serve as trusted advisors to them. As such we bring our full array of resources and capabilities to help build businesses of enduring value. Weve been operators and we understand the challenges you face on a daily basis as a leader of a growing business. We have been in your shoes and learned lessons along the way. We draw from our collective experience to help you navigate challenges as you build your business according to your vision. Our unique structure allows us to be a long-term investor and an aligned partner for sustainable growth. We invest equity capital in growing private companies that are achieving annual EBITDA of $5 million to $20 million and where there is an opportunity for accelerated growth through a partnership with Satori. We provide liquidity to current owners and/or expansion capital to support future growth. Our capital is invested from our committed funds and we primarily invest in companies that are family- or entrepreneur-owned. We partner with talented management teams to accelerate the growth of companies that are built to last. We act as advisors to help the companys management team accelerate the growth of the business and achieve the long-term vision. Satori Capital is a team of passionate CEOs investment professionals and sustainability leaders on a mission to invest in aligned private companies and help them grow - this is our lifes work. We assemble the partners resources tools and frameworks to help inspired leaders achieve their vision and continue building their business.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:35 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital Who We Are / Portfolio CEOs'],
 'pageurl': ['http://www.satoricapital.com/who_we_are/portfolio_ceos'],
 'siteurl': ['satoricapital.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:35 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001201684.PDF?Y=&O=PDF&D=&fid=1001201684&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:36 [scrapy] ERROR: Error processing {'pagetitle': [u'Analyst Coverage - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-analystsdiscl'],
 'siteurl': ['ir.mtge.com'],
 'text': ['We provide a list of brokers who follow American Capital Mortgage Investment Corp. ("The Company") as a matter of record only and do not endorse or recommend any brokerage firm. The Company does not endorse or adopt any of the opinions expressed by any individual broker or brokerage firm. Any statements made by a broker or brokerage firm express the views of the author not the Company. The views of the Company management with respect to the Companys historical performance and future prospects are contained in the Companys SEC filings. To join American Capital Mortgage Investment Corp.s broadcast e-mail list please click here. I understand and accept the disclaimer and want to view the broker list. I do not accept the disclaimer.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:36 [scrapy] ERROR: Error processing {'pagetitle': [u'Investor Alerts - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-alerts'],
 'siteurl': ['ir.mtge.com'],
 'text': ['1. Select the checkbox for each information category that interests you. 2. Enter your e-mail address in the space provided. American Capital Mortgage Investment Corp. provides a weekly summary alert of all the alerts sent during the prior week. Included in the weekly summary alert are: This weekly summary alert will include any alerts that are distributed after you subscribe to this weekly summary alert. You may set a Price & Volume alert that will be distributed when one or more specified thresholds have been crossed. Select Yes to view the conditions that may be set which are (1) upper and/or lower stock quote prices (2) upper and/or lower stock quote percentage changes based on the previous days closing price and/or (3) trading volume from the daily open. Please note the following: Only one set of Price & Volume alert conditions may be set per email address. When an alert has been sent your preferences will be cleared - at that time you may return to this page to create a new set of conditions. In the event a year elapses from the time you sign up for the Price & Volume alert your conditions will be deactivated without notice. Stock information is delayed for a minimum of 20 minutes. Current stock price for is and volume is . Notify me when:']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:36 [scrapy] ERROR: Error processing {'pagetitle': [u'Dividend Reinvestment and Direct Stock Purchase - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-drip'],
 'siteurl': ['ir.mtge.com'],
 'text': ['MTGEs Dividend Reinvestment Plan (DRIP) and Direct Stock Purchase Plan (DSPP) provide prospective investors and existing stockholders with a convenient and economical method to purchase shares of our common stock. By participating in the Plan investors may purchase additional shares of common stock by reinvesting some or all of the cash dividends received on shares of our common stock. If you elect to participate in the Plan you may also make optional cash purchases of shares of our common stock subject to certain limitations detailed in the Plan Prospectus. Prospective investors and existing stockholders are advised to read the Plan Prospectus before enrolling in the Plan and should carefully consider the risks described under "Risk Factors" in our most recent Annual Report on Form 10-K and any subsequent Quarterly Reports on Form 10-Q before investing in shares of our common stock. Our common stock is listed on the Nasdaq Global Select Market under the symbol "MTGE." To enroll in the Plan use the forms below and contact Computershare the Plan Administrator for further information: American Capital Mortgage Investment Corp. offers answers to frequently asked questions. Click on a subject or a specific question below. If you have additional questions please contact Investor Relations at (301) 968-9220 or IR@MTGE.com. Please click on the icon (+ or -) to the left to expand or collapse ALL answers.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:37 [scrapy] ERROR: Error processing {'pagetitle': [u'Contact Investor Relations - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-contact'],
 'siteurl': ['ir.mtge.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:38 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Dry Bulk Shipping Risks \u2013 A Concern'],
 'pageurl': ['http://www.blogcastalia.com/dry-bulk-shipping-risks-a-concern/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:39 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Tanker Market \u2013 Enjoy Being a Tanker Owner'],
 'pageurl': ['http://www.blogcastalia.com/tanker-market-enjoy-being-a-tanker-owner/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:40 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog China: Increasingly Fragile Economy and Financial Markets'],
 'pageurl': ['http://www.blogcastalia.com/china-increasingly-fragile-economy-and-financial-markets/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:41 [scrapy] ERROR: Error processing {'pagetitle': [u' Home : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/home/'],
 'siteurl': ['sciensam.com'],
 'text': ['We focus on providing bespoke and integrated investment solutions to institutional and private clients globally. Covering a large spectrum of alternative strategies Sciens offers investment solutions throughout market and economic cycles.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:41 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital What We Believe'],
 'pageurl': ['http://www.satoricapital.com/what_we_believe'],
 'siteurl': ['satoricapital.com'],
 'text': ['A capital partner should help create value and owners should be able to sell without selling out. Success is about more than money and investment is about more than capital. Theyre built on meaningful relationships inspired by vision and best when enjoyed as a journey. How a company behaves can create more enduring value than what it produces or provides alone. Over the long-term profit is a function of value created for stakeholders a measure of what was given not what was taken. Sustainable companies make better investments.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:41 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital What We Do'],
 'pageurl': ['http://www.satoricapital.com/what_we_do'],
 'siteurl': ['satoricapital.com'],
 'text': ['We invest equity capital in growing private companies that are achieving annual EBITDA of $5 million to $20 million and where there is an opportunity for accelerated growth through a partnership with Satori. We provide liquidity to current owners and/or expansion capital to support future growth. Our capital is invested from our committed funds and we primarily invest in companies that are family- or entrepreneur-owned. We partner with talented management teams to accelerate the growth of companies that are built to last. We act as advisors to help the companys management team accelerate the growth of the business and achieve the long-term vision.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:42 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital What We Believe / Conscious Capitalism'],
 'pageurl': ['http://www.satoricapital.com/what_we_believe/conscious_capitalism'],
 'siteurl': ['satoricapital.com'],
 'text': ['We believe that a mindset of sustainability or conscious capitalism is a foundational component of a thriving business and a force-multiplier on value creation. Sustainable companies operate with a long-term perspective have a strong sense of mission and purpose and are willing to make short-term sacrifices for the long-term success of the company and its ecosystem of stakeholders. Companies with sustainable business practices are better managed more innovative less risky and better positioned to create more enduring value over the long-term. We believe the status quo in business today is focused on short-term results. We believe that companies that take this approach and focus on profit first and at the expense of all else are unsustainable over the long-term. We believe that theres a better way to do business focused on creating value for all stakeholders not just shareholders. We believe in the power of conscious capitalism as a method of creating superior and sustainable long-term value. At Satori the tenets of conscious capitalism are central to our thinking throughout all phases of our investment process; these tenets inform us during investment sourcing and due diligence and they guide us as we work with our portfolio companies to help them achieve their vision. At the core of conscious capitalism is an emphasis on a companys mission and purpose a stakeholder-centric approach to doing business and a long-term perspective. There is a great deal of support both in theory and in practice for the integration of sustainability factors into investment analysis and decision-making. Recently Goldman Sachs through its GS Sustain effort began integrating a sustainability framework within its equity research. Additionally Firms of Endearment defined as those companies that explicitly take a stakeholder-centric approach returned 1025 percent over a 10-year period compared to 122 percent for the S&P 500 during the same period.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:42 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital Adding a Zero'],
 'pageurl': ['http://www.satoricapital.com/what_we_believe/adding_a_zero'],
 'siteurl': ['satoricapital.com'],
 'text': ['The entrepreneurs journey of building his or her business from $20 million to $200 million of revenue is arguably one of the most challenging. Achieving this new level of growth and scale requires a shift in mindset decision-making and organizational structure along with infrastructure improvements enhanced reporting and a clearly defined strategy. The day-to-day realities of being on the leadership team of a rapidly growing business will shift from working primarily in the business to working primarily on the business. Think about how differently your company operates now than during the first couple of years after it was founded. How has the team grown and changed? How have the skills of the team changed? How has the companys infrastructure changed? How many more customers are you able to serve? How many suppliers and partners do you now work with than when the business was in its initial stages of development? How has the nature of these relationships changed? How has your decision making shifted? What have you achieved that you never thought possible? Now think about how all of this will change again on your journey to $200 million. We believe that the business processes and management styles that make a business successful in growing from $2 million to $20 million in revenue are different than those that will allow it to reach the next level of scale. We believe that every company goes through the same evolutionary journey and faces similar challenges. Our team has been through this and now helps others navigate it as well. Learn more about how we help. Our strategic partner Stagen (http://www.stagen.com) describes this journey as one of scrambling to scaling. Stagen is a leadership development firm that specializes in helping mid-market companies scale.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:42 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital Satori Partnership'],
 'pageurl': ['http://www.satoricapital.com/what_we_do/partnership'],
 'siteurl': ['satoricapital.com'],
 'text': ['We build close and collaborative relationships with the management teams of our portfolio companies. These leaders are among our most important partners and our intent is to serve as trusted advisors to them. As such we bring our full array of resources and capabilities to help build businesses of enduring value. Learn more about our investment criteria.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:43 [scrapy] ERROR: Error processing {'pagetitle': [u' Login Page'],
 'pageurl': ['https://reports.sciensam.com/CP/login.aspx?ReturnUrl=%2fCP%2fTerms.aspx'],
 'siteurl': ['reports.sciensam.com'],
 'text': [' Access to the Sciens Information Portal is password protected. To obtain a password please register by To obtain a password please register by clicking here This section of the document portal requires a password and the information herein is intended solely for the recipient of the password. Neither the materials nor the password may be transmitted (in any form) to any other person without the consent of Sciens.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:43 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001191619.PDF?Y=&O=PDF&D=&FID=1001191619&T=&IID=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:43 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500071593.PDF?Y=&O=PDF&D=&FID=1500071593&T=&IID=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:44 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog First Half 2014 \u2013 J.C. Goodgal Inc. Client Letter and Commentary'],
 'pageurl': ['http://www.blogcastalia.com/first-half-2014-j-c-goodgal-inc-client-letter-and-commentary/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:45 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Chinese Shadow Banking Market: Recognition of Systemic Risk and a Real Impact on the Shipping Markets'],
 'pageurl': ['http://www.blogcastalia.com/chinese-shadow-banking-market-recognition-of-systemic-risk-and-a-real-impact-on-the-shipping-markets/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:46 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Due Like Yesterday\u2019s Bills'],
 'pageurl': ['http://www.blogcastalia.com/due-like-yesterdays-bills/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:46 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital What Makes Us Different'],
 'pageurl': ['http://www.satoricapital.com/what_makes_us_different'],
 'siteurl': ['satoricapital.com'],
 'text': ['Weve been operators and we understand the challenges you face on a daily basis as a leader of a growing business. We have been in your shoes and learned lessons along the way. We draw from our collective experience to help you navigate challenges as you build your business according to your vision. Our unique structure allows us to be a long-term investor and an aligned partner for sustainable growth.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:47 [scrapy] ERROR: Error processing {'pagetitle': [u'Satori Capital How We Help'],
 'pageurl': ['http://www.satoricapital.com/what_we_do/how_we_help'],
 'siteurl': ['satoricapital.com'],
 'text': ['While the day-to-day operations of our portfolio companies are led by talented executives in whom we place a high degree of trust we welcome the opportunity to play a significant and collaborative role in areas such as strategic planning leadership development corporate governance capital allocation strategic relationships and strategic acquisitions. Our goal is to use our experience and expertise to give our portfolio companies the guidance tools and resources they need to achieve their long-term vision. Each company and situation is unique but the following are the primary categories we help our portfolio companies with.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:47 [scrapy] ERROR: Error processing {'pagetitle': [u'Health Evolution Partners - Home'],
 'pageurl': ['http://www.healthevolutionpartners.com/'],
 'siteurl': ['healthevolutionpartners.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:48 [scrapy] ERROR: Error processing {'pagetitle': [u'Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-govmanage'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Malon Wilkus is our Chair and Chief Executive Officer and the Chief Executive Officer of American Capital MTGE Management LLC our Manager and its parent company American Capital Mortgage Management LLC. Mr. Wilkus is also the Chair and Chief Executive Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Chief Executive Officer of its manager American Capital AGNC Management LLC. In addition Mr. Wilkus is the Chief Executive Officer of American Capital Senior Floating Ltd. (NA... Gary Kain is our President and Chief Investment Officer and the President of our Manager with primary oversight for all of our investments. He is also the President and Chief Investment Officer of American Capital Agency Corp. (NASDAQ: AGNC). He was previously a Senior Vice President and Managing Director of American Capital Ltd. (NASDAQ: ACAS) until July 2009 after which time his employment was transferred to American Capital Mortgage Management LLC (f/k/a American Capital Agency Management... Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC John R. Erickson is our Executive Vice President and Chief Financial Officer and a member of our Board of Directors and Executive Vice President and Treasurer of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Erickson is also the Executive Vice President and Chief Financial Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President and Treasurer of its manager American Capital AGNC Management LLC. In addition he is the Exec... Samuel A. Flax is our Executive Vice President and Secretary and Executive Vice President Chief Compliance Officer and Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Flax is also a Director Executive Vice President and Secretary of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President Chief Compliance Officer and Secretary of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice Pre... Peter J. Federico is our Senior Vice President and Chief Risk Officer and the Senior Vice President and Chief Risk Officer of our Manager. He is also the Senior Vice President and Chief Risk Officer of the parent company of our Manager American Capital Mortgage Management LLC. Mr. Federico is also the Senior Vice President and Chief Risk Officer of American Capital Agency Corp. (NASDAQ: AGNC) and of its manager American Capital AGNC Management LLC. He is primarily responsible for overseeing ... Christopher Kuehl has served as our Senior Vice President Agency Portfolio Investments since March 2012 and as Senior Vice President of American Capital MTGE Management LLC our Manager since April 2011. He has also served as Senior Vice President of American Capital Mortgage Management LLC the parent company of our Manager since August 2010. Mr. Kuehl is also a Senior Vice President Agency Portfolio Investments of American Capital Agency Corp. (NASDAQ: AGNC) and Senior Vice Presiden... Aaron Pas is our Senior Vice President Non-Agency Portfolio Management. Mr. Pas is also a Senior Vice President of our Manager and American Capital Mortgage Management LLC. From 2003-2011 Mr. Pas worked at Freddie Mac where he most recently was the Director of Non-Agency Portfolio Management where he was primarily responsible for managing the firms non-agency residential securities portfolio. Mr. Pas holds a Bachelor of Science degree in Business from Washington University in St. Louis.... Malon Wilkus is our Chair and Chief Executive Officer and the Chief Executive Officer of American Capital MTGE Management LLC our Manager and its parent company American Capital Mortgage Management LLC. Mr. Wilkus is also the Chair and Chief Executive Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Chief Executive Officer of its manager American Capital AGNC Management LLC. In addition Mr. Wilkus is the founder of American Capital Ltd. (NASDAQ: ACAS) the indirect majori... Mr. Couch is Counsel to Bradley Arant Boult Cummings LLP a law firm based in Birmingham Alabama. Mr. Couch is also Chairman of ARK Real Estate Strategies LLC. ARK helps banks and financial institutions evaluate manage and market foreclosed residential real estate. ARK is also the manager of the ARK Real Estate Opportunity Fund I LLC an investment fund focused on distressed residential real estate. Mr. Couch is a member of the Board of Directors of Prospect Holding Company LLC the parent... Dr. Davis is an Associate Professor in the Department of Real Estate and Urban Land Economics at the University of Wisconsin-Madison School of Business. He has worked in the department since September 2006. He is currently on the Academic Advisory Council of the Federal Reserve Bank of Chicago and served in 2007 as a Research Associate at the Federal Reserve Bank of Cleveland. From July 2002 to August 2006 Dr. Davis was an economist at the Federal Reserve Board working in the Flow of Funds Sec... Mr. Dobbs has been a self-employed business consultant and business speaker since the end of 2010. Prior to that he was a Senior Operating Executive at Welsh Carson Anderson & Stowe or Welsh Carson a private equity firm. At Welsh Carson Mr. Dobbs was responsible for portfolio company operational oversight business acquisitions and equity opportunity development. From February 2005 to October 2008 he was the Chief Executive Officer of US Investigations Services Inc. and its subsidiaries... Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC John R. Erickson is our Executive Vice President and Chief Financial Officer and Executive Vice President and Treasurer of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Erickson is also a Director Executive Vice President and Chief Financial Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President and Treasurer of its manager American Capital AGNC Management LLC. In addition he is Executive Vice President and Treasurer o... Samuel A. Flax is our Executive Vice President and Secretary and Executive Vice President Chief Compliance Officer and Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Flax is also a Director Executive Vice President and Secretary of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President Chief Compliance Officer and Secretary of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice Pre... Ms. Larocca retired in 2011 from Royal Bank of Scotland (RBS) where from 1997 until her retirement she was a Managing Director in the firms Mortgage Backed and Asset Backed Finance Group. She is a widely recognized expert in the areas of housing finance and securitization and is a member of the board of the Housing Preservation Foundation as well as having previously served two terms on the board of the American Securitization Trade Association. Prior to joining RBS Ms. Larocca was a Senior ... Larry Harvey has served as Chief Financial Officer of Playa Hotels & Resorts B.V. since April 2015. From 2007 to 2013 Mr. Harvey served as Executive Vice President and Chief Financial Officer of Host Hotels & Resorts Inc. (NYSE: HST) (Host) and served as its Treasurer from 2007 to 2010. From 2006 to 2007 Mr. Harvey served as Senior Vice President Chief Accounting Officer of Host and from 2003 to 2006 he served as Hosts Senior Vice President and Corporate Controller. Prior to rejoining Ho... Dr. Puryear is Professor Emeritus of Management and Entrepreneurship at Baruch College of the City University of New York where he was the initial recipient of the Lawrence N. Field Professorship in Entrepreneurship. Dr. Puryear is also a management consultant who advises existing and new businesses with high-growth potential. Prior to his appointment at Baruch College Dr. Puryear was on the faculty of the graduate school of business administration at Rutgers University. During leaves of a... Other Officers of Our Manager Jason Campbell is a Senior Vice President of our Manager. He is primarily responsible for asset and liability management for us and other funds managed by American Capital Mortgage Management LLCs subsidiaries. Mr. Campbell is also a Senior Vice President of American Capital AGNC Management LLC the manger of American Capital Agency Corp. (NASDAQ: AGNC). Previously Mr. Campbell was a Vice President of American Capital Ltd. (NASDAQ: ACAS) in its Financial Services Investment Practice Group... Mr. Doshi is a Vice President of the parent company of our Manager and is primarily responsible for managing agency investment activities for us and American Capital Agency Corp. (Nasdaq: AGNC). Mr. Doshis primary focus is executing specific agency investment strategies for MTGE. Mr. Doshi was previously Vice President Global Securitized Markets in the Fixed Income Global Capital Markets Group at Citigroup Inc. where he was primarily responsible for making markets in Agency MBS pools. Mr. ... M. Song Jo is a Vice President of our Manager and is primarily responsible for mortgage structuring for us and other funds managed by American Capital Mortgage Management LLCs subsidiaries. Mr. Jo is also a Vice President of American Capital AGNC Management LLC the manager of American Capital Agency Corp (NASDAQ: AGNC). Mr. Jo previously served as Vice President Mortgage Structuring Investment & Capital Markets at Freddie Mac where he was primarily responsible for managing mortgage str... Vice President and Controller American Capital Mortgage Investment Corp. Vice President and Chief Financial Officer American Capital MTGE Management LLC Vice President and Controller American Capital Mortgage Investment Corp. Vice President and Chief Financial Officer American Capital MTGE Management LLC Donald Holley is Vice President and Chief Financial Officer of our Manager and Vice President and Controller of American Capital Mortgage Investment Corp. Prior to joining American Capital Mr. Holley was a Director at Freddie Mac responsible for the valuation and presentation of financial results for the companys investments debt and derivative portfolios. He has also worked as an accounting policy and assurance Director at Credit Suisse prepared corporate governance research at Deutsche B... Thomas A. McHale is Vice President and Assistant Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. McHale is also a Vice President of American Capital AGNC Management LLC the manager of American Capital Agency Corp. (NASDAQ: AGNC) and of American Capital LLC the fund management portfolio company of American Capital Ltd. (NASDAQ: ACAS). In addition Mr. McHale has served as the Senior Vice President Finance of American Capital Ltd. since May... Sean Reid is a Senior Vice President of our Manager and serves as Senior Vice President Corporate and Business Development of American Capital Mortgage Management LLC. Mr. Reid is also a Senior Vice President of American Capital AGNC Management LLC the manager of American Capital Agency Investment Corp. (NASDAQ: AGNC). Previously Mr. Reid served as Vice President of the Washington D.C. Buyouts group of American Capital Ltd. (NASDAQ: ACAS) and as an Assistant General Counsel with Americ...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:48 [scrapy] ERROR: Error processing {'pagetitle': [u'2015 American Capital Mortgage Investment Corp. Press Releases - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&nyo=0&p=irol-news'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.28 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of $(25.1) million or $(0.49) per common share and net book value of $19.93 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for... American Capital Mortgage Investment Corp. Will Report Third Quarter 2015 Results on October 28; Shareholder Call Scheduled for October 29 BETHESDA Md. Oct.13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on October 28 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 29 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free l... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. Sept.17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the third quarter 2015. The dividend is payable on October 15 2015 to preferred shareholders of record as of October 1 2015 with an ex-dividend date of Sep... American Capital Mortgage Investment Corp. Declares Third Quarter Common Stock Dividend of $0.40 Per Share and Announces the Repurchase of 1.2 Million Shares BETHESDA Md. Sept. 17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.40 per share of common stock for the third quarter 2015. The dividend is payable on October 27 2015 to common shareholders of record as of September 30 2015 with an ex-dividend date of September 28 2015. The Company also announced today that in the third quarter of 2015 it made ope... American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of $(41.1) million or $(0.80) per common share and net book value of $20.70 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarte... American Capital Mortgage Investment Corp. Will Report Second Quarter 2015 Results on July 29; Shareholder Call Scheduled for July 30 BETHESDA Md. July 8 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report second quarter 2015 earnings after market close on July 29 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on July 30 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast... American Capital Mortgage Investment Corp. Declares Second Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. June 15 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the second quarter 2015. The dividend is payable on July 15 2015 to preferred shareholders of record as of July 1 2015 with an ex-dividend date of June 29 ... American Capital Mortgage Investment Corp. Declares Second Quarter Common Stock Dividend of $0.50 Per Share BETHESDA Md. June 15 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.50 per share of common stock for the second quarter 2015. The dividend is payable on July 27 2015 to common shareholders of record as of June 30 2015 with an ex-dividend date of June 26 2015. For further information or questions please contact the Investor Relations Department at (3... American Capital Mortgage Investment Corp. Reports $0.59 Net Income Per Common Share For The First Quarter And $22.00 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. April29 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended March31 2015 of $29.9 million or $0.59 per common share and net book value of $22.00 per common share. Economic return for the period defined as dividends and change in net book value per common share was 2.7% for the qua... American Capital Mortgage Investment Corp. Will Report First Quarter 2015 Results on April 29 Shareholder Call Scheduled for April 30 BETHESDA Md. April 9 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report first quarter 2015 earnings after market close on April 29 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on April 30 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged ... American Capital Mortgage Investment Corp. Declares First Quarter Common Stock Dividend of $0.50 Per Share BETHESDA Md. March 19 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.50 per share of common stock for the first quarter 2015. The dividend is payable on April 27 2015 to common shareholders of record as of March 31 2015 with an ex-dividend date of March 27 2015. For further information or questions please contact the Investor Relations Department at... American Capital Mortgage Investment Corp. Reports $0.33 Net Income Per Common Share For The Fourth Quarter $3.06 Per Common Share For 2014 And $21.91 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Feb.4 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended December31 2014 of $16.7 million or $0.33 per common share and net book value of $21.91 per common share. Economic return for the period defined as dividends and change in net book value per common share was 1.4% for the quart... American Capital Mortgage Investment Corp. Will Report Fourth Quarter 2014 Results on February 4; Shareholder Call Scheduled for February 5 BETHESDA Md. Jan. 13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report fourth quarter 2014 earnings after market close on February 4 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on February 5 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live ...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:49 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001201469.PDF?Y=&O=PDF&D=&fid=1001201469&T=&iid=4147324> (referer: http://shareholders.fortress.com/CorporateProfile.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:49 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500071609.PDF?Y=&O=PDF&D=&fid=1500071609&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:49 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500068723.PDF?Y=&O=PDF&D=&FID=1500068723&T=&IID=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:50 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog The Dry Bulk Market \u2013 A Changing Horizon'],
 'pageurl': ['http://www.blogcastalia.com/the-dry-bulk-market-a-changing-horizon/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:51 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Greenshields Agri Holdings Plc'],
 'pageurl': ['http://www.blogcastalia.com/greenshields-agri-holdings-plc-merger-announcement/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:52 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Weather'],
 'pageurl': ['http://www.blogcastalia.com/category/weather/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:53 [scrapy] ERROR: Error processing {'pagetitle': [u'Pomona Capital'],
 'pageurl': ['http://www.pomonacapital.com/approach'],
 'siteurl': ['pomonacapital.com'],
 'text': ['Since Pomonas founding our fundamental strategy has been based on the following core tenets: focus relentlessly on quality; invest prudently and patiently; measure and learn from results; be open and responsive; and align our financial interest with those of our investors. Secondary Strategy For over 20 years Pomona has implemented a differentiated global and theme-based secondary strategy to acquire high-quality assets at meaningful discounts. Pomona maintains a consistent disciplined and nimble investment strategy based on: (i) proactively sourcing less competitive transactions; (ii) developing an understanding of target assets using granular company-level analysis; (iii) focusing on what we believe to be the highest quality of assets; (iv) sacrificing investment volume to maintain pricing discipline; and (v) diversifying investments to reduce risk. In our investment process we seek to mitigate risk and increase liquidity in buying mature high-quality private equity assets. We have raised funds sized in line with the market opportunity in order to allow us to be highly selective avoid competition and focus on middle market-sized transactions. We believe being principal investors rather than asset gatherers better aligns our interests with investors. The result is consistent growth and significant returns over a long period of time through multiple market cycles.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:53 [scrapy] ERROR: Error processing {'pagetitle': [u'Pomona Capital'],
 'pageurl': ['http://www.pomonacapital.com/investments'],
 'siteurl': ['pomonacapital.com'],
 'text': ['Pomonas primary investment activity that began in 1998 evolved from our unique perspective as secondary purchasers. Today we manage $3.4 billion in committed capital across various investment mandates with a specific focus on separate accounts. Our strategy is driven by individual company analysis to identify those groups proven to consistently generate superior returns. By leveraging our secondary relationships with leading managers we gain access insight and capacity to those partnerships. We make commitments to buyout venture capital and mezzanine funds located in the U.S. Europe and Asia and tailor our primary investing programs to investors specific goals.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Pomona Capital'],
 'pageurl': ['http://www.pomonacapital.com/'],
 'siteurl': ['pomonacapital.com'],
 'text': ['CEO Michael Granoff reflects on how Pomona Capital has - or has not -adapted to the changing secondaries market months after closing its latest fund.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Pomona Capital'],
 'pageurl': ['http://www.pomonacapital.com/contact'],
 'siteurl': ['pomonacapital.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:55 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Casino_ABS.aspx'],
 'siteurl': ['onex.com'],
 'text': ['How We Are Invested']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:56 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Mad as Hell \u2013 This is Inexcusable'],
 'pageurl': ['http://www.blogcastalia.com/mad-as-hell-this-is-inexcusable/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:57 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog The Chinese Shadow Banking Market: One Failure to Financial Crisis'],
 'pageurl': ['http://www.blogcastalia.com/the-chinese-shadow-banking-market-one-failure-to-financial-crisis/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:58 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001191616.PDF?Y=&O=PDF&D=&FID=1001191616&T=&IID=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:54:59 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Corn & Soybean Condition and Chinese Imports/Soybeans'],
 'pageurl': ['http://www.blogcastalia.com/corn-soybean-condition-and-chinese-importssoybeans/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:54:59 [scrapy] ERROR: Error processing {'pagetitle': [u'Investor Relations Home - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-irhome'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.28 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of $(25.1) million or $(0.49) per common share and net book value of $19.93 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for... American Capital Mortgage Investment Corp. Will Report Third Quarter 2015 Results on October 28; Shareholder Call Scheduled for October 29 BETHESDA Md. Oct.13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on October 28 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 29 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free l... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. Sept.17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the third quarter 2015. The dividend is payable on October 15 2015 to preferred shareholders of record as of October 1 2015 with an ex-dividend date of Sep... American Capital Mortgage Investment Corp. Declares Third Quarter Common Stock Dividend of $0.40 Per Share and Announces the Repurchase of 1.2 Million Shares BETHESDA Md. Sept. 17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.40 per share of common stock for the third quarter 2015. The dividend is payable on October 27 2015 to common shareholders of record as of September 30 2015 with an ex-dividend date of September 28 2015. The Company also announced today that in the third quarter of 2015 it made ope... American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of $(41.1) million or $(0.80) per common share and net book value of $20.70 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarte...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:01 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Agriculture \u2013 Soybeans 2015/2016 / Are Chinese Imports Going to Accelerate?'],
 'pageurl': ['http://www.blogcastalia.com/agriculture-soybeans-20152016-are-chinese-imports-going-to-accelerate/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:02 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog \u201cToo Big to Fail\u201d \u2013 Regulatory and Technology'],
 'pageurl': ['http://www.blogcastalia.com/too-big-to-fail-regulatory-and-technology/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:02 [scrapy] ERROR: Error downloading <GET http://www.arg>: DNS lookup failed: address 'www.arg' not found: [Errno -2] Name or service not known.
2015-11-04 06:55:02 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500072326.PDF?Y=&O=PDF&D=&FID=1500072326&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:02 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/00336710-71f5-4e70-9517-c6edb0be1217.pdf> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:03 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/admin/Magellan_Health_Services.aspx'],
 'siteurl': ['onex.com'],
 'text': ['How We Are Invested']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:04 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Genesis_Healthcare_Group.aspx'],
 'siteurl': ['onex.com'],
 'text': ['How We Are Invested']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:05 [scrapy] ERROR: Error processing {'pagetitle': [u' ONEX'],
 'pageurl': ['http://www.onex.com/Genesis_Healthcare.aspx'],
 'siteurl': ['onex.com'],
 'text': ['The amounts above include Onex share of the carried interest and are net of any payments under the Management Investment Plan. (1) In February 2015 Skilled Healthcare Group combined with Genesis HealthCare. The combined company operates under the Genesis Healthcare name. In late December 2005 Onex Onex Partners I and Onex management (together the "Onex Partners I Group") completed the acquisition of Skilled Healthcare Group Inc. ("Skilled Healthcare Group") in a transaction valued at $640 million. The Onex Partners I Group invested $207million for a 93% economic interest. Onex net investment in this acquisition was $49million for an initial 22% economic interest. In May 2007 Skilled Healthcare Group completed an initial public offering of 8.3 million new common shars. As part of the offering the Onex Partners I Group sold 10.6million shares of which Onex portion was 2.5million shares. In August 2014 Skilled Healthcare Group entered into an agreement to combine with Genesis HealthCare LLC ("Genesis HealthCare") a leading U.S. operator of long-term care facilities. The transaction was completed in February 2015. In accordance with the terms of the purchase and combination agreement each share of the Skilled Healthcare Group common stock issued and outstanding immediately prior to the closing of the transaction was converted into shares of the newly combined company. Skilled Healthcare Group shareholders own approximately 26 percent of the combined company of which the Onex Partners I Groups share of the economic ownership is 10 percent. The Onex Partners I Groups voting owndership has beenreduced to 10 percent from 86 percent before the combination. The combined company now operates under the Genesis Healthcare name and continues to be publicy traded (NYSE: GEN). Genesis Healthcare is a holding company with subsidiaries that on a combined basis comprise one of the nations largest post-acute care providers with more than 500 skilled nursing centers and assisted/senior living communities in 34 states nationwide. Genesis subsidiaries also supply rehabilitation and respitory therapy to more than 1800 healthcare providers in 47 states and the District of Columbia. In late December 2005 Onex Onex Partners I and Onex management (together the "Onex Partners I Group") completed the acquisition of Skilled Healthcare Group Inc. ("Skilled Healthcare Group") in a transaction valued at $640 million. The Onex Partners I Group invested $207million for a 93% economic interest. Onex net investment in this acquisition was $49million for an initial 22% economic interest.In May 2007 Skilled Healthcare Group completed an initial public offering of 8.3 million new common shars. As part of the offering the Onex Partners I Group sold 10.6million shares of which Onex portion was 2.5million shares.In August 2014 Skilled Healthcare Group entered into an agreement to combine with Genesis HealthCare LLC ("Genesis HealthCare") a leading U.S. operator of long-term care facilities. The transaction was completed in February 2015. In accordance with the terms of the purchase and combination agreement each share of the Skilled Healthcare Group common stock issued and outstanding immediately prior to the closing of the transaction was converted into shares of the newly combined company. Skilled Healthcare Group shareholders own approximately 26 percent of the combined company of which the Onex Partners I Groups share of the economic ownership is 10 percent. The Onex Partners I Groups voting owndership has beenreduced to 10 percent from 86 percent before the combination. The combined company now operates under the Genesis Healthcare name and continues to be publicy traded (NYSE: GEN).Genesis Healthcare is a holding company with subsidiaries that on a combined basis comprise one of the nations largest post-acute care providers with more than 500 skilled nursing centers and assisted/senior living communities in 34 states nationwide. Genesis subsidiaries also supply rehabilitation and respitory therapy to more than 1800 healthcare providers in 47 states and the District of Columbia.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:05 [scrapy] ERROR: Error processing {'pagetitle': [u' Sciens : Sciens Asset Management'],
 'pageurl': ['http://www.sciensam.com/news/page/2/'],
 'siteurl': ['sciensam.com'],
 'text': ['LONDON NEW YORK 5 December 2013 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announces that Eiffel Credit Opportunities a long-short sector focused European credit fund will be replicated in a new cell on the Sciens Managed [] LONDON NEW YORK 28 August 2013 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announced the continued growth of the Sciens Managed Account Platform with assets on the Platform increasing by circa 35% year-on-year to end June [] MIAMI LONDON DUBLIN SINGAPORE February 6 2013 Apollo Aviation Group (Apollo Aviation) and Sciens Capital Management LLC jointly announced today that Apollo Aviation raised approximately $595 million from a broad array of investors for its second aviation fund Sciens Aviation Special Opportunities Investment Fund II (SASOF II). Please click on the link below to [] NEW YORK LONDON 30th January 2013 Sciens Alternative Investments part of the Sciens Capital Management Group and provider of single- and multi-strategy funds of hedge funds and managed account services today announces its US launch of the Sciens Managed Account Risk Technologies (S.M.A.R.T.) an interactive risk analytics and portfolio construction engine that enables investors []']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:05 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500073087.PDF?Y=&O=PDF&D=&FID=1500073087&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:05 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500076664.PDF?Y=&O=PDF&D=&FID=1500076664&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:06 [scrapy] ERROR: Error processing {'pagetitle': [u'BlackSand Capital - What We Do'],
 'pageurl': ['http://www.blacksandcapital.com/whatwedo'],
 'siteurl': ['blacksandcapital.com'],
 'text': ['Founded in 2010 BlackSand Capital is a real estate private equity firm distinguished by its integrity innovation flexibility and intimate knowledge of the Hawaii market. It stands out as the only Hawaii-focused real estate private equity fund. BlackSand Capitals strong foundation rises from a family legacy that began with The MacNaughton Group and Kobayashi Group real estate development firms that have been active in Hawaii for over 70 years. The long-standing relationships emerging from decades of professional partnerships refine the firms acute understanding of real estate fundamentals and strengthen its ethical responsibility to the community. These enduring relationships and the depth of experience allow BlackSand to capture rare opportunities often not presented in the market.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.pomonacapital.com/about>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.pomonacapital.com/../about>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.tia>: DNS lookup failed: address 'www.tia' not found: [Errno -2] Name or service not known.
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.6800capitalllc.com/logout>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.pomonacapital.com/news>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.pomonacapital.com/team>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:55:06 [scrapy] ERROR: Error downloading <GET http://www.pomonacapital.com/team/flynn>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:55:07 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/510b3b47-48fe-48d5-8d7c-7b3cb978d19e.pdf> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:07 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001203733.PDF?Y=&O=PDF&D=&FID=1001203733&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:07 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500076663.PDF?Y=&O=PDF&D=&FID=1500076663&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:07 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500074337.PDF?Y=&O=PDF&D=&FID=1500074337&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:09 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog 2015'],
 'pageurl': ['http://www.blogcastalia.com/2015/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:11 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog'],
 'pageurl': ['http://www.blogcastalia.com/page/2/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:13 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Aircraft'],
 'pageurl': ['http://www.blogcastalia.com/category/aircraft/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:15 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Agricultural Markets'],
 'pageurl': ['http://www.blogcastalia.com/category/agriculture-markets/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:17 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Airlines'],
 'pageurl': ['http://www.blogcastalia.com/category/airlines/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:17 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2027177&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of per share of common stock for the first quarter 2015. The dividend is payable on to common shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-first-quarter-common-stock-dividend-of-050-per-share-300053235.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:17 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2027177&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of per share of common stock for the first quarter 2015. The dividend is payable on to common shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-first-quarter-common-stock-dividend-of-050-per-share-300053235.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:17 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2033657&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report first quarter 2015 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q1 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10063171. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-first-quarter-2015-results-on-april-29-300063709.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:18 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2033657&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report first quarter 2015 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q1 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10063171. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-first-quarter-2015-results-on-april-29-300063709.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:21 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2041742&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $0.59 Net Income Per Common Share For The First Quarter And $22.00 Net Book Value Per Common Share To download financial tables please click here. BETHESDA Md. April29 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended March31 2015 of or per common share and net book value of per common share. Economic return for the period defined as dividends and change in net book value per common share was 2.7% for the quarter or 11.0% on an annualized basis. net income per common share Includes all unrealized gains and losses on investment and hedging portfolios net spread and dollar roll income per common share Includes estimated dollar roll loss per common share associated with the Companys average net short position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market dividend per common share 11.1% annualized dividend yield based on closing stock price of per common share net book value per common share as of Increased per common share or 0.4% from per common share as of 2.7% economic return on common equity for the quarter or 11.0% annualized Comprised of dividend per common share and increase in net book value per common share 4.5x "at risk" leverage as of 7.7% agency securities actual CPR for the quarter 8.9% projected life CPR for agency securities as of 2.26% annualized net spread and dollar roll income for the quarter excluding estimated "catch-up" premium amortization "We are pleased with MTGEs performance this quarter as our portfolio generated an economic return of 11% against the backdrop of a very challenging interest rate environment" commented President and Chief Investment Officer. "Our solid performance this quarter was enhanced by the actions we took late last year to reposition our portfolio toward a more defensive posture by reducing our leverage and interest rate risk." "Both our agency and non-agency MBS performed well during the first quarter as we continued our focus on sound asset selection prudent risk management and an increased focus on prepayment risk" continued Mr. Kain. "We were especially pleased with the strong performance of the GSE credit risk sharing securities during the quarter (and into April) as we have increased our capital allocation to this sector." "MTGE continues to deliver solid risk-adjusted returns over a wide range of market conditions a testament to prudent capital allocation and disciplined risk management" commented Chair and Chief Executive Officer."MTGEs value proposition is even more compelling when viewed in the context of the historically low interest rate environment and the expected returns available in other fixed income asset classes both in the U.S. and around the world." INVESTMENT PORTFOLIO As of March31 2015 the Companys investment portfolio included of agency MBS of net long TBA securities of non-agency MBS and of MSR. As of March31 2015 the Companys agency investment portfolio inclusive of net long TBA was comprised of of fixed rate and of adjustable rate securities. As of March31 2015 the Companys agency fixed rate investments were comprised of 15 year securities 20 year securities 30 year securities 15 year net short TBA securities and 30 year net long TBA securities. As of March31 2015 15 year fixed rate investments represented 30% of the Companys agency investment portfolio a decrease from 43% as of and 30 year fixed rate investments represented 63% of the Companys agency investment portfolio an increase from 50% as of . As of March31 2015 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.35% compared to 3.23% as of comprised of the following weighted average coupons: As of March31 2015 the Companys non-agency portfolio was comprised of 38% Alt-A 16% prime 15% credit risk transfer 13% option ARM and 18% subprime securities. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of March31 2015 the Companys net long TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. AGENCY CONSTANT PREPAYMENT RATES The actual CPR for the Companys agency portfolio during the first quarter of 2015 was 7.7% down from 8.0% during the fourth quarter. The CPR published in for the Companys agency portfolio held as of March31 2015 was 9.5% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of March31 2015 was 8.9% compared to 8.2% as of 2014. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share with no significant impact from "catch-up" premium amortization during the quarter. The weighted average cost basis of the Companys agency securities was 104.9% of par and the unamortized agency net premium was as of March31 2015. NON-AGENCY DISCOUNT ACCRETION The weighted average cost basis of the Companys non-agency portfolio was 83.1% of par as of March31 2015. Accretion income on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of March31 2015 with designated as credit reserves. ASSET YIELDS COST OF FUNDS AND NET INTEREST RATE SPREAD The Companys average annualized net interest rate spread and dollar roll income for the first quarter was 2.25% consistent with the fourth quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.19% for the first quarter up 3 bps from 2.16% for the fourth quarter. The Companys average asset yield on its MBS portfolio for the first quarter was 3.15% compared to 3.18% for the fourth quarter. Excluding the impact of "catch-up" premium amortization expense recognized due to changes in projected CPR estimates the annualized weighted average yield on the Companys MBS portfolio was 3.16% for the first quarter compared to 3.31% for the fourth quarter. The Companys asset yield as of March31 2015 was 3.26% up 2 bps from 3.24% as of . The Companys average cost of funds was 0.96% for the first quarter (derived from the cost of repurchase agreements and effective interest rate swaps) compared to 1.02% for the fourth quarter. The Companys average cost of funds of 1.02% as of March31 2015 was consistent with . LEVERAGE AND HEDGING ACTIVITIES As of March31 2015 of the Companys repurchase agreements were used to fund purchases of agency and non-agency securities while the remaining were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including TBA securities the Companys "at risk" leverage ratio was 4.5x as of March31 2015 and averaged 4.5x during the first quarter. The borrowed under agency and non-agency repurchase agreements as of March31 2015 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; between six and nine months; and As of March31 2015 the Companys agency and non-agency repurchase agreements had an average of 183 days remaining to maturity down from 210 days as of . As of March31 2015 the Company had repurchase agreements with 32 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 21% of the Companys equity at risk. The Companys interest rate swap positions as of March31 2015 totaled in notional amount with a weighted average fixed pay rate of 2.14% a weighted average receive rate of 0.26% and a weighted average maturity of 4.6 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.37% as of March31 2015. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of March31 2015 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 1.2 years. These swaptions have an underlying weighted average interest rate swap term of 7.5 years and a weighted average pay rate of 3.43% as of March31 2015. In addition to its interest rate swaps and swaptions the Company held a net long position in U.S. Treasury securities and futures. As of March31 2015 67% of the Companys combined repurchase agreement and net TBA balance was hedged through a combination of interest rate swaps interest rate swaptions U.S. Treasury securities and futures and total return swaps. SERVICING As of March31 2015 ("RCS") managed a servicing portfolio of approximately 65000 residential mortgage loans representing approximately in unpaid principal balances. During the first quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. OTHER GAINS (LOSSES) NET The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the first quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized loss on periodic settlements of interest rate swaps; of net realized gain on other derivatives and securities; of net unrealized loss on other derivatives and securities; and Realized and unrealized net losses on other derivatives and securities during the first quarter include of net loss on interest rate swaps and swaptions of net gain on U.S. treasury securities and futures and of net gain on TBA mortgage positions (including of dollar roll loss). ESTIMATED TAXABLE INCOME REIT taxable income for the first quarter is estimated at per common share or lower than GAAP net income per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) timing differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the first quarter excludes per share of estimated net capital gains which are offset by the Companys capital loss carryforwards from prior periods. As of March31 2015 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of March31 2015 the Company had estimated remaining unutilized net capital losses of or per common share which may be carried forward and applied against future net capital gains through 2018. Additionally as of March31 2015 the Company had estimated net deferred gains from terminated swaps and swaptions of or per common share which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. FIRST QUARTER 2015 DIVIDEND DECLARATION On the Board of Directors of the Company declared a first quarter dividend on its common stock of per share which was paid on to common stockholders of record as of 2015. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a first quarter dividend on its Series A Preferred Stock of per share. The dividend was paid on to preferred stockholders of record as of 2015. Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. FINANCIAL STATEMENTS OPERATING PERFORMANCE AND PORTFOLIO STATISTICS The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Unrealized loss on other derivatives and securities net Net spread and dollar roll income available to Net spread and dollar roll income per common share basic and diluted Average cost of funds as of period end Net book value per common share as of Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. Excludes servicing expenses related to the Companys investment in RCS. The Companys estimated taxable income for the first quarter excludes $0.51 per common share of estimated net capital gains which are offset by the Companys capital loss carryforwards from prior periods. Excluding the Companys investment in RCS the average stockholders equity for the first quarter was $1.1 billion. Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency repurchase agreements for the period by the Companys average month-ended stockholders equity for the period less investments in RCS and REIT equity securities. Leverage excludes U.S. Treasury repurchase agreements. Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency repurchase agreements and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. STOCKHOLDER CALL MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who do plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q1 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10063171. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or . ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . ABOUT (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.americancapital.com. FORWARD LOOKING STATEMENTS This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-059-net-income-per-common-share-for-the-first-quarter-and-2200-net-book-value-per-common-share-300074590.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:25 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2041742&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $0.59 Net Income Per Common Share For The First Quarter And $22.00 Net Book Value Per Common Share To download financial tables please click here. BETHESDA Md. April29 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended March31 2015 of or per common share and net book value of per common share. Economic return for the period defined as dividends and change in net book value per common share was 2.7% for the quarter or 11.0% on an annualized basis. net income per common share Includes all unrealized gains and losses on investment and hedging portfolios net spread and dollar roll income per common share Includes estimated dollar roll loss per common share associated with the Companys average net short position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market dividend per common share 11.1% annualized dividend yield based on closing stock price of per common share net book value per common share as of Increased per common share or 0.4% from per common share as of 2.7% economic return on common equity for the quarter or 11.0% annualized Comprised of dividend per common share and increase in net book value per common share 4.5x "at risk" leverage as of 7.7% agency securities actual CPR for the quarter 8.9% projected life CPR for agency securities as of 2.26% annualized net spread and dollar roll income for the quarter excluding estimated "catch-up" premium amortization "We are pleased with MTGEs performance this quarter as our portfolio generated an economic return of 11% against the backdrop of a very challenging interest rate environment" commented President and Chief Investment Officer. "Our solid performance this quarter was enhanced by the actions we took late last year to reposition our portfolio toward a more defensive posture by reducing our leverage and interest rate risk." "Both our agency and non-agency MBS performed well during the first quarter as we continued our focus on sound asset selection prudent risk management and an increased focus on prepayment risk" continued Mr. Kain. "We were especially pleased with the strong performance of the GSE credit risk sharing securities during the quarter (and into April) as we have increased our capital allocation to this sector." "MTGE continues to deliver solid risk-adjusted returns over a wide range of market conditions a testament to prudent capital allocation and disciplined risk management" commented Chair and Chief Executive Officer."MTGEs value proposition is even more compelling when viewed in the context of the historically low interest rate environment and the expected returns available in other fixed income asset classes both in the U.S. and around the world." INVESTMENT PORTFOLIO As of March31 2015 the Companys investment portfolio included of agency MBS of net long TBA securities of non-agency MBS and of MSR. As of March31 2015 the Companys agency investment portfolio inclusive of net long TBA was comprised of of fixed rate and of adjustable rate securities. As of March31 2015 the Companys agency fixed rate investments were comprised of 15 year securities 20 year securities 30 year securities 15 year net short TBA securities and 30 year net long TBA securities. As of March31 2015 15 year fixed rate investments represented 30% of the Companys agency investment portfolio a decrease from 43% as of and 30 year fixed rate investments represented 63% of the Companys agency investment portfolio an increase from 50% as of . As of March31 2015 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.35% compared to 3.23% as of comprised of the following weighted average coupons: As of March31 2015 the Companys non-agency portfolio was comprised of 38% Alt-A 16% prime 15% credit risk transfer 13% option ARM and 18% subprime securities. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of March31 2015 the Companys net long TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. AGENCY CONSTANT PREPAYMENT RATES The actual CPR for the Companys agency portfolio during the first quarter of 2015 was 7.7% down from 8.0% during the fourth quarter. The CPR published in for the Companys agency portfolio held as of March31 2015 was 9.5% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of March31 2015 was 8.9% compared to 8.2% as of 2014. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share with no significant impact from "catch-up" premium amortization during the quarter. The weighted average cost basis of the Companys agency securities was 104.9% of par and the unamortized agency net premium was as of March31 2015. NON-AGENCY DISCOUNT ACCRETION The weighted average cost basis of the Companys non-agency portfolio was 83.1% of par as of March31 2015. Accretion income on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of March31 2015 with designated as credit reserves. ASSET YIELDS COST OF FUNDS AND NET INTEREST RATE SPREAD The Companys average annualized net interest rate spread and dollar roll income for the first quarter was 2.25% consistent with the fourth quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.19% for the first quarter up 3 bps from 2.16% for the fourth quarter. The Companys average asset yield on its MBS portfolio for the first quarter was 3.15% compared to 3.18% for the fourth quarter. Excluding the impact of "catch-up" premium amortization expense recognized due to changes in projected CPR estimates the annualized weighted average yield on the Companys MBS portfolio was 3.16% for the first quarter compared to 3.31% for the fourth quarter. The Companys asset yield as of March31 2015 was 3.26% up 2 bps from 3.24% as of . The Companys average cost of funds was 0.96% for the first quarter (derived from the cost of repurchase agreements and effective interest rate swaps) compared to 1.02% for the fourth quarter. The Companys average cost of funds of 1.02% as of March31 2015 was consistent with . LEVERAGE AND HEDGING ACTIVITIES As of March31 2015 of the Companys repurchase agreements were used to fund purchases of agency and non-agency securities while the remaining were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including TBA securities the Companys "at risk" leverage ratio was 4.5x as of March31 2015 and averaged 4.5x during the first quarter. The borrowed under agency and non-agency repurchase agreements as of March31 2015 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; between six and nine months; and As of March31 2015 the Companys agency and non-agency repurchase agreements had an average of 183 days remaining to maturity down from 210 days as of . As of March31 2015 the Company had repurchase agreements with 32 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 21% of the Companys equity at risk. The Companys interest rate swap positions as of March31 2015 totaled in notional amount with a weighted average fixed pay rate of 2.14% a weighted average receive rate of 0.26% and a weighted average maturity of 4.6 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.37% as of March31 2015. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of March31 2015 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 1.2 years. These swaptions have an underlying weighted average interest rate swap term of 7.5 years and a weighted average pay rate of 3.43% as of March31 2015. In addition to its interest rate swaps and swaptions the Company held a net long position in U.S. Treasury securities and futures. As of March31 2015 67% of the Companys combined repurchase agreement and net TBA balance was hedged through a combination of interest rate swaps interest rate swaptions U.S. Treasury securities and futures and total return swaps. SERVICING As of March31 2015 ("RCS") managed a servicing portfolio of approximately 65000 residential mortgage loans representing approximately in unpaid principal balances. During the first quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. OTHER GAINS (LOSSES) NET The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the first quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized loss on periodic settlements of interest rate swaps; of net realized gain on other derivatives and securities; of net unrealized loss on other derivatives and securities; and Realized and unrealized net losses on other derivatives and securities during the first quarter include of net loss on interest rate swaps and swaptions of net gain on U.S. treasury securities and futures and of net gain on TBA mortgage positions (including of dollar roll loss). ESTIMATED TAXABLE INCOME REIT taxable income for the first quarter is estimated at per common share or lower than GAAP net income per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) timing differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the first quarter excludes per share of estimated net capital gains which are offset by the Companys capital loss carryforwards from prior periods. As of March31 2015 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of March31 2015 the Company had estimated remaining unutilized net capital losses of or per common share which may be carried forward and applied against future net capital gains through 2018. Additionally as of March31 2015 the Company had estimated net deferred gains from terminated swaps and swaptions of or per common share which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. FIRST QUARTER 2015 DIVIDEND DECLARATION On the Board of Directors of the Company declared a first quarter dividend on its common stock of per share which was paid on to common stockholders of record as of 2015. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a first quarter dividend on its Series A Preferred Stock of per share. The dividend was paid on to preferred stockholders of record as of 2015. Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. FINANCIAL STATEMENTS OPERATING PERFORMANCE AND PORTFOLIO STATISTICS The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Unrealized loss on other derivatives and securities net Net spread and dollar roll income available to Net spread and dollar roll income per common share basic and diluted Average cost of funds as of period end Net book value per common share as of Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. Excludes servicing expenses related to the Companys investment in RCS. The Companys estimated taxable income for the first quarter excludes $0.51 per common share of estimated net capital gains which are offset by the Companys capital loss carryforwards from prior periods. Excluding the Companys investment in RCS the average stockholders equity for the first quarter was $1.1 billion. Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency repurchase agreements for the period by the Companys average month-ended stockholders equity for the period less investments in RCS and REIT equity securities. Leverage excludes U.S. Treasury repurchase agreements. Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency repurchase agreements and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. STOCKHOLDER CALL MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who do plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q1 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10063171. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or . ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . ABOUT (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.americancapital.com. FORWARD LOOKING STATEMENTS This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-059-net-income-per-common-share-for-the-first-quarter-and-2200-net-book-value-per-common-share-300074590.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:26 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2059438&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of per share of common stock for the second quarter 2015. The dividend is payable on to common shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-second-quarter-common-stock-dividend-of-050-per-share-300099357.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:26 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001203339.PDF?Y=&O=PDF&D=&FID=1001203339&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:26 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500077445.PDF?Y=&O=PDF&D=&FID=1500077445&T=&IID=4147324> (referer: http://shareholders.fortress.com/news.aspx?IID=4147324&mode=1)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:26 [scrapy] INFO: Crawled 1906 pages (at 91 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:55:26 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500074347.PDF?Y=&O=PDF&D=&fid=1500074347&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:27 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500077284.PDF?Y=&O=PDF&D=&fid=1500077284&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:27 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2059442&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of per share for the second quarter 2015. The dividend is payable on to preferred shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-second-quarter-dividend-on-its-series-a-preferred-stock-300099360.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:27 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2059442&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of per share for the second quarter 2015. The dividend is payable on to preferred shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-second-quarter-dividend-on-its-series-a-preferred-stock-300099360.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:27 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2065749&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report second quarter 2015 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q2 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10068977. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-second-quarter-2015-results-on-july-29-300110482.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:28 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2065749&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report second quarter 2015 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q2 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10068977. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-second-quarter-2015-results-on-july-29-300110482.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:32 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2072349&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of or per common share and net book value of per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarter or (14.7)% on an annualized basis. net loss per common share Includes all unrealized gains and losses on investment and hedging portfolios net spread and dollar roll income per common share excluding estimated "catch-up" premium amortization benefit Includes estimated dollar roll income per common share associated with the Companys average net long position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market Excludes per common share of estimated "catch-up" premium amortization benefit due to change in projected constant prepayment rate ("CPR") estimates dividend per common share 12.5% annualized dividend yield based on June30 2015 closing stock price of per common share net book value per common share as of June30 2015 Decreased per common share or (5.9)% from per common share as of (3.7)% economic loss on common equity for the quarter or (14.7)% annualized Comprised of dividend per common share and decrease in net book value per common share 4.1x "at risk" leverage as of June30 2015 10.2% agency securities actual CPR for the quarter 8.1% projected life CPR for agency securities as of June30 2015 2.48% annualized net interest rate spread for the quarter including TBA dollar roll income Includes 12 bps of "catch-up" premium amortization benefit due to change in projected CPR estimates "Fixed income markets around the globe continued to be extremely volatile during the second quarter" commented President and Chief Investment Officer. "This challenging environment led to negative economic earnings for the quarter as higher rates a steeper yield curve and wider spreads caused agency MBS to post their second worst quarterly performance over the last seven years. The performance of legacy non-agency MBS was more neutral as the increased interest rate and spread volatility was offset somewhat by continued improvement in several key housing fundamentals such as house prices employment growth and favorable delinquency trends." "In response to these volatile market conditions we continued to prioritize risk management and long term portfolio flexibility over short term returns" continued Mr. Kain. "Specifically we increased our capital allocation to the non-agency sector further reduced our agency leverage and enhanced the underlying credit quality of our non-agency portfolio. Looking ahead while the recent moves in interest rates and spreads adversely impacted our book value in the second quarter these changes also enhance the expected return on future investment opportunities. Moreover our current low risk profile gives us significant capacity to meaningfully grow our portfolio as more attractive investment opportunities arise." "Global and domestic bond markets continue to experience a period of unusually high volatility as uncertainty surrounding weakness in and parts of collides with the potential for near term rate hikes in the U.S." commented Chief Financial Officer and Executive Vice President. "Given this challenging environment we continue to be disciplined with regard to our deployment of capital. History teaches us that there are some environments where patience is critical with respect to both investment decisions and strategic positioning. I believe that our disciplined approach will enable us to generate attractive risk-adjusted returns for our shareholders over the long term." As of June30 2015 the Companys investment portfolio included of agency MBS of net short TBA securities of non-agency securities and of MSR. As of June30 2015 the Companys agency investment portfolio inclusive of net short TBA was comprised of of fixed rate and of adjustable rate securities. As of June30 2015 the Companys agency fixed rate investments were comprised of 15 year securities 20 year securities 30 year securities 15 year net short TBA securities and 30 year net long TBA securities. As of June30 2015 15 year fixed rate investments represented 25% of the Companys agency investment portfolio a decrease from 30% as of and 30 year fixed rate investments represented 66% of the Companys agency investment portfolio an increase from 63% as of . As of June30 2015 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.51% compared to 3.35% as of comprised of the following weighted average coupons: As of June30 2015 the Companys non-agency portfolio was comprised of 31% Alt-A 30% prime 15% subprime securities 13% credit risk transfer and 11% option ARM. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of June30 2015 the Companys net short TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. The actual CPR for the Companys agency portfolio during the second quarter of 2015 was 10.2% up from 7.7% during the first quarter. The CPR published in for the Companys agency portfolio held as of June30 2015 was 11.3% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of June30 2015 was 8.1% compared to 8.9% as of 2015. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share. The Company recognized approximately or per common share of "catch-up" premium amortization benefit during the quarter as projected CPR estimates fell for the Companys existing agency securities during the quarter. The weighted average cost basis of the Companys agency securities was 104.9% of par and the unamortized agency net premium was as of June30 2015. The weighted average cost basis of the Companys non-agency portfolio was 85.7% of par as of June30 2015. Accretion income on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of June30 2015 with designated as credit reserves. The Companys average annualized net interest rate spread and dollar roll income for the second quarter was 2.48% up from 2.25% in the first quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.36% for the second quarter up from 2.19% for the first quarter. The Companys average asset yield for the second quarter was 3.39% compared to 3.15% for the first quarter. Excluding the impact of "catch-up" premium amortization benefit (expense) recognized due to changes in projected CPR estimates the Companys annualized weighted average yield was 3.27% for the second quarter compared to 3.16% for the first quarter. The Companys asset yield as of June30 2015 was 3.47% up 21 bps from 3.26% as of . The Companys average cost of funds was 1.03% for the second quarter compared to 0.96% for the first quarter. The Companys average cost of funds includes the cost of repurchase agreements other debt and effective interest rate swaps (including interest rate swaps used to hedge the Companys dollar roll funded assets) measured against the Companys daily weighted average repurchase agreement and other debt balance outstanding. The Companys average cost of funds of 1.05% as of June30 2015 was up 3 bps from 1.02% at . As of June30 2015 of the Companys repurchase agreements and of advances were used to fund purchases of agency and non-agency securities while the remaining of borrowings under repurchase agreements were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including TBA securities the Companys "at risk" leverage ratio was 4.1x as of June30 2015 and averaged 4.5x during the second quarter. The borrowed under agency and non-agency repurchase agreements as of June30 2015 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; and As of June30 2015 the Companys agency and non-agency repurchase agreements had an average of 177 days remaining to maturity down from 183 days as of . During the second quarter a wholly-owned subsidiary of the Company was approved as a member of the . As of June30 2015 such subsidiary had financing with the with an outstanding balance of and an average of 23 days remaining to maturity. As of June30 2015 the Company had repurchase agreements with 32 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 22% of the Companys equity at risk. The Companys interest rate swap positions as of June30 2015 totaled in notional amount with a weighted average fixed pay rate of 2.03% a weighted average receive rate of 0.28% and a weighted average maturity of 4.3 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.30% as of June30 2015. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of June30 2015 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 0.9 years. These swaptions have an underlying weighted average interest rate swap term of 7.5 years and a weighted average pay rate of 3.43% as of June30 2015. In addition to its interest rate swaps and swaptions the Company held a net long position in U.S. Treasury securities and futures as of June30 2015. As of June30 2015 70% of the Companys combined funding and TBA balance was hedged through a combination of interest rate swaps interest rate swaptions U.S. Treasury securities and futures and interest only swaps. As of June30 2015 ("RCS") managed a servicing portfolio of approximately 64000 residential mortgage loans representing approximately in unpaid principal balances. During the second quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the second quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized loss on periodic settlements of interest rate swaps; of net realized loss on other derivatives and securities; of net unrealized gain on other derivatives and securities; and Realized and unrealized net losses on other derivatives and securities during the second quarter primarily include of net gain on interest rate swaps and swaptions of net loss on U.S. treasury securities and futures and of net loss on TBA mortgage positions (including of dollar roll income). REIT taxable income for the second quarter is estimated at per common share or higher than GAAP net loss of per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) timing differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the second quarter excludes per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. As of June30 2015 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of June30 2015 the Company had estimated remaining unutilized net capital losses of per common share compared to per common share as of which may be carried forward and applied against future net capital gains through 2018. Additionally as of June30 2015 the Company had estimated net deferred gains from terminated swaps and swaptions of per common share compared to per common share as of which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. On the Board of Directors of the Company declared a second quarter dividend on its common stock of per share which was paid on to common stockholders of record as of 2015. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a second quarter dividend on its Series A Preferred Stock of per share. The dividend was paid on to preferred stockholders of record as of 2015. Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Obligation to return securities borrowed under reverse repurchase agreements at fair value Common stock $0.01 par value; 300000 shares authorized 51192 51165 51165 51142 and 51142 issued and outstanding respectively Unrealized gain (loss) on other derivatives and securities net Net spread and dollar roll income available to common shareholders Net TBA portfolio - as of period end at fair value Net TBA portfolio - as of period end at cost Average cost of funds as of period end Average actual CPR for agency securities held during the period Average projected life CPR for agency securities as of period end Net book value per common share as of period end Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. Excludes servicing expenses related to the Companys investment in RCS. The Companys estimated taxable income for the second quarter excludes $(0.23) per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. Excluding the Companys investment in RCS the average stockholders equity for the second quarter was $1.1 billion. Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency financing for the period by the Companys average month-ended stockholders equity for the period less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency financing and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q2 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10068977. is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to . This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-080-net-loss-per-common-share-for-the-second-quarter-and-2070-net-book-value-per-common-share-300120757.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:35 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2072349&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of or per common share and net book value of per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarter or (14.7)% on an annualized basis. net loss per common share Includes all unrealized gains and losses on investment and hedging portfolios net spread and dollar roll income per common share excluding estimated "catch-up" premium amortization benefit Includes estimated dollar roll income per common share associated with the Companys average net long position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market Excludes per common share of estimated "catch-up" premium amortization benefit due to change in projected constant prepayment rate ("CPR") estimates dividend per common share 12.5% annualized dividend yield based on June30 2015 closing stock price of per common share net book value per common share as of June30 2015 Decreased per common share or (5.9)% from per common share as of (3.7)% economic loss on common equity for the quarter or (14.7)% annualized Comprised of dividend per common share and decrease in net book value per common share 4.1x "at risk" leverage as of June30 2015 10.2% agency securities actual CPR for the quarter 8.1% projected life CPR for agency securities as of June30 2015 2.48% annualized net interest rate spread for the quarter including TBA dollar roll income Includes 12 bps of "catch-up" premium amortization benefit due to change in projected CPR estimates "Fixed income markets around the globe continued to be extremely volatile during the second quarter" commented President and Chief Investment Officer. "This challenging environment led to negative economic earnings for the quarter as higher rates a steeper yield curve and wider spreads caused agency MBS to post their second worst quarterly performance over the last seven years. The performance of legacy non-agency MBS was more neutral as the increased interest rate and spread volatility was offset somewhat by continued improvement in several key housing fundamentals such as house prices employment growth and favorable delinquency trends." "In response to these volatile market conditions we continued to prioritize risk management and long term portfolio flexibility over short term returns" continued Mr. Kain. "Specifically we increased our capital allocation to the non-agency sector further reduced our agency leverage and enhanced the underlying credit quality of our non-agency portfolio. Looking ahead while the recent moves in interest rates and spreads adversely impacted our book value in the second quarter these changes also enhance the expected return on future investment opportunities. Moreover our current low risk profile gives us significant capacity to meaningfully grow our portfolio as more attractive investment opportunities arise." "Global and domestic bond markets continue to experience a period of unusually high volatility as uncertainty surrounding weakness in and parts of collides with the potential for near term rate hikes in the U.S." commented Chief Financial Officer and Executive Vice President. "Given this challenging environment we continue to be disciplined with regard to our deployment of capital. History teaches us that there are some environments where patience is critical with respect to both investment decisions and strategic positioning. I believe that our disciplined approach will enable us to generate attractive risk-adjusted returns for our shareholders over the long term." As of June30 2015 the Companys investment portfolio included of agency MBS of net short TBA securities of non-agency securities and of MSR. As of June30 2015 the Companys agency investment portfolio inclusive of net short TBA was comprised of of fixed rate and of adjustable rate securities. As of June30 2015 the Companys agency fixed rate investments were comprised of 15 year securities 20 year securities 30 year securities 15 year net short TBA securities and 30 year net long TBA securities. As of June30 2015 15 year fixed rate investments represented 25% of the Companys agency investment portfolio a decrease from 30% as of and 30 year fixed rate investments represented 66% of the Companys agency investment portfolio an increase from 63% as of . As of June30 2015 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.51% compared to 3.35% as of comprised of the following weighted average coupons: As of June30 2015 the Companys non-agency portfolio was comprised of 31% Alt-A 30% prime 15% subprime securities 13% credit risk transfer and 11% option ARM. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of June30 2015 the Companys net short TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. The actual CPR for the Companys agency portfolio during the second quarter of 2015 was 10.2% up from 7.7% during the first quarter. The CPR published in for the Companys agency portfolio held as of June30 2015 was 11.3% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of June30 2015 was 8.1% compared to 8.9% as of 2015. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share. The Company recognized approximately or per common share of "catch-up" premium amortization benefit during the quarter as projected CPR estimates fell for the Companys existing agency securities during the quarter. The weighted average cost basis of the Companys agency securities was 104.9% of par and the unamortized agency net premium was as of June30 2015. The weighted average cost basis of the Companys non-agency portfolio was 85.7% of par as of June30 2015. Accretion income on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of June30 2015 with designated as credit reserves. The Companys average annualized net interest rate spread and dollar roll income for the second quarter was 2.48% up from 2.25% in the first quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.36% for the second quarter up from 2.19% for the first quarter. The Companys average asset yield for the second quarter was 3.39% compared to 3.15% for the first quarter. Excluding the impact of "catch-up" premium amortization benefit (expense) recognized due to changes in projected CPR estimates the Companys annualized weighted average yield was 3.27% for the second quarter compared to 3.16% for the first quarter. The Companys asset yield as of June30 2015 was 3.47% up 21 bps from 3.26% as of . The Companys average cost of funds was 1.03% for the second quarter compared to 0.96% for the first quarter. The Companys average cost of funds includes the cost of repurchase agreements other debt and effective interest rate swaps (including interest rate swaps used to hedge the Companys dollar roll funded assets) measured against the Companys daily weighted average repurchase agreement and other debt balance outstanding. The Companys average cost of funds of 1.05% as of June30 2015 was up 3 bps from 1.02% at . As of June30 2015 of the Companys repurchase agreements and of advances were used to fund purchases of agency and non-agency securities while the remaining of borrowings under repurchase agreements were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including TBA securities the Companys "at risk" leverage ratio was 4.1x as of June30 2015 and averaged 4.5x during the second quarter. The borrowed under agency and non-agency repurchase agreements as of June30 2015 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; and As of June30 2015 the Companys agency and non-agency repurchase agreements had an average of 177 days remaining to maturity down from 183 days as of . During the second quarter a wholly-owned subsidiary of the Company was approved as a member of the . As of June30 2015 such subsidiary had financing with the with an outstanding balance of and an average of 23 days remaining to maturity. As of June30 2015 the Company had repurchase agreements with 32 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 22% of the Companys equity at risk. The Companys interest rate swap positions as of June30 2015 totaled in notional amount with a weighted average fixed pay rate of 2.03% a weighted average receive rate of 0.28% and a weighted average maturity of 4.3 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.30% as of June30 2015. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of June30 2015 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 0.9 years. These swaptions have an underlying weighted average interest rate swap term of 7.5 years and a weighted average pay rate of 3.43% as of June30 2015. In addition to its interest rate swaps and swaptions the Company held a net long position in U.S. Treasury securities and futures as of June30 2015. As of June30 2015 70% of the Companys combined funding and TBA balance was hedged through a combination of interest rate swaps interest rate swaptions U.S. Treasury securities and futures and interest only swaps. As of June30 2015 ("RCS") managed a servicing portfolio of approximately 64000 residential mortgage loans representing approximately in unpaid principal balances. During the second quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the second quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized loss on periodic settlements of interest rate swaps; of net realized loss on other derivatives and securities; of net unrealized gain on other derivatives and securities; and Realized and unrealized net losses on other derivatives and securities during the second quarter primarily include of net gain on interest rate swaps and swaptions of net loss on U.S. treasury securities and futures and of net loss on TBA mortgage positions (including of dollar roll income). REIT taxable income for the second quarter is estimated at per common share or higher than GAAP net loss of per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) timing differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the second quarter excludes per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. As of June30 2015 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of June30 2015 the Company had estimated remaining unutilized net capital losses of per common share compared to per common share as of which may be carried forward and applied against future net capital gains through 2018. Additionally as of June30 2015 the Company had estimated net deferred gains from terminated swaps and swaptions of per common share compared to per common share as of which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. On the Board of Directors of the Company declared a second quarter dividend on its common stock of per share which was paid on to common stockholders of record as of 2015. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a second quarter dividend on its Series A Preferred Stock of per share. The dividend was paid on to preferred stockholders of record as of 2015. Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Obligation to return securities borrowed under reverse repurchase agreements at fair value Common stock $0.01 par value; 300000 shares authorized 51192 51165 51165 51142 and 51142 issued and outstanding respectively Unrealized gain (loss) on other derivatives and securities net Net spread and dollar roll income available to common shareholders Net TBA portfolio - as of period end at fair value Net TBA portfolio - as of period end at cost Average cost of funds as of period end Average actual CPR for agency securities held during the period Average projected life CPR for agency securities as of period end Net book value per common share as of period end Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. Excludes servicing expenses related to the Companys investment in RCS. The Companys estimated taxable income for the second quarter excludes $(0.23) per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. Excluding the Companys investment in RCS the average stockholders equity for the second quarter was $1.1 billion. Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency financing for the period by the Companys average month-ended stockholders equity for the period less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency financing and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q2 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10068977. is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to . This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-080-net-loss-per-common-share-for-the-second-quarter-and-2070-net-book-value-per-common-share-300120757.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:35 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2088703&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of per share of common stock for the third quarter 2015. The dividend is payable on to common shareholders of record as of with an ex-dividend date of . The Company also announced today that in the third quarter of 2015 it made open market purchases of approximately 1.2 million shares of MTGE common stock or 2.3% of the Companys outstanding common shares as of 2015. The shares were purchased at an average price of per share including expenses totaling approximately . Since commencing a buyback program in the fourth quarter of 2012 the Company has purchased approximately 9.3 million shares of MTGE common stock for total consideration of approximately including expenses. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-third-quarter-common-stock-dividend-of-040-per-share-and-announces-the-repurchase-of-12-million-shares-300145176.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:35 [scrapy] ERROR: Error processing {'pagetitle': [u'BlackSand Capital - Sitemap'],
 'pageurl': ['http://www.blacksandcapital.com/sitemap'],
 'siteurl': ['blacksandcapital.com'],
 'text': ['What We Do']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:36 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2059438&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of per share of common stock for the second quarter 2015. The dividend is payable on to common shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-second-quarter-common-stock-dividend-of-050-per-share-300099357.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:37 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Lower Corn and Soybean Yield and Acreage'],
 'pageurl': ['http://www.blogcastalia.com/lower-corn-and-soybean-yield-and-acreage-indianawestern-ohio/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:38 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/0f3b1d2f-2362-49d8-a3a8-149692fcc754.pdf> (referer: http://shareholders.fortress.com/stockinfo.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:38 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/e51479ad-65b3-484a-a224-12404a03a044.pdf> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:38 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2088703&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of per share of common stock for the third quarter 2015. The dividend is payable on to common shareholders of record as of with an ex-dividend date of . The Company also announced today that in the third quarter of 2015 it made open market purchases of approximately 1.2 million shares of MTGE common stock or 2.3% of the Companys outstanding common shares as of 2015. The shares were purchased at an average price of per share including expenses totaling approximately . Since commencing a buyback program in the fourth quarter of 2012 the Company has purchased approximately 9.3 million shares of MTGE common stock for total consideration of approximately including expenses. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-third-quarter-common-stock-dividend-of-040-per-share-and-announces-the-repurchase-of-12-million-shares-300145176.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:38 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2088707&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Sept.17 2015 /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of per share for the third quarter 2015. The dividend is payable on to preferred shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-third-quarter-dividend-on-its-series-a-preferred-stock-300145177.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:39 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2088707&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Sept.17 2015 /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of per share for the third quarter 2015. The dividend is payable on to preferred shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-third-quarter-dividend-on-its-series-a-preferred-stock-300145177.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:39 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2096614&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Oct.13 2015 /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q3 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10073284. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-third-quarter-2015-results-on-october-28-300158776.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:39 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2096614&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Oct.13 2015 /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q3 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10073284. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly-owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly-owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-third-quarter-2015-results-on-october-28-300158776.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:42 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2103602&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share To download financial tables please click here. Oct.28 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of or per common share and net book value of per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for the quarter or (7.1)% on an annualized basis. net loss per common share Includes all unrealized gains and losses on investment and hedging portfolios Includes per common share impairment charge on intangible assets related to ("RCS") net spread and dollar roll income per common share excluding estimated "catch-up" premium amortization expense Includes estimated dollar roll income per common share associated with the Companys average net long position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market Excludes per common share of estimated "catch-up" premium amortization expense due to change in projected constant prepayment rate ("CPR") estimates dividend per common share 10.9% annualized dividend yield based on September30 2015 closing stock price of per common share 1.2 million shares of common stock repurchased during the quarter Represents 2.3% of common shares outstanding as of net book value per common share as of September30 2015 Decreased per common share or (3.7)% from as of (1.8)% economic loss on common equity for the quarter or (7.1)% annualized Comprised of dividend per common share and decrease in net book value per common share 4.7x "at risk" leverage as of 11.0% agency securities actual CPR for the quarter 8.6% projected life CPR for agency securities as of 2.27% annualized net interest rate spread for the quarter including TBA dollar roll income 2.40% annualized net interest rate spread and TBA dollar roll income for the quarter excluding 13 bps of "catch-up" premium amortization expense "Global fixed income and equity markets were volatile in the third quarter as investors struggled to weigh the impact of significant strains in emerging market economies and commodity price declines on U.S. economic growth and the Federal Reserves stance on monetary policy" commented President and Chief Investment Officer. "Against this backdrop spreads on agency and non-agency MBS investment grade corporate debt CMBS high-yield bonds and emerging market debt widened during the quarter. This spread widening was the primary driver of the decline in MTGEs book value during the quarter." "Importantly while spread widening negatively impacts near term book value it also improves our earnings outlook as investment returns on both agency and non-agency assets increase" continued Mr. Kain. "In addition the performance and return on our credit sensitive assets will be further supported by the continued strength in housing fundamentals with house prices delinquency rates and household formation all showing improvement." "During the third quarter we bought back 1.2 million shares of our common stock or approximately 2.3% of shares outstanding as of for " commented Chair and Chief Executive Officer. "We believe the accretion from share repurchases is an important element in maximizing long term shareholder value. Looking ahead we will continue to evaluate common stock repurchases against a number of variables including our price-to-book ratio current market conditions and other investment opportunities." As of September30 2015 the Companys investment portfolio included of agency MBS of net long TBA securities of non-agency securities and of MSR. As of September30 2015 the Companys agency investment portfolio inclusive of net long TBA was comprised of of fixed rate and of adjustable rate securities. As of September30 2015 the Companys fixed rate agency investments were comprised of 15 year securities 20 year securities 30 year securities 15 year net short TBA securities and 30 year net long TBA securities. As of September30 2015 15 year fixed rate investments represented 19% of the Companys agency investment portfolio a decrease from 25% as of and 30 year fixed rate investments represented 72% of the Companys agency investment portfolio an increase from 66% as of . As of September30 2015 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.54% compared to 3.51% as of comprised of the following weighted average coupons: As of September30 2015 the Companys non-agency portfolio was comprised of approximately 30% Alt-A 30% prime 15% subprime 14% credit risk transfer and 11% option ARM securities. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of September30 2015 the Companys net long TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. The actual CPR for the Companys agency portfolio during the third quarter of 2015 was 11.0% up from 10.2% during the second quarter. The CPR published in for the Companys agency portfolio held as of September30 2015 was 9.6% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of September30 2015 was 8.6% compared to 8.1% as of 2015. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share. The Company recognized approximately or per common share of "catch-up" premium amortization expense during the quarter as projected CPR estimates rose for the Companys existing agency securities during the quarter. The weighted average cost basis of the Companys agency securities was 104.9% of par and the unamortized agency net premium was as of September30 2015. The weighted average cost basis of the Companys non-agency portfolio was 86.4% of par as of September30 2015. Accretion income on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of September30 2015 with designated as credit reserves. The Companys average annualized net interest rate spread and dollar roll income for the third quarter was 2.27% down from 2.48% in the second quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.22% for the third quarter down from 2.36% for the second quarter. The Companys average asset yield for the third quarter was 3.31% compared to 3.39% for the second quarter. Excluding the impact of "catch-up" premium amortization benefit (expense) recognized due to changes in projected CPR estimates the Companys annualized weighted average yield was 3.44% for the third quarter compared to 3.27% for the second quarter. The Companys asset yield as of September30 2015 was 3.45% down 2 bps from 3.47% as of . The Companys average cost of funds was 1.09% for the third quarter compared to 1.03% for the second quarter. The Companys average cost of funds includes the cost of repurchase agreements FHLB advances and effective interest rate swaps (including those used to hedge the Companys dollar roll funded assets) measured against the Companys daily weighted average repurchase agreement and advances balances outstanding. The Companys average cost of funds of 1.16% as of September30 2015 was up 11 bps from 1.05% at . As of September30 2015 of the Companys repurchase agreements and of advances were used to fund purchases of agency and non-agency securities while the remaining of borrowings under repurchase agreements were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including TBA securities the Companys "at risk" leverage ratio was 4.7x as of September30 2015 and averaged 4.4x during the third quarter. The borrowed under agency and non-agency repurchase agreements as of September30 2015 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; between nine and twelve months; and As of September30 2015 the Companys agency and non-agency repurchase agreements had an average of 204 days remaining to maturity up from 177 days as of . During the second quarter a wholly-owned subsidiary of the Company was approved as a member of the . As of September30 2015 such subsidiary had financing with the with an outstanding balance of and an average of 4.9 years remaining to maturity. As of September30 2015 the Company had repurchase agreements with 32 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 23% of the Companys equity at risk. The Companys interest rate swap positions as of September30 2015 totaled in notional amount with a weighted average fixed pay rate of 1.98% a weighted average receive rate of 0.32% and a weighted average maturity of 4.2 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.36% as of September30 2015. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of September30 2015 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 0.8 years. These swaptions have an underlying weighted average interest rate swap term of 7.8 years and a weighted average pay rate of 3.40% as of September30 2015. In addition to its interest rate swaps and swaptions the Company held a net short position in U.S. Treasury securities and futures as of September30 2015. As of September30 2015 72% of the Companys combined funding and TBA balance was hedged through a combination of interest rate swaps interest rate swaptions U.S. Treasury securities and futures and interest only swaps. As of September30 2015 RCS managed a servicing portfolio of approximately 64000 residential mortgage loans representing approximately in unpaid principal balances. During the third quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the third quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized gain on agency and non-agency securities; of net realized loss on periodic settlements of interest rate swaps; of net realized loss on other derivatives and securities; of net unrealized loss on other derivatives and securities; of unrealized loss on mortgage servicing rights; and Realized and unrealized net losses on other derivatives and securities during the third quarter primarily include of net loss on interest rate swaps and swaptions and of net loss on U.S. Treasury securities and futures offset in part by of net gain on TBA mortgage positions (including of dollar roll income). During the third quarter the Company recorded a per common share impairment charge on intangible assets based on a revised estimated fair value of RCS servicing licenses and approvals. The Company reduced its estimate of the intangible assets fair value during the third quarter as a result of certain financial and economic factors including anticipated acquisitions of mortgage servicing rights. REIT taxable income for the third quarter is estimated at per common share or higher than GAAP net loss of per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) timing differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the third quarter excludes per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. As of September30 2015 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of September30 2015 the Company had estimated remaining unutilized net capital losses of per common share compared to per common share as of which may be carried forward and applied against future net capital gains through 2018. Additionally as of September30 2015 the Company had estimated net deferred gains from terminated swaps and swaptions of per common share compared to per common share as of which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. During the third quarter the Company made open market purchases of 1.2 million shares of its common stock or 2.3% of the Companys outstanding shares as of . The shares were purchased at an average net repurchase price of per share including expenses totaling . As of September30 2015 the Company had available under current board authorization for repurchases of its common stock. The Company also announced that its Board of Directors has extended its current share repurchase authorization through 2016. On the Board of Directors of the Company declared a third quarter dividend on its common stock of per share which was paid on to common stockholders of record as of 2015. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a third quarter dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock ("Series A Preferred Stock") of per share. The dividend was paid on to preferred stockholders of record as of 2015. Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Obligation to return securities borrowed under reverse repurchase agreements at fair value Common stock $0.01 par value; 300000 shares authorized 50010 51192 51165 51165 and 51142 issued and outstanding respectively Realized gain (loss) on other derivatives and securities net Unrealized gain (loss) on other derivatives and securities net Net spread and dollar roll income available to common shareholders Net spread and dollar roll income excluding "catch up" amortization per common share Net TBA portfolio - as of period end at fair value Net TBA portfolio - as of period end at cost Average cost of funds as of period end Average actual CPR for agency securities held during the period Average projected life CPR for agency securities as of period end Net book value per common share as of period end Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. Excludes servicing expenses related to the Companys investment in RCS. The Companys estimated taxable income for the third quarter excludes $(0.04) per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. Excluding the Companys investment in RCS the average stockholders equity for the third quarter was $1.0 billion. Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency financing for the period by the Companys average month-ended stockholders equity for the period less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency financing and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q3 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10073284. is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to . This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-049-net-loss-per-common-share-for-the-third-quarter-and-1993-net-book-value-per-common-share-300168171.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:46 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2103602&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share To download financial tables please click here. Oct.28 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of or per common share and net book value of per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for the quarter or (7.1)% on an annualized basis. net loss per common share Includes all unrealized gains and losses on investment and hedging portfolios Includes per common share impairment charge on intangible assets related to ("RCS") net spread and dollar roll income per common share excluding estimated "catch-up" premium amortization expense Includes estimated dollar roll income per common share associated with the Companys average net long position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market Excludes per common share of estimated "catch-up" premium amortization expense due to change in projected constant prepayment rate ("CPR") estimates dividend per common share 10.9% annualized dividend yield based on September30 2015 closing stock price of per common share 1.2 million shares of common stock repurchased during the quarter Represents 2.3% of common shares outstanding as of net book value per common share as of September30 2015 Decreased per common share or (3.7)% from as of (1.8)% economic loss on common equity for the quarter or (7.1)% annualized Comprised of dividend per common share and decrease in net book value per common share 4.7x "at risk" leverage as of 11.0% agency securities actual CPR for the quarter 8.6% projected life CPR for agency securities as of 2.27% annualized net interest rate spread for the quarter including TBA dollar roll income 2.40% annualized net interest rate spread and TBA dollar roll income for the quarter excluding 13 bps of "catch-up" premium amortization expense "Global fixed income and equity markets were volatile in the third quarter as investors struggled to weigh the impact of significant strains in emerging market economies and commodity price declines on U.S. economic growth and the Federal Reserves stance on monetary policy" commented President and Chief Investment Officer. "Against this backdrop spreads on agency and non-agency MBS investment grade corporate debt CMBS high-yield bonds and emerging market debt widened during the quarter. This spread widening was the primary driver of the decline in MTGEs book value during the quarter." "Importantly while spread widening negatively impacts near term book value it also improves our earnings outlook as investment returns on both agency and non-agency assets increase" continued Mr. Kain. "In addition the performance and return on our credit sensitive assets will be further supported by the continued strength in housing fundamentals with house prices delinquency rates and household formation all showing improvement." "During the third quarter we bought back 1.2 million shares of our common stock or approximately 2.3% of shares outstanding as of for " commented Chair and Chief Executive Officer. "We believe the accretion from share repurchases is an important element in maximizing long term shareholder value. Looking ahead we will continue to evaluate common stock repurchases against a number of variables including our price-to-book ratio current market conditions and other investment opportunities." As of September30 2015 the Companys investment portfolio included of agency MBS of net long TBA securities of non-agency securities and of MSR. As of September30 2015 the Companys agency investment portfolio inclusive of net long TBA was comprised of of fixed rate and of adjustable rate securities. As of September30 2015 the Companys fixed rate agency investments were comprised of 15 year securities 20 year securities 30 year securities 15 year net short TBA securities and 30 year net long TBA securities. As of September30 2015 15 year fixed rate investments represented 19% of the Companys agency investment portfolio a decrease from 25% as of and 30 year fixed rate investments represented 72% of the Companys agency investment portfolio an increase from 66% as of . As of September30 2015 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.54% compared to 3.51% as of comprised of the following weighted average coupons: As of September30 2015 the Companys non-agency portfolio was comprised of approximately 30% Alt-A 30% prime 15% subprime 14% credit risk transfer and 11% option ARM securities. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of September30 2015 the Companys net long TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. The actual CPR for the Companys agency portfolio during the third quarter of 2015 was 11.0% up from 10.2% during the second quarter. The CPR published in for the Companys agency portfolio held as of September30 2015 was 9.6% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of September30 2015 was 8.6% compared to 8.1% as of 2015. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share. The Company recognized approximately or per common share of "catch-up" premium amortization expense during the quarter as projected CPR estimates rose for the Companys existing agency securities during the quarter. The weighted average cost basis of the Companys agency securities was 104.9% of par and the unamortized agency net premium was as of September30 2015. The weighted average cost basis of the Companys non-agency portfolio was 86.4% of par as of September30 2015. Accretion income on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of September30 2015 with designated as credit reserves. The Companys average annualized net interest rate spread and dollar roll income for the third quarter was 2.27% down from 2.48% in the second quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.22% for the third quarter down from 2.36% for the second quarter. The Companys average asset yield for the third quarter was 3.31% compared to 3.39% for the second quarter. Excluding the impact of "catch-up" premium amortization benefit (expense) recognized due to changes in projected CPR estimates the Companys annualized weighted average yield was 3.44% for the third quarter compared to 3.27% for the second quarter. The Companys asset yield as of September30 2015 was 3.45% down 2 bps from 3.47% as of . The Companys average cost of funds was 1.09% for the third quarter compared to 1.03% for the second quarter. The Companys average cost of funds includes the cost of repurchase agreements FHLB advances and effective interest rate swaps (including those used to hedge the Companys dollar roll funded assets) measured against the Companys daily weighted average repurchase agreement and advances balances outstanding. The Companys average cost of funds of 1.16% as of September30 2015 was up 11 bps from 1.05% at . As of September30 2015 of the Companys repurchase agreements and of advances were used to fund purchases of agency and non-agency securities while the remaining of borrowings under repurchase agreements were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including TBA securities the Companys "at risk" leverage ratio was 4.7x as of September30 2015 and averaged 4.4x during the third quarter. The borrowed under agency and non-agency repurchase agreements as of September30 2015 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; between nine and twelve months; and As of September30 2015 the Companys agency and non-agency repurchase agreements had an average of 204 days remaining to maturity up from 177 days as of . During the second quarter a wholly-owned subsidiary of the Company was approved as a member of the . As of September30 2015 such subsidiary had financing with the with an outstanding balance of and an average of 4.9 years remaining to maturity. As of September30 2015 the Company had repurchase agreements with 32 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 23% of the Companys equity at risk. The Companys interest rate swap positions as of September30 2015 totaled in notional amount with a weighted average fixed pay rate of 1.98% a weighted average receive rate of 0.32% and a weighted average maturity of 4.2 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.36% as of September30 2015. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of September30 2015 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 0.8 years. These swaptions have an underlying weighted average interest rate swap term of 7.8 years and a weighted average pay rate of 3.40% as of September30 2015. In addition to its interest rate swaps and swaptions the Company held a net short position in U.S. Treasury securities and futures as of September30 2015. As of September30 2015 72% of the Companys combined funding and TBA balance was hedged through a combination of interest rate swaps interest rate swaptions U.S. Treasury securities and futures and interest only swaps. As of September30 2015 RCS managed a servicing portfolio of approximately 64000 residential mortgage loans representing approximately in unpaid principal balances. During the third quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the third quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized gain on agency and non-agency securities; of net realized loss on periodic settlements of interest rate swaps; of net realized loss on other derivatives and securities; of net unrealized loss on other derivatives and securities; of unrealized loss on mortgage servicing rights; and Realized and unrealized net losses on other derivatives and securities during the third quarter primarily include of net loss on interest rate swaps and swaptions and of net loss on U.S. Treasury securities and futures offset in part by of net gain on TBA mortgage positions (including of dollar roll income). During the third quarter the Company recorded a per common share impairment charge on intangible assets based on a revised estimated fair value of RCS servicing licenses and approvals. The Company reduced its estimate of the intangible assets fair value during the third quarter as a result of certain financial and economic factors including anticipated acquisitions of mortgage servicing rights. REIT taxable income for the third quarter is estimated at per common share or higher than GAAP net loss of per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) timing differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the third quarter excludes per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. As of September30 2015 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of September30 2015 the Company had estimated remaining unutilized net capital losses of per common share compared to per common share as of which may be carried forward and applied against future net capital gains through 2018. Additionally as of September30 2015 the Company had estimated net deferred gains from terminated swaps and swaptions of per common share compared to per common share as of which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. During the third quarter the Company made open market purchases of 1.2 million shares of its common stock or 2.3% of the Companys outstanding shares as of . The shares were purchased at an average net repurchase price of per share including expenses totaling . As of September30 2015 the Company had available under current board authorization for repurchases of its common stock. The Company also announced that its Board of Directors has extended its current share repurchase authorization through 2016. On the Board of Directors of the Company declared a third quarter dividend on its common stock of per share which was paid on to common stockholders of record as of 2015. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a third quarter dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock ("Series A Preferred Stock") of per share. The dividend was paid on to preferred stockholders of record as of 2015. Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Obligation to return securities borrowed under reverse repurchase agreements at fair value Common stock $0.01 par value; 300000 shares authorized 50010 51192 51165 51165 and 51142 issued and outstanding respectively Realized gain (loss) on other derivatives and securities net Unrealized gain (loss) on other derivatives and securities net Net spread and dollar roll income available to common shareholders Net spread and dollar roll income excluding "catch up" amortization per common share Net TBA portfolio - as of period end at fair value Net TBA portfolio - as of period end at cost Average cost of funds as of period end Average actual CPR for agency securities held during the period Average projected life CPR for agency securities as of period end Net book value per common share as of period end Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. Excludes servicing expenses related to the Companys investment in RCS. The Companys estimated taxable income for the third quarter excludes $(0.04) per common share of estimated net capital losses which will be added to the Companys net capital loss carryforwards from prior periods. Excluding the Companys investment in RCS the average stockholders equity for the third quarter was $1.0 billion. Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency financing for the period by the Companys average month-ended stockholders equity for the period less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency financing and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q3 2015 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10073284. is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to . This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-049-net-loss-per-common-share-for-the-third-quarter-and-1993-net-book-value-per-common-share-300168171.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:46 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:55:46 [scrapy] ERROR: Error downloading <GET http://www.pomonacapital.com/legal-disclaimer>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 06:55:47 [scrapy] ERROR: Error processing {'pagetitle': [u'2011 American Capital Mortgage Investment Corp. Press Releases - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&nyo=4&p=irol-news'],
 'siteurl': ['ir.mtge.com'],
 'text': ['BETHESDA Md. Dec. 12 2011 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.80 per share for the fourth quarter 2011. The dividend is payable on January 27 2012 to common shareholders of record as of December 22 2011 with an ex-dividend date of December 20 2011. ABOUT AMERICAN CAPITAL MORTGAGE INVESTMENT CORP. American Capital Mortgage In... BETHESDA Md. Nov. 8 2011 /PRNewswire via COMTEX/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that it has submitted a comment letter to the Securities and Exchange Commission regarding the SECs Concept Release concerning interpretive issues related to the Section 3( c )( 5 )( C ) exemption to the Investment Company Act of 1940. The comment letter will be available on MTGEs website at http://www.mtge.com/ and the SECs website at http... American Capital Mortgage Investment Reports $0.25 Earnings Per Share and $19.96 Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. Oct. 26 2011 /PRNewswire via COMTEX/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the period from the closing of its initial public offering ("IPO") on August 9 2011 through September 30 2011 (the "stub period") of $2.5 million or $0.25 per share and net book value of $19.96 per share. FINANCIAL HIGHLIGHTS Succe... American Capital Mortgage Investment Corp. Will Report Q3 2011 Results on October 26; Shareholder Call Scheduled for October 27 BETHESDA Md. Oct. 13 2011 /PRNewswire via COMTEX/ -- American Capital Mortgage Investment Corp. ("MTGE" or "the Company") (Nasdaq: MTGE) announced today it will report third quarter 2011 earnings after market close on October 26 2011. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 27 2011 at 11:00 am ET. The call can be accessed through a live webcast free of charge at http://www.mtge.com/ or by dialing (877) 569-8701 (U.S. ... BETHESDA Md. Sept. 13 2011 /PRNewswire via COMTEX/ -- American Capital Mortgage Investment Corp. ("MTGE" or "the Company") (Nasdaq: MTGE) announced today that its Board of Directors has declared a cash dividend of $0.20 per share for the shortened "stub" period from August 9 2011 through September 30 2011 following the closing of the Companys initial public offering ("IPO") and concurrent private placement. The third quarter 2011 dividend is payable on October 27 2011 to common sharehol... BETHESDA Md. Aug. 3 2011 /PRNewswire via COMTEX/ -- American Capital Mortgage Investment Corp. (the "Company") (Nasdaq: MTGE) announced today that it has priced its initial public offering of 8000000 shares of common stock at $20.00 per share for total gross proceeds of $160 million. All of the shares are being offered directly by American Capital Mortgage Investment Corp. The Company has granted the underwriters a 30-day option to purchase up to an additional 1200000 shares of common sto... BETHESDA Md. July 20 2011 /PRNewswire/ -- American Capital Mortgage Investment Corp. (the "Company") announced today that it plans to conduct an initial public offering of its common stock. The Company expects to offer 17500000 shares to the public at the proposed initial public offering price of $20.00 per share for estimated net proceeds of $349 million. The Company will grant the underwriters a 30-day option to purchase up to an additional 2625000 shares of common stock at the initial...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:47 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001197013.PDF?Y=&O=PDF&D=&fid=1001197013&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:47 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/25d497df-2976-4495-991c-5fa6378499d2.pdf> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=208605&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:55:48 [scrapy] ERROR: Error processing {'pagetitle': [u'2012 American Capital Mortgage Investment Corp. Press Releases - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&nyo=3&p=irol-news'],
 'siteurl': ['ir.mtge.com'],
 'text': ['BETHESDA Md. Dec. 14 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.90 per share for the fourth quarter 2012. The dividend is payable on January 28 2013 to common shareholders of record as of December 27 2012 with an ex-dividend date of December 24 2012. For further information or questions please contact our Investor Relations Department at (30... BETHESDA Md. Oct. 31 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has authorized the repurchase of up to $50 million of its outstanding shares of common stock through December 31 2013. The Company stated that it would be its intent only to repurchase shares when the repurchase price is less than its estimate of the current net book value of a share of common stock. When MTGE purchase... American Capital Mortgage Investment Corp. Reports $4.03 Earnings Per Share And $25.21 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. Oct. 31 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the three months ended September30 2012 of $146.2 million or $4.03 per share and net book value of $25.21 per share. THIRD QUARTER 2012 FINANCIAL HIGHLIGHTS $4.03 per share of net income Includes all unrealized gains and losses on... American Capital Mortgage Investment Corp. Will Report Third Quarter 2012 Results on October 31; Shareholder Call Scheduled for November 1 BETHESDA Md. Oct. 11 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2012 earnings after market close on October 31 2012. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on November 1 2012 at 11:00 am ET. The call can be accessed through a live webcast free of charge at www.MTGE.com or by dialing (877) 270-2148 (U.S. domes... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend Of $0.90 Per Share BETHESDA Md. Sept. 11 2012 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.90 per share for the third quarter 2012. The dividend is payable on October 26 2012 to common shareholders of record as of September 21 2012 with an ex-dividend date of September 19 2012. For further information or questions please contact our Investor Relations Department at (... American Capital Mortgage Investment Corp. to Ring NASDAQ Closing Bell in Celebration of One Year as a Publicly Traded Company BETHESDA Md. Aug. 6 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that in celebration of one year as a publicly traded company on the NASDAQ Global Select Market Chief Executive Officer and Chair Malon Wilkus and President and Chief Investment Officer Gary Kain will preside over the August 7 2012 NASDAQ closing bell ceremony. Joining Messrs. Wilkus and Kain will be members of the Board of Directors and managemen... American Capital Mortgage Investment Corp. Reports $1.15 Earnings Per Share and $22.08 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. Aug. 3 2012 /PRNewswire/ --American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the three months ended June30 2012 of $32.2 million or $1.15 per share and net book value of $22.08 per share. SECOND QUARTER 2012 FINANCIAL HIGHLIGHTS $1.15 per share of net income Includes all unrealized gains and losses o... American Capital Mortgage Investment Corp. Will Report Second Quarter 2012 Results On August 3; Shareholder Call Scheduled for August 6 BETHESDA Md. July 23 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report second quarter 2012 earnings after market close on August 3 2012. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on August 6 2012 at 11:00 am ET. The call can be accessed through a live webcast free of charge at http://www.mtge.com/ or by dialing (877) 270-2148 (U.S. do... American Capital Mortgage Investment Corp. Declares Second Quarter 2012 Dividend Of $0.90 Per Share BETHESDA Md. June 11 2012 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.90 per share for the second quarter 2012. The dividend is payable on July 27 2012 to common shareholders of record as of June 21 2012 with an ex-dividend date of June 19 2012. For further information or questions please contact our Investor Relations Department at (30... BETHESDA Md. May 30 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that Jeff Winkler Senior Vice President and Co-Chief Investment Officer is scheduled to make a presentation at the Keefe Bruyette & Woods ("KBW") Mortgage Finance Conference on June 5 2012 in New York City. The MTGE presentation is scheduled to begin at 5:00 pm ET. The presentation will be webcast live and archived on the MTGE website in th... BETHESDA Md. May 24 2012 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that it priced a public offering of 11000000 shares of common stock for total expected gross proceeds of approximately $253 millionbefore expenses. In connection with the offering the Company has granted the underwriters an option for 30 days to purchase up to an additional 1650000 shares of common stock to cover overallotments if any. The... BETHESDA Md. May 23 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that it plans to make a public offering of 10000000 shares of its common stock. In connection with the offering the Company intends to grant the underwriters an option for 30 days to purchase up to an additional 1500000 shares of common stock to cover overallotments. MTGE expects to use the net proceeds from this offering to invest as mark... BETHESDA Md. March 13 2012 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that it priced a public offering of 12000000 shares of common stock for total expected gross proceeds of approximately $260 million before expenses. In connection with the offering the Company has granted the underwriters an option for 30 days to purchase up to an additional 1791650 shares of common stock to cover overallotments if any. T... BETHESDA Md. March 13 2012 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that it plans to make a public offering of 10000000 shares of its common stock. In connection with the offering the Company intends to grant the underwriters an option for 30 days to purchase up to an additional 1500000 shares of common stock to cover overallotments. MTGE expects to use the net proceeds from this offering to invest in its... BETHESDA Md. March 5 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.90 per share for the first quarter 2012. The dividend is payable on April 27 2012 to common shareholders of record as of March 15 2012 with an ex-dividend date of March 13 2012. For further information or questions please contact our Investor Relations Department at (301) 968-92... American Capital Mortgage Investment Corp. Reports $1.72 Earnings Per Share and $20.87 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. Feb. 8 2012 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the three months ended December312011 of $17.2 million or $1.72 per share and net book value of $20.87 per share. FOURTH QUARTER 2011 FINANCIAL HIGHLIGHTS $1.72 per share of net income Includes all unrealized gains and losses on... American Capital Mortgage Investment Corp. Will Report Q4 2011 Results on February 8; holder Call Scheduled for February 9 BETHESDA Md. Jan. 20 2012 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report fourth quarter 2011 earnings after market close on February 8 2012. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on February 9 2012 at 8:30 am ET. The call can be accessed through a live webcast free of charge at http://www.mtge.com/ or by dialing (877) 569-8701 ...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:50 [scrapy] ERROR: Error processing {'pagetitle': [u'2013 American Capital Mortgage Investment Corp. Press Releases - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&nyo=2&p=irol-news'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Declares Fourth Quarter Dividend of $0.65 Per Share and Announces the Repurchase of 1.5 Million Shares BETHESDA Md. Dec. 18 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.65 per share for the fourth quarter 2013. The dividend is payable on January 28 2014 to common shareholders of record as of December 31 2013 with an ex-dividend date of December 27 2013. The Company also announced today that in the fourth quarter of 2013 it made open market purchase... BETHESDA Md. Dec. 4 2013 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that through a subsidiary it has acquired Residential Credit Solutions Inc. ("RCS") a licensed mortgage servicer based in Fort Worth Texas. "The acquisition of RCS a highly-regarded mortgage servicer marks a significant milestone for MTGE as it positions itself to meet the demands of the evolving mortgage market" said Gary Kain President and... American Capital Mortgage Investment Corp. Reports $0.25 Net Income Per Share And $22.37 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.30 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended September30 2013 of $13.7 million or $0.25 per share and net book value of $22.37 per share. THIRD QUARTER 2013 FINANCIAL HIGHLIGHTS $0.25 per share of net income Includes all unrealized gains and losses on investment... American Capital Mortgage Investment Corp. Will Report Third Quarter 2013 Results On October 30 Shareholder Call Scheduled for October 31 BETHESDA Md. Oct. 18 2013 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2013 earnings after market close on October 30 2013. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 31 2013 at 11:00 am ET. The call can be accessed through a live webcast free of charge at www.MTGE.com or by... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend Of $0.70 Per Share And Announces The Repurchase Of 3.2 Million Shares BETHESDA Md. Sept. 19 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.70 per share for the third quarter 2013. The dividend is payable on October 28 2013 to common shareholders of record as of September 30 2013 with an ex-dividend date of September 26 2013. The Company also announced today that in the third quarter of 2013 it made open market purchas... American Capital Mortgage Investment Corp. Reports $(0.94) Net Loss Per Share And $22.63 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. July30 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended June30 2013 of $(54.4) million or $(0.94) per share and net book value of $22.63 per share. SECOND QUARTER 2013 FINANCIAL HIGHLIGHTS $(0.94) per share of net loss Resulting mainly from $(5.05) per share in net... American Capital Mortgage Investment Corp. Will Report Second Quarter 2013 Results On July 30; Shareholder Call Scheduled for July 31 BETHESDA Md. July 16 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report second quarter 2013 earnings after market close on July 30 2013. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on July 31 2013 at 8:30 am ET. The call can be accessed through a live webcast free of charge at www.MTGE.com or by dialing (888) 317-6016 (U.S. domestic) or ... American Capital Mortgage Investment Corp. Declares Second Quarter Dividend of $0.80 Per Share BETHESDA Md. June 18 2013 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.80 per share for the second quarter 2013. The dividend is payable on July 26 2013 to common shareholders of record as of June 28 2013 with an ex-dividend date of June 26 2013. For further information or questions please contact our Investor Relations Department at (301) 968-9220 or ... BETHESDA Md. May 10 2013 /PRNewswire/ --American Capital Mortgage Investment Corp. (NASDAQ: MTGE) ("MTGE" or the "Company") announced today that Jeff Winkler Senior Vice President and Co-Chief Investment Officer is scheduled to make a presentation at the Wells Fargo Securities Specialty Finance Conference on May 16 2013 in New York City. The MTGE presentation is scheduled to begin at 3:30 pm ET. The presentation will be webcast live and archived on the MTGE website in the Investor Relati... BETHESDA Md. May 8 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. (NASDAQ: MTGE) ("MTGE" or the "Company") announced today that Jeff Winkler Senior Vice President and Co-Chief Investment Officer is scheduled to make a presentation at the 12th Annual JMP Securities Research Conference on May 13 2013 in San Francisco CA. The MTGE presentation is scheduled to begin at 10:00 am PT (1:00 pm ET). The presentation will be webcast live and archived on the MTGE website in the Inv... American Capital Mortgage Investment Corp. Reports $(0.56) Net Loss Per Share And $24.25 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. May3 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended March 31 2013 of $(26.6) million or $(0.56) per share and net book value of $24.25 per share. FIRST QUARTER 2013 FINANCIAL HIGHLIGHTS $(0.56) per share of net loss Resulting mainly from $(1.66) per share in net unrealized losse... American Capital Mortgage Investment Corp. Will Report First Quarter 2013 Results On May 3; Shareholder Call Scheduled for May 6 BETHESDA Md. April 24 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report first quarter 2013 earnings after market close on May 3 2013. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on May 6 2013 at 10:00 am ET. The call can be accessed through a live webcast free of charge at www.MTGE.com or by dialing (888) 317-6016 (U.S. domestic) or (41... American Capital Mortgage Investment Corp. Declares First Quarter Dividend of $0.90 Per Share BETHESDA Md. March 7 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.90 per share for the first quarter 2013. The dividend is payable on April 26 2013 to common shareholders of record as of March 20 2013 with an ex-dividend date of March 18 2013. For further information or questions please contact our Investor Relations Department at (301) 968-92... American Capital Mortgage Investment Corp. Reports $1.40 Net Income Per Share And $25.74 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. Feb. 8 2013 /PRNewswire/ --American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended December 31 2012 of $50.4 million or $1.40 per share and net book value of $25.74 per share. For the full year the Company reported a 41% economic return comprised of $3.60 per share in dividends and a $4.87 per share i... American Capital Mortgage Investment Corp. Adds Two Members To Its Board of Directors BETHESDA Md. Feb. 7 2013 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced that at a meeting today the Companys Board of Directors voted to increase the size of the Board by two and elected Prue B. Larocca and Samuel A. Flax to the new positions. "We are very excited to have Prue and Sam join our Board of Directors" said Malon Wilkus Chair and Chief Executive Officer of MTGE. "Both of these Directors bring a wealth of in... Shareholder Call Scheduled for February 11 BETHESDA Md. Jan. 30 2013 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report fourth quarter 2012 earnings after market close on February 8 2013. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on February 11 2013 at 8:30 am ET. The call can be accessed through a live webcast free of charge at www.MTGE.com or ...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:51 [scrapy] ERROR: Error processing {'pagetitle': [u'2014 American Capital Mortgage Investment Corp. Press Releases - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&nyo=1&p=irol-news'],
 'siteurl': ['ir.mtge.com'],
 'text': ['BETHESDA Md. Dec. 18 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the fourth quarter 2014. The dividend is payable on January 15 2015 to preferred shareholders of record as of January 1 2015 with an ex-dividend date of Dec... BETHESDA Md. Dec. 18 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.65 per share for the fourth quarter 2014. The dividend is payable on January 27 2015 to common shareholders of record as of December 31 2014 with an ex-dividend date of December 29 2014. For further information or questions please contact the Investor Relations Department at (301) 9... American Capital Mortgage Investment Corp. Reports $0.14 Net Income Per Common Share And $22.24 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.29 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended September30 2014 of $7.2 million or $0.14 per common share and net book value of $22.24 per common share. Economic return on common equity for the period defined as dividends and change in net book value was 0.7% for the... American Capital Mortgage Investment Corp. Will Report Third Quarter 2014 Results on October 29; Shareholder Call Scheduled for October 30 Shareholder Call Scheduled for October 30--> BETHESDA Md. Oct. 10 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2014 earnings after market close on October 29 2014. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 30 2014 at 11:00 am ET. Callers who do not plan on asking a question and have access to the inte... BETHESDA Md. Sept.22 2014 /PRNewswire/ --American Capital Agency Corp. (Nasdaq: AGNC) ("AGNC") and American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE") will hold their first Mortgage REIT Day on October 1 2014 from 9:15 am to 3:30 pm ET. The event will include presentations from AGNC and MTGE President and Chief Investment Officer Gary Kain other members of senior management and external industry experts. The event will be webcast live and archived on the Investor Relation... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. Sept. 18 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the third quarter 2014. The dividend is payable on October 15 2014 to preferred shareholders of record as of October 1 2014 with an ex-dividend date of Sep... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend of $0.65 Per Share BETHESDA Md. Sept. 18 2014 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.65 per share for the third quarter 2014. The dividend is payable on October 27 2014 to common shareholders of record as of September 30 2014 with an ex-dividend date of September 26 2014. For further information or questions please contact the Investor Relations Department at (301)... American Capital Mortgage Investment Corp. Reports $1.64 Net Income Per Common Share And $22.73 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. July30 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended June30 2014 of $83.8 million or $1.64 per common share and net book value of $22.73 per common share. Economic return for the period defined as dividends plus change in net book value was $1.60 per common share or 7.3% fo... American Capital Mortgage Investment Corp. Will Report Second Quarter 2014 Results On July 30; Shareholder Call Scheduled for July 31 BETHESDA Md. July 15 2014 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report second quarter 2014 earnings after market close on July 30 2014. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on July 31 2014 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at ... American Capital Mortgage Investment Corp. Announces Second Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. June 17 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.29905 per share for the second quarter 2014. The dividend is payable on July 15 2014 to preferred shareholders of record as of July 1 2014 with an ex-dividend date of June 27 201... American Capital Mortgage Investment Corp. Declares Second Quarter Dividend of $0.65 Per Share BETHESDA Md. June 17 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.65 per share for the second quarter 2014. The dividend is payable on July 28 2014 to common shareholders of record as of June 30 2014 with an ex-dividend date of June 26 2014. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or ... BETHESDA Md. May 15 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that it has priced a public offering of2 million shares of its 8.125% Series A Cumulative Redeemable Preferred Stock liquidation preference of $25.00 per share for estimated gross proceeds of $50 million. MTGE has also granted the underwriters an option to purchase up to an additional 300000 shares of Series A Preferred Stock for 30 days following t... BETHESDA Md. May 15 2014 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that it has commenced an underwritten public offering of its Series A Cumulative Redeemable Preferred Stock. The Company has applied to list the Series A Preferred Stock on the NASDAQ Global Select Market under the symbol "MTGEP". MTGE expects to use the net proceeds from this offering on a leveraged basis to invest in its targeted assets as market... American Capital Mortgage Investment Corp. Declares First Quarter Dividend of $0.65 Per Share BETHESDA Md. March 20 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.65 per share for the first quarter 2014. The dividend is payable on April 28 2014 to common shareholders of record as of March 31 2014 with an ex-dividend date of March 27 2014. The Company also announced today that in the first quarter of 2014 it made open market purchases of appr... American Capital Mortgage Investment Corp. Reports $(0.33) Net Loss Per Share And $21.47 Net Book Value Per Share PDF Version To download financial tables please click here. BETHESDA Md. Feb. 5 2014 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended December31 2013 of $(17.3) million or $(0.33) per share and net book value of $21.47 per share. Economic loss for the period defined as dividends per share plus the change in net book value per share was $(0.25) per share or (1.1)% ... American Capital Mortgage Investment Corp. Will Report Fourth Quarter 2013 Results On February 5; Shareholder Call Scheduled for February 6 BETHESDA Md. Jan.23 2014 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report fourth quarter 2013 earnings after market close on February 5 2014. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on February 6 2014 at 11:00 am ET. The call can be accessed through a live webcast free of charge at www.MTGE.com or by dialing (888) 317-6016 (U...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:51 [scrapy] ERROR: Error processing {'pagetitle': [u'2015 American Capital Mortgage Investment Corp. Press Releases - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&nyo=0&p=irol-news_pf'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.28 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of $(25.1) million or $(0.49) per common share and net book value of $19.93 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for... American Capital Mortgage Investment Corp. Will Report Third Quarter 2015 Results on October 28; Shareholder Call Scheduled for October 29 BETHESDA Md. Oct.13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on October 28 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 29 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free l... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. Sept.17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the third quarter 2015. The dividend is payable on October 15 2015 to preferred shareholders of record as of October 1 2015 with an ex-dividend date of Sep... American Capital Mortgage Investment Corp. Declares Third Quarter Common Stock Dividend of $0.40 Per Share and Announces the Repurchase of 1.2 Million Shares BETHESDA Md. Sept. 17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.40 per share of common stock for the third quarter 2015. The dividend is payable on October 27 2015 to common shareholders of record as of September 30 2015 with an ex-dividend date of September 28 2015. The Company also announced today that in the third quarter of 2015 it made ope... American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of $(41.1) million or $(0.80) per common share and net book value of $20.70 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarte... American Capital Mortgage Investment Corp. Will Report Second Quarter 2015 Results on July 29; Shareholder Call Scheduled for July 30 BETHESDA Md. July 8 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report second quarter 2015 earnings after market close on July 29 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on July 30 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast... American Capital Mortgage Investment Corp. Declares Second Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. June 15 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the second quarter 2015. The dividend is payable on July 15 2015 to preferred shareholders of record as of July 1 2015 with an ex-dividend date of June 29 ... American Capital Mortgage Investment Corp. Declares Second Quarter Common Stock Dividend of $0.50 Per Share BETHESDA Md. June 15 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.50 per share of common stock for the second quarter 2015. The dividend is payable on July 27 2015 to common shareholders of record as of June 30 2015 with an ex-dividend date of June 26 2015. For further information or questions please contact the Investor Relations Department at (3... American Capital Mortgage Investment Corp. Reports $0.59 Net Income Per Common Share For The First Quarter And $22.00 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. April29 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended March31 2015 of $29.9 million or $0.59 per common share and net book value of $22.00 per common share. Economic return for the period defined as dividends and change in net book value per common share was 2.7% for the qua... American Capital Mortgage Investment Corp. Will Report First Quarter 2015 Results on April 29 Shareholder Call Scheduled for April 30 BETHESDA Md. April 9 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report first quarter 2015 earnings after market close on April 29 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on April 30 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged ... American Capital Mortgage Investment Corp. Declares First Quarter Common Stock Dividend of $0.50 Per Share BETHESDA Md. March 19 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.50 per share of common stock for the first quarter 2015. The dividend is payable on April 27 2015 to common shareholders of record as of March 31 2015 with an ex-dividend date of March 27 2015. For further information or questions please contact the Investor Relations Department at... American Capital Mortgage Investment Corp. Reports $0.33 Net Income Per Common Share For The Fourth Quarter $3.06 Per Common Share For 2014 And $21.91 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Feb.4 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended December31 2014 of $16.7 million or $0.33 per common share and net book value of $21.91 per common share. Economic return for the period defined as dividends and change in net book value per common share was 1.4% for the quart... American Capital Mortgage Investment Corp. Will Report Fourth Quarter 2014 Results on February 4; Shareholder Call Scheduled for February 5 BETHESDA Md. Jan. 13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report fourth quarter 2014 earnings after market close on February 4 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on February 5 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live ...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:52 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=236193&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Sean Reid is a Senior Vice President of our Manager and serves as Senior Vice President Corporate and Business Development of American Capital Mortgage Management LLC. Mr. Reid is also a Senior Vice President of American Capital AGNC Management LLC the manager of American Capital Agency Investment Corp. (NASDAQ: AGNC). Previously Mr. Reid served as Vice President of the Washington D.C. Buyouts group of American Capital Ltd. (NASDAQ: ACAS) and as an Assistant General Counsel with American Capitals in-house legal team. Prior to joining American Capital Ltd. Mr. Reid was an Associate in the Corporate & Securities practice group of Covington & Burling in Washington D.C. Mr. Reid received his J.D. from the University of Virginia School of Law where he was a member of the Order of the Coif and an editor of the Virginia Law Review and his B.B.A. in Accounting from The College of William and Mary.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:52 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214167&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Thomas A. McHale is Vice President and Assistant Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. McHale is also a Vice President of American Capital AGNC Management LLC the manager of American Capital Agency Corp. (NASDAQ: AGNC) and of American Capital LLC the fund management portfolio company of American Capital Ltd. (NASDAQ: ACAS). In addition Mr. McHale has served as the Senior Vice President Finance of American Capital Ltd. since May 2006. He served as Vice President Finance and Investor Relations and Assistant Secretary of American Capital Ltd. from 2002 to May 2006. Mr. McHale joined American Capital in December 1998.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:53 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214166&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Vice President and Controller American Capital Mortgage Investment Corp. Vice President and Chief Financial Officer American Capital MTGE Management LLC Donald Holley is Vice President and Chief Financial Officer of our Manager and Vice President and Controller of American Capital Mortgage Investment Corp. Prior to joining American Capital Mr. Holley was a Director at Freddie Mac responsible for the valuation and presentation of financial results for the companys investments debt and derivative portfolios. He has also worked as an accounting policy and assurance Director at Credit Suisse prepared corporate governance research at Deutsche Bank and was an Audit Manager at Arthur Andersen. Mr. Holley holds a Bachelor of Science degree in Accounting from the University of Florida. He is a Certified Public Accountant and a CFA charterholder.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:53 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214163&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' M. Song Jo is a Vice President of our Manager and is primarily responsible for mortgage structuring for us and other funds managed by American Capital Mortgage Management LLCs subsidiaries. Mr. Jo is also a Vice President of American Capital AGNC Management LLC the manager of American Capital Agency Corp (NASDAQ: AGNC). Mr. Jo previously served as Vice President Mortgage Structuring Investment & Capital Markets at Freddie Mac where he was primarily responsible for managing mortgage structuring activities including creating managing and trading structured activities to enhance that entitys risk return and liquidity profile. Mr. Jo served Freddie Mac in various other capacities from 1997 to 2010. He previously worked at Alex. Brown & Sons and Bear Stearns & Co. Inc.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:54 [scrapy] ERROR: Error processing {'pagetitle': [u'BlackSand Capital - Investments'],
 'pageurl': ['http://www.blacksandcapital.com/investments'],
 'siteurl': ['blacksandcapital.com'],
 'text': ['BlackSand has invested in Park Lane Ala Moana an ultra-luxury residential condominium development located at Ala Moana Center with views overlooking Ala Moana Beach Park and the Pacific Ocean. The project is being developed in partnership by General Growth Properties Inc. The MacNaughton Group and Kobayashi Group. BlackSand has partnered to acquire the Ohana Waikiki West hotel and plans to make significant capital improvements that upon completion will transform the hotel into an upscale 620-room Hilton Garden Inn. The completely renovated hotel will also be located directly across the street from Hawaiis first planned Saks Fifth Avenue department store at a newly redeveloped International Market Place. BlackSand has provided high-yield mezzanine financing in support of the acquisition of Class A industrial warehouse space by an experienced institutional buyer. The warehouse space is centrally located and provides efficient access to the Honolulu International Airport Honolulus Central Business District and other major distribution hubs on Oahu. Pearlridge Uptown II is a 9.3-acre portion of Pearlridge Center the largest enclosed mall on the island of Oahu in Hawaii. Anchored by TJ Maxx and Hawaii Pacific Health Uptown II consists of approximately 150000 square feet of retail and medical office space. The center is centrally located in the Aiea / Pearl Harbor submarket with a population of 345000 in the surrounding area with an average household income of $82000. Maile Sky Court is a 600-room limited-service hotel located in the Waikiki district of Honolulu Hawaii. This 43-story hotel enjoys spectacular mountain ocean and city views from all sides of the hotel. BlackSand has invested in the $330 million luxury residential tower at Ala Moana Center being developed in partnership by The MacNaughton Group Kobayashi Group and The Howard Hughes Corporation. The project is located at Ala Moana Center one of the worlds most successful shopping malls and features ocean and mountain views in residential units priced up to $17 million. Kings Village Shopping Center is located in the heart of Waikiki steps away from the world famous Waikiki Beach. Adjacent to the Hyatt Regency Waikiki and the Sheraton Princess Kaiulani in one of the most highly trafficked corridors in Waikiki Kings Village enjoys the exposure of over 10000 cars and 25000 pedestrians on a daily basis. BlackSand has made a preferred equity investment in a $250 million upscale residential tower in partnership with a local development company. The project features 341 units with mountain and ocean views located in a mixed-use walkable community in urban Honolulu. BlackSand originated and participated in a structured senior credit facility used to acquire and renovate a 400-room upscale hotel in Waikiki. The ownership group made significant capital improvements to upgrade the asset from a 2-star to a 3-star hotel including a full refurbishment of guest suites the lobby and common area amenities.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:54 [scrapy] ERROR: Error processing {'pagetitle': [u'BlackSand Capital - Contact'],
 'pageurl': ['http://www.blacksandcapital.com/contact'],
 'siteurl': ['blacksandcapital.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:55 [scrapy] ERROR: Error processing {'pagetitle': [u'BlackSand Capital - Team'],
 'pageurl': ['http://www.blacksandcapital.com/overview'],
 'siteurl': ['blacksandcapital.com'],
 'text': ['BJ Kobayashi is the Managing Partner and Co-Founder of BlackSand Capital. Mr. Kobayashi is also a member of the firms investment committee and oversees the investment and operational activities of the company. In addition Mr. Kobayashi is a Founding Partner of the Kobayashi Group a property development and real estate investment firm based in Honolulu which he co-founded in 1996. Mr. Kobayashi has 18 years of experience in real estate acquisitions equity and debt financing development and entitlements. Born and raised in Honolulu Hawaii Mr. Kobayashi graduated from Punahou School Georgetown Universitys School of Business with a major in International Business and Georgetown Universitys Law Center where he earned a Juris Doctorate degree. Mr. Kobayashi serves on the board of directors of the Hawaiian Electric Company and is a Trustee of the Hawaiian Tax-Free Trust Hawaiis largest municipal bond fund. In 2011 the late U.S. Senator Daniel K. Inouye appointed Mr. Kobayashi to the Asia Pacific Economic Cooperation (APEC) Committee where he served as co-chair of the Budget and Finance sub-committee. Mr. Kobayashi is active in community service where he serves as the President of the Georgetown Club of Hawaii co-founder of the GIFT Foundation of Hawaii and Board member of the Shane Victorino Foundation.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=225692&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Mr. Doshi is a Vice President of the parent company of our Manager and is primarily responsible for managing agency investment activities for us and American Capital Agency Corp. (Nasdaq: AGNC). Mr. Doshis primary focus is executing specific agency investment strategies for MTGE. Mr. Doshi was previously Vice President Global Securitized Markets in the Fixed Income Global Capital Markets Group at Citigroup Inc. where he was primarily responsible for making markets in Agency MBS pools. Mr. Doshi also served in several other capacities at Citigroup including as Analyst in the Capital Markets Credit Division - High Yield Credit & Distressed Debt Trading Group and the Mortgage Pass-through Trading Group. Mr. Doshi holds a B.S. in Biomedical Engineering magna cum laude from Columbia Universitys School of Engineering and Applied Science.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214162&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Jason Campbell is a Senior Vice President of our Manager. He is primarily responsible for asset and liability management for us and other funds managed by American Capital Mortgage Management LLCs subsidiaries. Mr. Campbell is also a Senior Vice President of American Capital AGNC Management LLC the manger of American Capital Agency Corp. (NASDAQ: AGNC). Previously Mr. Campbell was a Vice President of American Capital Ltd. (NASDAQ: ACAS) in its Financial Services Investment Practice Group focusing on consumer and mortgage assets. From June 2005 to March 2007 he was a Vice President in Merrill Lynchs Global Structured Finance and Investments group in New York. Prior to Merrill Lynch Mr. Campbell held various finance and capital market positions at Eloan.com and Toyota Motor Credit Corporation and was an officer in the U.S. Army. Mr. Campbell holds a Bachelor of Science degree in Business from the University of Southern California and an M.B.A. from Indiana University with an emphasis in Finance. Mr. Campbell is a CFA charterholder.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:56 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214160&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Dr. Puryear is Professor Emeritus of Management and Entrepreneurship at Baruch College of the City University of New York where he was the initial recipient of the Lawrence N. Field Professorship in Entrepreneurship. Dr. Puryear is also a management consultant who advises existing and new businesses with high-growth potential. Prior to his appointment at Baruch College Dr. Puryear was on the faculty of the graduate school of business administration at Rutgers University. During leaves of absence from Baruch he served as Vice President at the Ford Foundation and First Deputy Comptroller for the City of New York. Before joining the academic community he held executive positions in finance and information technology with the Mobil Corporation and Allied Chemical Corporation respectively. He is also a member of the Board of Directors of American Capital Ltd. (NASDAQ: ACAS) and American Capital Agency Corp. (NASDAQ: AGNC). In the past five years Dr. Puryear also served as a director of North Fork Bancorporation North Fork Bank and the Bank of Tokyo-Mitsubishi UFG Trust Company.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:56 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2007347&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report fourth quarter 2014 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who do plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q4 2014 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10058862. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through an affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-fourth-quarter-2014-results-on-february-4-300020059.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:56 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2007347&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report fourth quarter 2014 earnings after market close on 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at www.MTGE.com. Those who do plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at www.MTGE.com. Select the Q4 2014 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10058862. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through an affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-will-report-fourth-quarter-2014-results-on-february-4-300020059.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:55:59 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2013678&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $0.33 Net Income Per Common Share For The Fourth Quarter $3.06 Per Common Share For 2014 And $21.91 Net Book Value Per Common Share To download financial tables please click here. BETHESDA Md. Feb.4 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended December31 2014 of or per common share and net book value of per common share. Economic return for the period defined as dividends and change in net book value per common share was 1.4% for the quarter. For the full year 2014 MTGE reported an economic return of 14.1%. net income per common share Includes all unrealized gains and losses on investment and hedging portfolios net spread and dollar roll income per common share excluding estimated "catch-up" premium amortization Includes estimated dollar roll income per common share associated with the Companys average net long position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market Excludes estimated "catch-up" premium amortization cost per common share due to change in projected constant prepayment rate ("CPR") estimates dividend declared per common share on 13.8% annualized dividend yield based on closing stock price of per common share net book value per common share as of Decreased per common share or (1.5)% from per common share as of 1.4% economic return on common equity for the quarter or 5.8% annualized Comprised of dividend per common share and decrease in net book value per common share 4.6x "at risk" leverage as of 8.0% agency securities actual CPR for the quarter 8.2% projected life CPR for agency securities as of 2.35% annualized net spread and dollar roll income for the quarter excluding estimated "catch-up" premium amortization 14.1% economic return on common equity comprised of: increase in net book value per common share of net proceeds raised from 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") "We are pleased with MTGEs 2014 performance. Our portfolio generated an economic return of 14% and our total stock return exceeded 23% for the year" commented President and Chief Investment Officer. "At the beginning of 2014 the consensus view was that interest rates would increase and agency MBS spreads would widen as the economy strengthened and the Federal Reserve ended its asset purchases. We took a more balanced approach to managing the portfolio than this consensus view might suggest maintaining a larger duration gap allocating a relatively high proportion of our capital to agency investments and operating with moderate leverage. Interest rates ultimately declined in 2014 and agency MBS performed well contrary to popular sentiment." "Our non-agency MBS also performed well during 2014 modestly lagging our agency investments as the US housing market and the broader economy continued to show improvement while supply and demand fundamentals remained favorable" continued Mr. Kain. "With respect to our mortgage servicing investments and operations we were patient in acquiring MSR despite the negative impact on the operating performance of our servicing platform as elevated market valuations did not adequately reflect our view of the underlying risks. Looking ahead with longer term rates near historic lows managing prepayment and extension risk will be critical to producing attractive risk-adjusted returns. These are familiar challenges for us and we are confident that our disciplined and active approach to portfolio management will serve us well in this environment. Moreover we believe our non-agency portfolio is well positioned and we expect to have attractive investment opportunities in GSE risk sharing transactions in 2015." "2014 was another strong year for MTGE" commented Chief Financial Officer and Executive Vice President. "We are very pleased with last years economic return as well as the outstanding 61% economic return that MTGE has generated since its IPO in late 2011. These are great results for MTGE shareholders and are a testament to our investment philosophy that stresses active management prudent asset selection and careful diversification. We are happy with our performance to date but we are even more excited about MTGEs future. We are confident that MTGE is well positioned to take full advantage of the ever changing landscape in the US housing market." As of December31 2014 the Companys investment portfolio included of agency MBS of net long TBA securities of non-agency MBS and of MSR. As of December31 2014 the Companys agency investment portfolio inclusive of net long TBA was comprised of of fixed rate and of adjustable rate securities. As of December31 2014 the Companys agency fixed rate investments were comprised of 15 year securities 20 year securities 30 year securities and 15 year net long TBA securities. As of December31 2014 15 year fixed rate investments represented 43% of the Companys agency investment portfolio an increase from 27% as of and 30 year fixed rate investments represented 50% of the Companys agency investment portfolio a decrease from 66% as of . As of December31 2014 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.23% comprised of a weighted average coupon of 2.98% for 15 year securities 3.35% for 20 year securities and 3.44% for 30 year securities. As of December31 2014 the Companys non-agency portfolio was comprised of 42% Alt-A 24% prime 15% option ARM and 19% subprime securities.The Companysnon-agency securities collateralized by prime mortgage loans had a total fair value of of which related to GSE issued credit risk transfer securities. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of December31 2014 the Companys net long TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. The actual CPR for the Companys agency portfolio during the fourth quarter of 2014 was 8.0% down from 8.9% during the third quarter. The CPR published in for the Companys agency portfolio held as of December31 2014 was 7.7% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of December31 2014 was 8.2% compared to 7.6% as of 2014. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share. The Company recognized approximately or per common share of "catch-up" premium amortization expense during the quarter as projected CPR estimates rose for the Companys existing agency securities during the quarter. The weighted average cost basis of the Companys agency securities was 104.4% of par and the unamortized agency net premium was as of December31 2014. The weighted average cost basis of the Companys non-agency portfolio was 80.9% of par as of December31 2014. Discount accretion on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of December31 2014 with designated as credit reserves. The Companys average annualized net interest rate spread and dollar roll income for the fourth quarter was 2.24% compared to 2.34% for the third quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.16% for the fourth quarter down 10 bps from 2.26% for the third quarter. The Companys average asset yield on its MBS portfolio for the fourth quarter was 3.18% compared to 3.28% for the third quarter. Excluding the impact of "catch-up" premium amortization expense recognized due to changes in projected CPR estimates the annualized weighted average yield on the Companys MBS portfolio was 3.31% for the fourth quarter compared to 3.34% for the third quarter. The Companys asset yield as of December31 2014 was 3.24% down 9 bps from 3.33% as of . The Companys average cost of funds of 1.02% for the fourth quarter (derived from the cost of repurchase agreements and effective interest rate swaps) was consistent with the third quarter. The Companys average cost of funds of 1.02% as of December31 2014 was down 1 bp from 1.03% as of . As of December31 2014 of the Companys repurchase agreements were used to fund purchases of agency and non-agency securities while the remaining were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including net long TBA securities the Companys "at risk" leverage ratio was 4.6x as of December31 2014 and averaged 5.3x during the fourth quarter. The borrowed under agency and non-agency repurchase agreements as of December31 2014 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; between six and nine months; between nine and twelve months; and As of December31 2014 the Companys agency and non-agency repurchase agreements had an average of 210 days remaining to maturity up from 87 days as of . As of December31 2014 the Company had repurchase agreements with 31 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 21% of the Companys equity at risk. The Companys interest rate swap positions as of December31 2014 totaled in notional amount with a weighted average fixed pay rate of 2.08% a weighted average receive rate of 0.23% and a weighted average maturity of 4.7 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.24% as of December31 2014. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of December31 2014 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 1.1 years. These swaptions have an underlying weighted average interest rate swap term of 6.5 years and a weighted average pay rate of 3.29% as of December31 2014. In addition to its interest rate swaps and swaptions the Company held a net long position in U.S. Treasury securities and futures. As of December31 2014 84% of the Companys combined repurchase agreement and net long TBA balance was hedged through a combination of interest rate swaps interest rate swaptions and U.S. Treasury securities and futures. As of December31 2014 ("RCS") managed a servicing portfolio of approximately 66000 residential mortgage loans representing approximately in unpaid principal balances. During the fourth quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the fourth quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized loss on periodic settlements of interest rate swaps; of net realized loss on other derivatives and securities; of net unrealized loss on other derivatives and securities; and Realized and unrealized net losses on other derivatives and securities during the fourth quarter include of net loss on interest rate swaps and swaptions of net loss on U.S. treasury securities and futures and of net gain on TBA mortgage positions (including of dollar roll income). REIT taxable income for the fourth quarter is estimated at per common share or higher than GAAP net income per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) temporary differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the fourth quarter excludes per share of estimated net capital losses which are not included in the Companys ordinary taxable income. As of December31 2014 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share net of dividends declared. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of December31 2014 the Company had estimated remaining unutilized net capital losses of or per common share which may be carried forward and applied against future net capital gains through 2018. Additionally as of December31 2014 the Company had estimated net deferred gains from terminated swaps and swaptions of or per common share which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. On the Board of Directors of the Company declared a fourth quarter dividend on its common stock of per share unchanged from the prior quarter which was paid on to common stockholders of record as of 2014. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a fourth quarter dividend on its Series A Preferred Stock of per share. The dividend was paid on to preferred stockholders of record as of . Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. The Company also announced the tax characteristics of its 2014 common stock and Series A Preferred Stock dividends. The Companys 2014 dividends of per common share and per share of Series A PreferredStock consisted of ordinary dividends for federal income tax purposes. Stockholders should receive an 1099-DIV containing this information from their brokers transfer agents or other institutions. For additional detail please visit the Companys website at www.MTGE.com. The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Obligation to return securities borrowed under reverse repurchaseagreements at fair value Common stock $0.01 par value; 300000 shares authorized 51165 51142 51142 51142 and 51356 issued and outstanding respectively Realized gain (loss) on other derivatives and securities net Unrealized loss on other derivatives and securities net Net spread and dollar roll income excluding "catch up" amortization per common share - basic and diluted Net TBA portfolio - as of period end at fair value Net TBA portfolio - as of period end at cost Average cost of funds as of period end Average actual CPR for agency securities held during the period Average projected life CPR for agency securities as of period end Net book value per common share as of period end (1) Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. (2) Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. (3) Excludes servicing expenses related to the Companys investment in RCS. (4) The Companys estimated taxable income for the fourth quarter excludes per common share of estimated net capital losses which are not included in the Companys ordinary taxable income but are applied against previously recognized net capital losses. (5) Excluding our investments in RCS and REIT equity securities the average stockholders equity for the fourth quarter was . (6) Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. (7) Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. (8) Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency repurchase agreements for the period by the Companys average month-ended stockholders equity for the period less investments in RCS and REIT equity securities. Leverage excludes U.S. Treasury repurchase agreements. (9) Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency repurchase agreements and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who do plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q4 2014 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10058862. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or . is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-033-net-income-per-common-share-for-the-fourth-quarter-306-per-common-share-for-2014-and-2191-net-book-value-per-common-share-300031054.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:04 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2013678&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $0.33 Net Income Per Common Share For The Fourth Quarter $3.06 Per Common Share For 2014 And $21.91 Net Book Value Per Common Share To download financial tables please click here. BETHESDA Md. Feb.4 2015 /PRNewswire/ -- ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net income for the quarter ended December31 2014 of or per common share and net book value of per common share. Economic return for the period defined as dividends and change in net book value per common share was 1.4% for the quarter. For the full year 2014 MTGE reported an economic return of 14.1%. net income per common share Includes all unrealized gains and losses on investment and hedging portfolios net spread and dollar roll income per common share excluding estimated "catch-up" premium amortization Includes estimated dollar roll income per common share associated with the Companys average net long position in agency mortgage-backed securities ("MBS") in the "to-be-announced" ("TBA") market Excludes estimated "catch-up" premium amortization cost per common share due to change in projected constant prepayment rate ("CPR") estimates dividend declared per common share on 13.8% annualized dividend yield based on closing stock price of per common share net book value per common share as of Decreased per common share or (1.5)% from per common share as of 1.4% economic return on common equity for the quarter or 5.8% annualized Comprised of dividend per common share and decrease in net book value per common share 4.6x "at risk" leverage as of 8.0% agency securities actual CPR for the quarter 8.2% projected life CPR for agency securities as of 2.35% annualized net spread and dollar roll income for the quarter excluding estimated "catch-up" premium amortization 14.1% economic return on common equity comprised of: increase in net book value per common share of net proceeds raised from 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") "We are pleased with MTGEs 2014 performance. Our portfolio generated an economic return of 14% and our total stock return exceeded 23% for the year" commented President and Chief Investment Officer. "At the beginning of 2014 the consensus view was that interest rates would increase and agency MBS spreads would widen as the economy strengthened and the Federal Reserve ended its asset purchases. We took a more balanced approach to managing the portfolio than this consensus view might suggest maintaining a larger duration gap allocating a relatively high proportion of our capital to agency investments and operating with moderate leverage. Interest rates ultimately declined in 2014 and agency MBS performed well contrary to popular sentiment." "Our non-agency MBS also performed well during 2014 modestly lagging our agency investments as the US housing market and the broader economy continued to show improvement while supply and demand fundamentals remained favorable" continued Mr. Kain. "With respect to our mortgage servicing investments and operations we were patient in acquiring MSR despite the negative impact on the operating performance of our servicing platform as elevated market valuations did not adequately reflect our view of the underlying risks. Looking ahead with longer term rates near historic lows managing prepayment and extension risk will be critical to producing attractive risk-adjusted returns. These are familiar challenges for us and we are confident that our disciplined and active approach to portfolio management will serve us well in this environment. Moreover we believe our non-agency portfolio is well positioned and we expect to have attractive investment opportunities in GSE risk sharing transactions in 2015." "2014 was another strong year for MTGE" commented Chief Financial Officer and Executive Vice President. "We are very pleased with last years economic return as well as the outstanding 61% economic return that MTGE has generated since its IPO in late 2011. These are great results for MTGE shareholders and are a testament to our investment philosophy that stresses active management prudent asset selection and careful diversification. We are happy with our performance to date but we are even more excited about MTGEs future. We are confident that MTGE is well positioned to take full advantage of the ever changing landscape in the US housing market." As of December31 2014 the Companys investment portfolio included of agency MBS of net long TBA securities of non-agency MBS and of MSR. As of December31 2014 the Companys agency investment portfolio inclusive of net long TBA was comprised of of fixed rate and of adjustable rate securities. As of December31 2014 the Companys agency fixed rate investments were comprised of 15 year securities 20 year securities 30 year securities and 15 year net long TBA securities. As of December31 2014 15 year fixed rate investments represented 43% of the Companys agency investment portfolio an increase from 27% as of and 30 year fixed rate investments represented 50% of the Companys agency investment portfolio a decrease from 66% as of . As of December31 2014 the Companys agency fixed rate mortgage assets inclusive of the net TBA position had a weighted average coupon of 3.23% comprised of a weighted average coupon of 2.98% for 15 year securities 3.35% for 20 year securities and 3.44% for 30 year securities. As of December31 2014 the Companys non-agency portfolio was comprised of 42% Alt-A 24% prime 15% option ARM and 19% subprime securities.The Companysnon-agency securities collateralized by prime mortgage loans had a total fair value of of which related to GSE issued credit risk transfer securities. The Company accounts for TBA securities as derivative instruments and recognizes dollar roll income and other realized and unrealized gains and losses on TBA securities in other gains (losses) net on the Companys consolidated statements of operations. As of December31 2014 the Companys net long TBA mortgage portfolio had a fair value and cost basis of approximately with a net carrying value of reported in derivative assets/(liabilities) on the Companys consolidated balance sheets. The actual CPR for the Companys agency portfolio during the fourth quarter of 2014 was 8.0% down from 8.9% during the third quarter. The CPR published in for the Companys agency portfolio held as of December31 2014 was 7.7% and the weighted average projected CPR for the remaining life of the Companys agency securities held as of December31 2014 was 8.2% compared to 7.6% as of 2014. The Company amortizes and accretes premiums and discounts associated with purchases of agency securities into interest income over the estimated life of such securities based on actual and projected CPRs using the effective yield method. As such slower actual and projected prepayments can have a meaningful positive impact while faster actual or projected prepayments can have a meaningful negative impact on the Companys agency asset yields. The amortization of premiums (net of any accretion of discounts) on the agency portfolio for the quarter was or per common share. The Company recognized approximately or per common share of "catch-up" premium amortization expense during the quarter as projected CPR estimates rose for the Companys existing agency securities during the quarter. The weighted average cost basis of the Companys agency securities was 104.4% of par and the unamortized agency net premium was as of December31 2014. The weighted average cost basis of the Companys non-agency portfolio was 80.9% of par as of December31 2014. Discount accretion on the non-agency portfolio for the quarter was or per common share. The total net discount remaining was as of December31 2014 with designated as credit reserves. The Companys average annualized net interest rate spread and dollar roll income for the fourth quarter was 2.24% compared to 2.34% for the third quarter. Excluding dollar rolls the Companys average net interest rate spread was 2.16% for the fourth quarter down 10 bps from 2.26% for the third quarter. The Companys average asset yield on its MBS portfolio for the fourth quarter was 3.18% compared to 3.28% for the third quarter. Excluding the impact of "catch-up" premium amortization expense recognized due to changes in projected CPR estimates the annualized weighted average yield on the Companys MBS portfolio was 3.31% for the fourth quarter compared to 3.34% for the third quarter. The Companys asset yield as of December31 2014 was 3.24% down 9 bps from 3.33% as of . The Companys average cost of funds of 1.02% for the fourth quarter (derived from the cost of repurchase agreements and effective interest rate swaps) was consistent with the third quarter. The Companys average cost of funds of 1.02% as of December31 2014 was down 1 bp from 1.03% as of . As of December31 2014 of the Companys repurchase agreements were used to fund purchases of agency and non-agency securities while the remaining were used to fund purchases of U.S. Treasury securities and are not included in the Companys measurements of leverage. Including net long TBA securities the Companys "at risk" leverage ratio was 4.6x as of December31 2014 and averaged 5.3x during the fourth quarter. The borrowed under agency and non-agency repurchase agreements as of December31 2014 had remaining maturities consisting of: of one month or less; between one and two months; between two and three months; between three and six months; between six and nine months; between nine and twelve months; and As of December31 2014 the Companys agency and non-agency repurchase agreements had an average of 210 days remaining to maturity up from 87 days as of . As of December31 2014 the Company had repurchase agreements with 31 financial institutions and less than 5% of the Companys equity was at risk with any one counterparty with the top five counterparties representing less than 21% of the Companys equity at risk. The Companys interest rate swap positions as of December31 2014 totaled in notional amount with a weighted average fixed pay rate of 2.08% a weighted average receive rate of 0.23% and a weighted average maturity of 4.7 years. Excluding forward starting swaps the Companys interest rate swap portfolio had a notional balance of and an average fixed pay rate of 1.24% as of December31 2014. The Company enters into interest rate swaps with longer maturities with the intention of protecting its net book value and longer term earnings potential. The Company utilizes interest rate swaptions to mitigate the Companys exposure to larger more rapid increases in interest rates. As of December31 2014 the Company held payer swaption contracts with a total notional amount of and a weighted average expiration of 1.1 years. These swaptions have an underlying weighted average interest rate swap term of 6.5 years and a weighted average pay rate of 3.29% as of December31 2014. In addition to its interest rate swaps and swaptions the Company held a net long position in U.S. Treasury securities and futures. As of December31 2014 84% of the Companys combined repurchase agreement and net long TBA balance was hedged through a combination of interest rate swaps interest rate swaptions and U.S. Treasury securities and futures. As of December31 2014 ("RCS") managed a servicing portfolio of approximately 66000 residential mortgage loans representing approximately in unpaid principal balances. During the fourth quarter the Company recorded in servicing income and in servicing expense which included in realization of cash flows on MSR. The Company has elected to record all investments at fair value with all changes in fair value recorded in current GAAP earnings as other gains (losses). In addition the Company has not designated any derivatives as hedges for GAAP accounting purposes and therefore all changes in the fair value of derivatives are recorded in current GAAP earnings as other gains (losses). During the fourth quarter the Company recorded in other gains (losses) net or per common share. Other gains (losses) net for the quarter are comprised of: of net realized loss on periodic settlements of interest rate swaps; of net realized loss on other derivatives and securities; of net unrealized loss on other derivatives and securities; and Realized and unrealized net losses on other derivatives and securities during the fourth quarter include of net loss on interest rate swaps and swaptions of net loss on U.S. treasury securities and futures and of net gain on TBA mortgage positions (including of dollar roll income). REIT taxable income for the fourth quarter is estimated at per common share or higher than GAAP net income per common share. The primary differences between GAAP net income and estimated REIT taxable net income are (i) unrealized gains and losses associated with investment securities interest rate swaps and other derivatives and securities marked-to-market in current income for GAAP purposes but excluded from taxable income until realized or settled (ii) timing differences both temporary and potentially permanent in the recognition of certain realized gains and losses (iii) losses or undistributed income of taxable REIT subsidiaries and (iv) temporary differences related to the amortization and accretion of net premiums and discounts paid on investments. The Companys estimated taxable income for the fourth quarter excludes per share of estimated net capital losses which are not included in the Companys ordinary taxable income. As of December31 2014 the Company had approximately of estimated undistributed taxable income ("UTI") or per common share net of dividends declared. UTI excludes the Companys remaining unutilized net capital loss carryforwards and net deferred gains from terminated or expired swaps and swaptions. As of December31 2014 the Company had estimated remaining unutilized net capital losses of or per common share which may be carried forward and applied against future net capital gains through 2018. Additionally as of December31 2014 the Company had estimated net deferred gains from terminated swaps and swaptions of or per common share which will be amortized into future ordinary taxable income over the remaining terms of the underlying swaps. On the Board of Directors of the Company declared a fourth quarter dividend on its common stock of per share unchanged from the prior quarter which was paid on to common stockholders of record as of 2014. Since its initial public offering the Company has declared and paid a total of in common stock dividends or per common share. On the Board of Directors of the Company declared a fourth quarter dividend on its Series A Preferred Stock of per share. The dividend was paid on to preferred stockholders of record as of . Since the Series A Preferred Stock offering the Company has declared and paid a total of in Series A Preferred Stock dividends or per share. The Company also announced the tax characteristics of its 2014 common stock and Series A Preferred Stock dividends. The Companys 2014 dividends of per common share and per share of Series A PreferredStock consisted of ordinary dividends for federal income tax purposes. Stockholders should receive an 1099-DIV containing this information from their brokers transfer agents or other institutions. For additional detail please visit the Companys website at www.MTGE.com. The following tables include certain measures of operating performance such as net spread income and estimated taxable income which are non-GAAP financial measures. Please refer to "Use of Non-GAAP Financial Information" later in this release for further discussion of non-GAAP measures. Obligation to return securities borrowed under reverse repurchaseagreements at fair value Common stock $0.01 par value; 300000 shares authorized 51165 51142 51142 51142 and 51356 issued and outstanding respectively Realized gain (loss) on other derivatives and securities net Unrealized loss on other derivatives and securities net Net spread and dollar roll income excluding "catch up" amortization per common share - basic and diluted Net TBA portfolio - as of period end at fair value Net TBA portfolio - as of period end at cost Average cost of funds as of period end Average actual CPR for agency securities held during the period Average projected life CPR for agency securities as of period end Net book value per common share as of period end (1) Table includes non-GAAP financial measures. Average numbers for each period are weighted based on days on the Companys books and records. All percentages are annualized. Refer to "Use of Non-GAAP Financial Information" for additional discussion of non-GAAP financial measures. (2) Dividend income from investments in REIT equity securities is included in realized gain (loss) on other derivatives and securities net on the consolidated statements of operations. (3) Excludes servicing expenses related to the Companys investment in RCS. (4) The Companys estimated taxable income for the fourth quarter excludes per common share of estimated net capital losses which are not included in the Companys ordinary taxable income but are applied against previously recognized net capital losses. (5) Excluding our investments in RCS and REIT equity securities the average stockholders equity for the fourth quarter was . (6) Weighted average cost of funds includes periodic settlements of interest rate swaps and excludes U.S. Treasury repurchase agreements. (7) Estimated dollar roll income excludes the impact of other supplemental hedges and is recognized in gain (loss) on derivative instruments and other securities net. (8) Leverage during the period was calculated by dividing the Companys daily weighted average agency and non-agency repurchase agreements for the period by the Companys average month-ended stockholders equity for the period less investments in RCS and REIT equity securities. Leverage excludes U.S. Treasury repurchase agreements. (9) Leverage at period end was calculated by dividing the sum of the amount outstanding under the Companys agency and non-agency repurchase agreements and the net receivable/payable for unsettled securities at period end by the Companys stockholders equity at period end less investment in RCS. Leverage excludes U.S. Treasury repurchase agreements. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on at . Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free live webcast at . Those who do plan on participating in the Q&A or do not have the internet available may access the call by dialing (877) 503-6874 (U.S. domestic) or (412) 902-6600 (international). Please advise the operator you are dialing in for the shareholder call. A slide presentation will accompany the call and will be available at . Select the Q4 2014 Earnings Presentation link to download and print the presentation in advance of the shareholder call. An archived audio of the shareholder call combined with the slide presentation will be made available on the MTGE website after the call on . In addition there will be a phone recording available one hour after the live call on through . If you are interested in hearing the recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10058862. For further information or questions please contact the Investor Relations Department at (301) 968-9220 or . is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to . (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to This press release contains forward-looking statements. Forward-looking statements are based on estimates projections beliefs and assumptions of management of the Company at the time of such statements and are not guarantees of future performance or results. Forward-looking statements involve risks and uncertainties in predicting future results and conditions. Actual results could differ materially from those projected in these forward-looking statements due to a variety of important factors including without limitation changes in interest rates changes in the yield curve changes in prepayment rates the availability and terms of financing changes in the market value of the Companys assets the receipt of regulatory approval or other closing conditions for a transaction general economic conditions market conditions conditions in the market for agency and non-agency securities and mortgage related investments and legislative and regulatory changes that could adversely affect the business of the Company. Certain important factors that could cause actual results to differ materially from those contained in the forward-looking statements are included in the Companys periodic reports filed with the (" "). Copies are available on the website . The Company disclaims any obligation to update or revise any forward-looking statements based on the occurrence of future events the receipt or new information or otherwise. USE OF NON-GAAP FINANCIAL INFORMATION In addition to the results presented in accordance with GAAP our results of operations discussed herein include certain non-GAAP financial information including "adjusted net interest income" (including the periodic interest rate costs of our interest rate swaps reported in gain (loss) on derivatives and other securities net in our consolidated statements of operations and dividends from REIT equity securities) and "estimated taxable income" and certain financial metrics derived from non-GAAP information such as "cost of funds" and "estimated undistributed taxable income." By providing users of our financial information with such measures in addition to the related GAAP measures we believe it gives users greater transparency into the information used by our management in its financial and operational decision-making and that it is meaningful information to consider related to: (i) the economic costs of financing our investment portfolio inclusive of interest rate swaps used to economically hedge against fluctuations in our borrowing costs (ii) in the case of net spread income our current financial performance without the effects of certain transactions that are not necessarily indicative of our current investment portfolio and operations and (iii) in the case of estimated taxable income and estimated undistributed taxable income information that is directly related to the amount of dividends we are required to distribute in order to maintain our REIT qualification status. However because such measures are incomplete measures of our financial performance and involve differences from results computed in accordance with GAAP they should be considered as supplementary to and not as a substitute for our results computed in accordance with GAAP. In addition because not all companies use identical calculations our presentation of such non-GAAP measures may not be comparable to other similarly-titled measures of other companies. Furthermore estimated taxable income can include certain information that is subject to potential adjustments up to the time of filing our income tax returns which occurs after the end of our fiscal year. A reconciliation of GAAP net interest income to non-GAAP net spread and dollar roll income and a reconciliation of GAAP net income to non-GAAP estimated taxable income is included in this release. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-reports-033-net-income-per-common-share-for-the-fourth-quarter-306-per-common-share-for-2014-and-2191-net-book-value-per-common-share-300031054.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:05 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog 2015'],
 'pageurl': ['http://www.blogcastalia.com/2015/page/2/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:06 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog China: Unsustainable Growth'],
 'pageurl': ['http://www.blogcastalia.com/china-unsustainable-growth/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:07 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2027176&c=245595&p=irol-newsArticle_Print'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of per share for the first quarter 2015. The dividend is payable on to preferred shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-first-quarter-dividend-on-its-series-a-preferred-stock-300053234.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:08 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog'],
 'pageurl': ['http://www.blogcastalia.com/page/3/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:09 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog New Contributor: Roger King \u2013 CreditSights'],
 'pageurl': ['http://www.blogcastalia.com/new-contributor-roger-king-creditsights/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:11 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog Agricultural Markets: Finely Balanced'],
 'pageurl': ['http://www.blogcastalia.com/agricultural-markets-finely-balanced/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:13 [scrapy] ERROR: Error processing {'pagetitle': [u'Castalia Blog U.S. Agricultural Markets: Weather Sensitive'],
 'pageurl': ['http://www.blogcastalia.com/u-s-agricultural-markets-weather-sensitive/'],
 'siteurl': ['blogcastalia.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:13 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500077266.PDF?Y=&O=PDF&D=&FID=1500077266&T=&IID=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:56:13 [scrapy] ERROR: Error processing {'pagetitle': [u'News Release - News - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=2027176&c=245595&p=irol-newsArticle'],
 'siteurl': ['ir.mtge.com'],
 'text': [' /PRNewswire/ -- (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of per share for the first quarter 2015. The dividend is payable on to preferred shareholders of record as of with an ex-dividend date of . For further information or questions please contact the Investor Relations Department at (301) 968-9220 or IR@MTGE.com. ABOUT is a real estate investment trust that invests in and manages a leveraged portfolio of agency mortgage investments non-agency mortgage investments and other mortgage-related investments. The Company is externally managed and advised by an affiliate of (" "). For further information please refer to www.MTGE.com. ABOUT AMERICAN CAPITAL (Nasdaq: ACAS) is a publicly traded private equity firm and global asset manager. both directly and through its asset management business originates underwrites and manages investments in middle market private equity leveraged finance real estate energy & infrastructure and structured products. manages of assets including assets on its balance sheet and fee earning assets under management by affiliated managers with of total assets under management (including levered assets). Through a wholly owned affiliate manages publicly traded (Nasdaq: AGNC) (Nasdaq: MTGE) and (Nasdaq: ACSF) with approximately of total net book value. From its eight offices in the U.S. and and its wholly owned affiliate will consider investment opportunities from . For further information please refer to www.AmericanCapital.com. To view the original version on PR Newswire visit:http://www.prnewswire.com/news-releases/american-capital-mortgage-investment-corp-declares-first-quarter-dividend-on-its-series-a-preferred-stock-300053234.html']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:13 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214159&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Larry Harvey has served as Chief Financial Officer of Playa Hotels & Resorts B.V. since April 2015. From 2007 to 2013 Mr. Harvey served as Executive Vice President and Chief Financial Officer of Host Hotels & Resorts Inc. (NYSE: HST) (Host) and served as its Treasurer from 2007 to 2010. From 2006 to 2007 Mr. Harvey served as Senior Vice President Chief Accounting Officer of Host and from 2003 to 2006 he served as Hosts Senior Vice President and Corporate Controller. Prior to rejoining Host in 2003 he served as Chief Financial Officer of Barcel Crestline Corporation formerly Crestline Capital Corporation. Prior to that Mr. Harvey was Hosts Vice President of Corporate Accounting before the spin-off of Crestline in 1998. Mr. Harvey also serves on the board of directors of American Capital Agency Corp. (NASDAQ: AGNC) and American Capital Senior Floating Ltd. (NASDAQ: ACSF).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:14 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=228083&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Ms. Larocca retired in 2011 from Royal Bank of Scotland (RBS) where from 1997 until her retirement she was a Managing Director in the firms Mortgage Backed and Asset Backed Finance Group. She is a widely recognized expert in the areas of housing finance and securitization and is a member of the board of the Housing Preservation Foundation as well as having previously served two terms on the board of the American Securitization Trade Association. Prior to joining RBS Ms. Larocca was a Senior Vice President at Lehman Brothers in the mortgage finance business managed the consumer and single family securitization business for the Resolution Trust Corporation practiced law with the firms of Milbank Tweed Hadley and McCloy as well as Kutak Rock. She is a graduate of the Georgetown University Law Center and Indiana University. Ms. Larocca also serves on the board of directors of American Capital Agency Corp. (NASDAQ: AGNC).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:14 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=228082&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Samuel A. Flax is our Executive Vice President and Secretary and Executive Vice President Chief Compliance Officer and Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Flax is also a Director Executive Vice President and Secretary of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President Chief Compliance Officer and Secretary of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice President Chief Compliance Officer and Secretary of American Capital Asset Management LLC (f/k/a American Capital LLC) the asset fund management portfolio company of American Capital Ltd. (NASDAQ: ACAS). Mr. Flax has also served as the Executive Vice President General Counsel Chief Compliance Officer and Secretary of American Capital Ltd. since January 2005. Mr. Flax was a partner in the corporate and securities practice group of the Washington D.C. law firm of Arnold & Porter LLP from 1990 to January 2005. At Arnold & Porter LLP he represented American Capital Ltd. in raising debt and equity capital advised the company on corporate securities and other legal matters and represented the company in many of its investment transactions.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:14 [scrapy] ERROR: Error processing {'pagetitle': [u'Quarterly Shareholder Presentation - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?EventId=5171691&c=245595&p=irol-EventDetails'],
 'siteurl': ['ir.mtge.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:15 [scrapy] ERROR: Error processing {'pagetitle': [u'Quarterly Shareholder Presentation - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?EventId=5182279&c=245595&p=irol-EventDetails'],
 'siteurl': ['ir.mtge.com'],
 'text': [' To hear a recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10058862 and the recording is available through February 19.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:15 [scrapy] ERROR: Error processing {'pagetitle': [u'Quarterly Shareholder Presentation - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?EventId=5189650&c=245595&p=irol-EventDetails'],
 'siteurl': ['ir.mtge.com'],
 'text': [' To hear a recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The conference number is 10063171 and the recording is available through May 14.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:16 [scrapy] ERROR: Error processing {'pagetitle': [u'Quarterly Shareholder Presentation - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?EventId=5197842&c=245595&p=irol-EventDetails'],
 'siteurl': ['ir.mtge.com'],
 'text': [' To hear a recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The access code for both domestic and international callers is 10068977 and the recording is available through August 13th.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:21 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports First Quarter 2011 Financial Results'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=11145922&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE: FIG) today reported its first quarter 2011 results. We had a very solid first quarter with our Credit business in a sweet spot private equity valuations again marching up and our macro funds delivering top-tier returns said Daniel Mudd Chief Executive Officer. More and more Fortress cylinders are firing investor interest in our strategies is strong and growing and we see tremendous opportunities to put capital to work. Please see the exhibits to this press release for a reconciliation of non-GAAP measures referred to in this press release to the related GAAP measures. As of March 31 2011 assets under management totaled $43.1 billion up 43% from $30.2 billion as of March 31 2010. The net increase was primarily from the April 2010 acquisition of Logan Circle Partners which had approximately $12.5 billion of assets under management as of March 31 2011. Pre-tax DE was $103 million in the first quarter of 2011 up from $96 million in the first quarter of 2010. On a dividend paying share basis pre-tax DE was $0.20 per share in the first quarter of 2010 up 5% from $0.19 per share in the first quarter of 2010. Pre-tax DE increased primarily as a result of improved performance in our Principal Investment segment as well as increased incentive income across the Liquid Hedge Fund and the Credit Hedge Fund segments. The increase in incentive income in the hedge funds was the result of substantially all of the capital eligible to earn incentive income within the main Credit and Liquid Hedge Funds being above their respective high water marks as of March 31 2011. In the first quarter of 2011 fund management distributable earnings were $95 million up 3% from $92 million in the first quarter of 2010. This increase was driven by a $37 million increase in total segment revenues offset by a $34 million increase in total segment expenses. Total segment revenues were $244 million in the first quarter of 2011 up 18% from $207 million in the first quarter of 2010. Notably we had (i) $38 million of incentive income recognized from our Credit Hedge Funds compared to $8 million in the first quarter of 2010 and (ii) $22 million of incentive income recognized from our Liquid Hedge Funds compared to $6 million in the first quarter of 2010. Total segment expenses were $149 million in the first quarter of 2011 up 30% from $115 million in the first quarter of 2010 largely driven by increases in compensation and benefits including profit sharing expenses. In addition Fortress had a $7 million increase in unallocated expenses primarily from Logan Circle Partners which was not part of Fortress in the first quarter of 2010. The Companys quarterly segment revenues and distributable earnings will fluctuate materially depending upon the performance of our funds and the realization events within our private equity businesses as well as other factors. Accordingly the revenues and profits in any particular quarter should not be expected to be indicative of future results. Fortress had a GAAP net loss of $255 million in the first quarter of 2011 compared to a loss of $261 million in the first quarter of 2010. This included revenues of $197 million in the first quarter of 2011 and $160 million in the first quarter of 2010. Excluding principals agreement compensation Fortress had a net loss of $20 million in the first quarter of 2011 compared to a net loss of $27 million in the first quarter of 2010. In the first quarter of 2011 our GAAP net loss attributable to Class A Shareholders was $0.58 per diluted share compared to $0.58 per diluted share in the first quarter of 2010. The table below details Fortresss Distributable Earnings and GAAP Net Income (Loss) for the first quarter of 2011 and 2010: The following discussion of our results is based on segment reporting as presented in our Quarterly Report on Form 10-Q. Our GAAP statement of operations and balance sheet are presented following this discussion. The following table is a summary presentation of our segment performance with supplemental data provided for informational purposes. Supplemental Data for the First Quarter 2011 and 2010: Assets under management for the Private Equity Funds was $10.0 billion as of March 31 2011 compared to $11.9 billion as of December 31 2010 and $11.6 billion as of March 31 2010. During the first quarter of 2011 three of our Private Equity Funds reached the end of their investment periods which reset the basis upon which management fees are earned. This reset occurs in the ordinary course pursuant to the Funds governing documentation and is typical for similarly structured private equity funds. The basis was reduced from committed capital to the lower of invested capital or net asset value (NAV). Therefore the change in basis is comprised of two components: (i) the difference between committed capital and the amount of that commitment that was actually invested and (ii) historical reductions in NAV which in this case generally occurred in 2008 and 2009. These resets resulted in a $2.0 billion dollar decrease and were the primary driver in the decline in assets under management. With respect to investments held at the beginning of the period the Fortress Private Equity investments appreciated 10.4% during the first quarter of 2011. The Private Equity Funds generated pre-tax DE of $22 million in the first quarter of 2011 down 8% from $24 million in the first quarter of 2010 as a result of an increase in segment expenses of $5 million partially offset by an increase in segment revenues of $3 million. Segment revenues were $36 million in the first quarter of 2011 up 9% from $33 million in the first quarter of 2010. The year-over-year increase in segment revenue was driven by a $2 million increase in fund management fees and a $1 million increase in incentive income. Assets under management for the Castles which are comprised of two managed publicly traded companies (Newcastle Investment Corp. and Eurocastle Investment Limited) was $3.2 billion as of March 31 2011 compared to $3.0 billion as of December 31 2010 and $3.1 billion as of March 31 2010. The Castles generated pre-tax DE of $5 million in the first quarter of 2011 up 25% from $4 million in the first quarter of 2010 as a result of a decrease in segment expenses of $1 million. Segment revenues were $12 million in the first quarter of 2011 flat compared to $12 million in the first quarter of 2010. Assets under management for the Liquid Hedge Funds was $4.8 billion as of March 31 2011 compared to $4.7 billion as of December 31 2010 and $4.3 billion as of March 31 2010. Capital raised and redemption payouts in April 2011 will affect assets under management in the second quarter of 2011. The following table shows our Assets Under Management by fund: The following table shows our gross and net returns by fund:5 The Liquid Hedge Funds generated pre-tax DE of $13 million in the first quarter of 2011 up 63% from $8 million in the first quarter of 2010 as a result of an increase in segment revenues of $19 million partially offset by an increase in segment expenses of $14 million. Segment revenues were $44 million in the first quarter of 2011 up 76% from $25 million in the first quarter of 2010. The year-over-year increase in segment revenues was a result of a $16 million increase in incentive income and a $3 million increase in management fees. As a result of the positive performance across each of our Liquid Hedge Funds virtually all of the capital that is eligible to earn incentive income within these funds continued to exceed their respective high water marks and therefore incentive income was recognized. Assets under management for the Credit Hedge Funds was $8.0 billion as of March 31 2011 compared to $8.4 billion as of December 31 2010 and $9.1 billion as of March 31 2010. Excluding the Value Recovery Funds which are third-party originated funds for which we were engaged to manage the orderly liquidation of existing portfolios assets under management was flat in the Credit Hedge Funds at $6.8 billion as of March 31 2011 compared to March 31 2010. Capital raised and redemption payouts in April 2011 will affect assets under management in the second quarter of 2011. The following table shows our Assets Under Management by fund: The following table shows our gross and net returns by fund:10 The Credit Hedge Funds generated pre-tax DE of $20 million in the first quarter of 2011 up 82% from $11 million in the first quarter of 2010 as a result of an increase in segment revenues of $32 million partially offset by an increase in segment expenses of $23 million. Segment revenues were $74 million in the first quarter of 2011 up 76% from $42 million in the first quarter of 2010. The year-over-year increase in segment revenues resulted from a $30 million increase in incentive income and a $2 million increase in management fees. As a result of the positive performance across each of the Credit Hedge Funds the majority of the capital that is eligible to earn incentive income within these funds continued to exceed their respective high water marks and therefore incentive income was recognized. Assets under management for the Credit Private Equity Funds was $4.5 billion as of March 31 2011 compared to $4.8 billion as of December 31 2010 and $2.2 billion as of March 31 2010. As of March 31 2011 the Credit Private Equity Funds had approximately $3.6 billion of uncalled or recallable committed capital that will become assets under management when invested. The Credit Private Equity Funds generated pre-tax DE of $38 million in the first quarter of 2011 down 17% from $46 million in the first quarter of 2010 as a result of a decrease in segment revenues of $22 million partially offset by a decrease in segment expenses of $14 million. Segment revenues were $73 million in the first quarter of 2011 down 23% from $95 million in the first quarter of 2010. The year-over-year decrease in segment revenues was largely driven by a $28 million decrease in incentive income partially offset by a $6 million increase in management fees. During the first quarter of 2011 Fortress had profit realizations in its Credit Private Equity Funds and generated a total of $57 million of incentive income in the segment. As of March 31 2011 Principal Investments had segment assets (excluding cash and cash equivalents) totaling $1077 million up from $865 million as of March 31 201012. As of March 31 2011 Fortresss share of the net asset value of its direct and indirect investments exceeded its segment cost basis by $311 million representing unrealized gains. Principal Investments which is comprised of investments in Fortresss underlying funds generated pre-tax DE of $8 million in the first quarter of 2011 up from $4 million in the first quarter of 2010 as a result of an increase in investment income of $4 million and flat year-over-year interest expense. As of March 31 2011 we had a total of $102 million of outstanding commitments to our funds as detailed in the table below. As of March 31 2011 Fortress had cash on hand of $219 million and debt obligations of $275 million. As of March 31 2011 Fortress had availability for further borrowing under its revolving credit facility of $51 million. Each quarter we evaluate whether to pay quarterly dividends on our Class A shares. The amount of any dividends will be determined by our board of directors. However no assurance can be given that any dividends whether quarterly or otherwise will or can be paid. In determining the amount of any dividends our board will take into account various factors including our financial performance on both an actual and projected basis earnings liquidity and the operating performance of our segments as assessed by management. Fortress discloses certain non-GAAP financial information which management believes provides a meaningful basis for comparison among present and future periods. The following are non-GAAP measures used in the accompanying financial information: We urge you to read the reconciliation of such data to the related GAAP measures appearing in the exhibits to this release. Management will host a conference call today Thursday May 5 2011 at 8:30 A.M. Eastern Time. A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. All interested parties are welcome to participate on the live call. The conference call may be accessed by dialing 1-877-717-3044 (from within the U.S.) or 1-706-679-1521 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress First Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Thursday May 12 2011 by dialing 1-800-642-1687 (from within the U.S.) or 1-706-645-9291 (from outside of the U.S.); please reference access code 60798471. Fortress is a leading global investment manager with approximately $43.1 billion in assets under management as of March 31 2011. Fortress offers alternative and traditional investment products and was founded in 1998. For more information regarding Fortress Investment Group LLC or to be added to our e-mail distribution list please visit www.fortress.com. CAUTIONARY NOTE REGARDING FORWARD-LOOKING STATEMENTS Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding our sources of management fees incentive income and investment income (loss) estimated fund performance the amount and source of expected capital commitments and amount of redemptions. These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements and any such differences could cause our actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysQuarterly Report on Form 10-Q which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. Distributable earnings is our supplemental measure of operating performance. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income and it is not necessarily indicative of liquidity or cash available to fund our operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 10 to our financial statements included in our Quarterly Report on Form 10-Q for the quarter ended March 31 2011. Growing distributable earnings is a key component to our business strategy and distributable earnings is the supplemental measure used by our management to evaluate the economic profitability of each of our businesses and our total operations. Therefore we believe that it provides useful information to our investors in evaluating our operating performance. Our definition of distributable earnings is not based on any definition contained in our amended and restated operating agreement.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:30 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Year End & Fourth Quarter 2010 Financial Results'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=10814762&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE: FIG) today reported its year end and fourth quarter 2010 results. Our fourth quarter and full year results were very solid and reflect broad momentum that has carried into 2011 said Daniel Mudd Chief Executive Officer. Our funds continued to deliver strong returns capital raising increased apace and we have attracted a significant number of new investors to Fortress. We are also taking important steps to answer demand for our investment capabilities on an increasingly global basis. As we work through the continued shallow choppy economic recovery and periodic event-driven volatility we believe our businesses are very well-positioned to deliver strong results for our investors. Please see the exhibits to this press release for a reconciliation of non-GAAP measures referred to in this press release to the related GAAP measures. As of December 31 2010 assets under management totaled $44.6 billion up 42% from $31.5 billion as of December 31 2009. The net increase was principally from the acquisition of Logan Circle Partners in April 2010. Logan Circle Partners which had $11.7 billion of assets under management as of December 31 2010 will not be reported as a separate segment until such time as its operations become material to our overall performance. Currently it is included under Unallocated Revenues and Unallocated Expenses in the tables contained in this press release. Pre-tax DE was $372 million in 2010 up from $126 million in 2009. On a dividend paying share basis pre-tax DE was $0.72 per share in 2010 up 177% from $0.26 per share in 2009. Pre-tax DE increased primarily as a result of higher incentive income across the Credit Private Equity Funds Credit Hedge Funds and Liquid Hedge Funds. Certain capital in the Credit Hedge Funds started earning incentive income in the second quarter which increased significantly in the third and fourth quarters while substantially all of the Liquid Hedge Funds started earning significant incentive income in the second half of the year. As of December 31 2010 substantially all of the assets under management eligible to earn incentive income in the main Credit and Liquid Hedge Funds were above their high water marks. In 2010 fund management distributable earnings were $358 million up 72% from $208 million in 2009. This increase was driven by a $341 million increase in total segment revenues offset by a $191 million increase in total segment expenses. Total segment revenues were $840 million in 2010 up 68% from $499 million in 2009. The increase was primarily a result of (i) $170 million of incentive income recognized from our Credit Hedge Funds and Liquid Hedge Funds compared to $16 million in 2009 (ii) $158 million of incentive income recognized from our Credit Private Equity Funds compared to $23 million in 2009 and (iii) $41 million of incentive income recognized from our Private Equity Funds in 2010 compared to $36 million in 2009. Total segment expenses were $482 million in 2010 up 66% from $291 million in 2009 largely driven by higher profit sharing compensation expenses. In 2010 profit sharing compensation expenses were $168 million compared to $47 million in 2009. The increase is primarily a result of higher incentive income distributions from our funds. The remaining $70 million increase in segment expenses was made up of $28 million from Logan Circle Partners which was not part of Fortress in prior years and a $42 million increase in other operating expenses. The Companys quarterly segment revenues and distributable earnings will fluctuate materially depending upon the performance of our funds and the realization events within our private equity businesses as well as other factors. Accordingly the revenues and profits in any particular quarter should not be expected to be indicative of future results. Fortress had a GAAP net loss of $782 million in 2010 compared to a loss of $909 million in 2009. This included revenues of $950 million in 2010 and $584 million in 2009. Excluding principals agreement compensation Fortress had net income of $170 million in 2010 compared to net income of $43 million in 2009. In 2010 our GAAP net loss attributable to Class A Shareholders was $1.83 per diluted share compared to $2.08 per diluted share in 2009. The table below details Fortresss Distributable Earnings and GAAP Net Income (Loss) for the full year and fourth quarter of 2010 and 2009: The following discussion of our results is based on segment reporting as presented in our Annual Report on Form 10-K. Our GAAP statement of operations and balance sheet are presented following this discussion. The following table is a summary presentation of our segment performance with supplemental data provided for informational purposes. 1 Represents $11.4 billion of capital under management due to Fortresss acquisition of Logan Circle Partners. 2 Includes $142 million of SPV capital that ceased paying management fees in the third quarter. 3 Represents distributions from (i) assets held by redeeming capital accounts in our Drawbridge Special Opportunities Funds and (ii) the Value Recovery Funds. 4 Represents $3.3 billion of capital under management due to Fortresss take over of management of the D.B. Zwirn funds and related investment vehicles. 5 Represents distributions from (i) assets held by redeeming capital accounts in our Drawbridge Special Opportunities Funds and (ii) the Value Recovery Funds. Assets under management for the Private Equity Funds was $11.9 billion as of December 31 2010 compared to $11.6 billion as of September 30 2010 and $11.3 billion as of December 31 2009. With respect to investments held at the beginning of each period the Fortress Private Equity investments appreciated 17.0% in value during the full year 2010 and 10.5% during the fourth quarter of 2010. The Private Equity Funds generated pre-tax DE of $127 million in 2010 up 9% from $116 million in 2009 as a result of an increase in segment revenues of $13 million partially offset by an increase in segment expenses of $2 million. Segment revenues were $180 million in 2010 up 8% from $167 million in 2009. The year-over-year increase in segment revenue was driven by an $8 million increase in fund management fees and a $5 million increase in incentive income. The Private Equity Funds had realization events in 2010 that contributed to $41 million of incentive income generated from Fund I and Fund II. Assets under management for the Castles which are comprised of two managed publicly traded companies (Newcastle Investment Corp. and Eurocastle Investment Limited) was $3.0 billion as of December 31 2010 compared to $3.1 billion as of September 30 2010 and $3.2 billion as of December 31 2009. The Castles generated pre-tax DE of $18 million in 2010 down 14% from $21 million in 2009 as a result of a decrease in segment revenues of $2 million and an increase in segment expenses of $1 million. Segment revenues were $48 million in 2010 down 4% from $50 million in 2009. The year-over-year decrease in segment revenues was primarily attributable to changes in foreign currency exchange rates and to a lesser extent a reduction in assets under management. Assets under management for the Liquid Hedge Funds was $4.7 billion as of December 31 2010 compared to $4.3 billion as of September 30 2010 and $4.3 billion as of December 31 2009. The Liquid Hedge Funds had redemption payouts of approximately $150 million in January 2011 related to 2010 redemption notices which will reduce assets under management for the first quarter of 2011. The following table shows our Assets Under Management by fund: 6 Combined AUM for Fortress Macro Onshore Fund L.P. Fortress Macro Offshore Fund L.P. Fortress Macro Fund Ltd. Fortress Macro managed accounts Drawbridge Global Macro Fund L.P. Drawbridge Global Macro Intermediate Fund L.P. Drawbridge Global Macro Alpha Intermediate Fund L.P. DBGM Offshore Ltd. DBGM Onshore L.P. and DBGM Alpha V Ltd. 7 Combined AUM for Fortress Commodities Fund L.P. Fortress Commodities MA1 L.P. and Fortress Commodities managed accounts. The following table shows our gross and net returns by fund:8 8 The performance data contained herein reflects returns for a "new issue eligible" single investor class as of the close of business on the last day of the relevant period. Gross returns reflect performance data prior to management fees borne by the Fund and incentive allocations while net returns reflect performance data after taking into account management fees borne by the Fund and incentive allocations. The Liquid Hedge Funds generated pre-tax DE of $55 million in 2010 up 162% from $21 million in 2009 as a result of an increase in segment revenues of $50 million partially offset by an increase in segment expenses of $16 million. Segment revenues were $144 million in 2010 up 53% from $94 million in 2009. The year-over-year increase in segment revenues was a result of a $51 million increase in incentive income offset by a $1 million decrease in management fees. The main Macro Funds exceeded virtually all their high water marks in the second half of 2010 generating significant incentive income in that period. Assets under management for the Credit Hedge Funds was $8.4 billion as of December 31 2010 compared to $8.6 billion as of September 30 2010 and $9.3 billion as of December 31 2009. Effective December 31 2010 the Credit Hedge Funds which have an annual notice date for redemptions received redemption notices for fee-paying capital of approximately $1.0 billion using December 31 2010 values. This amount is subject to change based on performance. For the majority of these notices redemptions will be paid out over time as the underlying investments are liquidated in accordance with the governing documents of the applicable funds. During this period such amounts continue to be subject to management fees and as applicable incentive income. The following table shows our Assets Under Management by fund: 9 Combined AUM for Drawbridge Special Opportunities Fund Ltd. Drawbridge Special Opportunities Fund L.P. Drawbridge Special Opportunities Fund managed accounts Worden Fund L.P. and Worden Fund II L.P. 10 Combined AUM for Fortress Partners Offshore Fund L.P. and Fortress Partners Fund L.P. 11 Fortress will receive management fees from these funds equal to 1% of cash receipts and up to 1% per annum on certain managed assets subject to collectability and may receive limited incentive income if aggregate realizations exceed an agreed threshold. The following table shows our gross and net returns by fund:12 12 The performance data contained herein reflects returns for a "new issue eligible" single investor class as of the close of business on the last day of the relevant period. Gross returns reflect performance data prior to management fees borne by the Fund and incentive allocations while net returns reflect performance data after taking into account management fees borne by the Fund and incentive allocations. Specific performance may vary based on among other things whether fund investors are invested in one or more special investments. 13 The returns for the Drawbridge Special Opportunities Funds reflect the performance of each fund excluding the performance of the redeeming capital accounts which relate to December 31 2008 and December 31 2009 redemptions. The Credit Hedge Funds generated pre-tax DE of $81 million in 2010 up 268% from $22 million in 2009 as a result of an increase in segment revenues of $124 million partially offset by an increase in segment expenses of $65 million. Segment revenues were $249 million in 2010 up nearly 100% from $125 million in 2009. The year-over-year increase in segment revenues resulted from a $103 million increase in incentive income and a $21 million increase in management fees. Incentive income increased as certain capital within the Credit Hedge Funds passed its high water marks by June 30 2010 and generated significant incentive income during the second half of the year. Assets under management for the Credit Private Equity Funds was $4.8 billion as of December 31 2010 compared to $4.1 billion as of September 30 2010 and $3.3 billion as of December 31 2009. As of December 31 2010 the Credit Private Equity Funds had approximately $3.2 billion of uncalled or recallable committed capital that will become assets under management when invested. The Credit Private Equity Funds generated pre-tax DE of $96 million in 2010 up 231% from $29 million in 2009 as a result of an increase in segment revenues of $143 million partially offset by an increase in segment expenses of $76 million. Segment revenues were $206 million in 2010 up 227% from $63 million in 2009. The year-over-year increase in segment revenues was largely driven by a $135 million increase in incentive income as well as an $8 million increase in management fees. The Credit Private Equity Funds had incentive income of $158 million during 2010 which was generated by certain realization events during the year primarily from the Credit Opportunities Funds. As of December 31 2010 Principal Investments had segment assets (excluding cash and cash equivalents) totaling $780 million up from $727 million as of December 31 2009. As of December 31 2010 Fortresss share of the net asset value of its direct and indirect investments exceeded its segment cost basis by $244 million representing unrealized gains. Principal Investments which is comprised of investments in Fortresss underlying funds generated pre-tax DE of $14 million in 2010 up from a loss of $82 million in 2009 as a result of an increase in investment income of $92 million and a decrease in interest expense of $4 million. Investment income was $34 million in 2010 up from a loss of $58 million in 2009. The increase in investment income was largely a result of a decrease in impairments related to our private equity investments as well as increased earnings from our investments during 2010. As of December 31 2010 we had a total of $102 million of outstanding commitments to our funds as detailed in the table below. As of December 31 2010 Fortress had cash on hand of $211 million and debt obligations of $278 million. As of December 31 2010 Fortress had availability for further borrowing under its revolving credit facility of $52 million. In October 2010 Fortress entered into a new credit agreement and repaid its existing credit agreement in full. The credit agreement includes a $280 million term loan facility and a $60 million revolving credit facility (of which approximately $8 million was outstanding under a letter of credit subfacility as of December 31 2010). The term loan is subject to future mandatory repayments of $2.5 million at each quarter end through September 30 2011 and $8.75 million at each quarter end from December 31 2011 through September 30 2015 and matures in October 2015 (revolving credity facility matures in October 2013). For further detail on the credit agreement please see our Annual Report on Form 10-K. Each quarter we evaluate whether to pay quarterly dividends on our Class A shares. The amount of any dividends will be determined by our board of directors. However no assurance can be given that any dividends whether quarterly or otherwise will or can be paid. In determining the amount of any dividends our board will take into account various factors including our financial performance on both an actual and projected basis earnings liquidity and the operating performance of our segments as assessed by management. Fortress discloses certain non-GAAP financial information which management believes provides a meaningful basis for comparison among present and future periods. The following are non-GAAP measures used in the accompanying financial information: We urge you to read the reconciliation of such data to the related GAAP measures appearing in the exhibits to this release. Management will host a conference call today Tuesday March 1 2011 at 8:30 a.m. (Eastern Time). A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. All interested parties are welcome to participate on the live call. The conference call may be accessed by dialing 1-877-252-8576 (from within the U.S.) or 1-706-679-1521 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Fourth Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Monday March 14 2011 by dialing 1-800-642-1687 (from within the U.S.) or 1-706-645-9291 (from outside of the U.S.); please reference access code 43426668. Fortress is a leading global investment manager with approximately $44.6 billion in assets under management as of December 31 2010. Fortress offers alternative and traditional investment products and was founded in 1998. For more information regarding Fortress Investment Group LLC or to be added to our e-mail distribution list please visit www.fortress.com. CAUTIONARY NOTE REGARDING FORWARD-LOOKING STATEMENTS Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding our sources of management fees incentive income and investment income (loss) estimated fund performance the amount and source of expected capital commitments and amount of redemptions. These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements and any such differences could cause our actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysAnnual Report on Form 10-K which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. Distributable earnings is our supplemental measure of operating performance. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income and it is not necessarily indicative of liquidity or cash available to fund our operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 11 to our financial statements included in our Annual Report on Form 10-K for the year ended December 31 2010. Growing distributable earnings is a key component to our business strategy and distributable earnings is the supplemental measure used by our management to evaluate the economic profitability of each of our businesses and our total operations. Therefore we believe that it provides useful information to our investors in evaluating our operating performance. Our definition of distributable earnings is not based on any definition contained in our amended and restated operating agreement.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:30 [scrapy] ERROR: Error processing {'pagetitle': [u'Quarterly Shareholder Presentation - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?EventId=5207104&c=245595&p=irol-EventDetails'],
 'siteurl': ['ir.mtge.com'],
 'text': [' To hear a recording of the presentation please dial (877) 344-7529 (U.S. domestic) or (412) 317-0088 (international). The access code for both domestic and international callers is 10073284 and the recording is available through November 13th.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:31 [scrapy] ERROR: Error processing {'pagetitle': [u'Investor Relations Home - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-irhome_pf'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.28 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of $(25.1) million or $(0.49) per common share and net book value of $19.93 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for... American Capital Mortgage Investment Corp. Will Report Third Quarter 2015 Results on October 28; Shareholder Call Scheduled for October 29 BETHESDA Md. Oct.13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on October 28 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 29 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free l... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. Sept.17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the third quarter 2015. The dividend is payable on October 15 2015 to preferred shareholders of record as of October 1 2015 with an ex-dividend date of Sep... American Capital Mortgage Investment Corp. Declares Third Quarter Common Stock Dividend of $0.40 Per Share and Announces the Repurchase of 1.2 Million Shares BETHESDA Md. Sept. 17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.40 per share of common stock for the third quarter 2015. The dividend is payable on October 27 2015 to common shareholders of record as of September 30 2015 with an ex-dividend date of September 28 2015. The Company also announced today that in the third quarter of 2015 it made ope... American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of $(41.1) million or $(0.80) per common share and net book value of $20.70 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarte...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:31 [scrapy] ERROR: Error processing {'pagetitle': [u'Preferred Stock - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-preferred'],
 'siteurl': ['ir.mtge.com'],
 'text': [' (1) The Company may redeem series of preferred stock on or after the first call date in whole or in part at its option at the liquidation preference plus any accumulated and unpaid dividends.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:32 [scrapy] ERROR: Error processing {'pagetitle': [u'Investor Relations Home - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&g=true&p=irol-irhome'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.28 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of $(25.1) million or $(0.49) per common share and net book value of $19.93 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for... American Capital Mortgage Investment Corp. Will Report Third Quarter 2015 Results on October 28; Shareholder Call Scheduled for October 29 BETHESDA Md. Oct.13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on October 28 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 29 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free l... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. Sept.17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the third quarter 2015. The dividend is payable on October 15 2015 to preferred shareholders of record as of October 1 2015 with an ex-dividend date of Sep... American Capital Mortgage Investment Corp. Declares Third Quarter Common Stock Dividend of $0.40 Per Share and Announces the Repurchase of 1.2 Million Shares BETHESDA Md. Sept. 17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.40 per share of common stock for the third quarter 2015. The dividend is payable on October 27 2015 to common shareholders of record as of September 30 2015 with an ex-dividend date of September 28 2015. The Company also announced today that in the third quarter of 2015 it made ope... American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of $(41.1) million or $(0.80) per common share and net book value of $20.70 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarte...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:32 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214158&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC John R. Erickson is our Executive Vice President and Chief Financial Officer and Executive Vice President and Treasurer of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Erickson is also a Director Executive Vice President and Chief Financial Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President and Treasurer of its manager American Capital AGNC Management LLC. In addition he is Executive Vice President and Treasurer of American Capital Asset Management LLC (f/k/a American Capital LLC) the asset fund management portfolio company of American Capital Ltd. (NASDAQ: ACAS). Mr. Erickson has also served as President Structured Finance of American Capital Ltd. since 2008 and as its Chief Financial Officer since 1998. From 1991 to 1998 Mr. Erickson was the Chief Financial Officer of Storage USA Inc. a REIT formerly traded on the New York Stock Exchange (NYSE: SUS).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:32 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214157&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Mr. Dobbs has been a self-employed business consultant and business speaker since the end of 2010. Prior to that he was a Senior Operating Executive at Welsh Carson Anderson & Stowe or Welsh Carson a private equity firm. At Welsh Carson Mr. Dobbs was responsible for portfolio company operational oversight business acquisitions and equity opportunity development. From February 2005 to October 2008 he was the Chief Executive Officer of US Investigations Services Inc. and its subsidiaries or USIS. USIS provides business intelligence and risk management solutions security and related services and expert staffing solutions for businesses and federal agencies. From April 2003 to February 2005 Mr. Dobbs was President and Chief Executive Officer of Philips Medical Systems North America a manufacturer of systems for imaging radiation oncology and patient monitoring as well as information management and resuscitation products. Prior to April 2003 Mr. Dobbs spent 27 years with General Electric Company where he held various senior level positions including President and Chief Executive Officer of GE Capital IT Solutions. Mr. Dobbs also serves on the board of directors of American Capital Agency Corp. (NASDAQ: AGNC) and the board of directors of various private equity companies. He previously served on the board of directos of Savvis Inc. (NASDAQ: SVVS) from November 2010 to August 2011.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:32 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214156&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Dr. Davis is an Associate Professor in the Department of Real Estate and Urban Land Economics at the University of Wisconsin-Madison School of Business. He has worked in the department since September 2006. He is currently on the Academic Advisory Council of the Federal Reserve Bank of Chicago and served in 2007 as a Research Associate at the Federal Reserve Bank of Cleveland. From July 2002 to August 2006 Dr. Davis was an economist at the Federal Reserve Board working in the Flow of Funds Section. From October 2001 to July 2002 he was Director of Yield Optimization at Return Buy Inc. and from August 1998 to October 2001 Dr. Davis was an economist at the Macroeconomics and Quantitative Studies Section of the Federal Reserve Board. Dr. Davis is widely published on issues related to the U.S. housing markets and a frequent lecturer. He holds a Ph.D. in Economics from the University of Pennsylvania. Dr. Davis also serves on the board of directors of American Capital Agency Corp. (NASDAQ: AGNC).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:35 [scrapy] ERROR: Error processing {'pagetitle': [u'Arlington Value Management / Capital \u203a Log In'],
 'pageurl': ['http://arlingtonvaluemanagement.com/wordpress/wp-login.php'],
 'siteurl': ['arlingtonvaluemanagement.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:35 [scrapy] ERROR: Error processing {'pagetitle': [u'Contact Arlington Value Management / Capital'],
 'pageurl': ['http://arlingtonvaluemanagement.com/contact/'],
 'siteurl': ['arlingtonvaluemanagement.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:35 [scrapy] INFO: Crawled 1977 pages (at 71 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:56:36 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214155&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Mr. Couch is Counsel to Bradley Arant Boult Cummings LLP a law firm based in Birmingham Alabama. Mr. Couch is also Chairman of ARK Real Estate Strategies LLC. ARK helps banks and financial institutions evaluate manage and market foreclosed residential real estate. ARK is also the manager of the ARK Real Estate Opportunity Fund I LLC an investment fund focused on distressed residential real estate. Mr. Couch is a member of the Board of Directors of Prospect Holding Company LLC the parent company of Prospect Mortgage of Sherman Oaks California. In December 2011 Mr. Couch was also appointed to the Bipartisan Policy Center Housing Commision. From June 2007 to November 2008 Mr. Couch served as General Counsel of the United States Department of Housing and Urban Development or HUD. From December 2006 until June 2007 Mr. Couch served as Acting General Counsel of HUD. Mr. Couch began his service at HUD as President of Ginnie Mae from June 2006 until June 2007. Prior to his government service Mr. Couch served as President and Chief Executive Officer of New South Federal Savings Bank. He holds a Juris Doctor degree from Washington and Lee University. Mr. Couch also serves on the board of directors of American Capital Agency Corp. (NASDAQ: AGNC).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:45 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Third Quarter 2010 Financial Results'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=10325615&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE: FIG) today reported its third quarter 2010 results. For the quarter ended September 30 2010 our GAAP net loss was $272 million compared to a loss of $190 million for third quarter 2009. Our GAAP net loss attributable to Class A Shareholders was $95 million or $0.62 loss per diluted share as compared to a loss of $59 million or $0.43 loss per diluted share for the third quarter 2009. Excluding principals agreement compensation third quarter GAAP net loss was $32 million as compared to net income of $50 million for third quarter 2009. For the third quarter fund management distributable earnings was $71 million compared to $51 million in the third quarter of 2009. Pre-tax DE for the third quarter was $78 million or $0.15 per dividend paying share versus $57 million for the third quarter of 2009 or $0.11 per dividend paying share. Our segment distributable earnings increased period-over-period while our GAAP results decreased. This is primarily a result of the way we record incentive income from PE style funds as well as from hedge funds in interim periods. In determining distributable earnings we generally recognize PE style incentive income when gains are realized and hedge fund incentive income based on current returns and we recognize our employees share of this income as compensation expense at the same time. In contrast GAAP requires that we recognize the compensation when incurred but we must defer the recognition of the revenue until all contingencies primarily minimum returns over the lives of the PE style funds and annual performance requirements of the hedge funds are resolved regardless of the probability of such returns being met. As a result when we have significant PE style realizations or positive returns in our hedge funds as we had in the first nine months of 2010 and which we regard as a positive event the related incentive income impact improves our segment distributable earnings while reducing our GAAP results for the same period. This was a very solid quarter for Fortress driven by strong investment performance stated Daniel Mudd Chief Executive Officer. Our hedge funds produced significant incentive income our credit business remains in a sweet spot attractive private equity opportunities are emerging and assets under management continue to build. Fortresss strengthsstructuring expertise operational depth broad deal flow and experienced managementplay to both the Great Deleveraging and choppy markets. This combination can produce exceptional results for our investors. The table below details Fortresss GAAP Net Income (Loss) and Distributable Earnings for the third quarter and nine months ended 2010 and 2009: For reconciliations of non-GAAP measures please see Exhibit 2 Reconciliation of Fund Management DE to Pre-tax DE and GAAP Net Income (Loss) and Reconciliation of Segment Revenues to GAAP Revenues Exhibit 3 Reconciliation of GAAP Net Income (Loss) Excluding Principals Agreement Compensation to GAAP Net Income (Loss) and Exhibit 4 Reconciliation of Weighted Average Class A Shares Outstanding (Used for Basic EPS) to Weighted Average Dividend Paying Shares and Units Outstanding (Used for DEPS) at the end of this release. Distributable earnings is a supplemental measure of our operating performance that we believe provides a meaningful basis for comparison between present and future periods. The Companys quarterly segment revenues and distributable earnings will fluctuate materially depending upon the performance of our funds and the realization events within our private equity businesses as well as other factors. Accordingly the revenues and profits in any particular quarter should not be expected to be indicative of future results. The following discussion of our results is based on segment reporting as presented in our Quarterly Report on Form 10-Q. Our GAAP statement of operations and balance sheet are presented following this discussion. The following table is a summary presentation of our segment performance with supplemental data provided for informational purposes. Supplemental Data for Third Quarter 2010 and 2009: Supplemental Data for the Nine Month Ended 2010 and 2009: We managed $44.0 billion of assets in private equity funds liquid hedge funds credit funds and our traditional asset management business at September 30 2010. We earn management fees based on the amount of capital we manage incentive income based on the performance of our alternative investment funds and investment income (loss) from our principal investments. In the third quarter of 2010 we generated fund management distributable earnings of $71 million. Including principal investments Fortress generated pre-tax DE of $78 million. For the quarter ended September 30 2010 the private equity segments accounted for approximately 28% of total segment revenues the liquid hedge funds segment accounted for approximately 20% of total segment revenues the credit funds segments accounted for approximately 49% of total segment revenues and the traditional asset management business accounted for approximately 3% of total segment revenues. For the quarter ended September 30 2010 the Companys private equity funds had pre-tax DE of $31 million compared to pre-tax DE of $25 million for the quarter ended September 30 2009. Assets under management for private equity funds was $11.6 billion at September 30 2010 compared to $11.1 billion at September 30 2009. For the quarter ended September 30 2010 the Companys Castles generated pre-tax DE of $4 million compared to $5 million for the quarter ended September 30 2009. Assets under management for the Castles was $3.1 billion at September 30 2010 compared to $3.3 billion at September 30 2009. For the quarter ended September 30 2010 the Companys liquid hedge fund business generated pre-tax DE of $16 million compared to $6 million for the quarter ended September 30 2009. Assets under management for the liquid hedge funds was $4.3 billion at September 30 2010 compared to $4.5 billion at September 30 2009. The following table shows our Assets Under Management by fund: The following table shows our gross and net returns by fund:9 For the quarter ended September 30 2010 the Companys credit hedge fund business generated pre-tax DE of $18 million compared to $6 million for the quarter ended September 30 2009. Assets under management for the credit hedge funds was $8.6 billion at September 30 2010 compared to $9.6 billion at September 30 2009. The following table shows our Assets Under Management by fund: The following table shows our gross and net returns by fund:13 For the quarter ended September 30 2010 the Companys credit private equity fund business generated pre-tax DE of $7 million as compared to $9 million for the quarter ended September 30 2009. Assets under management for the credit private equity funds was $4.1 billion at September 30 2010 compared to $3.4 billion at September 30 2009. As of September 30 2010 the credit private equity funds have approximately $3.0 billion of uncalled or recallable committed capital that will become assets under management when invested. At September 30 2010 we had $744 million of segment assets (excluding cash and cash equivalents) in our principal investments segment compared to $805 million (excluding cash and cash equivalents) at September 30 2009. Segment asset totals do not include net unrealized gains of $172 million primarily on private equity type investments which would be recognized as segment investment income when realized. Our principal investments segment generated a net gain of $7 million for the three months ended September 30 2010 due primarily to gains of $10 million from our balance sheet investments offset by $3 million of interest expense. We had $113 million of unfunded commitments to our principal investments as of September 30 2010 of which we estimate that approximately $29 million will never be funded based on the funds operating agreements. Segment expenses were $120 million in the third quarter of 2010 up from $67 million for the third quarter of 2009. Segment expenses for the third quarter of 2010 included $32 million of profit sharing compensation which is a function of revenues earned from our various funds and managed accounts. The Company had $296 million of share-based compensation expense (primarily relating to expense recorded in connection with the principals agreement the issuance of restricted stock units to Fortress employees and the issuance of restricted partnership units) for the quarter ended September 30 2010 which contributed to our reporting a GAAP net loss. Share-based compensation expense is not included in segment expenses or in the calculation of distributable earnings. In October 2010 we executed a new credit agreement and repaid the previous credit agreement in full. The new credit agreement includes a $280 million term loan and a $60 million revolving credit facility. The new term loan expires in October 2015 the new revolver expires in October 2013 and the credit agreement bears an interest rate of LIBOR plus 400 basis points (with a minimum LIBOR rate of 1.75%). As of October 31 2010 we have $280 million of debt outstanding and have capacity available of $50 million under our revolving credit facility. Each quarter we evaluate whether to pay quarterly dividends on our Class A shares. The amount of any dividends will be determined by our board of directors. However no assurance can be given that any dividends whether quarterly or otherwise will or can be paid. In determining the amount of any dividends our board will take into account various factors including our financial performance on both an actual and projected basis earnings liquidity and the operating performance of our segments as assessed by management. Fortress discloses certain non-GAAP financial information which management believes provides a meaningful basis for comparison among present and future periods. The following are non-GAAP measures used in the accompanying financial information: We urge you to read the reconciliation of such data to the related GAAP measures appearing in the exhibits to this release. Management will host a conference call today Friday November 5 2010 at 8:30 a.m. (Eastern Time). A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. All interested parties are welcome to participate on the live call. The conference call may be accessed by dialing 1-877-252-8576 (from within the U.S.) or 1-706-679-1521 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Third Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Friday November 12 2010 by dialing 1-800-642-1687 (from within the U.S.) or 1-706-645-9291 (from outside of the U.S.); please reference access code 18392742. Fortress is a leading global investment manager with approximately $44.0 billion in assets under management as of September 30 2010. Fortress offers alternative and traditional investment products and was founded in 1998. For more information regarding Fortress Investment Group LLC or to be added to our e-mail distribution list please visit www.fortress.com. Cautionary Note Regarding Forward-Looking Statements Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding our sources of management fees incentive income and investment income (loss) estimated fund performance the amount and source of expected capital commitments and amount of redemptions. These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially from these forward-looking statements and any such differences could cause our actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the Companys Quarterly Report on Form 10-Q which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. Distributable earnings is our supplemental measure of operating performance. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income and it is not necessarily indicative of liquidity or cash available to fund our operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 10 to our financial statements included in our Quarterly Report on Form 10-Q for the quarter ended September 30 2010. Growing distributable earnings is a key component to our business strategy and distributable earnings is the supplemental measure used by our management to evaluate the economic profitability of each of our businesses and our total operations. Therefore we believe that it provides useful information to our investors in evaluating our operating performance. Our definition of distributable earnings is not based on any definition contained in our amended and restated operating agreement.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Second Quarter 2010 Financial Results'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=9924551&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE: FIG) today reported its second quarter 2010 results. Subsequent Events in the Third Quarter For the quarter ended June 30 2010 our GAAP net loss was $251 million compared to a loss of $171 million for second quarter 2009. Our GAAP net loss attributable to Class A Shareholders was $92 million or $0.57 loss per diluted share as compared to a loss of $45 million or $0.41 loss per diluted share for the second quarter 2009. Excluding principals agreement compensation second quarter GAAP net loss was $14 million as compared to net income of $66 million for second quarter 2009. For the second quarter fund management distributable earnings was $73 million compared to $53 million in the second quarter of 2009. Pre-tax DE for the second quarter was $73 million or $0.14 per dividend paying share versus $59 million for the second quarter of 2009 or $0.12 per dividend paying share. We delivered solid results in a quarter marked by extremely challenging market dynamics with strong momentum in capital raising stable management fees and continued recognition of incentive income stated Daniel H. Mudd Chief Executive Officer. Equally important we continued to grow and diversify our business while capitalizing on historically attractive opportunities to deploy capital on our partners behalf. Weve been opportunistic in a market that continues to align with the core strengths of Fortress. The table below details Fortresss GAAP Net Income (Loss) and Distributable Earnings for the second quarter and first half of 2010 and 2009: For reconciliations of non-GAAP measures please see Exhibit 2 Reconciliation of Fund Management DE to Pre-tax DE and GAAP Net Income (Loss) and Reconciliation of Segment Revenues to GAAP Revenues Exhibit 3 Reconciliation of GAAP Net Income (Loss) Excluding Principals Agreement Compensation to GAAP Net Income (Loss) and Exhibit 4 Reconciliation of Weighted Average Class A Shares Outstanding (Used for Basic EPS) to Weighted Average Dividend Paying Shares and Units Outstanding (Used for DEPS) at the end of this release. Distributable earnings is a supplemental measure of our operating performance that we believe provides a meaningful basis for comparison between present and future periods. The Companys quarterly segment revenues and distributable earnings will fluctuate materially depending upon the performance of our funds and the realization events within our private equity businesses as well as other factors. Accordingly the revenues and profits in any particular quarter should not be expected to be indicative of future results. The following discussion of our results is based on segment reporting as presented in our Quarterly Report on Form 10-Q. Our GAAP statement of operations and balance sheet are presented following this discussion. The following table is a summary presentation of our segment performance with supplemental data provided for informational purposes. 1 Represents distributions from (i) assets held by redeeming capital accounts in our Drawbridge Special Opportunities Funds and (ii) the Value Recovery Funds. 2 Includes $3.1 billion of capital under management due to Fortresss take over of management of the D.B. Zwirn funds and related investment vehicles. 3 Represents distributions from (i) assets held by redeeming capital accounts in our Drawbridge Special Opportunities Funds and (ii) the Value Recovery Funds. 4 Includes $3.1 billion of capital under management due to Fortresss take over of management of the D.B. Zwirn funds and related investment vehicles. We managed $41.7 billion of assets in private equity funds liquid hedge funds credit funds and our traditional asset management business at June 30 2010. We earn management fees based on the amount of capital we manage incentive income based on the performance of our alternative investment funds and investment income (loss) from our principal investments. In the second quarter of 2010 we generated fund management distributable earnings of $73 million. Including principal investments Fortress generated pre-tax DE of $73 million. For the quarter ended June 30 2010 the private equity segments accounted for approximately 26% of total segment revenues the liquid hedge funds segment accounted for approximately 11% of total segment revenues the credit funds segments accounted for approximately 61% of total segment revenues and the traditional asset management business accounted for approximately 2% of total segment revenues. For the quarter ended June 30 2010 the Companys private equity funds had pre-tax DE of $25 million compared to pre-tax DE of $28 million for the quarter ended June 30 2009. Assets under management for private equity funds was $11.5 billion at June 30 2010 compared to $10.6 billion at June 30 2009. For the quarter ended June 30 2010 the Companys Castles generated pre-tax DE of $5 million compared to $7 million for the quarter ended June 30 2009. Assets under management for the Castles was $2.9 billion at June 30 2010 compared to $3.2 billion at June 30 2009. For the quarter ended June 30 2010 the Companys liquid hedge fund business generated pre-tax DE of $1 million compared to $7 million for the quarter ended June 30 2009. Assets under management for the liquid hedge funds was $4.2 billion at June 30 2010 compared to $4.6 billion at June 30 2009. The following table shows our Assets Under Management by fund: 5 Combined AUM for Fortress Macro Onshore Fund LP Fortress Macro Offshore Fund L.P. Fortress Macro Fund Ltd Fortress Macro managed accounts Drawbridge Global Macro Fund L.P. Drawbridge Global Macro Intermediate Fund L.P. Drawbridge Global Macro Alpha Intermediate Fund L.P. DBGM Offshore Ltd DBGM Onshore LP DBGM Alpha V Ltd and Drawbridge Global Macro managed accounts. 6 Combined AUM for Fortress Commodities Fund L.P. Fortress Commodities Fund MA1 Ltd and Fortress Commodities managed accounts. The following table shows our gross and net returns by fund:7 7 The performance data contained herein reflects returns for a "new issue eligible" single investor class as of the close of business on the last day of the relevant period. Gross returns reflect performance data prior to management fees borne by the Fund and incentive allocations while net returns reflect performance data after taking into account management fees borne by the Fund and incentive allocations. For the quarter ended June 30 2010 the Companys credit hedge fund business generated pre-tax DE of $18 million compared to $7 million for the quarter ended June 30 2009. Assets under management for the credit hedge funds was $8.7 billion at June 30 2010 compared to $9.7 billion at June 30 2009. The following table shows our Assets Under Management by fund: 8 Combined AUM for Drawbridge Special Opportunities Fund Ltd Drawbridge Special Opportunities Fund LP Drawbridge Special Opportunities Fund managed accounts and Worden Fund LP. 10 Fortress will receive management fees from these funds equal to 1% of cash receipts and up to 1% per annum on certain managed assets subject to collectability and may receive limited incentive income if aggregate realizations exceed an agreed threshold. The following table shows our gross and net returns by fund:11 11 The performance data contained herein reflects returns for a "new issue eligible" single investor class as of the close of business on the last day of the relevant period. Gross returns reflect performance data prior to management fees borne by the Fund and incentive allocations while net returns reflect performance data after taking into account management fees borne by the Fund and incentive allocations. Specific performance may vary based on among other things whether fund investors are invested in one or more special investments. 12 The returns for the Drawbridge Special Opportunities Funds reflect the performance of each fund excluding the performance of the redeeming capital accounts which relate to December 31 2008 and December 31 2009 redemptions. For the quarter ended June 30 2010 the Companys credit private equity fund business generated pre-tax DE of $29 million as compared to $4 million for the quarter ended June 30 2009. Assets under management for the credit private equity funds was $2.9 billion at June 30 2010 compared to $2.8 billion at June 30 2009. Currently the credit private equity funds have approximately $4.1 billion of uncalled or recallable committed capital that will become assets under management when invested. At June 30 2010 we had $0.7 billion of assets (excluding cash and cash equivalents) in our principal investments segment compared to $0.8 billion (excluding cash and cash equivalents) at June 30 2009. During the second quarter of 2010 we increased commitments to our principal investments by $20 million and funded $6 million of our commitments. We had $130 million of unfunded commitments to our principal investments as of June 30 2010. Our principal investments segment pre-tax DE was unchanged for the three months ended June 30 2010 due primarily to gains and distributions of $4 million from our balance sheet investments offset by $4 million of interest expense. Segment expenses were $100 million in the second quarter of 2010 up from $64 million for the second quarter of 2009. Segment expenses for the second quarter of 2010 included $30 million of profit sharing compensation which is a function of revenues received from our various funds. The Company had $286 million of share-based compensation expense (primarily relating to expense recorded in connection with the principals agreement the issuance of restricted stock units to Fortress employees and the issuance of restricted partnership units) for the quarter ended June 30 2010 which contributed to our reporting a GAAP net loss. Share-based compensation expense is not included in segment expenses or in the calculation of distributable earnings. During the second quarter we paid down the credit facility by $14 million. As of June 30 2010 we have $356 million of debt outstanding and have capacity available of approximately $60 million under our revolving credit facility. Each quarter we evaluate whether to pay quarterly dividends on our Class A shares. The amount of any dividends will be determined by our board of directors. However no assurance can be given that any dividends whether quarterly or otherwise will or can be paid. In determining the amount of any dividends our board will take into account various factors including our financial performance on both an actual and projected basis earnings liquidity and the operating performance of our segments as assessed by management. Fortress discloses certain non-GAAP financial information which management believes provides a meaningful basis for comparison among present and future periods. The following are non-GAAP measures used in the accompanying financial information: We urge you to read the reconciliation of such data to the related GAAP measures appearing in the exhibits to this release. Management will host a conference call today Thursday August 5 2010 at 8:00 a.m. (Eastern Time). A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. All interested parties are welcome to participate on the live call. The conference call may be accessed by dialing 1-877-252-8576 (from within the U.S.) or 1-706-679-1521 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Second Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available until 11:59 P.M. Eastern Time on Thursday August 12 2010 by dialing 1-800-642-1687 (from within the U.S.) or 1-706-645-9291 (from outside of the U.S.); please reference access code 89300732. Fortress is a leading global investment manager with approximately $41.7 billion in assets under management as of June 30 2010. Fortress offers alternative and traditional investment products and was founded in 1998. For more information regarding Fortress Investment Group LLC or to be added to our e-mail distribution list please visit www.fortress.com. Cautionary Note Regarding Forward-Looking Statements Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding our sources of management fees incentive income and investment income (loss) estimated fund performance the amount and source of expected capital commitments and amount of redemptions. These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements and any such differences could cause our actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysQuarterly Report on Form 10-Q which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. 13 Combined AUM for Fortress Macro Onshore Fund LP Fortress Macro Offshore Fund L.P. Fortress Macro Fund Ltd and Fortress Macro managed accounts. 14 Combined AUM for Drawbridge Global Macro Fund L.P. Drawbridge Global Macro Intermediate Fund L.P. Drawbridge Global Macro Alpha Intermediate Fund L.P. DBGM Offshore Ltd DBGM Onshore LP DBGM Alpha V Ltd and Drawbridge Global Macro managed accounts. 15 Combined AUM for Fortress Commodities Fund L.P. Fortress Commodities Fund MA1 Ltd and Fortress Commodities managed accounts. 16 The performance data contained herein reflects returns for a "new issue eligible" single investor class as of the close of business on the last day of the relevant period. Gross returns reflect performance data prior to management fees borne by the Fund and incentive allocations while net returns reflect performance data after taking into account management fees borne by the Fund and incentive allocations. 17 Combined AUM for Drawbridge Special Opportunities Fund Ltd Drawbridge Special Opportunities Fund LP Drawbridge Special Opportunities Fund managed accounts and Worden Fund LP. 19 Fortress will receive management fees from these funds equal to 1% of cash receipts and up to 1% per annum on certain managed assets subject to collectability and may receive limited incentive income if aggregate realizations exceed an agreed threshold. 20 The performance data contained herein reflects returns for a "new issue eligible" single investor class as of the close of business on the last day of the relevant period. Gross returns reflect performance data prior to management fees borne by the Fund and incentive allocations while net returns reflect performance data after taking into account management fees borne by the Fund and incentive allocations. Specific performance may vary based on among other things whether fund investors are invested in one or more special investments. 21 The returns for the Drawbridge Special Opportunities Funds reflect the performance of each fund excluding the performance of the redeeming capital accounts which relate to December 31 2008 and December 31 2009 redemptions. Distributable earnings is our supplemental measure of operating performance. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income and it is not necessarily indicative of liquidity or cash available to fund our operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 10 to our financial statements included in our Quarterly Report on Form 10-Q for the quarter ended June 30 2010. Growing distributable earnings is a key component to our business strategy and distributable earnings is the supplemental measure used by our management to evaluate the economic profitability of each of our businesses and our total operations. Therefore we believe that it provides useful information to our investors in evaluating our operating performance. Our definition of distributable earnings is not based on any definition contained in our amended and restated operating agreement.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214154&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Malon Wilkus is our Chair and Chief Executive Officer and the Chief Executive Officer of American Capital MTGE Management LLC our Manager and its parent company American Capital Mortgage Management LLC. Mr. Wilkus is also the Chair and Chief Executive Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Chief Executive Officer of its manager American Capital AGNC Management LLC. In addition Mr. Wilkus is the founder of American Capital Ltd. (NASDAQ: ACAS) the indirect majority owner of our Manager and has served as its Chief Executive Officer and Chairman of the Board of Directors since 1986 except for the period from 1997 to 1998 during which he served as Chief Executive Officer and Vice Chairman of the Board of Directors. He also served as President of American Capital Ltd. from 2001 to 2008 and from 1986 to 1999. Mr. Wilkus has also been the Chairman of European Capital Limited a European private equity and mezzanine fund since its formation in 2005. Additionally Mr. Wilkus is the Chief Executive Officer and President of American Capital LLC which is the asset fund management portfolio company of American Capital Ltd. He has also served on the board of directors of over a dozen middle-market companies in various industries.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:54 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214164&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Aaron Pas is our Senior Vice President Non-Agency Portfolio Management. Mr. Pas is also a Senior Vice President of our Manager and American Capital Mortgage Management LLC. From 2003-2011 Mr. Pas worked at Freddie Mac where he most recently was the Director of Non-Agency Portfolio Management where he was primarily responsible for managing the firms non-agency residential securities portfolio. Mr. Pas holds a Bachelor of Science degree in Business from Washington University in St. Louis. Mr. Pas is a CFA charterholder.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214161&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': ['Christopher Kuehl has served as our Senior Vice President Agency Portfolio Investments since March 2012 and as Senior Vice President of American Capital MTGE Management LLC our Manager since April 2011. He has also served as Senior Vice President of American Capital Mortgage Management LLC the parent company of our Manager since August 2010. Mr. Kuehl is also a Senior Vice President Agency Portfolio Investments of American Capital Agency Corp. (NASDAQ: AGNC) and Senior Vice President of its manager American Capital AGNC Management LLC. Prior to joining American Capital Mortgage Management LLC Mr. Kuehl served as Vice President of Mortgage Investments & Structuring of Freddie Mac. In this capacity Mr. Kuehl was responsible for directing Freddie Macs purchases sales and structuring activities and structuring activities for all MBS products including fixed-rate mortgages ARMs and CMOs. Prior to joining Freddie Mac in 2000 Mr. Kuehl was a Portfolio Manager with TeleBanc/Etrade Bank.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214152&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Peter J. Federico is our Senior Vice President and Chief Risk Officer and the Senior Vice President and Chief Risk Officer of our Manager. He is also the Senior Vice President and Chief Risk Officer of the parent company of our Manager American Capital Mortgage Management LLC. Mr. Federico is also the Senior Vice President and Chief Risk Officer of American Capital Agency Corp. (NASDAQ: AGNC) and of its manager American Capital AGNC Management LLC. He is primarily responsible for overseeing risk management activities for us and other funds managed by American Capital Mortgage Management LLCs subsidiaries. Mr. Federico joined the parent company of our Manager in May 2011. Prior to that Mr. Federico served as Executive Vice President and Treasurer of Freddie Mac from October 2010 through May 2011 where he was primarily responsible for managing the companys investment activities for its retained portfolio and developing implementing and managing risk mitigation strategies. He was also responsible for managing Freddie Macs $1.2 trillion interest rate derivative portfolio and short and long-term debt issuance programs. Mr. Federico also served in a number of other capacities at Freddie Mac including as Senior Vice President Asset & Liability Management since joining the company in 1988.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214150&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Samuel A. Flax is our Executive Vice President and Secretary and Executive Vice President Chief Compliance Officer and Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Flax is also a Director Executive Vice President and Secretary of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President Chief Compliance Officer and Secretary of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice President Chief Compliance Officer and Secretary of American Capital Asset Management LLC (f/k/a American Capital LLC) the asset fund management portfolio company of American Capital Ltd. (NASDAQ: ACAS). Mr. Flax has also served as the Executive Vice President General Counsel Chief Compliance Officer and Secretary of American Capital Ltd. since January 2005. Mr. Flax was a partner in the corporate and securities practice group of the Washington D.C. law firm of Arnold & Porter LLP from 1990 to January 2005. At Arnold & Porter LLP he represented American Capital Ltd. in raising debt and equity capital advised the company on corporate securities and other legal matters and represented the company in many of its investment transactions.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:55 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214149&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC John R. Erickson is our Executive Vice President and Chief Financial Officer and a member of our Board of Directors and Executive Vice President and Treasurer of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Erickson is also the Executive Vice President and Chief Financial Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President and Treasurer of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice President and Treasurer of American Capital Asset Management LLC (f/k/a American Capital LLC) the asset fund management portfolio company of American Capital Ltd. (NASDAQ: ACAS). Mr. Erickson has also served as President Structured Finance of American Capital Ltd. since 2008 and as its Chief Financial Officer since 1998. From 1991 to 1998 Mr. Erickson was the Chief Financial Officer of Storage USA Inc. a REIT formerly traded on the New York Stock Exchange (NYSE: SUS).']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:56 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214148&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Gary Kain is our President and Chief Investment Officer and the President of our Manager with primary oversight for all of our investments. He is also the President and Chief Investment Officer of American Capital Agency Corp. (NASDAQ: AGNC). He was previously a Senior Vice President and Managing Director of American Capital Ltd. (NASDAQ: ACAS) until July 2009 after which time his employment was transferred to American Capital Mortgage Management LLC (f/k/a American Capital Agency Management LLC) the parent company of our Manager. Mr. Kain is also the President of American Capital Mortgage Management LLC and American Capital AGNC Management LLC the sister company of our Manager and the manager of American Capital Agency Corp. Prior to joining American Capital Mr. Kain served as Senior Vice President of Investments and Capital Markets of Federal Home Loan Mortgage Corporation (Freddie Mac) from May 2008 to January 2009. He also served as Senior Vice President of Mortgage Investments & Structuring of Freddie Mac from February 2005 to April 2008 during which time he was responsible for managing all of Freddie Macs mortgage investment activities for the companys $700 billion retained portfolio. From 2001 to 2005 Mr. Kain served as Vice President of Mortgage Portfolio Strategy at Freddie Mac. From 1995 to 2001 he served as head trader in Freddie Macs Securities Sales & Trading Group where he was responsible for managing all trading decisions including REMIC structuring and underwriting hedging all mortgage positions income generation and risk management. Prior to that he served as a senior trader responsible for managing the adjustable-rate mortgage and REMIC sectors.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:56 [scrapy] ERROR: Error processing {'pagetitle': [u'Biography - Our People - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?ID=214147&c=245595&p=irol-govBio'],
 'siteurl': ['ir.mtge.com'],
 'text': [' Malon Wilkus is our Chair and Chief Executive Officer and the Chief Executive Officer of American Capital MTGE Management LLC our Manager and its parent company American Capital Mortgage Management LLC. Mr. Wilkus is also the Chair and Chief Executive Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Chief Executive Officer of its manager American Capital AGNC Management LLC. In addition Mr. Wilkus is the Chief Executive Officer of American Capital Senior Floating Ltd. (NASDAQ: ACSF). Mr. Wilkus is also the founder of American Capital Ltd. (NASDAQ: ACAS) the indirect majority owner of our Manager and has served as its Chief Executive Officer and Chairman of the Board of Directors since 1986 except for the period from 1997 to 1998 during which he served as Chief Executive Officer and Vice Chairman of the Board of Directors. He also served as President of American Capital Ltd. from 2001 to 2008 and from 1986 to 1999. Mr. Wilkus has also been the Chairman of European Capital Limited a European private equity and mezzanine fund since its formation in 2005. Additionally Mr. Wilkus is the Chief Executive Officer and President of American Capital Asset Management LLC (f/k/a American Capital LLC) which is the asset fund management portfolio company of American Capital Ltd. He has also served on the board of directors of over a dozen middle-market companies in various industries.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:57 [scrapy] ERROR: Error processing {'pagetitle': [u' - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/GenPage.aspx?GKP=1073747862&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:57 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/574bd4ee-c544-4e10-ac98-2aca4f954355.pdf> (referer: http://shareholders.fortress.com/file.aspx?FID=31616972&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:56:58 [scrapy] ERROR: Error processing {'pagetitle': [u'Shareholder Information'],
 'pageurl': ['http://shareholders.fortress.com/GenPage.aspx?GKP=1073748403&IID=4147324&print=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:58 [scrapy] ERROR: Error processing {'pagetitle': [u'Our People - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-govmanage_pf'],
 'siteurl': ['ir.mtge.com'],
 'text': ['Aaron Pas is our Senior Vice President Non-Agency Portfolio Management. Mr. Pas is also a Senior Vice President of our Manager and American Capital Mortgage Management LLC. From 2003-2011 Mr. Pas worked at Freddie Mac where he most recently was the Director of Non-Agency Portfolio Management where he was primarily responsible for managing the firms non-agency residential securities portfolio. Mr. Pas holds a Bachelor of Science degree in Business from Washington University in St. Louis.... Christopher Kuehl has served as our Senior Vice President Agency Portfolio Investments since March 2012 and as Senior Vice President of American Capital MTGE Management LLC our Manager since April 2011. He has also served as Senior Vice President of American Capital Mortgage Management LLC the parent company of our Manager since August 2010. Mr. Kuehl is also a Senior Vice President Agency Portfolio Investments of American Capital Agency Corp. (NASDAQ: AGNC) and Senior Vice Presiden... Peter J. Federico is our Senior Vice President and Chief Risk Officer and the Senior Vice President and Chief Risk Officer of our Manager. He is also the Senior Vice President and Chief Risk Officer of the parent company of our Manager American Capital Mortgage Management LLC. Mr. Federico is also the Senior Vice President and Chief Risk Officer of American Capital Agency Corp. (NASDAQ: AGNC) and of its manager American Capital AGNC Management LLC. He is primarily responsible for overseeing ... Peter J. Federico is our Senior Vice President and Chief Risk Officer and the Senior Vice President and Chief Risk Officer of our Manager. He is also the Senior Vice President and Chief Risk Officer of the parent company of our Manager American Capital Mortgage Management LLC. Mr. Federico is also the Senior Vice President and Chief Risk Officer of American Capital Agency Corp. (NASDAQ: AGNC) and of its manager American Capital AGNC Management LLC. He is primarily responsible for overseeing ... Samuel A. Flax is our Executive Vice President and Secretary and Executive Vice President Chief Compliance Officer and Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Flax is also a Director Executive Vice President and Secretary of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President Chief Compliance Officer and Secretary of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice Pre... Samuel A. Flax is our Executive Vice President and Secretary and Executive Vice President Chief Compliance Officer and Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Flax is also a Director Executive Vice President and Secretary of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President Chief Compliance Officer and Secretary of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice Pre... John R. Erickson is our Executive Vice President and Chief Financial Officer and a member of our Board of Directors and Executive Vice President and Treasurer of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Erickson is also the Executive Vice President and Chief Financial Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President and Treasurer of its manager American Capital AGNC Management LLC. In addition he is the Exec... John R. Erickson is our Executive Vice President and Chief Financial Officer and a member of our Board of Directors and Executive Vice President and Treasurer of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Erickson is also the Executive Vice President and Chief Financial Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President and Treasurer of its manager American Capital AGNC Management LLC. In addition he is the Exec... Gary Kain is our President and Chief Investment Officer and the President of our Manager with primary oversight for all of our investments. He is also the President and Chief Investment Officer of American Capital Agency Corp. (NASDAQ: AGNC). He was previously a Senior Vice President and Managing Director of American Capital Ltd. (NASDAQ: ACAS) until July 2009 after which time his employment was transferred to American Capital Mortgage Management LLC (f/k/a American Capital Agency Management... Gary Kain is our President and Chief Investment Officer and the President of our Manager with primary oversight for all of our investments. He is also the President and Chief Investment Officer of American Capital Agency Corp. (NASDAQ: AGNC). He was previously a Senior Vice President and Managing Director of American Capital Ltd. (NASDAQ: ACAS) until July 2009 after which time his employment was transferred to American Capital Mortgage Management LLC (f/k/a American Capital Agency Management... Malon Wilkus is our Chair and Chief Executive Officer and the Chief Executive Officer of American Capital MTGE Management LLC our Manager and its parent company American Capital Mortgage Management LLC. Mr. Wilkus is also the Chair and Chief Executive Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Chief Executive Officer of its manager American Capital AGNC Management LLC. In addition Mr. Wilkus is the Chief Executive Officer of American Capital Senior Floating Ltd. (NA... Malon Wilkus is our Chair and Chief Executive Officer and the Chief Executive Officer of American Capital MTGE Management LLC our Manager and its parent company American Capital Mortgage Management LLC. Mr. Wilkus is also the Chair and Chief Executive Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Chief Executive Officer of its manager American Capital AGNC Management LLC. In addition Mr. Wilkus is the Chief Executive Officer of American Capital Senior Floating Ltd. (NA... Malon Wilkus is our Chair and Chief Executive Officer and the Chief Executive Officer of American Capital MTGE Management LLC our Manager and its parent company American Capital Mortgage Management LLC. Mr. Wilkus is also the Chair and Chief Executive Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Chief Executive Officer of its manager American Capital AGNC Management LLC. In addition Mr. Wilkus is the founder of American Capital Ltd. (NASDAQ: ACAS) the indirect majori... Mr. Couch is Counsel to Bradley Arant Boult Cummings LLP a law firm based in Birmingham Alabama. Mr. Couch is also Chairman of ARK Real Estate Strategies LLC. ARK helps banks and financial institutions evaluate manage and market foreclosed residential real estate. ARK is also the manager of the ARK Real Estate Opportunity Fund I LLC an investment fund focused on distressed residential real estate. Mr. Couch is a member of the Board of Directors of Prospect Holding Company LLC the parent... Dr. Davis is an Associate Professor in the Department of Real Estate and Urban Land Economics at the University of Wisconsin-Madison School of Business. He has worked in the department since September 2006. He is currently on the Academic Advisory Council of the Federal Reserve Bank of Chicago and served in 2007 as a Research Associate at the Federal Reserve Bank of Cleveland. From July 2002 to August 2006 Dr. Davis was an economist at the Federal Reserve Board working in the Flow of Funds Sec... Mr. Dobbs has been a self-employed business consultant and business speaker since the end of 2010. Prior to that he was a Senior Operating Executive at Welsh Carson Anderson & Stowe or Welsh Carson a private equity firm. At Welsh Carson Mr. Dobbs was responsible for portfolio company operational oversight business acquisitions and equity opportunity development. From February 2005 to October 2008 he was the Chief Executive Officer of US Investigations Services Inc. and its subsidiaries... Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC Director Executive Vice President Chief Financial Officer and Assistant Secretary American Capital Mortgage Investment Corp. Executive Vice President and Treasurer American Capital MTGE Management LLC John R. Erickson is our Executive Vice President and Chief Financial Officer and Executive Vice President and Treasurer of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Erickson is also a Director Executive Vice President and Chief Financial Officer of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President and Treasurer of its manager American Capital AGNC Management LLC. In addition he is Executive Vice President and Treasurer o... Samuel A. Flax is our Executive Vice President and Secretary and Executive Vice President Chief Compliance Officer and Secretary of our Manager and of its parent company American Capital Mortgage Management LLC. Mr. Flax is also a Director Executive Vice President and Secretary of American Capital Agency Corp. (NASDAQ: AGNC) and the Executive Vice President Chief Compliance Officer and Secretary of its manager American Capital AGNC Management LLC. In addition he is the Executive Vice Pre... Ms. Larocca retired in 2011 from Royal Bank of Scotland (RBS) where from 1997 until her retirement she was a Managing Director in the firms Mortgage Backed and Asset Backed Finance Group. She is a widely recognized expert in the areas of housing finance and securitization and is a member of the board of the Housing Preservation Foundation as well as having previously served two terms on the board of the American Securitization Trade Association. Prior to joining RBS Ms. Larocca was a Senior ... Larry Harvey has served as Chief Financial Officer of Playa Hotels & Resorts B.V. since April 2015. From 2007 to 2013 Mr. Harvey served as Executive Vice President and Chief Financial Officer of Host Hotels & Resorts Inc. (NYSE: HST) (Host) and served as its Treasurer from 2007 to 2010. From 2006 to 2007 Mr. Harvey served as Senior Vice President Chief Accounting Officer of Host and from 2003 to 2006 he served as Hosts Senior Vice President and Corporate Controller. Prior to rejoining Ho... Dr. Puryear is Professor Emeritus of Management and Entrepreneurship at Baruch College of the City University of New York where he was the initial recipient of the Lawrence N. Field Professorship in Entrepreneurship. Dr. Puryear is also a management consultant who advises existing and new businesses with high-growth potential. Prior to his appointment at Baruch College Dr. Puryear was on the faculty of the graduate school of business administration at Rutgers University. During leaves of a...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:59 [scrapy] ERROR: Error processing {'pagetitle': [u'Investor Relations Home - Investor Relations - American Capital Mortgage Investment Corp.'],
 'pageurl': ['http://ir.mtge.com/phoenix.zhtml?c=245595&p=irol-irhome'],
 'siteurl': ['ir.mtge.com'],
 'text': ['American Capital Mortgage Investment Corp. Reports $(0.49) Net Loss Per Common Share For The Third Quarter And $19.93 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. Oct.28 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported a net loss for the quarter ended September30 2015 of $(25.1) million or $(0.49) per common share and net book value of $19.93 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (1.8)% for... American Capital Mortgage Investment Corp. Will Report Third Quarter 2015 Results on October 28; Shareholder Call Scheduled for October 29 BETHESDA Md. Oct.13 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today it will report third quarter 2015 earnings after market close on October 28 2015. MTGE invites shareholders prospective shareholders and analysts to attend the MTGE shareholder call on October 29 2015 at 11:00 am ET. Callers who do not plan on asking a question and have access to the internet are encouraged to utilize the free l... American Capital Mortgage Investment Corp. Declares Third Quarter Dividend on Its Series A Preferred Stock BETHESDA Md. Sept.17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend on its 8.125% Series A Cumulative Redeemable Preferred Stock (the "Series A Preferred Stock") (Nasdaq: MTGEP) of $0.5078125 per share for the third quarter 2015. The dividend is payable on October 15 2015 to preferred shareholders of record as of October 1 2015 with an ex-dividend date of Sep... American Capital Mortgage Investment Corp. Declares Third Quarter Common Stock Dividend of $0.40 Per Share and Announces the Repurchase of 1.2 Million Shares BETHESDA Md. Sept. 17 2015 /PRNewswire/ --American Capital Mortgage Investment Corp. (Nasdaq: MTGE) ("MTGE" or the "Company") announced today that its Board of Directors has declared a cash dividend of $0.40 per share of common stock for the third quarter 2015. The dividend is payable on October 27 2015 to common shareholders of record as of September 30 2015 with an ex-dividend date of September 28 2015. The Company also announced today that in the third quarter of 2015 it made ope... American Capital Mortgage Investment Corp. Reports $(0.80) Net Loss Per Common Share For The Second Quarter And $20.70 Net Book Value Per Common Share PDF Version To download financial tables please click here. BETHESDA Md. July29 2015 /PRNewswire/ -- American Capital Mortgage Investment Corp. ("MTGE" or the "Company") (Nasdaq: MTGE) today reported net loss for the quarter ended June30 2015 of $(41.1) million or $(0.80) per common share and net book value of $20.70 per common share. Economic loss for the period defined as dividends and change in net book value per common share was (3.7)% for the quarte...']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:56:59 [scrapy] ERROR: Error processing {'pagetitle': [u'Arlington Value Management / Capital'],
 'pageurl': ['http://arlingtonvalue.com'],
 'siteurl': ['arlingtonvalue.co'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:57:00 [scrapy] ERROR: Error processing {'pagetitle': [u' - FORTRESS INVESTMENT GROUP LLC'],
 'pageurl': ['http://shareholders.fortress.com/GenPage.aspx?GKP=1073747863&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:57:00 [scrapy] ERROR: Error processing {'pagetitle': [u'Arlington Value Management / Capital \u203a Lost Password'],
 'pageurl': ['http://arlingtonvaluemanagement.com/wordpress/wp-login.php?action=lostpassword'],
 'siteurl': ['arlingtonvaluemanagement.com'],
 'text': ['Please enter your username or email address. You will receive a link to create a new password via email.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:57:01 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001197014.PDF?Y=&O=PDF&D=&fid=1001197014&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:19 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Third Quarter 2015 Results and Announces Dividend of $0.08 per Share'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=31644499&IID=4147324'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) (Fortress or the Company) today reported its third quarter 2015 financial results. It has been a very active and productive year for Fortress said Chief Executive Officer Randal Nardone. We closed the third quarter with record assets under management more new commitments to our alternative strategies through September than in any full year since 2007 and embedded value in our funds and on our balance sheet that now represents over 70% of our share price today. These results underscore the strength of our core businesses and potential for significant earnings upside in future periods. Our recently announced agreement to repurchase nearly 57 million Fortress shares represents a very positive use of capital that will provide ongoing economic benefit to shareholders. We remain committed to distributing substantially all of our after-tax DE to shareholders as dividends or in the event additional opportunities arise as buybacks. Fortresss business model is highly diversified and management believes that this positions the Company to capitalize on opportunities for investing capital formation and harvesting profits that can occur at different points in any cycle for our individual businesses. Fortresss business model generates stable and predictable management fees which is a function of the majority of Fortresss alternative AUM residing in long-term investment structures. Fortresss alternative investment businesses also generate variable incentive income based on performance and this incentive income can contribute meaningfully to financial results. Balance sheet investments represent a third component of Fortresss business model and the Company has built substantial value in these investments which are made in Fortress funds alongside the funds limited partners. The table below summarizes Fortresss operating results for the three and nine months ended September 30 2015. The consolidated GAAP statement of operations and balance sheet are presented on pages 13-14 of this press release. Fortress recorded a GAAP net loss of $26 million or $(0.07) per diluted Class A share for the third quarter of 2015 compared to GAAP net income of $17 million or $0.02 per diluted Class A share for the third quarter of 2014. Our diluted earnings per share for all periods presented includes the income tax effects to net income (loss) attributable to Class A shareholders from the assumed conversion of Fortress Operating Group units and fully vested restricted partnership units to Class A shares. The year-over-year decrease in Fortresss third quarter 2015 GAAP net income was primarily driven by a $73 million decrease in other income partially offset by a $21 million increase in revenues and an $11 million decrease in expenses. The decrease in other income was primarily related to a $61 million net decrease in earnings from equity method investees primarily related to investments in our alternative investment funds. The increase in revenues was primarily related to higher incentive income and expense reimbursements from affiliates while the decrease in expenses was primarily related to lower compensation and benefits expense. This section provides information about each of Fortresss businesses: (i) Credit Hedge Funds and Credit PE Funds (ii) Private Equity Funds and Permanent Capital Vehicles (iii) Liquid Hedge Funds and (iv) Logan Circle. Fortress uses DE as the primary metric to manage its businesses and gauge the Companys performance and it uses DE exclusively to report segment results. All DE figures are presented on a pre-tax basis. Consolidated segment results are non-GAAP information and are not presented as a substitute for Fortresss GAAP results. Fortress urges you to read Non-GAAP Information below. Pre-tax DE was $69 million in the third quarter of 2015 up from $55 million in the third quarter of 2014 primarily due to lower profit-sharing expenses and higher net investment income partially offset by lower incentive income. Management fees were $151 million in the third quarter of 2015 up slightly from $149 million in the third quarter of 2014. Management fees for the third quarter of 2014 included approximately $15 million attributable to the former Fortress Asia Macro Funds (FAMF) which transitioned to an autonomous business in which we have a minority interest named Graticule Asset Management Asia L.P. (Graticule) in the first quarter of 2015. Adjusting for this transition Fortresss management fees increased by approximately $17 million year-over-year in the third quarter of 2015. The increase was primarily due to higher management fees from the Permanent Capital Vehicles Credit Hedge Funds and Credit PE Funds partially offset by lower management fees from the Private Equity Funds and Liquid Hedge Funds. Incentive income in the third quarter of 2015 totaled $70 million down from $88 million in the third quarter of 2014 primarily due to lower incentive income from the Credit Hedge Funds and Permanent Capital Vehicles partially offset by higher incentive income from the Credit PE Funds. Earnings from Affiliated Managers totaled $2 million in the third quarter of 2015 up from a loss of $1 million in the second quarter of 2015 related to our interests in Graticule. Additionally Fortress had $938 million in gross undistributed unrecognized incentive income based on investment valuations as of September 30 2015. This includes $913 million from our funds $20 million from options in our permanent capital vehicles and $5 million in common shares Fortress received in connection with the IPO of Fortress Transportation and Infrastructure Investors LLC (NYSE: FTAI). The Companys segment revenues and distributable earnings will fluctuate materially depending upon the performance of its funds and the realization events within its Private Equity businesses as well as other factors. Accordingly the revenues and distributable earnings in any particular period should not be expected to be indicative of future results. As of September 30 2015 AUM totaled $74.3 billion up from $72.0 billion as of June 30 2015. During the quarter Fortress recorded a $3.6 billion positive change in AUM of Affiliated Managers and co-managed funds had a $0.6 billion increase in invested capital raised $0.2 billion of capital that was directly added to AUM and recorded $0.1 billion of net client inflows for Logan Circle. These increases to AUM were partially offset by (i) $0.9 billion of capital distributions to investors (ii) $0.6 billion of market-driven valuation losses (iii) $0.5 billion of Liquid Hedge Fund redemptions and (iv) $0.2 billion of Credit Hedge Fund redemptions and payments to Credit Hedge Fund investors from redeeming capital accounts. As of September 30 2015 the Credit Funds and Private Equity Funds had $8.1 billion and $0.7 billion of uncalled capital respectively that will become AUM if deployed/called. Uncalled capital or dry powder capital committed to the funds but not invested and generating management fees includes $3.0 billion that is only available for follow-on investments management fees and other fund expenses. Notably approximately 82% of alternative AUM was in funds with long-term investment structures as of September 30 2015 which provides for a stable predictable base of management fees. Below is a discussion of third quarter 2015 segment results and business highlights. The Credit business which includes our Credit PE Funds and Credit Hedge Funds generated pre-tax DE of $51 million in the third quarter of 2015 up from $36 million in the third quarter of 2014. The 42% year-over-year increase in DE was primarily driven by higher management fees and investment income partially offset by higher expenses. Credit incentive income totaled $71 million in the quarter up $1 million from the third quarter of 2014 as higher incentive income from the Credit PE Funds were largely offset by lower incentive income from the Credit Hedge Funds. The Credit PE Funds generated pre-tax DE of $38 million in the quarter up significantly from $18 million in the third quarter of 2014 as increased realization activity resulted in $70 million of incentive income in the quarter. For the nine months ended September 30 2015 the Credit PE Funds generated $154 million of incentive income greater than the amount generated in any comparable period since the Credit PE Funds were launched in 2008. Gross unrecognized Credit PE incentive income totaled $818 million at quarter end representing the amount of gross incentive income that would be recorded in DE if the related Credit funds were liquidated on September 30 2015 at their net asset values. The Credit Hedge Funds generated pre-tax DE of $13 million in the quarter compared to $18 million in the third quarter of 2014 primarily due to lower incentive income earned in the quarter. Fortresss flagship credit hedge fund DBSO LP had net returns of 0.3% in the third quarter bringing net returns for the nine months ended September 30 2015 to 4.9%. During the quarter Fortress agreed to become co-manager of the Mount Kellett Funds which have been included as part of the Credit Hedge Fund segment. As of September 30 2015 the Mount Kellett Funds had $3.0 billion of AUM. Post quarter end the Credit PE Funds expect to close their third Japan Real Estate fund FJOF III at its cap of approximately $1.1 billion. At quarter end approximately 14% of the funds capital has already been committed or invested in real estate related investments in Japan. The Private Equity business recorded pre-tax DE of $24 million in the third quarter of 2015 including $16 million for the Private Equity Funds and $8 million for the Permanent Capital Vehicles. Pre-tax DE declined from $25 million in the third quarter of 2014 primarily due to a $17 million decrease in Permanent Capital Vehicle incentive income year-over-year partially offset by a $10 million year-over-year increase in Permanent Capital Vehicle management fees. Private Equity Fund valuations declined 3.9% in the third quarter primarily due to depreciation of Nationstar Mortgage Holdings Inc. (NYSE: NSM) and Springleaf Holdings Inc. (NYSE: LEAF). As of October 28 2015 public company valuations had increased 7.3% in the fourth quarter primarily due to a 10.9% increase in Springleafs stock price. The Liquid Hedge Funds recorded a pre-tax DE loss of $4 million in the third quarter of 2015 flat compared to the third quarter of 2014. The pre-tax DE loss in the quarter was primarily due to an $11 million impairment of certain software and technology-related assets. Earnings from Affiliated Managers totaled $2 million in the third quarter of 2015 compared to a loss of $1 million in the second quarter of 2015. Third quarter 2015 net returns for the Fortress Macro Funds Fortress Convex Asia Funds Fortress Centaurus Global Funds and Fortress Partners Funds were (7.6)% 3.3% (3.1)% and (4.8)% respectively. Net returns year-to-date through October 23 2015 for the Fortress Convex Asia Funds and Fortress Centaurus Global Funds were (1.0)% and (2.3)% respectively.* Post quarter end Fortress announced it will close the Fortress Macro Funds and expects to return most capital to investors by the end of 2015. The Fortress Macro Funds accounted for 2% of Fortresss total AUM at quarter end and had generated a pre-tax DE loss of $8 million year-to-date through September 30 2015. Michael Novogratz a Fortress principal officer and director and Macro Fund CIO is expected to retire at the end of 2015. Excluding $1.8 billion of AUM related to the Fortress Macro Funds the Liquid Hedge Funds had $5.6 billion of AUM at quarter end including $4.5 billion related to Affiliated Managers. Logan Circle our traditional asset management business recorded breakeven pre-tax DE in the quarter as $0.3 million of fund management DE was offset by $0.3 million of net investment losses. Pre-tax DE increased from a pre-tax DE loss of $2 million in the third quarter of 2014 primarily due to higher revenues and lower operating expenses. Logan Circle ended the quarter with $33.4 billion in AUM a slight decline compared to the prior quarter and an 8% increase compared to the third quarter of 2014. AUM declined slightly quarter-over-quarter as $0.1 billion of net inflows were offset by $0.2 billion of market-driven valuation losses. Since inception 15 of 16 Logan Circle fixed income strategies have outperformed their respective benchmarks and seven were ranked in the top quartile of performance for their competitor universe. For the year-to-date period through September 30 2015 8 out of Logan Circles 16 fixed income strategies outperformed their respective benchmarks. As of September 30 2015 Fortress had cash and cash equivalents of $337 million and debt obligations of $75 million. As of September 30 2015 Fortress had $1.2 billion of investments in Fortress funds and options in publicly traded permanent capital vehicles. As of September 30 2015 Fortress had a total of $153 million of outstanding commitments to its funds. In addition the NAV of Fortresss investments in its own funds exceeded its segment cost basis by $506 million at quarter end representing net unrealized gains that have not yet been recognized for segment reporting purposes. In October 2015 Fortress announced that it has agreed to purchase 56.8 million Class A equivalent shares at $4.50 per share or an aggregate amount of $255.7 million. In connection with this purchase it is anticipated that Fortress will pay $100.0 million of cash in November 2015 and issue an $155.7 million promissory note. In addition Fortress expects to enter into a consent and waiver with lenders under its credit agreement for their consent to this transaction. Fortresss Board of Directors declared a third quarter 2015 cash dividend of $0.08 per dividend paying share. The dividend is payable on November 13 2015 to Class A shareholders of record as of the close of business on November 10 2015. The declaration and payment of any dividends are at the sole discretion of the Board of Directors which may decide to change its dividend policy at any time. Please see below for information on the U.S. federal income tax implications of the dividend. DE is a primary metric used by management to measure Fortresss operating performance.Consistent with GAAP DE is the sole measure that management uses to manage and thus report on Fortresss segments namely: Private Equity Permanent Capital Vehicles Credit Hedge Funds Credit PE Funds Liquid Hedge Funds and Logan Circle.DE differs from GAAP net income in a number of material ways. For a detailed description of the calculation of pre-tax DE and fund management DE see Exhibit 3 to this release and note 10 to the financial statements included in the Companys most recent quarterly report on Form 10-Q. Fortress aggregates its segment results to report consolidated segment results as shown in the table under Summary Financial Results and in the Total column of the table under Consolidated Segment Results (Non-GAAP). The consolidated segment results are non-GAAP financial information. Management believes that consolidated segment results provide a meaningful basis for comparison among present and future periods. However consolidated segment results should not be considered a substitute for Fortresss consolidated GAAP results. The exhibits to this release contain reconciliations of the components of Fortresss consolidated segment results to the comparable GAAP measures and Fortress urges you to review these exhibits. Fortress also uses weighted average dividend paying shares and units outstanding (used to calculate pre-tax DE per dividend paying share) and net cash and investments. The exhibits to this release contain reconciliations of these measures to the comparable GAAP measures and Fortress urges you to review these exhibits. Management will host a conference call today Thursday October 29th at 10:00 A.M. Eastern Time. A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Third Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 57023583. Fortress Investment Group LLC (NYSE: FIG) is a leading highly diversified global investment management firm with $74.3 billion in assets under management as of September 30 2015. Fortress applies its deep experience and specialized expertise across a range of investment strategies - private equity credit liquid hedge funds and traditional asset management - on behalf of over 1800 institutional clients and private investors worldwide. For more information regarding Fortress Investment Group LLC or to be added to its e-mail distribution list please visit www.fortress.com. Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding Fortresss sources of management fees incentive income and investment income (loss) estimated fund performance the amount and source of expected capital commitments the closing of the Fortress Macro Funds and repurchase of shares/units from a retiring principal (the transaction).These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements. Such differences or other changes to forward looking statements including with respect to the transaction could cause the Companys actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysQuarterly Report on Form 10-Q which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. This announcement is intended to be a qualified notice as provided in the Internal Revenue Code (the Code) and the Regulations thereunder. For U.S. federal income tax purposes the dividend declared in October 2015 will be treated as a partnership distribution. The per share distribution components are as follows: Distributable earnings is Fortresss supplemental measure of operating performance used by management in analyzing segment and overall results. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income in accordance with GAAP and it is not necessarily indicative of liquidity or cash available to fund the Companys operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 10 to the financial statements included in the Companys Quarterly Report on Form 10-Q for the quarter ended September 30 2015. Growing distributable earnings is a key component to the Companys business strategy and distributable earnings is the supplemental measure used by management to evaluate the economic profitability of each of the Companys businesses and total operations. Therefore Fortress believes that it provides useful information to investors in evaluating its operating performance. Fortresss definition of distributable earnings is not based on any definition contained in its amended and restated operating agreement. Fund management DE is equal to pre-tax distributable earnings excluding our direct investment-related results. Fund management DE is comprised of Pre-tax Distributable Earnings excluding Investment Loss (Income) and Interest Expense. Fund management DE and its components are used by management to analyze and measure the performance of our investment management business on a stand-alone basis. Fortress defines segment operating margin to be equal to fund management DE divided by segment revenues. The Company believes that it is useful to provide investors with the opportunity to review our investment management business using the same metrics. Fund management DE and its components are subject to the same limitations as pre-tax distributable earnings as described above. Dividend paying shares and units represents the number of shares and units outstanding at the end of the period which were entitled to receive dividends or related distributions. The Company believes it is useful for investors in computing the aggregate amount of cash required to make a current per share distribution of a given amount per share. It excludes certain potentially dilutive equity instruments primarily non-dividend paying restricted Class A share units and therefore is limited in its usefulness in computing per share amounts. Accordingly dividend paying shares and units should be considered only as a supplement and not an alternative to GAAP basic and diluted shares outstanding. The Companys calculation of dividend paying shares and units may be different from the calculation used by other companies and therefore comparability may be limited. Net cash and investments represents cash and cash equivalents plus investments less debt outstanding. The Company believes that net cash and investments is a useful supplemental measure because it provides investors with information regarding the Companys net investment assets. Net cash and investments excludes certain assets (investments in options due from affiliates deferred tax asset other assets) and liabilities (due to affiliates accrued compensation and benefits deferred incentive income and other liabilities) and its utility as a measure of financial position is limited. Accordingly net cash and investments should be considered only as a supplement and not an alternative to GAAP book value as a measure of the Companys financial position. The Companys calculation of net cash and investments may be different from the calculation used by other companies and therefore comparability may be limited.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:57:19 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045734.PDF?Y=&O=PDF&D=&fid=1500045734&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045733.PDF?Y=&O=PDF&D=&fid=1500045733&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] INFO: Crawled 1993 pages (at 16 pages/min), scraped 1290 items (at 0 items/min)
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045735.PDF?Y=&O=PDF&D=&fid=1500045735&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045732.PDF?Y=&O=PDF&D=&fid=1500045732&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045737.PDF?Y=&O=PDF&D=&fid=1500045737&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045736.PDF?Y=&O=PDF&D=&fid=1500045736&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045738.PDF?Y=&O=PDF&D=&fid=1500045738&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500045739.PDF?Y=&O=PDF&D=&fid=1500045739&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001171636.PDF?Y=&O=PDF&D=&fid=1001171636&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:20 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/eef09732-14fa-4405-8c19-ee1ff913e9e1.pdf> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073748403&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:21 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001171637.PDF?Y=&O=PDF&D=&fid=1001171637&T=&iid=4147324> (referer: http://shareholders.fortress.com/calendar.aspx?iid=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:21 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/23197133.PDF?Y=&O=PDF&D=&FID=23197133&T=&OSID=9&IID=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:22 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1001201149.PDF?Y=&O=PDF&D=&fid=1001201149&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:23 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Third Quarter 2015 Results and Announces Dividend of $0.08 per Share'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=31644499&IID=4147324&printable=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['Thank you for your interest in IR Solutions. You have reached this page because the URL you have requested is no longer available or you do not have permission to view it. For more information on the most comprehensive IR web site management tool available please click here. If you have any questions e-mail IRSupport@snl.com or call (866) 802-9128 and ask for an IR Solutions sales representative. to return to the main SNL Financial homepage. Please click here to return to the main SNL Financial homepage.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:57:24 [scrapy] ERROR: Spider error processing <GET http://shareholders.fortress.com/Cache/1500068784.PDF?Y=&O=PDF&D=&fid=1500068784&T=&iid=4147324> (referer: http://shareholders.fortress.com/GenPage.aspx?GKP=1073744786&IID=4147324)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 06:57:41 [scrapy] ERROR: Error processing {'pagetitle': [u'Fortress Reports Third Quarter 2015 Results and Announces Dividend of $0.08 per Share'],
 'pageurl': ['http://shareholders.fortress.com/file.aspx?FID=31644499&IID=4147324&Print=1&PDF=1'],
 'siteurl': ['shareholders.fortress.com'],
 'text': ['NEW YORK--(BUSINESS WIRE)-- Fortress Investment Group LLC (NYSE:FIG) (Fortress or the Company) today reported its third quarter 2015 financial results. It has been a very active and productive year for Fortress said Chief Executive Officer Randal Nardone. We closed the third quarter with record assets under management more new commitments to our alternative strategies through September than in any full year since 2007 and embedded value in our funds and on our balance sheet that now represents over 70% of our share price today. These results underscore the strength of our core businesses and potential for significant earnings upside in future periods. Our recently announced agreement to repurchase nearly 57 million Fortress shares represents a very positive use of capital that will provide ongoing economic benefit to shareholders. We remain committed to distributing substantially all of our after-tax DE to shareholders as dividends or in the event additional opportunities arise as buybacks. Fortresss business model is highly diversified and management believes that this positions the Company to capitalize on opportunities for investing capital formation and harvesting profits that can occur at different points in any cycle for our individual businesses. Fortresss business model generates stable and predictable management fees which is a function of the majority of Fortresss alternative AUM residing in long-term investment structures. Fortresss alternative investment businesses also generate variable incentive income based on performance and this incentive income can contribute meaningfully to financial results. Balance sheet investments represent a third component of Fortresss business model and the Company has built substantial value in these investments which are made in Fortress funds alongside the funds limited partners. The table below summarizes Fortresss operating results for the three and nine months ended September 30 2015. The consolidated GAAP statement of operations and balance sheet are presented on pages 13-14 of this press release. Fortress recorded a GAAP net loss of $26 million or $(0.07) per diluted Class A share for the third quarter of 2015 compared to GAAP net income of $17 million or $0.02 per diluted Class A share for the third quarter of 2014. Our diluted earnings per share for all periods presented includes the income tax effects to net income (loss) attributable to Class A shareholders from the assumed conversion of Fortress Operating Group units and fully vested restricted partnership units to Class A shares. The year-over-year decrease in Fortresss third quarter 2015 GAAP net income was primarily driven by a $73 million decrease in other income partially offset by a $21 million increase in revenues and an $11 million decrease in expenses. The decrease in other income was primarily related to a $61 million net decrease in earnings from equity method investees primarily related to investments in our alternative investment funds. The increase in revenues was primarily related to higher incentive income and expense reimbursements from affiliates while the decrease in expenses was primarily related to lower compensation and benefits expense. This section provides information about each of Fortresss businesses: (i) Credit Hedge Funds and Credit PE Funds (ii) Private Equity Funds and Permanent Capital Vehicles (iii) Liquid Hedge Funds and (iv) Logan Circle. Fortress uses DE as the primary metric to manage its businesses and gauge the Companys performance and it uses DE exclusively to report segment results. All DE figures are presented on a pre-tax basis. Consolidated segment results are non-GAAP information and are not presented as a substitute for Fortresss GAAP results. Fortress urges you to read Non-GAAP Information below. Pre-tax DE was $69 million in the third quarter of 2015 up from $55 million in the third quarter of 2014 primarily due to lower profit-sharing expenses and higher net investment income partially offset by lower incentive income. Management fees were $151 million in the third quarter of 2015 up slightly from $149 million in the third quarter of 2014. Management fees for the third quarter of 2014 included approximately $15 million attributable to the former Fortress Asia Macro Funds (FAMF) which transitioned to an autonomous business in which we have a minority interest named Graticule Asset Management Asia L.P. (Graticule) in the first quarter of 2015. Adjusting for this transition Fortresss management fees increased by approximately $17 million year-over-year in the third quarter of 2015. The increase was primarily due to higher management fees from the Permanent Capital Vehicles Credit Hedge Funds and Credit PE Funds partially offset by lower management fees from the Private Equity Funds and Liquid Hedge Funds. Incentive income in the third quarter of 2015 totaled $70 million down from $88 million in the third quarter of 2014 primarily due to lower incentive income from the Credit Hedge Funds and Permanent Capital Vehicles partially offset by higher incentive income from the Credit PE Funds. Earnings from Affiliated Managers totaled $2 million in the third quarter of 2015 up from a loss of $1 million in the second quarter of 2015 related to our interests in Graticule. Additionally Fortress had $938 million in gross undistributed unrecognized incentive income based on investment valuations as of September 30 2015. This includes $913 million from our funds $20 million from options in our permanent capital vehicles and $5 million in common shares Fortress received in connection with the IPO of Fortress Transportation and Infrastructure Investors LLC (NYSE: FTAI). The Companys segment revenues and distributable earnings will fluctuate materially depending upon the performance of its funds and the realization events within its Private Equity businesses as well as other factors. Accordingly the revenues and distributable earnings in any particular period should not be expected to be indicative of future results. As of September 30 2015 AUM totaled $74.3 billion up from $72.0 billion as of June 30 2015. During the quarter Fortress recorded a $3.6 billion positive change in AUM of Affiliated Managers and co-managed funds had a $0.6 billion increase in invested capital raised $0.2 billion of capital that was directly added to AUM and recorded $0.1 billion of net client inflows for Logan Circle. These increases to AUM were partially offset by (i) $0.9 billion of capital distributions to investors (ii) $0.6 billion of market-driven valuation losses (iii) $0.5 billion of Liquid Hedge Fund redemptions and (iv) $0.2 billion of Credit Hedge Fund redemptions and payments to Credit Hedge Fund investors from redeeming capital accounts. As of September 30 2015 the Credit Funds and Private Equity Funds had $8.1 billion and $0.7 billion of uncalled capital respectively that will become AUM if deployed/called. Uncalled capital or dry powder capital committed to the funds but not invested and generating management fees includes $3.0 billion that is only available for follow-on investments management fees and other fund expenses. Notably approximately 82% of alternative AUM was in funds with long-term investment structures as of September 30 2015 which provides for a stable predictable base of management fees. Below is a discussion of third quarter 2015 segment results and business highlights. The Credit business which includes our Credit PE Funds and Credit Hedge Funds generated pre-tax DE of $51 million in the third quarter of 2015 up from $36 million in the third quarter of 2014. The 42% year-over-year increase in DE was primarily driven by higher management fees and investment income partially offset by higher expenses. Credit incentive income totaled $71 million in the quarter up $1 million from the third quarter of 2014 as higher incentive income from the Credit PE Funds were largely offset by lower incentive income from the Credit Hedge Funds. The Credit PE Funds generated pre-tax DE of $38 million in the quarter up significantly from $18 million in the third quarter of 2014 as increased realization activity resulted in $70 million of incentive income in the quarter. For the nine months ended September 30 2015 the Credit PE Funds generated $154 million of incentive income greater than the amount generated in any comparable period since the Credit PE Funds were launched in 2008. Gross unrecognized Credit PE incentive income totaled $818 million at quarter end representing the amount of gross incentive income that would be recorded in DE if the related Credit funds were liquidated on September 30 2015 at their net asset values. The Credit Hedge Funds generated pre-tax DE of $13 million in the quarter compared to $18 million in the third quarter of 2014 primarily due to lower incentive income earned in the quarter. Fortresss flagship credit hedge fund DBSO LP had net returns of 0.3% in the third quarter bringing net returns for the nine months ended September 30 2015 to 4.9%. During the quarter Fortress agreed to become co-manager of the Mount Kellett Funds which have been included as part of the Credit Hedge Fund segment. As of September 30 2015 the Mount Kellett Funds had $3.0 billion of AUM. Post quarter end the Credit PE Funds expect to close their third Japan Real Estate fund FJOF III at its cap of approximately $1.1 billion. At quarter end approximately 14% of the funds capital has already been committed or invested in real estate related investments in Japan. The Private Equity business recorded pre-tax DE of $24 million in the third quarter of 2015 including $16 million for the Private Equity Funds and $8 million for the Permanent Capital Vehicles. Pre-tax DE declined from $25 million in the third quarter of 2014 primarily due to a $17 million decrease in Permanent Capital Vehicle incentive income year-over-year partially offset by a $10 million year-over-year increase in Permanent Capital Vehicle management fees. Private Equity Fund valuations declined 3.9% in the third quarter primarily due to depreciation of Nationstar Mortgage Holdings Inc. (NYSE: NSM) and Springleaf Holdings Inc. (NYSE: LEAF). As of October 28 2015 public company valuations had increased 7.3% in the fourth quarter primarily due to a 10.9% increase in Springleafs stock price. The Liquid Hedge Funds recorded a pre-tax DE loss of $4 million in the third quarter of 2015 flat compared to the third quarter of 2014. The pre-tax DE loss in the quarter was primarily due to an $11 million impairment of certain software and technology-related assets. Earnings from Affiliated Managers totaled $2 million in the third quarter of 2015 compared to a loss of $1 million in the second quarter of 2015. Third quarter 2015 net returns for the Fortress Macro Funds Fortress Convex Asia Funds Fortress Centaurus Global Funds and Fortress Partners Funds were (7.6)% 3.3% (3.1)% and (4.8)% respectively. Net returns year-to-date through October 23 2015 for the Fortress Convex Asia Funds and Fortress Centaurus Global Funds were (1.0)% and (2.3)% respectively.* Post quarter end Fortress announced it will close the Fortress Macro Funds and expects to return most capital to investors by the end of 2015. The Fortress Macro Funds accounted for 2% of Fortresss total AUM at quarter end and had generated a pre-tax DE loss of $8 million year-to-date through September 30 2015. Michael Novogratz a Fortress principal officer and director and Macro Fund CIO is expected to retire at the end of 2015. Excluding $1.8 billion of AUM related to the Fortress Macro Funds the Liquid Hedge Funds had $5.6 billion of AUM at quarter end including $4.5 billion related to Affiliated Managers. Logan Circle our traditional asset management business recorded breakeven pre-tax DE in the quarter as $0.3 million of fund management DE was offset by $0.3 million of net investment losses. Pre-tax DE increased from a pre-tax DE loss of $2 million in the third quarter of 2014 primarily due to higher revenues and lower operating expenses. Logan Circle ended the quarter with $33.4 billion in AUM a slight decline compared to the prior quarter and an 8% increase compared to the third quarter of 2014. AUM declined slightly quarter-over-quarter as $0.1 billion of net inflows were offset by $0.2 billion of market-driven valuation losses. Since inception 15 of 16 Logan Circle fixed income strategies have outperformed their respective benchmarks and seven were ranked in the top quartile of performance for their competitor universe. For the year-to-date period through September 30 2015 8 out of Logan Circles 16 fixed income strategies outperformed their respective benchmarks. As of September 30 2015 Fortress had cash and cash equivalents of $337 million and debt obligations of $75 million. As of September 30 2015 Fortress had $1.2 billion of investments in Fortress funds and options in publicly traded permanent capital vehicles. As of September 30 2015 Fortress had a total of $153 million of outstanding commitments to its funds. In addition the NAV of Fortresss investments in its own funds exceeded its segment cost basis by $506 million at quarter end representing net unrealized gains that have not yet been recognized for segment reporting purposes. In October 2015 Fortress announced that it has agreed to purchase 56.8 million Class A equivalent shares at $4.50 per share or an aggregate amount of $255.7 million. In connection with this purchase it is anticipated that Fortress will pay $100.0 million of cash in November 2015 and issue an $155.7 million promissory note. In addition Fortress expects to enter into a consent and waiver with lenders under its credit agreement for their consent to this transaction. Fortresss Board of Directors declared a third quarter 2015 cash dividend of $0.08 per dividend paying share. The dividend is payable on November 13 2015 to Class A shareholders of record as of the close of business on November 10 2015. The declaration and payment of any dividends are at the sole discretion of the Board of Directors which may decide to change its dividend policy at any time. Please see below for information on the U.S. federal income tax implications of the dividend. DE is a primary metric used by management to measure Fortresss operating performance.Consistent with GAAP DE is the sole measure that management uses to manage and thus report on Fortresss segments namely: Private Equity Permanent Capital Vehicles Credit Hedge Funds Credit PE Funds Liquid Hedge Funds and Logan Circle.DE differs from GAAP net income in a number of material ways. For a detailed description of the calculation of pre-tax DE and fund management DE see Exhibit 3 to this release and note 10 to the financial statements included in the Companys most recent quarterly report on Form 10-Q. Fortress aggregates its segment results to report consolidated segment results as shown in the table under Summary Financial Results and in the Total column of the table under Consolidated Segment Results (Non-GAAP). The consolidated segment results are non-GAAP financial information. Management believes that consolidated segment results provide a meaningful basis for comparison among present and future periods. However consolidated segment results should not be considered a substitute for Fortresss consolidated GAAP results. The exhibits to this release contain reconciliations of the components of Fortresss consolidated segment results to the comparable GAAP measures and Fortress urges you to review these exhibits. Fortress also uses weighted average dividend paying shares and units outstanding (used to calculate pre-tax DE per dividend paying share) and net cash and investments. The exhibits to this release contain reconciliations of these measures to the comparable GAAP measures and Fortress urges you to review these exhibits. Management will host a conference call today Thursday October 29th at 10:00 A.M. Eastern Time. A copy of the earnings release is posted to the Investor Relations section of Fortresss website www.fortress.com. The conference call may be accessed by dialing 1-877-694-6694 (from within the U.S.) or 1-970-315-0985 (from outside of the U.S.) ten minutes prior to the scheduled start of the call; please reference Fortress Third Quarter Earnings Call. A simultaneous webcast of the conference call will be available to the public on a listen-only basis at www.fortress.com. Please allow extra time prior to the call to visit the site and download the necessary software required to listen to the internet broadcast. A telephonic replay of the conference call will also be available by dialing 1-855-859-2056 (from within the U.S.) or 1-404-537-3406 (from outside of the U.S.); please reference access code 57023583. Fortress Investment Group LLC (NYSE: FIG) is a leading highly diversified global investment management firm with $74.3 billion in assets under management as of September 30 2015. Fortress applies its deep experience and specialized expertise across a range of investment strategies - private equity credit liquid hedge funds and traditional asset management - on behalf of over 1800 institutional clients and private investors worldwide. For more information regarding Fortress Investment Group LLC or to be added to its e-mail distribution list please visit www.fortress.com. Certain statements in this press release may constitute forward-looking statements within the meaning of the Private Securities Litigation Reform Act of 1995 including statements regarding Fortresss sources of management fees incentive income and investment income (loss) estimated fund performance the amount and source of expected capital commitments the closing of the Fortress Macro Funds and repurchase of shares/units from a retiring principal (the transaction).These statements are not historical facts but instead represent only the Companys beliefs regarding future events many of which by their nature are inherently uncertain and outside of the Companys control. It is possible that the sources and amounts of management fees incentive income and investment income the amount and source of expected capital commitments for any new fund or redemption amounts may differ possibly materially fromthese forward-looking statements. Such differences or other changes to forward looking statements including with respect to the transaction could cause the Companys actual results to differ materially from the results expressed or implied by these forward-looking statements. For a discussion of some of the risks and important factors that could affect such forward-looking statements see the sections entitled Risk Factors and Managements Discussion and Analysis of Financial Condition and Results of Operations in the CompanysQuarterly Report on Form 10-Q which is or will be available on the Companys website (www.fortress.com). In addition new risks and uncertainties emerge from time to time and it is not possible for the Company to predict or assess the impact of every factor that may cause its actual results to differ from those contained in any forward-looking statements. Accordingly you should not place undue reliance on any forward-looking statements contained in this press release. The Company can give no assurance that the expectations of any forward-looking statement will be obtained. Such forward-looking statements speak only as of the date of this press release. The Company expressly disclaims any obligation to release publicly any updates or revisions to any forward-looking statements contained herein to reflect any change in the Companys expectations with regard thereto or any change in events conditions or circumstances on which any statement is based. This announcement is intended to be a qualified notice as provided in the Internal Revenue Code (the Code) and the Regulations thereunder. For U.S. federal income tax purposes the dividend declared in October 2015 will be treated as a partnership distribution. The per share distribution components are as follows: Distributable earnings is Fortresss supplemental measure of operating performance used by management in analyzing segment and overall results. It reflects the value created which management considers available for distribution during any period. As compared to generally accepted accounting principles (GAAP) net income distributable earnings excludes the effects of unrealized gains (or losses) on illiquid investments reflects contingent revenue which has been received as income to the extent it is not expected to be reversed and disregards expenses which do not require an outlay of assets whether currently or on an accrued basis. Distributable earnings is reflected on an unconsolidated and pre-tax basis and therefore the interests in consolidated subsidiaries related to Fortress Operating Group units (held by the principals) and income tax expense are added back in its calculation. Distributable earnings is not a measure of cash generated by operations which is available for distribution nor should it be considered in isolation or as an alternative to cash flow or net income in accordance with GAAP and it is not necessarily indicative of liquidity or cash available to fund the Companys operations. For a complete discussion of distributable earnings and its reconciliation to GAAP as well as an explanation of the calculation of distributable earnings impairment see note 10 to the financial statements included in the Companys Quarterly Report on Form 10-Q for the quarter ended September 30 2015. Growing distributable earnings is a key component to the Companys business strategy and distributable earnings is the supplemental measure used by management to evaluate the economic profitability of each of the Companys businesses and total operations. Therefore Fortress believes that it provides useful information to investors in evaluating its operating performance. Fortresss definition of distributable earnings is not based on any definition contained in its amended and restated operating agreement. Fund management DE is equal to pre-tax distributable earnings excluding our direct investment-related results. Fund management DE is comprised of Pre-tax Distributable Earnings excluding Investment Loss (Income) and Interest Expense. Fund management DE and its components are used by management to analyze and measure the performance of our investment management business on a stand-alone basis. Fortress defines segment operating margin to be equal to fund management DE divided by segment revenues. The Company believes that it is useful to provide investors with the opportunity to review our investment management business using the same metrics. Fund management DE and its components are subject to the same limitations as pre-tax distributable earnings as described above. Dividend paying shares and units represents the number of shares and units outstanding at the end of the period which were entitled to receive dividends or related distributions. The Company believes it is useful for investors in computing the aggregate amount of cash required to make a current per share distribution of a given amount per share. It excludes certain potentially dilutive equity instruments primarily non-dividend paying restricted Class A share units and therefore is limited in its usefulness in computing per share amounts. Accordingly dividend paying shares and units should be considered only as a supplement and not an alternative to GAAP basic and diluted shares outstanding. The Companys calculation of dividend paying shares and units may be different from the calculation used by other companies and therefore comparability may be limited. Net cash and investments represents cash and cash equivalents plus investments less debt outstanding. The Company believes that net cash and investments is a useful supplemental measure because it provides investors with information regarding the Companys net investment assets. Net cash and investments excludes certain assets (investments in options due from affiliates deferred tax asset other assets) and liabilities (due to affiliates accrued compensation and benefits deferred incentive income and other liabilities) and its utility as a measure of financial position is limited. Accordingly net cash and investments should be considered only as a supplement and not an alternative to GAAP book value as a measure of the Companys financial position. The Companys calculation of net cash and investments may be different from the calculation used by other companies and therefore comparability may be limited.']}
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/pipelines.py", line 42, in process_item
    self.cur.execute(sql, dc)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/cursor.py", line 515, in execute
    self._handle_result(self._connection.cmd_query(stmt))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 488, in cmd_query
    result = self._handle_result(self._send_cmd(ServerCmd.QUERY, query))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/connection.py", line 261, in _send_cmd
    packet_number)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/mysql/connector/network.py", line 133, in send_plain
    errno=2055, values=(self.get_address(), _strioerror(err)))
OperationalError: 2055: Lost connection to MySQL server at '130.211.154.93:3306', system error: 32 Broken pipe
2015-11-04 06:57:42 [scrapy] INFO: Closing spider (finished)
2015-11-04 06:57:42 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 154,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 10,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 19,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 53,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 69,
 'downloader/request_bytes': 882656,
 'downloader/request_count': 2497,
 'downloader/request_method_count/GET': 2497,
 'downloader/response_bytes': 94632269,
 'downloader/response_count': 2343,
 'downloader/response_status_count/200': 1889,
 'downloader/response_status_count/301': 103,
 'downloader/response_status_count/302': 206,
 'downloader/response_status_count/400': 31,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 104,
 'downloader/response_status_count/408': 7,
 'dupefilter/filtered': 15667,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 6, 57, 42, 45303),
 'item_scraped_count': 1290,
 'log_count/ERROR': 523,
 'log_count/INFO': 35,
 'offsite/domains': 544,
 'offsite/filtered': 3308,
 'request_depth_max': 2,
 'response_received_count': 2005,
 'scheduler/dequeued': 2497,
 'scheduler/dequeued/memory': 2497,
 'scheduler/enqueued': 2497,
 'scheduler/enqueued/memory': 2497,
 'spider_exceptions/AttributeError': 4,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/TypeError': 111,
 'start_time': datetime.datetime(2015, 11, 4, 1, 56, 19, 841064)}
2015-11-04 06:57:42 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 06:58:44 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 06:58:44 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 06:58:44 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 06:58:44 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 06:58:44 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 06:58:44 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 06:58:45 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 06:58:45 [scrapy] INFO: Spider opened
2015-11-04 06:58:45 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.par>: DNS lookup failed: address 'www.par' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.gim>: DNS lookup failed: address 'www.gim' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.investor.pccpllc.amiesdigital.com>: DNS lookup failed: address 'www.investor.pccpllc.amiesdigital.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:45 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:46 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 06:58:46 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 06:58:46 [scrapy] ERROR: Error downloading <GET http://www.torshencapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:01:49 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:01:49 [scrapy] INFO: Crawled 76 pages (at 76 pages/min), scraped 19 items (at 19 items/min)
2015-11-04 07:04:50 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.ostracap.com took longer than 180.0 seconds..
2015-11-04 07:04:50 [scrapy] INFO: Crawled 77 pages (at 1 pages/min), scraped 25 items (at 6 items/min)
2015-11-04 07:04:51 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 07:04:51 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 07:04:51 [scrapy] ERROR: Error downloading <GET http://www.cfm>: DNS lookup failed: address 'www.cfm' not found: [Errno -2] Name or service not known.
2015-11-04 07:04:51 [scrapy] ERROR: Error downloading <GET http://www.fsc>: DNS lookup failed: address 'www.fsc' not found: [Errno -2] Name or service not known.
2015-11-04 07:04:51 [scrapy] ERROR: Error downloading <GET http://www.freshfordcapital.com>: DNS lookup failed: address 'www.freshfordcapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:04:57 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:05:47 [scrapy] ERROR: Error downloading <GET https://deutscheawm.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:05:47 [scrapy] INFO: Crawled 140 pages (at 63 pages/min), scraped 54 items (at 29 items/min)
2015-11-04 07:08:49 [scrapy] INFO: Crawled 149 pages (at 9 pages/min), scraped 65 items (at 11 items/min)
2015-11-04 07:08:49 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:08:49 [scrapy] ERROR: Error downloading <GET http://www.intrepidcap.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:09:55 [scrapy] ERROR: Spider error processing <GET http://www.fosuncapital.com/index.php/investment/default/page/6> (referer: http://www.fosuncapital.com/index.php/investment)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 07:09:55 [scrapy] INFO: Crawled 156 pages (at 7 pages/min), scraped 77 items (at 12 items/min)
2015-11-04 07:11:07 [scrapy] INFO: Crawled 171 pages (at 15 pages/min), scraped 92 items (at 15 items/min)
2015-11-04 07:12:18 [scrapy] ERROR: Spider error processing <GET http://www.fosuncapital.com/index.php/fund/default/category/22> (referer: http://www.fosuncapital.com/index.php/fund)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 07:12:18 [scrapy] INFO: Crawled 172 pages (at 1 pages/min), scraped 92 items (at 0 items/min)
2015-11-04 07:13:01 [scrapy] INFO: Crawled 180 pages (at 8 pages/min), scraped 99 items (at 7 items/min)
2015-11-04 07:13:33 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:14:30 [scrapy] INFO: Crawled 193 pages (at 13 pages/min), scraped 113 items (at 14 items/min)
2015-11-04 07:15:47 [scrapy] INFO: Crawled 197 pages (at 4 pages/min), scraped 117 items (at 4 items/min)
2015-11-04 07:16:45 [scrapy] INFO: Crawled 198 pages (at 1 pages/min), scraped 118 items (at 1 items/min)
2015-11-04 07:17:45 [scrapy] INFO: Crawled 198 pages (at 0 pages/min), scraped 118 items (at 0 items/min)
2015-11-04 07:17:54 [scrapy] ERROR: Error downloading <GET http://madisonint.com/de/news/madison-international-realty-announces-sale-of-trianon-iconic-frankfurt-office-tower/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:17:54 [scrapy] ERROR: Error downloading <GET http://madisonint.com/news/the-hostile-takeover-of-canary-wharf-thrust-madison-international-realty-and-its-unusual-brand-of-real-estate-secondaries-investing-into-the-spotlight/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:17:54 [scrapy] ERROR: Error downloading <GET http://madisonint.com/de/limited-partners/investment-opportunities/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:17:54 [scrapy] ERROR: Error downloading <GET http://madisonint.com/de/limited-partners/the-team/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:18:45 [scrapy] INFO: Crawled 198 pages (at 0 pages/min), scraped 118 items (at 0 items/min)
2015-11-04 07:19:45 [scrapy] INFO: Crawled 198 pages (at 0 pages/min), scraped 118 items (at 0 items/min)
2015-11-04 07:20:02 [scrapy] ERROR: Error downloading <GET http://madisonint.com/news/page/2/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:02 [scrapy] ERROR: Error downloading <GET http://madisonint.com/news/madison-international-realty-acquires-stake-in-monogram-residential-trust/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:02 [scrapy] ERROR: Error downloading <GET http://madisonint.com/de/news/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:20:02 [scrapy] ERROR: Error downloading <GET http://madisonint.com/de/contact/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:25:34 [scrapy] INFO: Crawled 218 pages (at 20 pages/min), scraped 138 items (at 20 items/min)
2015-11-04 07:25:34 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:25:34 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 189,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 12,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 46,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 49,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 61,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 7,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 14,
 'downloader/request_bytes': 132353,
 'downloader/request_count': 453,
 'downloader/request_method_count/GET': 453,
 'downloader/response_bytes': 3620861,
 'downloader/response_count': 264,
 'downloader/response_status_count/200': 213,
 'downloader/response_status_count/301': 25,
 'downloader/response_status_count/302': 16,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 4,
 'downloader/response_status_count/404': 2,
 'downloader/response_status_count/408': 1,
 'downloader/response_status_count/503': 1,
 'dupefilter/filtered': 836,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 25, 34, 826275),
 'item_scraped_count': 138,
 'log_count/ERROR': 35,
 'log_count/INFO': 22,
 'offsite/domains': 86,
 'offsite/filtered': 377,
 'request_depth_max': 2,
 'response_received_count': 218,
 'scheduler/dequeued': 453,
 'scheduler/dequeued/memory': 453,
 'scheduler/enqueued': 453,
 'scheduler/enqueued/memory': 453,
 'spider_exceptions/timeout': 2,
 'start_time': datetime.datetime(2015, 11, 4, 6, 58, 45, 209715)}
2015-11-04 07:25:34 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:26:36 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:26:36 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:26:36 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:26:36 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:26:36 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:26:36 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:26:37 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:26:37 [scrapy] INFO: Spider opened
2015-11-04 07:26:37 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.beckerdrapkin.com>: DNS lookup failed: address 'www.beckerdrapkin.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.aetherip.applicationexperts.com>: DNS lookup failed: address 'www.aetherip.applicationexperts.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.due>: DNS lookup failed: address 'www.due' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.cor>: DNS lookup failed: address 'www.cor' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:37 [scrapy] ERROR: Error downloading <GET http://www.adamshillpartners.com>: DNS lookup failed: address 'www.adamshillpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:38 [scrapy] ERROR: Error downloading <GET http://www.harvpartners.com>: DNS lookup failed: address 'www.harvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:38 [scrapy] ERROR: Error downloading <GET http://www.imc>: DNS lookup failed: address 'www.imc' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:42 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:26:42 [scrapy] ERROR: Error downloading <GET https://www.magnitudecapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]
2015-11-04 07:26:42 [scrapy] ERROR: Error downloading <GET http://www.omn>: DNS lookup failed: address 'www.omn' not found: [Errno -2] Name or service not known.
2015-11-04 07:26:55 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:26:56 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:27:01 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:27:01 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:27:01 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:27:01 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:27:37 [scrapy] INFO: Crawled 236 pages (at 236 pages/min), scraped 126 items (at 126 items/min)
2015-11-04 07:28:42 [scrapy] INFO: Crawled 342 pages (at 106 pages/min), scraped 232 items (at 106 items/min)
2015-11-04 07:29:37 [scrapy] INFO: Crawled 356 pages (at 14 pages/min), scraped 253 items (at 21 items/min)
2015-11-04 07:30:37 [scrapy] INFO: Crawled 356 pages (at 0 pages/min), scraped 253 items (at 0 items/min)
2015-11-04 07:31:37 [scrapy] INFO: Crawled 356 pages (at 0 pages/min), scraped 253 items (at 0 items/min)
2015-11-04 07:32:37 [scrapy] INFO: Crawled 356 pages (at 0 pages/min), scraped 253 items (at 0 items/min)
2015-11-04 07:33:03 [scrapy] ERROR: Error downloading <GET http://www.adelphi-europe.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:33:03 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:33:03 [scrapy] ERROR: Error downloading <GET http://www.oldmutualus.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:33:03 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:33:03 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 95,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 5,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 39,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 9,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 39,
 'downloader/request_bytes': 182860,
 'downloader/request_count': 499,
 'downloader/request_method_count/GET': 499,
 'downloader/response_bytes': 5448389,
 'downloader/response_count': 404,
 'downloader/response_status_count/200': 335,
 'downloader/response_status_count/301': 20,
 'downloader/response_status_count/302': 22,
 'downloader/response_status_count/400': 6,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 18,
 'dupefilter/filtered': 538,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 33, 3, 229905),
 'item_scraped_count': 253,
 'log_count/ERROR': 31,
 'log_count/INFO': 13,
 'offsite/domains': 100,
 'offsite/filtered': 468,
 'request_depth_max': 2,
 'response_received_count': 356,
 'scheduler/dequeued': 499,
 'scheduler/dequeued/memory': 499,
 'scheduler/enqueued': 499,
 'scheduler/enqueued/memory': 499,
 'start_time': datetime.datetime(2015, 11, 4, 7, 26, 37, 207045)}
2015-11-04 07:33:03 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:34:05 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:34:05 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:34:05 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:34:05 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:34:05 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:34:05 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:34:05 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:34:05 [scrapy] INFO: Spider opened
2015-11-04 07:34:05 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.lar>: DNS lookup failed: address 'www.lar' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.santanderasset.com>: DNS lookup failed: address 'www.santanderasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.cap>: DNS lookup failed: address 'www.cap' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.inglesideadvisors.com>: DNS lookup failed: address 'www.inglesideadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.mountainpacificadvisors.com>: DNS lookup failed: address 'www.mountainpacificadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.inc>: DNS lookup failed: address 'www.inc' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.tia>: DNS lookup failed: address 'www.tia' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.harvpartners.com>: DNS lookup failed: address 'www.harvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:05 [scrapy] ERROR: Error downloading <GET http://www.pro>: DNS lookup failed: address 'www.pro' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.zad>: DNS lookup failed: address 'www.zad' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.omn>: DNS lookup failed: address 'www.omn' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.dai>: DNS lookup failed: address 'www.dai' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.alphametrix.com>: DNS lookup failed: address 'www.alphametrix.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.riverside-pm.com>: DNS lookup failed: address 'www.riverside-pm.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.sco>: DNS lookup failed: address 'www.sco' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET http://www.freshfordcapital.com>: DNS lookup failed: address 'www.freshfordcapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:06 [scrapy] ERROR: Error downloading <GET https://www.magnitudecapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]
2015-11-04 07:34:14 [scrapy] ERROR: Error downloading <GET http://www.meridianfunds.com>: DNS lookup failed: address 'www.meridianfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:34:25 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 07:34:34 [scrapy] ERROR: Error downloading <GET http://www.lightstreetcap.com>: DNS lookup failed: address 'www.lightstreetcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:35:05 [scrapy] INFO: Crawled 225 pages (at 225 pages/min), scraped 152 items (at 152 items/min)
2015-11-04 07:36:05 [scrapy] INFO: Crawled 225 pages (at 0 pages/min), scraped 152 items (at 0 items/min)
2015-11-04 07:37:05 [scrapy] INFO: Crawled 225 pages (at 0 pages/min), scraped 152 items (at 0 items/min)
2015-11-04 07:37:13 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:38:05 [scrapy] INFO: Crawled 225 pages (at 0 pages/min), scraped 152 items (at 0 items/min)
2015-11-04 07:39:05 [scrapy] INFO: Crawled 225 pages (at 0 pages/min), scraped 152 items (at 0 items/min)
2015-11-04 07:40:05 [scrapy] INFO: Crawled 225 pages (at 0 pages/min), scraped 152 items (at 0 items/min)
2015-11-04 07:40:27 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:40:27 [scrapy] ERROR: Error downloading <GET http://www.emffp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:40:27 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:40:27 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 94,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 7,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 75,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 88537,
 'downloader/request_count': 348,
 'downloader/request_method_count/GET': 348,
 'downloader/response_bytes': 1876311,
 'downloader/response_count': 254,
 'downloader/response_status_count/200': 215,
 'downloader/response_status_count/301': 13,
 'downloader/response_status_count/302': 14,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 6,
 'downloader/response_status_count/404': 4,
 'dupefilter/filtered': 299,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 40, 27, 902082),
 'item_scraped_count': 152,
 'log_count/ERROR': 31,
 'log_count/INFO': 13,
 'offsite/domains': 97,
 'offsite/filtered': 357,
 'request_depth_max': 2,
 'response_received_count': 225,
 'scheduler/dequeued': 348,
 'scheduler/dequeued/memory': 348,
 'scheduler/enqueued': 348,
 'scheduler/enqueued/memory': 348,
 'start_time': datetime.datetime(2015, 11, 4, 7, 34, 5, 373722)}
2015-11-04 07:40:27 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:41:29 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:41:29 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:41:29 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:41:29 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:41:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:41:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:41:29 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:41:29 [scrapy] INFO: Spider opened
2015-11-04 07:41:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.jrc>: DNS lookup failed: address 'www.jrc' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.nom>: DNS lookup failed: address 'www.nom' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.harvpartners.com>: DNS lookup failed: address 'www.harvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.portal.krfs.com>: DNS lookup failed: address 'www.portal.krfs.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.mountainpacificadvisors.com>: DNS lookup failed: address 'www.mountainpacificadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.car>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.adamshillpartners.com>: DNS lookup failed: address 'www.adamshillpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:30 [scrapy] ERROR: Error downloading <GET http://www.alphatitans.com>: DNS lookup failed: address 'www.alphatitans.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:33 [scrapy] ERROR: Error downloading <GET http://www.gim>: DNS lookup failed: address 'www.gim' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:33 [scrapy] ERROR: Error downloading <GET http://www.ellislake.com>: DNS lookup failed: address 'www.ellislake.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:34 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 07:41:34 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:34 [scrapy] ERROR: Error downloading <GET http://www.arg>: DNS lookup failed: address 'www.arg' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:34 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:35 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:35 [scrapy] ERROR: Error downloading <GET http://www.ccm>: DNS lookup failed: address 'www.ccm' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:35 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:35 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:35 [scrapy] ERROR: Error downloading <GET http://www.mad>: DNS lookup failed: address 'www.mad' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:35 [scrapy] ERROR: Error downloading <GET http://www.citicapitaladvisors.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:41:35 [scrapy] ERROR: Error downloading <GET http://www.clerestorycapital.com>: DNS lookup failed: address 'www.clerestorycapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:41:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 07:42:29 [scrapy] INFO: Crawled 136 pages (at 136 pages/min), scraped 52 items (at 52 items/min)
2015-11-04 07:43:29 [scrapy] INFO: Crawled 136 pages (at 0 pages/min), scraped 52 items (at 0 items/min)
2015-11-04 07:44:29 [scrapy] INFO: Crawled 136 pages (at 0 pages/min), scraped 52 items (at 0 items/min)
2015-11-04 07:45:29 [scrapy] INFO: Crawled 136 pages (at 0 pages/min), scraped 52 items (at 0 items/min)
2015-11-04 07:46:29 [scrapy] INFO: Crawled 136 pages (at 0 pages/min), scraped 52 items (at 0 items/min)
2015-11-04 07:47:29 [scrapy] INFO: Crawled 136 pages (at 0 pages/min), scraped 52 items (at 0 items/min)
2015-11-04 07:47:51 [scrapy] ERROR: Error downloading <GET http://www.emffp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:47:54 [scrapy] ERROR: Error downloading <GET http://www.permalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:47:55 [scrapy] ERROR: Error downloading <GET http://www.vscapitalpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 07:47:55 [scrapy] INFO: Closing spider (finished)
2015-11-04 07:47:55 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 117,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 63,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 9,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 36,
 'downloader/request_bytes': 72330,
 'downloader/request_count': 296,
 'downloader/request_method_count/GET': 296,
 'downloader/response_bytes': 772563,
 'downloader/response_count': 179,
 'downloader/response_status_count/200': 121,
 'downloader/response_status_count/301': 16,
 'downloader/response_status_count/302': 20,
 'downloader/response_status_count/400': 6,
 'downloader/response_status_count/401': 5,
 'downloader/response_status_count/403': 3,
 'downloader/response_status_count/404': 8,
 'dupefilter/filtered': 182,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 7, 47, 55, 261681),
 'item_scraped_count': 52,
 'log_count/ERROR': 39,
 'log_count/INFO': 13,
 'offsite/domains': 89,
 'offsite/filtered': 299,
 'request_depth_max': 2,
 'response_received_count': 136,
 'scheduler/dequeued': 296,
 'scheduler/dequeued/memory': 296,
 'scheduler/enqueued': 296,
 'scheduler/enqueued/memory': 296,
 'start_time': datetime.datetime(2015, 11, 4, 7, 41, 29, 999890)}
2015-11-04 07:47:55 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 07:48:57 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 07:48:57 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 07:48:57 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 07:48:57 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 07:48:57 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 07:48:57 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 07:48:57 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 07:48:57 [scrapy] INFO: Spider opened
2015-11-04 07:48:57 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 07:48:57 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:57 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:57 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:48:57 [scrapy] ERROR: Error downloading <GET http://www.zca>: DNS lookup failed: address 'www.zca' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:57 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:57 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:58 [scrapy] ERROR: Error downloading <GET http://www.tit>: DNS lookup failed: address 'www.tit' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:58 [scrapy] ERROR: Error downloading <GET http://www.adamshillpartners.com>: DNS lookup failed: address 'www.adamshillpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:58 [scrapy] ERROR: Error downloading <GET http://www.visicap.com>: DNS lookup failed: address 'www.visicap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:58 [scrapy] ERROR: Error downloading <GET http://www.aboutyou.bwater.com>: DNS lookup failed: address 'www.aboutyou.bwater.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:58 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 07:48:58 [scrapy] ERROR: Error downloading <GET http://www.5tides.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 07:48:59 [scrapy] ERROR: Error downloading <GET http://emergingcapitalmarket.com>: DNS lookup failed: address 'emergingcapitalmarket.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:49:00 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 07:49:07 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 07:49:15 [scrapy] ERROR: Error downloading <GET http://www.esemplia.com>: DNS lookup failed: address 'www.esemplia.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:49:17 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:50:34 [scrapy] INFO: Crawled 168 pages (at 168 pages/min), scraped 48 items (at 48 items/min)
2015-11-04 07:50:50 [scrapy] ERROR: Error downloading <GET http://www.lightstreetcap.com>: DNS lookup failed: address 'www.lightstreetcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 07:53:04 [scrapy] INFO: Crawled 176 pages (at 8 pages/min), scraped 82 items (at 34 items/min)
2015-11-04 07:53:59 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/fund/boshiyazhoupiaoxishouyizhaiquan.html> (referer: http://www.bosera.com/index.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 07:54:24 [scrapy] INFO: Crawled 222 pages (at 46 pages/min), scraped 111 items (at 29 items/min)
2015-11-04 07:55:29 [scrapy] INFO: Crawled 224 pages (at 2 pages/min), scraped 125 items (at 14 items/min)
2015-11-04 07:56:04 [scrapy] INFO: Crawled 261 pages (at 37 pages/min), scraped 145 items (at 20 items/min)
2015-11-04 07:56:59 [scrapy] INFO: Crawled 276 pages (at 15 pages/min), scraped 164 items (at 19 items/min)
2015-11-04 07:58:46 [scrapy] INFO: Crawled 287 pages (at 11 pages/min), scraped 181 items (at 17 items/min)
2015-11-04 07:59:00 [scrapy] INFO: Crawled 289 pages (at 2 pages/min), scraped 190 items (at 9 items/min)
2015-11-04 08:01:41 [scrapy] INFO: Crawled 326 pages (at 37 pages/min), scraped 214 items (at 24 items/min)
2015-11-04 08:02:13 [scrapy] INFO: Crawled 327 pages (at 1 pages/min), scraped 229 items (at 15 items/min)
2015-11-04 08:02:14 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/926698.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/926698.pdf took longer than 180.0 seconds..
2015-11-04 08:03:08 [scrapy] INFO: Crawled 374 pages (at 47 pages/min), scraped 261 items (at 32 items/min)
2015-11-04 08:04:10 [scrapy] INFO: Crawled 408 pages (at 34 pages/min), scraped 300 items (at 39 items/min)
2015-11-04 08:04:59 [scrapy] INFO: Crawled 466 pages (at 58 pages/min), scraped 361 items (at 61 items/min)
2015-11-04 08:05:57 [scrapy] INFO: Crawled 524 pages (at 58 pages/min), scraped 419 items (at 58 items/min)
2015-11-04 08:07:25 [scrapy] INFO: Crawled 574 pages (at 50 pages/min), scraped 454 items (at 35 items/min)
2015-11-04 08:07:33 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228798> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:07:48 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228800> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:07:57 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228801> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:07:57 [scrapy] INFO: Crawled 575 pages (at 1 pages/min), scraped 466 items (at 12 items/min)
2015-11-04 08:08:07 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228799> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623312)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:09:03 [scrapy] INFO: Crawled 615 pages (at 40 pages/min), scraped 503 items (at 37 items/min)
2015-11-04 08:10:07 [scrapy] INFO: Crawled 631 pages (at 16 pages/min), scraped 514 items (at 11 items/min)
2015-11-04 08:10:59 [scrapy] INFO: Crawled 668 pages (at 37 pages/min), scraped 556 items (at 42 items/min)
2015-11-04 08:12:18 [scrapy] INFO: Crawled 691 pages (at 23 pages/min), scraped 583 items (at 27 items/min)
2015-11-04 08:12:59 [scrapy] INFO: Crawled 700 pages (at 9 pages/min), scraped 590 items (at 7 items/min)
2015-11-04 08:14:25 [scrapy] INFO: Crawled 732 pages (at 32 pages/min), scraped 619 items (at 29 items/min)
2015-11-04 08:15:24 [scrapy] INFO: Crawled 753 pages (at 21 pages/min), scraped 635 items (at 16 items/min)
2015-11-04 08:15:57 [scrapy] INFO: Crawled 753 pages (at 0 pages/min), scraped 644 items (at 9 items/min)
2015-11-04 08:17:01 [scrapy] INFO: Crawled 763 pages (at 10 pages/min), scraped 652 items (at 8 items/min)
2015-11-04 08:18:05 [scrapy] INFO: Crawled 813 pages (at 50 pages/min), scraped 692 items (at 40 items/min)
2015-11-04 08:19:04 [scrapy] INFO: Crawled 837 pages (at 24 pages/min), scraped 724 items (at 32 items/min)
2015-11-04 08:20:00 [scrapy] INFO: Crawled 863 pages (at 26 pages/min), scraped 750 items (at 26 items/min)
2015-11-04 08:21:04 [scrapy] INFO: Crawled 902 pages (at 39 pages/min), scraped 791 items (at 41 items/min)
2015-11-04 08:22:03 [scrapy] INFO: Crawled 952 pages (at 50 pages/min), scraped 831 items (at 40 items/min)
2015-11-04 08:23:03 [scrapy] INFO: Crawled 995 pages (at 43 pages/min), scraped 876 items (at 45 items/min)
2015-11-04 08:24:05 [scrapy] INFO: Crawled 1033 pages (at 38 pages/min), scraped 919 items (at 43 items/min)
2015-11-04 08:25:02 [scrapy] INFO: Crawled 1089 pages (at 56 pages/min), scraped 978 items (at 59 items/min)
2015-11-04 08:26:02 [scrapy] INFO: Crawled 1153 pages (at 64 pages/min), scraped 1042 items (at 64 items/min)
2015-11-04 08:27:03 [scrapy] INFO: Crawled 1217 pages (at 64 pages/min), scraped 1106 items (at 64 items/min)
2015-11-04 08:28:02 [scrapy] INFO: Crawled 1281 pages (at 64 pages/min), scraped 1170 items (at 64 items/min)
2015-11-04 08:28:59 [scrapy] INFO: Crawled 1350 pages (at 69 pages/min), scraped 1234 items (at 64 items/min)
2015-11-04 08:29:58 [scrapy] INFO: Crawled 1414 pages (at 64 pages/min), scraped 1298 items (at 64 items/min)
2015-11-04 08:31:01 [scrapy] INFO: Crawled 1473 pages (at 59 pages/min), scraped 1362 items (at 64 items/min)
2015-11-04 08:32:04 [scrapy] INFO: Crawled 1545 pages (at 72 pages/min), scraped 1434 items (at 72 items/min)
2015-11-04 08:33:02 [scrapy] INFO: Crawled 1609 pages (at 64 pages/min), scraped 1498 items (at 64 items/min)
2015-11-04 08:34:01 [scrapy] INFO: Crawled 1671 pages (at 62 pages/min), scraped 1560 items (at 62 items/min)
2015-11-04 08:34:59 [scrapy] INFO: Crawled 1737 pages (at 66 pages/min), scraped 1624 items (at 64 items/min)
2015-11-04 08:35:57 [scrapy] INFO: Crawled 1799 pages (at 62 pages/min), scraped 1684 items (at 60 items/min)
2015-11-04 08:37:02 [scrapy] INFO: Crawled 1866 pages (at 67 pages/min), scraped 1755 items (at 71 items/min)
2015-11-04 08:38:01 [scrapy] INFO: Crawled 1930 pages (at 64 pages/min), scraped 1819 items (at 64 items/min)
2015-11-04 08:39:01 [scrapy] INFO: Crawled 1990 pages (at 60 pages/min), scraped 1875 items (at 56 items/min)
2015-11-04 08:39:57 [scrapy] INFO: Crawled 2038 pages (at 48 pages/min), scraped 1923 items (at 48 items/min)
2015-11-04 08:40:57 [scrapy] INFO: Crawled 2094 pages (at 56 pages/min), scraped 1979 items (at 56 items/min)
2015-11-04 08:42:01 [scrapy] INFO: Crawled 2159 pages (at 65 pages/min), scraped 2043 items (at 64 items/min)
2015-11-04 08:43:05 [scrapy] INFO: Crawled 2217 pages (at 58 pages/min), scraped 2090 items (at 47 items/min)
2015-11-04 08:44:00 [scrapy] INFO: Crawled 2280 pages (at 63 pages/min), scraped 2135 items (at 45 items/min)
2015-11-04 08:45:12 [scrapy] INFO: Crawled 2317 pages (at 37 pages/min), scraped 2198 items (at 63 items/min)
2015-11-04 08:45:51 [scrapy] ERROR: Error downloading <GET https://www.ardian-investment.com/en/videos>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:46:18 [scrapy] INFO: Crawled 2339 pages (at 22 pages/min), scraped 2231 items (at 33 items/min)
2015-11-04 08:46:29 [scrapy] ERROR: Error downloading <GET https://www.ardian-investment.com/fr/videos>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:47:10 [scrapy] INFO: Crawled 2380 pages (at 41 pages/min), scraped 2265 items (at 34 items/min)
2015-11-04 08:47:48 [scrapy] ERROR: Error downloading <GET http://www.nokomiscapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 08:47:58 [scrapy] INFO: Crawled 2406 pages (at 26 pages/min), scraped 2290 items (at 25 items/min)
2015-11-04 08:49:04 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=SPDB> (referer: https://trade.bosera.com/acctMgr/openAcct/selectBankCard)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 08:49:23 [scrapy] INFO: Crawled 2435 pages (at 29 pages/min), scraped 2317 items (at 27 items/min)
2015-11-04 08:49:27 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/www/fundInfoDetail?flag=info&fundCode=059056>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:49:27 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/www/fundInfoDetail?flag=info&fundCode=059071>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:49:43 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 08:50:13 [scrapy] INFO: Crawled 2448 pages (at 13 pages/min), scraped 2331 items (at 14 items/min)
2015-11-04 08:50:13 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_duty.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 08:50:13 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=tty>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:50:13 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=alipay>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:50:13 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=CMBC>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 08:50:33 [scrapy] ERROR: Spider error processing <GET https://www.greenlightcapital.com/922828.pdf> (referer: https://www.greenlightcapital.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 08:51:27 [scrapy] INFO: Crawled 2473 pages (at 25 pages/min), scraped 2346 items (at 15 items/min)
2015-11-04 08:52:24 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/www/fundInfoDetail?flag=info&fundCode=059026>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/www/fundInfoDetail?flag=info&fundCode=059026 took longer than 180.0 seconds..
2015-11-04 08:52:24 [scrapy] INFO: Crawled 2474 pages (at 1 pages/min), scraped 2354 items (at 8 items/min)
2015-11-04 08:52:28 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/feedbackRecord>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctMgr/feedbackRecord took longer than 180.0 seconds..
2015-11-04 08:52:28 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/922676.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/922676.pdf took longer than 180.0 seconds..
2015-11-04 08:52:28 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/926211.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/926211.pdf took longer than 180.0 seconds..
2015-11-04 08:53:06 [scrapy] INFO: Crawled 2485 pages (at 11 pages/min), scraped 2365 items (at 11 items/min)
2015-11-04 08:53:10 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_risk.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:53:10 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index.jsp>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:53:10 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/scheduleBuy/scheduleBuyFundList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:53:10 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/specialFund/mySpecialFundDetail>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:53:40 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/column/index.jsp?classid=000200020007000400020003> (referer: http://www.bosera.com/vip/zhuanhulicai.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 08:54:23 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/column/index.jsp?classid=000200020007000400020001> (referer: http://www.bosera.com/vip/zhuanhulicai.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 08:54:23 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=CMB>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctMgr/openAcct/selectPayType?bankCode=CMB took longer than 180.0 seconds..
2015-11-04 08:54:24 [scrapy] INFO: Crawled 2502 pages (at 17 pages/min), scraped 2371 items (at 6 items/min)
2015-11-04 08:54:46 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/ took longer than 180.0 seconds..
2015-11-04 08:54:46 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050003>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 08:54:46 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=000936>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 08:54:46 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/cashbox/myCashboxDetail>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 08:55:04 [scrapy] INFO: Crawled 2505 pages (at 3 pages/min), scraped 2382 items (at 11 items/min)
2015-11-04 08:55:31 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/scheduleBuy/scheduleBuyList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:55:31 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index_s.jsp?tgtUrl=%2FacctAsset%2FmyFund%2FmyFundList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:55:59 [scrapy] INFO: Crawled 2513 pages (at 8 pages/min), scraped 2391 items (at 9 items/min)
2015-11-04 08:56:02 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=050015>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 08:56:57 [scrapy] INFO: Crawled 2514 pages (at 1 pages/min), scraped 2393 items (at 2 items/min)
2015-11-04 08:57:57 [scrapy] INFO: Crawled 2514 pages (at 0 pages/min), scraped 2393 items (at 0 items/min)
2015-11-04 08:59:04 [scrapy] INFO: Crawled 2515 pages (at 1 pages/min), scraped 2394 items (at 1 items/min)
2015-11-04 08:59:04 [scrapy] INFO: Closing spider (finished)
2015-11-04 08:59:04 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 266,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 2,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 48,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 4,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 28,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 178,
 'downloader/request_bytes': 1334430,
 'downloader/request_count': 2880,
 'downloader/request_method_count/GET': 2880,
 'downloader/response_bytes': 33890422,
 'downloader/response_count': 2614,
 'downloader/response_status_count/200': 2498,
 'downloader/response_status_count/301': 25,
 'downloader/response_status_count/302': 30,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 5,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 19,
 'downloader/response_status_count/500': 29,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 6921,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 8, 59, 4, 872475),
 'item_scraped_count': 2394,
 'log_count/ERROR': 54,
 'log_count/INFO': 74,
 'offsite/domains': 345,
 'offsite/filtered': 1006,
 'request_depth_max': 2,
 'response_received_count': 2515,
 'scheduler/dequeued': 2880,
 'scheduler/dequeued/memory': 2880,
 'scheduler/enqueued': 2880,
 'scheduler/enqueued/memory': 2880,
 'spider_exceptions/AttributeError': 5,
 'spider_exceptions/SSLError': 1,
 'spider_exceptions/error': 2,
 'spider_exceptions/timeout': 1,
 'start_time': datetime.datetime(2015, 11, 4, 7, 48, 57, 337342)}
2015-11-04 08:59:04 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 09:00:06 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 09:00:06 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 09:00:06 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 09:00:06 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 09:00:06 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 09:00:06 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 09:00:07 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 09:00:07 [scrapy] INFO: Spider opened
2015-11-04 09:00:07 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.coo>: DNS lookup failed: address 'www.coo' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.dam>: DNS lookup failed: address 'www.dam' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.bnp>: DNS lookup failed: address 'www.bnp' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.visicap.com>: DNS lookup failed: address 'www.visicap.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.bpc>: DNS lookup failed: address 'www.bpc' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.ccm>: DNS lookup failed: address 'www.ccm' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.cor>: DNS lookup failed: address 'www.cor' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.ellislake.com>: DNS lookup failed: address 'www.ellislake.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:07 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:09 [scrapy] ERROR: Error downloading <GET http://www.investor.gppfunds.com>: DNS lookup failed: address 'www.investor.gppfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 09:00:22 [scrapy] ERROR: Error downloading <GET https://www.miopartners.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:01:20 [scrapy] INFO: Crawled 209 pages (at 209 pages/min), scraped 115 items (at 115 items/min)
2015-11-04 09:02:12 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39827/17-spot.pdf> (referer: http://www.berkshire-group.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:02:15 [scrapy] INFO: Crawled 277 pages (at 68 pages/min), scraped 188 items (at 73 items/min)
2015-11-04 09:02:34 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/SpurVentures>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:03:13 [scrapy] ERROR: Error downloading <GET https://plus.google.com/108306343581548568740>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:03:13 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.godaddy.mobile.android>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:03:15 [scrapy] INFO: Crawled 337 pages (at 60 pages/min), scraped 245 items (at 57 items/min)
2015-11-04 09:03:15 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:03:40 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/ssctechnologies>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:03:52 [scrapy] ERROR: Spider error processing <GET http://blakesblog.com/> (referer: https://www.godaddy.com/?isc=instantpage_311&showip=true)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:04:03 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/subscription_center?add_user=godaddy>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:04:03 [scrapy] ERROR: Error downloading <GET https://itunes.apple.com/us/app/godaddy.com-mobile-domain/id333201813?mt=8>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:04:03 [scrapy] ERROR: Error downloading <GET https://twitter.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 09:04:03 [scrapy] ERROR: Error downloading <GET https://twitter.com/ssctechnologies>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:04:03 [scrapy] ERROR: Error downloading <GET https://www.invesco.com/portal/site/us/gateway/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:04:03 [scrapy] ERROR: Error downloading <GET https://www.intermedia.net/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:04:19 [scrapy] INFO: Crawled 405 pages (at 68 pages/min), scraped 293 items (at 48 items/min)
2015-11-04 09:05:11 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:05:21 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/godaddy>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:05:21 [scrapy] INFO: Crawled 475 pages (at 70 pages/min), scraped 350 items (at 57 items/min)
2015-11-04 09:06:31 [scrapy] INFO: Crawled 506 pages (at 31 pages/min), scraped 404 items (at 54 items/min)
2015-11-04 09:07:24 [scrapy] INFO: Crawled 524 pages (at 18 pages/min), scraped 441 items (at 37 items/min)
2015-11-04 09:07:43 [scrapy] ERROR: Error downloading <GET https://www-us.computershare.com/investor/?gcc=us>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:08:18 [scrapy] INFO: Crawled 572 pages (at 48 pages/min), scraped 486 items (at 45 items/min)
2015-11-04 09:16:00 [scrapy] ERROR: Spider error processing <GET http://www.macromedia.com/support/documentation/en/flashplayer/help/settings_manager07.html> (referer: http://www.webtrends.com/terms-policies/privacy/privacy-statement/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 69, in crawl
    return self.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 104, in feed
    self.goahead(0)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 138, in goahead
    k = self.parse_starttag(i)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 296, in parse_starttag
    self.finish_starttag(tag, attrs)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 345, in finish_starttag
    self.handle_starttag(tag, method, attrs)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 381, in handle_starttag
    method(attrs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1597, in start_meta
    if (self.declaredHTMLEncoding is not None or
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1204, in __getattr__
    return Tag.__getattr__(self, methodName)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 666, in __getattr__
    return self.find(tag)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 829, in find
    l = self.findAll(name, attrs, recursive, text, 1, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 849, in findAll
    return self._findAll(name, attrs, text, limit, generator, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 368, in _findAll
    strainer = SoupStrainer(name, attrs, text, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 895, in __init__
    if isinstance(attrs, basestring):
RuntimeError: maximum recursion depth exceeded in __instancecheck__
2015-11-04 09:16:06 [scrapy] INFO: Crawled 651 pages (at 79 pages/min), scraped 549 items (at 63 items/min)
2015-11-04 09:16:18 [scrapy] INFO: Crawled 651 pages (at 0 pages/min), scraped 560 items (at 11 items/min)
2015-11-04 09:17:08 [scrapy] INFO: Crawled 718 pages (at 67 pages/min), scraped 610 items (at 50 items/min)
2015-11-04 09:18:01 [scrapy] ERROR: Error downloading <GET https://clarus.man.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 09:18:01 [scrapy] ERROR: Error downloading <GET https://www.maninvestments.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 09:18:09 [scrapy] INFO: Crawled 799 pages (at 81 pages/min), scraped 677 items (at 67 items/min)
2015-11-04 09:18:24 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51201762&id=smartlink&index=2&lan=en-US&md5=7debb5b50ba2b3e82184d018d846e791&newsitemid=20151014006687&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Company-Update-for-September-2015/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 821: Tag footer invalid
2015-11-04 09:18:24 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=2&lan=en-US&md5=5158266120384eecdd44b68c41040b1f&newsitemid=20151103007039&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 821: Tag footer invalid
2015-11-04 09:18:24 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=1&lan=en-US&md5=3b5f5d52f28fddf355051fd8768bf656&newsitemid=20151103007039&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com%2F> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 821: Tag footer invalid
2015-11-04 09:18:25 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51201762&id=smartlink&index=1&lan=en-US&md5=72e6b29e66ffbd45c2cc962332ab5694&newsitemid=20151014006687&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Company-Update-for-September-2015/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 821: Tag footer invalid
2015-11-04 09:18:28 [scrapy] ERROR: Error downloading <GET https://segments.webtrends.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 09:18:31 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51193482&id=smartlink&index=1&lan=en-US&md5=35501886b80f5ce4fea6c8e2127ad6f5&newsitemid=20151001006977&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Fourth-Quarter-2015-Preferred-Distributions/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 77: Tag nav invalid
2015-11-04 09:18:31 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51193482&id=smartlink&index=2&lan=en-US&md5=1d97f15d0085b2ad9ff18003c6868369&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Fourth-Quarter-2015-Preferred-Distributions/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 77: Tag nav invalid
2015-11-04 09:19:04 [scrapy] ERROR: Error downloading <GET https://ondemand.webtrends.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:19:04 [scrapy] ERROR: Error downloading <GET https://analytics.webtrends.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:19:15 [scrapy] INFO: Crawled 893 pages (at 94 pages/min), scraped 785 items (at 108 items/min)
2015-11-04 09:20:11 [scrapy] INFO: Crawled 937 pages (at 44 pages/min), scraped 818 items (at 33 items/min)
2015-11-04 09:20:37 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/q4websystems>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:21:15 [scrapy] INFO: Crawled 967 pages (at 30 pages/min), scraped 874 items (at 56 items/min)
2015-11-04 09:22:27 [scrapy] INFO: Crawled 1006 pages (at 39 pages/min), scraped 891 items (at 17 items/min)
2015-11-04 09:24:04 [scrapy] INFO: Crawled 1010 pages (at 4 pages/min), scraped 913 items (at 22 items/min)
2015-11-04 09:24:10 [scrapy] INFO: Crawled 1019 pages (at 9 pages/min), scraped 917 items (at 4 items/min)
2015-11-04 09:26:09 [scrapy] INFO: Crawled 1039 pages (at 20 pages/min), scraped 941 items (at 24 items/min)
2015-11-04 09:26:51 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10940438> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:26:52 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10966005> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:26:52 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10974225> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:26:52 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10940361> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:26:53 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10940405> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:26:53 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10974202> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:26:54 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10984074> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:27:46 [scrapy] INFO: Crawled 1066 pages (at 27 pages/min), scraped 955 items (at 14 items/min)
2015-11-04 09:28:21 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10940188> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:28:21 [scrapy] INFO: Crawled 1067 pages (at 1 pages/min), scraped 961 items (at 6 items/min)
2015-11-04 09:28:40 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10984162> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:30:01 [scrapy] INFO: Crawled 1082 pages (at 15 pages/min), scraped 977 items (at 16 items/min)
2015-11-04 09:30:19 [scrapy] INFO: Crawled 1092 pages (at 10 pages/min), scraped 980 items (at 3 items/min)
2015-11-04 09:31:09 [scrapy] INFO: Crawled 1094 pages (at 2 pages/min), scraped 990 items (at 10 items/min)
2015-11-04 09:31:12 [scrapy] ERROR: Spider error processing <GET http://shareholder.api.edgar-online.com/efx_dll/edgarpro.dll?FetchFilingXLS1?sessionid=esc4eyrHZhNPCBf&ID=10986344> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:31:17 [scrapy] ERROR: Spider error processing <GET http://apps.shareholder.com/captcha/audio.aspx?CompanyID=JMPG&t=.wav&verifyid=1160331819> (referer: http://investor.jmpg.com/contactus.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:32:51 [scrapy] INFO: Crawled 1131 pages (at 37 pages/min), scraped 1017 items (at 27 items/min)
2015-11-04 09:33:09 [scrapy] INFO: Crawled 1132 pages (at 1 pages/min), scraped 1024 items (at 7 items/min)
2015-11-04 09:34:10 [scrapy] INFO: Crawled 1161 pages (at 29 pages/min), scraped 1042 items (at 18 items/min)
2015-11-04 09:34:53 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x832032/DD937596-525F-4FD1-8CB7-BBA577E18AE2/JMP_News_2015_5_27_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:35:00 [scrapy] ERROR: Error downloading <GET https://twitter.com/q4websystems>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:35:57 [scrapy] INFO: Crawled 1179 pages (at 18 pages/min), scraped 1061 items (at 19 items/min)
2015-11-04 09:37:13 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/place/335+Bryant+St,+Palo+Alto,+CA+94301/data=!4m2!3m1!1s0x808fbb3788735f0d:0xde282a76de7c2507>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:37:13 [scrapy] INFO: Crawled 1179 pages (at 0 pages/min), scraped 1069 items (at 8 items/min)
2015-11-04 09:37:26 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/1603+Orrington+Avenue,+Suite+815,+Evanston,+IL+60201/@42.047492,-87.680962,16z?source=s_q&hl=en>: TCP connection timed out: 110: Connection timed out.
2015-11-04 09:37:26 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x839197/85FD8C36-9081-49CA-AD68-747A2470E28A/JMP_News_2015_7_14_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:37:43 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x839911/AF045F98-C1FA-4FB9-8F07-142B21476EC5/JMP_News_2015_7_20_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:37:47 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/47%2FF+Cheung+Kong+Center,+2+Queen's+Road+Central,+Hong+Kong/@22.27954,114.160548,3285m/data=!3m1!4b1?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:37:48 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/8+Century+Boulevard+,+Shanghai,+China+200121/@31.240545,121.491365,15z?source=s_q&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:38:25 [scrapy] INFO: Crawled 1212 pages (at 33 pages/min), scraped 1092 items (at 23 items/min)
2015-11-04 09:39:12 [scrapy] INFO: Crawled 1228 pages (at 16 pages/min), scraped 1100 items (at 8 items/min)
2015-11-04 09:39:42 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x839993/0C4CB9F6-071A-4582-9CBD-BB9A65371278/JMP_News_2015_7_20_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:40:15 [scrapy] INFO: Crawled 1247 pages (at 19 pages/min), scraped 1119 items (at 19 items/min)
2015-11-04 09:41:17 [scrapy] INFO: Crawled 1258 pages (at 11 pages/min), scraped 1130 items (at 11 items/min)
2015-11-04 09:41:59 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x841025/2B87CA5D-D65A-4F01-802A-5377E293E1DE/JMP_News_2015_7_24_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:42:03 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x854875/A59C960A-325C-4197-8087-B4D2C225C426/JMP_News_2015_10_15_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:42:08 [scrapy] INFO: Crawled 1270 pages (at 12 pages/min), scraped 1147 items (at 17 items/min)
2015-11-04 09:42:12 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x855207/A8C88956-FD5D-4833-B087-EC0AB3CCAA3D/JMP_News_2015_10_19_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:43:13 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=1670> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:43:43 [scrapy] ERROR: Error downloading <GET https://www.bmo.com/privatebank/asia/sc>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 09:43:43 [scrapy] INFO: Crawled 1289 pages (at 19 pages/min), scraped 1163 items (at 16 items/min)
2015-11-04 09:43:44 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x857790/6ED9B17E-68A6-46EE-865C-B38C387D549E/JMP_News_2015_10_30_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:44:19 [scrapy] INFO: Crawled 1303 pages (at 14 pages/min), scraped 1168 items (at 5 items/min)
2015-11-04 09:44:20 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2842> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:44:20 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=1991> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:44:20 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2995> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:44:40 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x856085/6E4D19CC-2BE0-4298-9B1C-22EB2E7D514F/JMP_News_2015_10_23_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:45:11 [scrapy] INFO: Crawled 1308 pages (at 5 pages/min), scraped 1174 items (at 6 items/min)
2015-11-04 09:45:11 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2976> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:45:12 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2890> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:45:17 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=1998> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:45:18 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2151> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:45:28 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1246360%2D15%2D3219/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:45:49 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1246360%2D15%2D3222/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:46:19 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2920> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:46:20 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1246360%2D15%2D3403/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:46:30 [scrapy] INFO: Crawled 1354 pages (at 46 pages/min), scraped 1206 items (at 32 items/min)
2015-11-04 09:46:56 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1157523%2D15%2D3404/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:47:11 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1246360%2D15%2D3405/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:47:12 [scrapy] INFO: Crawled 1368 pages (at 14 pages/min), scraped 1217 items (at 11 items/min)
2015-11-04 09:47:12 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x824181/78C18505-7F34-4B76-859C-1A46D2C81D72/JMP_News_2015_4_28_General_Releases.pdf> (referer: http://investor.jmpg.com/releases.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:47:13 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1246360%2D15%2D3443/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:47:47 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x851116/DC2FA412-FFE6-4958-9E4E-1B3C76C46C07/JMP_Group_LLC_09-21-15_.pdf> (referer: http://investor.jmpg.com/events.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:48:14 [scrapy] INFO: Crawled 1443 pages (at 75 pages/min), scraped 1282 items (at 65 items/min)
2015-11-04 09:48:15 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x292461/1095F624-295F-4CE2-9D8A-1EBCE8461415/JMP08AR.pdf> (referer: http://investor.jmpg.com/results.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:49:03 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x372463/61157357-7239-4BE2-ABB9-64906A2F5316/JMP09AR.pdf> (referer: http://investor.jmpg.com/results.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:49:13 [scrapy] INFO: Crawled 1472 pages (at 29 pages/min), scraped 1308 items (at 26 items/min)
2015-11-04 09:50:17 [scrapy] INFO: Crawled 1486 pages (at 14 pages/min), scraped 1335 items (at 27 items/min)
2015-11-04 09:51:23 [scrapy] INFO: Crawled 1529 pages (at 43 pages/min), scraped 1374 items (at 39 items/min)
2015-11-04 09:51:53 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0x465122/827B7BD3-DAB4-44A1-B2D7-67108FD1E81C/JMP_Group_2010_Annual_Report.pdf> (referer: http://investor.jmpg.com/results.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 09:52:15 [scrapy] INFO: Crawled 1613 pages (at 84 pages/min), scraped 1444 items (at 70 items/min)
2015-11-04 09:53:41 [scrapy] INFO: Crawled 1673 pages (at 60 pages/min), scraped 1509 items (at 65 items/min)
2015-11-04 09:55:06 [scrapy] INFO: Crawled 1753 pages (at 80 pages/min), scraped 1568 items (at 59 items/min)
2015-11-04 09:56:14 [scrapy] INFO: Crawled 1767 pages (at 14 pages/min), scraped 1598 items (at 30 items/min)
2015-11-04 09:57:50 [scrapy] INFO: Crawled 1805 pages (at 38 pages/min), scraped 1635 items (at 37 items/min)
2015-11-04 09:58:25 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 09:58:25 [scrapy] INFO: Crawled 1836 pages (at 31 pages/min), scraped 1650 items (at 15 items/min)
2015-11-04 09:59:07 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/dir//3440+Preston+Ridge+Road+Suite+350+Alpharetta,+GA+30005>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:59:07 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/dir//12+Paoli+Pike+Suite+1A+Paoli,+PA+19301>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:59:07 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/dir//265+Franklin+Street+10th+Floor+Boston,+MA+02110>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:59:07 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/dir//190+South+LaSalle+Street+Suite+1610+Chicago,+IL+60603>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:59:07 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/dir//600+Montgomery+Street+Suite+1100+San+Francisco,+CA+94111>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:59:07 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/dir//450+Park+Avenue+5th+Floor+New+York,+NY+10022>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 09:59:08 [scrapy] INFO: Crawled 1907 pages (at 71 pages/min), scraped 1725 items (at 75 items/min)
2015-11-04 09:59:41 [scrapy] ERROR: Error downloading <GET https://support.microsoft.com/kb/2536204>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 09:59:50 [scrapy] ERROR: Error downloading <GET https://www.greendot.com/greendot>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:00:28 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39764/berkshireraises-1615mfornewstrategy.pdf> (referer: http://www.berkshire-group.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:00:28 [scrapy] ERROR: Error downloading <GET https://www.lookout.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:00:28 [scrapy] INFO: Crawled 2014 pages (at 107 pages/min), scraped 1840 items (at 115 items/min)
2015-11-04 10:00:55 [scrapy] ERROR: Error downloading <GET https://www.bettercloud.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_bytes', 'ssl handshake failure')]>]
2015-11-04 10:01:17 [scrapy] INFO: Crawled 2068 pages (at 54 pages/min), scraped 1894 items (at 54 items/min)
2015-11-04 10:02:02 [scrapy] ERROR: Spider error processing <GET http://www.zend.com/> (referer: http://sapphireventures.com/companies/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 10:02:26 [scrapy] INFO: Crawled 2127 pages (at 59 pages/min), scraped 1954 items (at 60 items/min)
2015-11-04 10:03:18 [scrapy] INFO: Crawled 2152 pages (at 25 pages/min), scraped 1981 items (at 27 items/min)
2015-11-04 10:03:43 [scrapy] ERROR: Error downloading <GET http://www.recommind.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:03:43 [scrapy] ERROR: Error downloading <GET https://squareup.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:04:05 [scrapy] ERROR: Error downloading <GET https://www.mirantis.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:04:05 [scrapy] ERROR: Error downloading <GET https://qumu.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 10:04:17 [scrapy] ERROR: Error downloading <GET https://business.twitter.com/help/how-twitter-ads-work>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:04:17 [scrapy] ERROR: Error downloading <GET https://pingidentity.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:04:17 [scrapy] INFO: Crawled 2199 pages (at 47 pages/min), scraped 2029 items (at 48 items/min)
2015-11-04 10:05:15 [scrapy] INFO: Crawled 2230 pages (at 31 pages/min), scraped 2061 items (at 32 items/min)
2015-11-04 10:06:14 [scrapy] INFO: Crawled 2262 pages (at 32 pages/min), scraped 2089 items (at 28 items/min)
2015-11-04 10:07:12 [scrapy] INFO: Crawled 2288 pages (at 26 pages/min), scraped 2118 items (at 29 items/min)
2015-11-04 10:08:18 [scrapy] INFO: Crawled 2320 pages (at 32 pages/min), scraped 2150 items (at 32 items/min)
2015-11-04 10:09:14 [scrapy] INFO: Crawled 2355 pages (at 35 pages/min), scraped 2182 items (at 32 items/min)
2015-11-04 10:09:52 [scrapy] ERROR: Error downloading <GET http://www.berkshire-group.com/media/39800/berkshireinter_am0615_2.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.berkshire-group.com/media/39800/berkshireinter_am0615_2.pdf took longer than 180.0 seconds..
2015-11-04 10:10:17 [scrapy] INFO: Crawled 2384 pages (at 29 pages/min), scraped 2214 items (at 32 items/min)
2015-11-04 10:11:10 [scrapy] INFO: Crawled 2415 pages (at 31 pages/min), scraped 2242 items (at 28 items/min)
2015-11-04 10:12:23 [scrapy] INFO: Crawled 2444 pages (at 29 pages/min), scraped 2274 items (at 32 items/min)
2015-11-04 10:13:16 [scrapy] INFO: Crawled 2470 pages (at 26 pages/min), scraped 2301 items (at 27 items/min)
2015-11-04 10:14:07 [scrapy] INFO: Crawled 2505 pages (at 35 pages/min), scraped 2327 items (at 26 items/min)
2015-11-04 10:15:16 [scrapy] INFO: Crawled 2541 pages (at 36 pages/min), scraped 2362 items (at 35 items/min)
2015-11-04 10:16:25 [scrapy] INFO: Crawled 2567 pages (at 26 pages/min), scraped 2397 items (at 35 items/min)
2015-11-04 10:17:10 [scrapy] INFO: Crawled 2587 pages (at 20 pages/min), scraped 2417 items (at 20 items/min)
2015-11-04 10:18:25 [scrapy] INFO: Crawled 2619 pages (at 32 pages/min), scraped 2449 items (at 32 items/min)
2015-11-04 10:19:16 [scrapy] INFO: Crawled 2652 pages (at 33 pages/min), scraped 2479 items (at 30 items/min)
2015-11-04 10:19:32 [scrapy] ERROR: Error downloading <GET https://www.mozilla.org/firefox/?utm_source=firefox-com&utm_medium=referral>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:20:17 [scrapy] INFO: Crawled 2693 pages (at 41 pages/min), scraped 2507 items (at 28 items/min)
2015-11-04 10:21:19 [scrapy] INFO: Crawled 2720 pages (at 27 pages/min), scraped 2544 items (at 37 items/min)
2015-11-04 10:22:23 [scrapy] INFO: Crawled 2774 pages (at 54 pages/min), scraped 2596 items (at 52 items/min)
2015-11-04 10:22:42 [scrapy] ERROR: Error downloading <GET https://www.lifelock.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:23:01 [scrapy] ERROR: Error downloading <GET https://www.fitbit.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:23:20 [scrapy] INFO: Crawled 2813 pages (at 39 pages/min), scraped 2639 items (at 43 items/min)
2015-11-04 10:24:08 [scrapy] ERROR: Error downloading <GET https://asug.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:24:08 [scrapy] ERROR: Error downloading <GET https://www.iovation.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 10:24:08 [scrapy] INFO: Crawled 2845 pages (at 32 pages/min), scraped 2671 items (at 32 items/min)
2015-11-04 10:24:46 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/WatchDox>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:25:17 [scrapy] INFO: Crawled 2894 pages (at 49 pages/min), scraped 2723 items (at 52 items/min)
2015-11-04 10:26:01 [scrapy] ERROR: Error downloading <GET https://www.currencycloud.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:26:20 [scrapy] INFO: Crawled 2947 pages (at 53 pages/min), scraped 2775 items (at 52 items/min)
2015-11-04 10:27:18 [scrapy] INFO: Crawled 3015 pages (at 68 pages/min), scraped 2836 items (at 61 items/min)
2015-11-04 10:27:44 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2112> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:28:40 [scrapy] INFO: Crawled 3061 pages (at 46 pages/min), scraped 2872 items (at 36 items/min)
2015-11-04 10:29:28 [scrapy] ERROR: Error downloading <GET https://www21.bmo.com?eaiLocaleString=fr>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:29:28 [scrapy] INFO: Crawled 3062 pages (at 1 pages/min), scraped 2892 items (at 20 items/min)
2015-11-04 10:30:25 [scrapy] INFO: Crawled 3062 pages (at 0 pages/min), scraped 2897 items (at 5 items/min)
2015-11-04 10:30:36 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1246360%2D15%2D3442/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:31:07 [scrapy] INFO: Crawled 3131 pages (at 69 pages/min), scraped 2939 items (at 42 items/min)
2015-11-04 10:32:16 [scrapy] INFO: Crawled 3227 pages (at 96 pages/min), scraped 3022 items (at 83 items/min)
2015-11-04 10:33:09 [scrapy] ERROR: Error downloading <GET https://newsroom.bmo.com/email-registration>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:33:09 [scrapy] INFO: Crawled 3312 pages (at 85 pages/min), scraped 3094 items (at 72 items/min)
2015-11-04 10:33:16 [scrapy] ERROR: Spider error processing <GET http://files.shareholder.com/downloads/JMPG/964053395x0xS1437749%2D15%2D19501/1302350/filing.pdf> (referer: http://investor.jmpg.com/sec.cfm)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:33:44 [scrapy] ERROR: Error downloading <GET https://sysys.hostpilot.com/>: DNS lookup failed: address 'sysys.hostpilot.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:33:45 [scrapy] ERROR: Error downloading <GET http://www.hedgeweek.com/2013/03/01/180965/fourth-hedgeweek-awards-celebrate-winners-tough-year>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://feedback-form.truste.com/watchdog/request>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://itunes.apple.com/ca/artist/q4-web-systems/id479636699>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://itunes.apple.com/ca/app/q4-touch/id944844669?mt=8>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/developer?hl=en&id=Q4+Web+Systems>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?hl=en&id=com.q4websystems.Q4App>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?hl=en&hnear=17+State+St%2C+New+York%2C+10004&ll=40.716883%2C-73.989744&oq=17+&q=17+State+St%2C+New+York%2C+NY&sll=40.75701%2C-73.972063&spn=0.046839%2C0.104628&sspn=0.023406%2C0.052314&t=m&z=14>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://www.man.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://plus.google.com/114288036615758881029/about>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/place/3485+N+Pines+Way/@43.5304,-110.843,17z/data=!3m1!4b1!4m2!3m1!1s0x53530ff2ab32d793:0x6be7eec3e0f1b2f3>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET http://www.patch.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:34:00 [scrapy] ERROR: Error downloading <GET https://www.man.com/products>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:07 [scrapy] ERROR: Error downloading <GET https://services.intralinks.com/servlets/Reset?step=start>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:07 [scrapy] ERROR: Error downloading <GET https://services.intralinks.com/branding/2752428579/?clientID=2752428579>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:09 [scrapy] INFO: Crawled 3369 pages (at 57 pages/min), scraped 3169 items (at 75 items/min)
2015-11-04 10:34:09 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:09 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:09 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:09 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:34:09 [scrapy] ERROR: Error downloading <GET https://www.dashboard3.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:34:36 [scrapy] ERROR: Error downloading <GET https://www.nyse.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:34:53 [scrapy] ERROR: Error downloading <GET https://www.secure.bmoinvestorline.com>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:08 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/site/printcart.php>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:08 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_corvetti?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:08 [scrapy] INFO: Crawled 3420 pages (at 51 pages/min), scraped 3210 items (at 41 items/min)
2015-11-04 10:35:19 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET http://www.mainlineco.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_janchar?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/investment-team>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/portfolio-company-interaction>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/investment-strategy>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/stockbridge-investors-team>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:32 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_ascione?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:33 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:33 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:35:33 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/industry-expertise>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:33 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/types-of-investments>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:41 [scrapy] ERROR: Spider error processing <GET http://www.glgpartners.com/Download?guid=dd7f4a8d-ff48-4b8c-9a5b-d3c832e024d9> (referer: http://www.glgpartners.com/res/leadership-team)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:35:43 [scrapy] ERROR: Spider error processing <GET http://www.glgpartners.com/Download?guid=8a577d9c-cdb8-4694-9374-fdc2f28374cb> (referer: http://www.glgpartners.com/res/leadership-team)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:35:49 [scrapy] ERROR: Error downloading <GET https://spurcapital.firmex.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:35:52 [scrapy] ERROR: Spider error processing <GET http://www.glgpartners.com/Download?guid=85116473-bb31-4387-8436-0f65ae08b464> (referer: http://www.glgpartners.com/res/leadership-team)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:35:54 [scrapy] ERROR: Spider error processing <GET http://www.glgpartners.com/Download?guid=41dade46-2a01-4c9c-92d4-9c07ca03e3e7> (referer: http://www.glgpartners.com/res/leadership-team)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:35:57 [scrapy] ERROR: Spider error processing <GET http://www.glgpartners.com/Download?guid=4035f1bb-240a-4b8f-ac9e-caf5a30b597d> (referer: http://www.glgpartners.com/res/leadership-team)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:36:12 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/websites/web-design>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:36:12 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/hosting/wordpress-hosting>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:36:12 [scrapy] INFO: Crawled 3468 pages (at 48 pages/min), scraped 3243 items (at 33 items/min)
2015-11-04 10:36:44 [scrapy] ERROR: Error downloading <GET https://www.godaddy.com/websites/ecommerce-website-design>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:36:45 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_kuo>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:36:58 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_mckenna>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/approach-to-realizations>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_scheinfeld?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/investment-parameters>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/how-we-work>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/investment-principles>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/investment-approach>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/in-the-community>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://retailsolutions.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] ERROR: Error downloading <GET http://imprivata.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:12 [scrapy] INFO: Crawled 3495 pages (at 27 pages/min), scraped 3276 items (at 33 items/min)
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://returnpath.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://docusign.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://abovethecrowd.com/2012/09/04/the-dangerous-seduction-of-the-lifetime-value-ltv-formula/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://abovethecrowd.com/2015/02/25/investors-beware/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://www.localytics.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET https://www.bmorewards.com/en/landing.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/for-the-media>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://jibe.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:22 [scrapy] ERROR: Error downloading <GET http://www.bvp.com/cloud/law2>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?gl=us&hnear=4+Rue+Lou+Hemmer%2C+Niederanven%2C+Luxembourg&q=4+rue+Lou+Hemmer+L-1748+Luxembourg&t=m&z=16>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?aq=&f=q&geocode=&hl=en&ie=UTF8&q=35.685189%2C+139.761516&sll=42.036922%2C-71.683501&source=s_q&sspn=2.680322%2C8.371582&t=m&vpsrc=3&z=16>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://www.bmomutualfunds.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://m2.bmo.com/mobile/apps/services/www/BMOMobileBanking/mobilewebapp/default/BMOMobileBanking.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://www.cbinsights.com/blog/misinterpretations-cltv-cac-saas-metrics/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://www22.bmo.com/ctpauth/CTPEAILogin/CustUserPasswordAuthServlet?AUTHNLEVEL=&ERROR_CODE=0x00000000&HOSTNAME=www22.bmo.com&PROTOCOL=https&TAM_OP=login&URL=%2Fen>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/place/Ebene+Heights,+34,+Quatre+Bornes,+Mauritius/@-20.2428285,57.4922798,18z/data=!3m1!4b1!4m7!1m4!3m3!1s0x217c5ae214052969:0x59f599a32899287a!2sEbene+Heights,+34,+Quatre+Bornes,+Mauritius!3b1!3m1!1s0x217c5ae214052969:0x59f599a32899287a>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET https://www.google.com/maps/place/5+Fitzwilliam+Square+E,+Dublin+2,+Ireland/@53.335146,-6.2504329,17z/data=!3m1!4b1!4m2!3m1!1s0x48670e97e1e3cb83:0x8ce7d42294b72180>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET http://loglogic.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/investment-in-opening-ceremony>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_pappas?cid=11313>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET http://dssd.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET http://criteo.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET http://blackducksoftware.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:23 [scrapy] ERROR: Error downloading <GET http://alteryx.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:31 [scrapy] ERROR: Error downloading <GET http://convercent.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:47 [scrapy] ERROR: Error downloading <GET https://www.saastr.com/the-dry-bubble-we-may-be-in-what-that-means/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:37:50 [scrapy] ERROR: Error downloading <GET http://www.tellme.com/>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:38:00 [scrapy] ERROR: Spider error processing <GET https://in.godaddy.com/hosting/web-hosting> (referer: https://www.godaddy.com/?isc=instantpage_311&showip=true)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 642, in _read_chunked
    raise IncompleteRead(''.join(value))
IncompleteRead: IncompleteRead(5317 bytes read)
2015-11-04 10:38:02 [scrapy] ERROR: Spider error processing <GET https://in.godaddy.com/hosting/dedicated-ip> (referer: https://www.godaddy.com/?isc=instantpage_311&showip=true)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 642, in _read_chunked
    raise IncompleteRead(''.join(value))
IncompleteRead: IncompleteRead(7012 bytes read)
2015-11-04 10:38:52 [scrapy] INFO: Crawled 3587 pages (at 92 pages/min), scraped 3363 items (at 87 items/min)
2015-11-04 10:38:53 [scrapy] ERROR: Spider error processing <GET http://www.mutualfundsadvisor.bmo.com/advisor/marketing/document?documentId=2955> (referer: http://www.bmo.com/privatebanking/our-services/investment-management)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:39:05 [scrapy] ERROR: Error downloading <GET https://in.godaddy.com/websites/online-store>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:39:05 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/dick-portillo-reflects-on-sale-of-eponymous-restaurant-chain>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:05 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/in-the-news-1?year=1991>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:05 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/in-the-news-1?year=1990>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:05 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/in-the-news-1?year=2011>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:36 [scrapy] INFO: Crawled 3602 pages (at 15 pages/min), scraped 3383 items (at 20 items/min)
2015-11-04 10:39:37 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/coty>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:37 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/current_and_former_portfolio_companies>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:37 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:39:37 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:39:37 [scrapy] ERROR: Error downloading <GET http://www.oldmutualus.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:39:37 [scrapy] ERROR: Error downloading <GET http://www.vscapitalpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:39:47 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_b_levy?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:47 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_spirn?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:47 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/case-studies-ews>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:47 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_nuger?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:47 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_johnson?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:47 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_kuo?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:47 [scrapy] ERROR: Error downloading <GET http://www.reardencommerce.com/>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?video=3000387982>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?play=1&video=3000394401>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?play=1&video=3000389690>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?play=1&video=3000420316>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?play=1&video=3000424343>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.meetmax.com/jmptech2016.html>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.janrain.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/channel/UCU15ELvJ1AcdL-c_-n_JCjQ>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET https://maps.google.com/maps?aq=0&f=q&geocode=&hl=en&hnear=32+6th+Ave%2C+New+York%2C+10013&hq=&ie=UTF8&ll=40.72009%2C-74.004833&oq=32+Avenue+of+the+Americas&q=32+Avenue+of+the+Americas%2C+New+York%2C+NY&sll=37.0625%2C-95.677068&source=embed&sspn=59.986788%2C62.138672&t=m&z=14>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/press-releases-2?year=2001>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/press-releases-2?year=2007>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/press-releases-2?year=2010>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/press-releases-2?year=2009>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/press-releases-2?year=2008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:39:59 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/telx>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:15 [scrapy] ERROR: Error downloading <GET https://ie.godaddy.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:15 [scrapy] ERROR: Error downloading <GET https://ar.godaddy.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:15 [scrapy] ERROR: Error downloading <GET https://de.godaddy.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:15 [scrapy] ERROR: Error downloading <GET https://br.godaddy.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:15 [scrapy] INFO: Crawled 3634 pages (at 32 pages/min), scraped 3413 items (at 30 items/min)
2015-11-04 10:40:27 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/portillo%E2%80%99s>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:27 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_janchar?cid=11309>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:27 [scrapy] ERROR: Error downloading <GET http://www.hig>: DNS lookup failed: address 'www.hig' not found: [Errno -2] Name or service not known.
2015-11-04 10:40:27 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/srs>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:40:27 [scrapy] ERROR: Error downloading <GET http://www.berkshirepartners.com/bio_dacey?d=10008>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 10:41:09 [scrapy] INFO: Crawled 3745 pages (at 111 pages/min), scraped 3508 items (at 95 items/min)
2015-11-04 10:42:22 [scrapy] INFO: Crawled 3833 pages (at 88 pages/min), scraped 3589 items (at 81 items/min)
2015-11-04 10:42:40 [scrapy] ERROR: Spider error processing <GET http://blog.parallels.com> (referer: http://www.parallels.com/products/desktop/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 10:42:55 [scrapy] ERROR: Error downloading <GET https://plus.google.com/108886344408285245645/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:42:55 [scrapy] ERROR: Error downloading <GET https://plus.google.com/communities/109881979300958500728>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:42:56 [scrapy] ERROR: Error downloading <GET https://plus.google.com/115664609675970560466>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:43:08 [scrapy] INFO: Crawled 3871 pages (at 38 pages/min), scraped 3632 items (at 43 items/min)
2015-11-04 10:43:52 [scrapy] INFO: Closing spider (finished)
2015-11-04 10:43:52 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1095,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 19,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 57,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 37,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 29,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 12,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 938,
 'downloader/request_bytes': 2811310,
 'downloader/request_count': 5825,
 'downloader/request_method_count/GET': 5825,
 'downloader/response_bytes': 122606254,
 'downloader/response_count': 4730,
 'downloader/response_status_count/200': 3797,
 'downloader/response_status_count/301': 470,
 'downloader/response_status_count/302': 320,
 'downloader/response_status_count/303': 6,
 'downloader/response_status_count/400': 4,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 3,
 'downloader/response_status_count/404': 71,
 'downloader/response_status_count/408': 34,
 'downloader/response_status_count/416': 1,
 'downloader/response_status_count/429': 1,
 'downloader/response_status_count/504': 6,
 'downloader/response_status_count/999': 15,
 'dupefilter/filtered': 9692,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 10, 43, 52, 380463),
 'item_scraped_count': 3656,
 'log_count/ERROR': 264,
 'log_count/INFO': 101,
 'offsite/domains': 94,
 'offsite/filtered': 647,
 'request_depth_max': 2,
 'response_received_count': 3886,
 'scheduler/dequeued': 5825,
 'scheduler/dequeued/memory': 5825,
 'scheduler/enqueued': 5825,
 'scheduler/enqueued/memory': 5825,
 'spider_exceptions/AttributeError': 34,
 'spider_exceptions/IncompleteRead': 2,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/RuntimeError': 1,
 'spider_exceptions/TypeError': 19,
 'spider_exceptions/XMLSyntaxError': 6,
 'spider_exceptions/error': 1,
 'start_time': datetime.datetime(2015, 11, 4, 9, 0, 7, 186578)}
2015-11-04 10:43:52 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 10:44:55 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 10:44:55 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 10:44:55 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 10:44:55 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 10:44:55 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 10:44:55 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 10:44:55 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 10:44:55 [scrapy] INFO: Spider opened
2015-11-04 10:44:55 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.imc>: DNS lookup failed: address 'www.imc' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.hig>: DNS lookup failed: address 'www.hig' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.bpc>: DNS lookup failed: address 'www.bpc' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.ecp.altareturn.com>: DNS lookup failed: address 'www.ecp.altareturn.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.tet>: DNS lookup failed: address 'www.tet' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.formulainvesting.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:44:55 [scrapy] ERROR: Error downloading <GET http://www.lp.lcpartners.com>: DNS lookup failed: address 'www.lp.lcpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:56 [scrapy] ERROR: Error downloading <GET https://www.magnitudecapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL3_GET_RECORD', 'wrong version number')]>]
2015-11-04 10:44:56 [scrapy] ERROR: Error downloading <GET http://www.dai>: DNS lookup failed: address 'www.dai' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:56 [scrapy] ERROR: Error downloading <GET http://www.exp>: DNS lookup failed: address 'www.exp' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:56 [scrapy] ERROR: Error downloading <GET http://www.5tides.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:44:56 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 10:44:57 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 10:45:02 [scrapy] ERROR: Error downloading <GET http://www.glaxisllc.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:45:02 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 10:45:03 [scrapy] ERROR: Error downloading <GET http://www.tit>: DNS lookup failed: address 'www.tit' not found: [Errno -2] Name or service not known.
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/philosophy/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/investing-for-impact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/contact/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/investing/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:35 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/solutions/transaction-types/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/overview/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/about/executive-team/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:44 [scrapy] ERROR: Error downloading <GET https://brevetcapital.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:45:55 [scrapy] INFO: Crawled 189 pages (at 189 pages/min), scraped 111 items (at 111 items/min)
2015-11-04 10:46:55 [scrapy] INFO: Crawled 189 pages (at 0 pages/min), scraped 111 items (at 0 items/min)
2015-11-04 10:47:55 [scrapy] INFO: Crawled 189 pages (at 0 pages/min), scraped 111 items (at 0 items/min)
2015-11-04 10:48:03 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:48:55 [scrapy] INFO: Crawled 189 pages (at 0 pages/min), scraped 111 items (at 0 items/min)
2015-11-04 10:49:55 [scrapy] INFO: Crawled 189 pages (at 0 pages/min), scraped 111 items (at 0 items/min)
2015-11-04 10:50:55 [scrapy] INFO: Crawled 189 pages (at 0 pages/min), scraped 111 items (at 0 items/min)
2015-11-04 10:51:17 [scrapy] ERROR: Error downloading <GET http://www.intrepidcap.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:51:17 [scrapy] ERROR: Error downloading <GET http://www.ironsidespartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:51:17 [scrapy] ERROR: Error downloading <GET http://www.feplp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:51:17 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 10:51:18 [scrapy] INFO: Closing spider (finished)
2015-11-04 10:51:18 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 111,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 9,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 42,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 12,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 39,
 'downloader/request_bytes': 88925,
 'downloader/request_count': 356,
 'downloader/request_method_count/GET': 356,
 'downloader/response_bytes': 1091741,
 'downloader/response_count': 245,
 'downloader/response_status_count/200': 187,
 'downloader/response_status_count/301': 23,
 'downloader/response_status_count/302': 29,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 1,
 'dupefilter/filtered': 229,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 10, 51, 18, 13782),
 'item_scraped_count': 111,
 'log_count/ERROR': 37,
 'log_count/INFO': 13,
 'offsite/domains': 61,
 'offsite/filtered': 299,
 'request_depth_max': 2,
 'response_received_count': 189,
 'scheduler/dequeued': 356,
 'scheduler/dequeued/memory': 356,
 'scheduler/enqueued': 356,
 'scheduler/enqueued/memory': 356,
 'start_time': datetime.datetime(2015, 11, 4, 10, 44, 55, 632708)}
2015-11-04 10:51:18 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 10:52:19 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 10:52:19 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 10:52:19 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 10:52:19 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 10:52:19 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 10:52:19 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 10:52:20 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 10:52:20 [scrapy] INFO: Spider opened
2015-11-04 10:52:20 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.riv>: DNS lookup failed: address 'www.riv' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.cmsco.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.inglesideadvisors.com>: DNS lookup failed: address 'www.inglesideadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.enhancedcapct.com>: DNS lookup failed: address 'www.enhancedcapct.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.bnp>: DNS lookup failed: address 'www.bnp' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.lar>: DNS lookup failed: address 'www.lar' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:20 [scrapy] ERROR: Error downloading <GET http://www.citicapitaladvisors.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 10:52:21 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:22 [scrapy] ERROR: Error downloading <GET http://www.investor.pccpllc.amiesdigital.com>: DNS lookup failed: address 'www.investor.pccpllc.amiesdigital.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:22 [scrapy] ERROR: Error downloading <GET http://www.gra>: DNS lookup failed: address 'www.gra' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:26 [scrapy] ERROR: Error downloading <GET http://www.lp.lcpartners.com>: DNS lookup failed: address 'www.lp.lcpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:26 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:26 [scrapy] ERROR: Error downloading <GET http://www.omn>: DNS lookup failed: address 'www.omn' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:30 [scrapy] ERROR: Error downloading <GET http://www.riverside-pm.com>: DNS lookup failed: address 'www.riverside-pm.com' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:30 [scrapy] ERROR: Error downloading <GET http://www.sec>: DNS lookup failed: address 'www.sec' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:30 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 10:52:30 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 10:53:23 [scrapy] INFO: Crawled 226 pages (at 226 pages/min), scraped 119 items (at 119 items/min)
2015-11-04 10:54:04 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39827/17-spot.pdf> (referer: http://www.berkshire-group.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:54:15 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51193482&id=smartlink&index=2&lan=en-US&md5=1d97f15d0085b2ad9ff18003c6868369&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Fourth-Quarter-2015-Preferred-Distributions/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 611: htmlParseEntityRef: no name
2015-11-04 10:54:15 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51201762&id=smartlink&index=2&lan=en-US&md5=7debb5b50ba2b3e82184d018d846e791&newsitemid=20151014006687&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Company-Update-for-September-2015/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 611: htmlParseEntityRef: no name
2015-11-04 10:54:16 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=2&lan=en-US&md5=5158266120384eecdd44b68c41040b1f&newsitemid=20151103007039&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 611: htmlParseEntityRef: no name
2015-11-04 10:54:16 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=1&lan=en-US&md5=3b5f5d52f28fddf355051fd8768bf656&newsitemid=20151103007039&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com%2F> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 611: htmlParseEntityRef: no name
2015-11-04 10:54:16 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51201762&id=smartlink&index=1&lan=en-US&md5=72e6b29e66ffbd45c2cc962332ab5694&newsitemid=20151014006687&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Company-Update-for-September-2015/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 611: htmlParseEntityRef: no name
2015-11-04 10:54:17 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51216562&id=smartlink&index=3&lan=en-US&md5=b2c03c2adec48fc1257e632f5893599f&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Schedules-Release-of-Third-Quarter-2015-Financial-Results/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 611: htmlParseEntityRef: no name
2015-11-04 10:54:17 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.eaglepointcreditcompany.com&esheet=51193482&id=smartlink&index=1&lan=en-US&md5=35501886b80f5ce4fea6c8e2127ad6f5&newsitemid=20151001006977&url=http%3A%2F%2Fwww.eaglepointcreditcompany.com> (referer: http://eaglepointcreditcompany.com/investor-relations/Documents/press-releases/press-release-details/2015/Eagle-Point-Credit-Company-Inc-Announces-Fourth-Quarter-2015-Preferred-Distributions/default.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 611: htmlParseEntityRef: no name
2015-11-04 10:54:22 [scrapy] ERROR: Error downloading <GET https://deutscheawm.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:54:22 [scrapy] INFO: Crawled 319 pages (at 93 pages/min), scraped 217 items (at 98 items/min)
2015-11-04 10:54:54 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/documents/public/global/neuberger_berman_business_continuity_disclosure.pdf> (referer: https://caa.nb.com/?aa=trust)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:55:09 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=%2fDocuments%2fPublic%2fen-us%2fNB_Statement_of_Financial_Condition.pdf> (referer: https://caa.nb.com/?aa=trust)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 10:55:31 [scrapy] INFO: Crawled 430 pages (at 111 pages/min), scraped 324 items (at 107 items/min)
2015-11-04 10:56:22 [scrapy] INFO: Crawled 468 pages (at 38 pages/min), scraped 366 items (at 42 items/min)
2015-11-04 10:56:28 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer/otherversions/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 10:57:21 [scrapy] INFO: Crawled 569 pages (at 101 pages/min), scraped 471 items (at 105 items/min)
2015-11-04 10:58:22 [scrapy] INFO: Crawled 686 pages (at 117 pages/min), scraped 591 items (at 120 items/min)
2015-11-04 10:59:22 [scrapy] INFO: Crawled 798 pages (at 112 pages/min), scraped 704 items (at 113 items/min)
2015-11-04 11:00:20 [scrapy] INFO: Crawled 866 pages (at 68 pages/min), scraped 771 items (at 67 items/min)
2015-11-04 11:00:35 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Prelim-release-Q1FY12.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:00:37 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/open-letter-shareholders.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:00:43 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY11-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:01:03 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY12-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:01:12 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:01:21 [scrapy] INFO: Crawled 952 pages (at 86 pages/min), scraped 849 items (at 78 items/min)
2015-11-04 11:01:58 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:02:26 [scrapy] INFO: Crawled 1031 pages (at 79 pages/min), scraped 927 items (at 78 items/min)
2015-11-04 11:02:27 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:03:04 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:03:17 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:03:21 [scrapy] INFO: Crawled 1111 pages (at 80 pages/min), scraped 1006 items (at 79 items/min)
2015-11-04 11:03:59 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:04:12 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2013-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:04:14 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/misc/company/news/embedded-extends-support-development-boards.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:04:21 [scrapy] INFO: Crawled 1209 pages (at 98 pages/min), scraped 1097 items (at 91 items/min)
2015-11-04 11:05:07 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:05:14 [scrapy] ERROR: Spider error processing <GET https://co.godaddy.com/> (referer: https://sso.godaddy.com?app=idp&path=%2flogin.aspx%3fauto%3dTrue%26domain%3dbluesprucelp.com%26lcid%3dTrue%26prog_id%3dGoDaddy%26redirect%3dtrue%26sd%3d%26spkey%3dGDSIMPLESITEWEB.PROD)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 642, in _read_chunked
    raise IncompleteRead(''.join(value))
IncompleteRead: IncompleteRead(7354 bytes read)
2015-11-04 11:05:17 [scrapy] ERROR: Spider error processing <GET https://cl.godaddy.com/> (referer: https://sso.godaddy.com?app=idp&path=%2flogin.aspx%3fauto%3dTrue%26domain%3dbluesprucelp.com%26lcid%3dTrue%26prog_id%3dGoDaddy%26redirect%3dtrue%26sd%3d%26spkey%3dGDSIMPLESITEWEB.PROD)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 642, in _read_chunked
    raise IncompleteRead(''.join(value))
IncompleteRead: IncompleteRead(7079 bytes read)
2015-11-04 11:05:21 [scrapy] INFO: Crawled 1263 pages (at 54 pages/min), scraped 1143 items (at 46 items/min)
2015-11-04 11:05:53 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:06:26 [scrapy] INFO: Crawled 1331 pages (at 68 pages/min), scraped 1213 items (at 70 items/min)
2015-11-04 11:07:22 [scrapy] INFO: Crawled 1383 pages (at 52 pages/min), scraped 1264 items (at 51 items/min)
2015-11-04 11:07:25 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:07:26 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2014-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:07:53 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:07:59 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:08:13 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:08:13 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2015-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:08:32 [scrapy] INFO: Crawled 1463 pages (at 80 pages/min), scraped 1334 items (at 70 items/min)
2015-11-04 11:08:41 [scrapy] ERROR: Error downloading <GET https://www.northerntrust.com/asset-servicing/united-states/services/fund-services/hedge-funds>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:08:50 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY2016-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:09:38 [scrapy] INFO: Crawled 1535 pages (at 72 pages/min), scraped 1406 items (at 72 items/min)
2015-11-04 11:09:38 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2016-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:10:40 [scrapy] INFO: Crawled 1579 pages (at 44 pages/min), scraped 1445 items (at 39 items/min)
2015-11-04 11:11:28 [scrapy] INFO: Crawled 1616 pages (at 37 pages/min), scraped 1472 items (at 27 items/min)
2015-11-04 11:12:21 [scrapy] INFO: Crawled 1664 pages (at 48 pages/min), scraped 1517 items (at 45 items/min)
2015-11-04 11:13:20 [scrapy] INFO: Crawled 1707 pages (at 43 pages/min), scraped 1557 items (at 40 items/min)
2015-11-04 11:14:07 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/certus-ds.pdf> (referer: https://www.mentor.com/products/fv/certus-silicon-debug)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:14:16 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/presentation/products/fv/formal-model-checking.pdf> (referer: https://www.mentor.com/products/fv/questa-property-checking)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:14:21 [scrapy] INFO: Crawled 1739 pages (at 32 pages/min), scraped 1589 items (at 32 items/min)
2015-11-04 11:14:28 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/codelink/codelink-hve-ds.pdf> (referer: https://www.mentor.com/products/fv/codelink/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:14:52 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/presentation/products/fv/questa-covercheck/adv-verification-mgmt-coverage-closure.pdf> (referer: https://www.mentor.com/products/fv/questa-covercheck/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:15:34 [scrapy] INFO: Crawled 1765 pages (at 26 pages/min), scraped 1615 items (at 26 items/min)
2015-11-04 11:15:44 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/visualizer-debug-environment.pdf> (referer: https://www.mentor.com/products/fv/visualizer-debug)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:15:47 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce2-brochure.pdf> (referer: https://www.mentor.com/products/fv/verification-debug)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:16:38 [scrapy] INFO: Crawled 1802 pages (at 37 pages/min), scraped 1647 items (at 32 items/min)
2015-11-04 11:17:23 [scrapy] INFO: Crawled 1830 pages (at 28 pages/min), scraped 1676 items (at 29 items/min)
2015-11-04 11:18:21 [scrapy] INFO: Crawled 1863 pages (at 33 pages/min), scraped 1708 items (at 32 items/min)
2015-11-04 11:19:30 [scrapy] INFO: Crawled 1899 pages (at 36 pages/min), scraped 1743 items (at 35 items/min)
2015-11-04 11:20:22 [scrapy] INFO: Crawled 1928 pages (at 29 pages/min), scraped 1768 items (at 25 items/min)
2015-11-04 11:20:43 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/questa-formal-Based-technology-ds.pdf> (referer: https://www.mentor.com/products/fv/questa-x-check)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:21:22 [scrapy] INFO: Crawled 1964 pages (at 36 pages/min), scraped 1806 items (at 38 items/min)
2015-11-04 11:22:08 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/transforming-verification.pdf> (referer: https://www.mentor.com/products/fv/questa-x-check)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:22:23 [scrapy] INFO: Crawled 1997 pages (at 33 pages/min), scraped 1837 items (at 31 items/min)
2015-11-04 11:23:20 [scrapy] INFO: Crawled 2025 pages (at 28 pages/min), scraped 1865 items (at 28 items/min)
2015-11-04 11:24:20 [scrapy] INFO: Crawled 2065 pages (at 40 pages/min), scraped 1909 items (at 44 items/min)
2015-11-04 11:25:27 [scrapy] INFO: Crawled 2134 pages (at 69 pages/min), scraped 1959 items (at 50 items/min)
2015-11-04 11:26:24 [scrapy] INFO: Crawled 2166 pages (at 32 pages/min), scraped 1993 items (at 34 items/min)
2015-11-04 11:27:34 [scrapy] INFO: Crawled 2236 pages (at 70 pages/min), scraped 2057 items (at 64 items/min)
2015-11-04 11:28:23 [scrapy] INFO: Crawled 2266 pages (at 30 pages/min), scraped 2094 items (at 37 items/min)
2015-11-04 11:29:20 [scrapy] INFO: Crawled 2303 pages (at 37 pages/min), scraped 2122 items (at 28 items/min)
2015-11-04 11:30:23 [scrapy] INFO: Crawled 2330 pages (at 27 pages/min), scraped 2157 items (at 35 items/min)
2015-11-04 11:30:23 [scrapy] ERROR: Error downloading <GET https://www.altera.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:30:38 [scrapy] ERROR: Error downloading <GET https://www.codasip.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:31:25 [scrapy] INFO: Crawled 2389 pages (at 59 pages/min), scraped 2206 items (at 49 items/min)
2015-11-04 11:32:22 [scrapy] INFO: Crawled 2495 pages (at 106 pages/min), scraped 2288 items (at 82 items/min)
2015-11-04 11:33:23 [scrapy] INFO: Crawled 2554 pages (at 59 pages/min), scraped 2367 items (at 79 items/min)
2015-11-04 11:34:25 [scrapy] INFO: Crawled 2617 pages (at 63 pages/min), scraped 2433 items (at 66 items/min)
2015-11-04 11:35:03 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/questa-cdc-ds.pdf> (referer: https://www.mentor.com/products/fv/success/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:35:22 [scrapy] INFO: Crawled 2698 pages (at 81 pages/min), scraped 2504 items (at 71 items/min)
2015-11-04 11:36:28 [scrapy] INFO: Crawled 2758 pages (at 60 pages/min), scraped 2562 items (at 58 items/min)
2015-11-04 11:37:25 [scrapy] INFO: Crawled 2801 pages (at 43 pages/min), scraped 2604 items (at 42 items/min)
2015-11-04 11:38:23 [scrapy] INFO: Crawled 2885 pages (at 84 pages/min), scraped 2677 items (at 73 items/min)
2015-11-04 11:39:01 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/virtual-pcie-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:39:05 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtual-usb-3-0-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:39:13 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtual-usb-2-0-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:39:21 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtual-ethernet-device-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:39:21 [scrapy] INFO: Crawled 2931 pages (at 46 pages/min), scraped 2724 items (at 47 items/min)
2015-11-04 11:39:31 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/virtual-mulitmedia-device.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:39:33 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/veloce-virtualab-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/virtual-devices)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:40:23 [scrapy] INFO: Crawled 3006 pages (at 75 pages/min), scraped 2790 items (at 66 items/min)
2015-11-04 11:41:20 [scrapy] INFO: Crawled 3063 pages (at 57 pages/min), scraped 2849 items (at 59 items/min)
2015-11-04 11:42:31 [scrapy] INFO: Crawled 3137 pages (at 74 pages/min), scraped 2918 items (at 69 items/min)
2015-11-04 11:43:25 [scrapy] INFO: Crawled 3188 pages (at 51 pages/min), scraped 2968 items (at 50 items/min)
2015-11-04 11:44:28 [scrapy] INFO: Crawled 3279 pages (at 91 pages/min), scraped 3044 items (at 76 items/min)
2015-11-04 11:45:24 [scrapy] INFO: Crawled 3380 pages (at 101 pages/min), scraped 3129 items (at 85 items/min)
2015-11-04 11:46:09 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=documents%2Fpublic%2Fen-us%2Fethics_and_compliance_committee_charter.pdf> (referer: http://www.nb.com/pages/public/global/fund-governance.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:14 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=documents%2Fpublic%2Fen-us%2Fcontract_review_committee_charter.pdf> (referer: http://www.nb.com/pages/public/global/fund-governance.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:14 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=documents%2Fpublic%2Fen-us%2Faudit_committee_charter.pdf> (referer: http://www.nb.com/pages/public/global/fund-governance.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:15 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=documents%2Fpublic%2Fen-us%2Finvestment_performance_committee_charter.pdf> (referer: http://www.nb.com/pages/public/global/fund-governance.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:15 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=documents%2Fpublic%2Fen-us%2Fgovernance_and_nominating_committee_charter.pdf> (referer: http://www.nb.com/pages/public/global/fund-governance.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:15 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=documents%2Fpublic%2Fen-us%2Fexecutive_committee_charter.pdf> (referer: http://www.nb.com/pages/public/global/fund-governance.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:17 [scrapy] ERROR: Spider error processing <GET http://www.nb.com/_layouts/www/ap/downloadasset.aspx?asset=documents%2Fpublic%2Fen-us%2Fclosed-end_funds_committee_charter.pdf> (referer: http://www.nb.com/pages/public/global/fund-governance.aspx)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 11:46:28 [scrapy] INFO: Crawled 3477 pages (at 97 pages/min), scraped 3218 items (at 89 items/min)
2015-11-04 11:46:47 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/industry_article/products/fv/hardware-emulators.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/veloce)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:46:47 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/industry_article/resources/industry_articles/fv-thine-electronics.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/veloce)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:47:12 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/tannereda/ams-ic-design-flow-ds.pdf> (referer: https://www.mentor.com/tannereda/ams-ic)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:47:36 [scrapy] INFO: Crawled 3559 pages (at 82 pages/min), scraped 3293 items (at 75 items/min)
2015-11-04 11:47:52 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/tannereda/t-spice-ams-simulation.pdf> (referer: https://www.mentor.com/tannereda/ams-ic)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:48:08 [scrapy] ERROR: Error downloading <GET https://mentor1.adobeconnect.com/_a781163502/p51755750/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 11:48:34 [scrapy] ERROR: Error downloading <GET https://www.chrome.com/cubelab>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:48:34 [scrapy] ERROR: Error downloading <GET https://chrome.com/racer>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:48:34 [scrapy] ERROR: Error downloading <GET https://chrome.com/supersyncsports>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:48:34 [scrapy] ERROR: Error downloading <GET https://kickwithchrome.withgoogle.com/?utm_campaign=en&utm_medium=et&utm_source=en-et-na-us-crmhp>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:48:34 [scrapy] ERROR: Error downloading <GET https://www.chromeexperiments.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:48:34 [scrapy] INFO: Crawled 3572 pages (at 13 pages/min), scraped 3326 items (at 33 items/min)
2015-11-04 11:48:40 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-usb-3-0-host-controller-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:48:54 [scrapy] ERROR: Error downloading <GET https://www.cubeslam.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:49:12 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/silicon-yield/products/tessent.pdf> (referer: https://www.mentor.com/products/silicon-yield/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:49:14 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-softmodel-memories-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:49:15 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-ethernet-switch.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:49:35 [scrapy] ERROR: Error downloading <GET https://www.ticket-surf.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:49:36 [scrapy] INFO: Crawled 3643 pages (at 71 pages/min), scraped 3375 items (at 49 items/min)
2015-11-04 11:49:36 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/silicon-yield/products/ijtag-ds.pdf> (referer: https://www.mentor.com/products/silicon-yield/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:49:36 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/testbench-xpress-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/testbench-xpress)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:49:55 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-usb-peripheral.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:50:22 [scrapy] ERROR: Error downloading <GET https://www.ziggo.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 11:50:34 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-sata-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:50:34 [scrapy] ERROR: Error downloading <GET http://www.satcom-airbusds.com>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:50:34 [scrapy] INFO: Crawled 3726 pages (at 83 pages/min), scraped 3433 items (at 58 items/min)
2015-11-04 11:50:36 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-sas-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:50:58 [scrapy] ERROR: Error downloading <GET https://www.planisware.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:51:14 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-arm-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:51:26 [scrapy] INFO: Crawled 3773 pages (at 47 pages/min), scraped 3495 items (at 62 items/min)
2015-11-04 11:51:32 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-usb-host-controller-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:51:37 [scrapy] ERROR: Error downloading <GET https://www.photonis.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:51:39 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-multimedia-exerciser-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:51:40 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-multimedia-analyzer-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:51:44 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve-pci-express-ds.pdf> (referer: https://www.mentor.com/products/fv/emulation-systems/isolve)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:51:50 [scrapy] ERROR: Error downloading <GET https://bulkypix.com>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:51:50 [scrapy] ERROR: Error downloading <GET https://www.arkadin.com/fr>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 11:53:02 [scrapy] INFO: Crawled 3847 pages (at 74 pages/min), scraped 3555 items (at 60 items/min)
2015-11-04 11:53:31 [scrapy] ERROR: Error downloading <GET https://www.placedesleads.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:53:31 [scrapy] INFO: Crawled 3856 pages (at 9 pages/min), scraped 3578 items (at 23 items/min)
2015-11-04 11:54:11 [scrapy] ERROR: Error downloading <GET https://www.charter.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:54:59 [scrapy] ERROR: Spider error processing <GET http://www.berkshire-group.com/media/39764/berkshireraises-1615mfornewstrategy.pdf> (referer: http://www.berkshire-group.com/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:55:15 [scrapy] INFO: Crawled 3927 pages (at 71 pages/min), scraped 3658 items (at 80 items/min)
2015-11-04 11:55:22 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/922828.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/922828.pdf took longer than 180.0 seconds..
2015-11-04 11:55:22 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/922676.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/922676.pdf took longer than 180.0 seconds..
2015-11-04 11:55:22 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/926698.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/926698.pdf took longer than 180.0 seconds..
2015-11-04 11:55:22 [scrapy] ERROR: Error downloading <GET https://www.greenlightcapital.com/926211.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.greenlightcapital.com/926211.pdf took longer than 180.0 seconds..
2015-11-04 11:55:25 [scrapy] INFO: Crawled 3946 pages (at 19 pages/min), scraped 3668 items (at 10 items/min)
2015-11-04 11:56:30 [scrapy] INFO: Crawled 3958 pages (at 12 pages/min), scraped 3687 items (at 19 items/min)
2015-11-04 11:57:20 [scrapy] ERROR: Error downloading <GET http://s3.mentor.com/public_documents/datasheet/pcb-manufacturing-assembly/valor-mss-brochure-2.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://s3.mentor.com/public_documents/datasheet/pcb-manufacturing-assembly/valor-mss-brochure-2.pdf took longer than 180.0 seconds..
2015-11-04 11:57:21 [scrapy] INFO: Crawled 3970 pages (at 12 pages/min), scraped 3700 items (at 13 items/min)
2015-11-04 11:57:22 [scrapy] ERROR: Error downloading <GET http://s3.mentor.com/public_documents/datasheet/tannereda/mems-design-ds.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://s3.mentor.com/public_documents/datasheet/tannereda/mems-design-ds.pdf took longer than 180.0 seconds..
2015-11-04 11:57:36 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/193> (referer: http://www.elementpartners.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:57:37 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/217> (referer: http://www.elementpartners.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:57:37 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/139> (referer: http://www.elementpartners.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:57:41 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/137> (referer: http://www.elementpartners.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 11:58:30 [scrapy] INFO: Crawled 4115 pages (at 145 pages/min), scraped 3830 items (at 130 items/min)
2015-11-04 11:58:50 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/Scripts_Signup/Signup.dll?CMD=CMDChoosePackage&SUBMIT=SUBMIT&SEL_PACKAGE=26&SETCOUPON=>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:59:24 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/Signup/SignupJavaScriptTest.ffhtml?PID=947&URL=%2f%7esite%2fSignup%2fStartCCRSignup%2effhtml>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:59:24 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/jws/myEmailsLogin.action>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:59:24 [scrapy] ERROR: Error downloading <GET https://www.homestead.com/~site/go/jump.ffhtml?SPTID=LoWebsitePackages_BuyGold&TARGET=/~site/Signup/gold.ffhtml>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:59:24 [scrapy] INFO: Crawled 4189 pages (at 74 pages/min), scraped 3897 items (at 67 items/min)
2015-11-04 11:59:34 [scrapy] ERROR: Error downloading <GET https://www.google.com:443/maps/search/274+Riverside+Ave,+Westport,+CT/@41.130533,-73.368949,14z?source=embed&hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 11:59:50 [scrapy] ERROR: Error downloading <GET http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve_custom_solutions-ds.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://s3.mentor.com/public_documents/datasheet/products/fv/emulation-systems/isolve_custom_solutions-ds.pdf took longer than 180.0 seconds..
2015-11-04 11:59:50 [scrapy] ERROR: Error downloading <GET http://s3.mentor.com/public_documents/datasheet/products/ic_nanometer_design/analog-mixed-signal-verification/advance-ms/ADMS_Datasheet.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://s3.mentor.com/public_documents/datasheet/products/ic_nanometer_design/analog-mixed-signal-verification/advance-ms/ADMS_Datasheet.pdf took longer than 180.0 seconds..
2015-11-04 12:00:12 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/178> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:00:12 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/182> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:00:14 [scrapy] ERROR: Error downloading <GET https://twitter.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:00:14 [scrapy] ERROR: Error downloading <GET https://twitter.com/NBCDFW>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:00:14 [scrapy] ERROR: Error downloading <GET https://twitter.com/udfd71/status/661537246810472448>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:00:14 [scrapy] ERROR: Error downloading <GET https://twitter.com/intent/favorite?tweet_id=661537246810472448>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:00:16 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/180> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:00:17 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/17> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:00:17 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/13> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:00:17 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/133> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:00:22 [scrapy] INFO: Crawled 4276 pages (at 87 pages/min), scraped 3967 items (at 70 items/min)
2015-11-04 12:00:25 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/15> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:00:47 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/176> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:01:01 [scrapy] ERROR: Spider error processing <GET http://www.elementpartners.com/media/open/135> (referer: http://www.elementpartners.com/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:01:05 [scrapy] ERROR: Spider error processing <GET http://www.bluehost.com/blog/> (referer: http://www.bluehost.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 12:01:21 [scrapy] INFO: Crawled 4333 pages (at 57 pages/min), scraped 4035 items (at 68 items/min)
2015-11-04 12:01:24 [scrapy] ERROR: Error downloading <GET http://www.onelinkpr.com/>: An error occurred while connecting: 113: No route to host.
2015-11-04 12:01:24 [scrapy] ERROR: Error downloading <GET http://www.%20logicircuit.com/>: DNS lookup failed: address 'www.%20logicircuit.com' not found: [Errno -5] No address associated with hostname.
2015-11-04 12:01:29 [scrapy] ERROR: Error downloading <GET http://www.ikoula.com/PXmeZ/NeXKZ/QNfpZ/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.ikoula.com/PXmeZ/NeXKZ/QNfpZ/ took longer than 180.0 seconds..
2015-11-04 12:01:47 [scrapy] ERROR: Error downloading <GET http://video.cnbc.com/gallery/?play=1&video=3000081659>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 12:01:47 [scrapy] ERROR: Error downloading <GET http://www.%20sondrel.com/>: DNS lookup failed: address 'www.%20sondrel.com' not found: [Errno -5] No address associated with hostname.
2015-11-04 12:01:47 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/air?promoid=KLXMG>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:01:47 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer?promoid=KLXMF>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:01:47 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/shockwave?promoid=KLXMH>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:01:47 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/reader?promoid=KLXME>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:01:47 [scrapy] ERROR: Error downloading <GET http://www.%20pronesistech.com/>: DNS lookup failed: address 'www.%20pronesistech.com' not found: [Errno -5] No address associated with hostname.
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://www.ardian-investment.com/fr/videos>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://learn.mentor.com/library/xpedition/011282b>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://learn.mentor.com/library/vesys/527e4c6>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://learn.mentor.com/library/calibre-circuit-verification/78acdba>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://learn.mentor.com/library/pads-professional/5d9e213>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://learn.mentor.com/library/capital/36d4b30>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET http://www.inno-logic.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://carl-ng.nbalternatives.com/Login/Default.aspx?ru=https%3A%2F%2Fcarl-ng.nbalternatives.com%2Fdefault.aspx>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://erecruiting.nb.com/sap/bc/webdynpro/sap/hrrcf_a_startpage_ext_cand?sap-language=EN>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://erecruiting.nb.com/sap/bc/webdynpro/sap/hrrcf_a_candidate_registration?sap-language=EN>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://erecruiting.nb.com/sap/bc/webdynpro/sap/hrrcf_a_unreg_job_search?sap-language=EN&sap-wd-configId=ZSEARCHEXT>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://support.google.com/chrome/?p=mktg_help>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://developers.google.com/chrome/web-store/?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://developers.google.com/?hl=en>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://plus.google.com/100585555255542998765?prsrc=3>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://plus.google.com/100585555255542998765/posts>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET http://www.aumraj.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://learn.mentor.com/library/high-speed-pcb/347e67c>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://plus.google.com/112038386121410654017/posts>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://www.hfrdatabase.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://www.hfrdatabase.com/?q=reg>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://www.semiwiki.com/forum/content/4787-power-management-gets-tricky-ip-driven-world.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://www.semiwiki.com/forum/content/5126-shifting-low-power-verification-ip-soc-flow.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://www.semiwiki.com/forum/content/5043-something-old-something-new%C2%85eda-verification.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET http://www.orora.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:04 [scrapy] ERROR: Error downloading <GET https://www.ardian-investment.com/en/videos>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:09 [scrapy] ERROR: Error downloading <GET https://spurcapital.firmex.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:02:09 [scrapy] ERROR: Error downloading <GET http://www.acielectronics.com/>: An error occurred while connecting: 104: Connection reset by peer.
2015-11-04 12:02:16 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:02:34 [scrapy] INFO: Crawled 4415 pages (at 82 pages/min), scraped 4107 items (at 72 items/min)
2015-11-04 12:02:53 [scrapy] ERROR: Error downloading <GET http://www.myinsight.com/>: DNS lookup failed: address 'www.myinsight.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:02:53 [scrapy] ERROR: Error downloading <GET http://detechtion.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:53 [scrapy] ERROR: Error downloading <GET http://aquaventure.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:02:53 [scrapy] ERROR: Error downloading <GET http://quenchonline.com/>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:03:30 [scrapy] INFO: Crawled 4451 pages (at 36 pages/min), scraped 4152 items (at 45 items/min)
2015-11-04 12:03:30 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/photoshop-lightroom.html?promoid=KLXLX>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:03:30 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/business/teams.html?promoid=KQQSE>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:03:30 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/creativecloud/catalog/desktop.html?promoid=KOVFF>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:03:40 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/premiere.html?promoid=KLXLV>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:03:40 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/indesign.html?promoid=KLXLU>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:03:40 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/illustrator.html?promoid=KLXLT>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:03:40 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/aftereffects.html?promoid=KLXLW>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:03:40 [scrapy] ERROR: Error downloading <GET http://www.berkshire-group.com/media/39800/berkshireinter_am0615_2.pdf>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.berkshire-group.com/media/39800/berkshireinter_am0615_2.pdf took longer than 180.0 seconds..
2015-11-04 12:03:54 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/products/photoshop.html?promoid=KLXLS>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:04:31 [scrapy] INFO: Crawled 4499 pages (at 48 pages/min), scraped 4201 items (at 49 items/min)
2015-11-04 12:05:04 [scrapy] ERROR: Error downloading <GET https://support.twitter.com/articles/14226-how-to-find-your-twitter-short-code-or-long-code>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:05:04 [scrapy] ERROR: Error downloading <GET https://support.twitter.com/forums/26810/entries/78525>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:05:31 [scrapy] INFO: Crawled 4543 pages (at 44 pages/min), scraped 4245 items (at 44 items/min)
2015-11-04 12:05:48 [scrapy] ERROR: Error downloading <GET https://www-pi.capitalanalytics.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:06:00 [scrapy] ERROR: Error downloading <GET http://www.nokomiscapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 12:06:30 [scrapy] INFO: Crawled 4614 pages (at 71 pages/min), scraped 4305 items (at 60 items/min)
2015-11-04 12:07:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/careers.html?promoid=KLXNE>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:07:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/online-marketing-solutions.html?promoid=KRVUY>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:07:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/primetime-tv-platform.html?promoid=KOUEV>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:07:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/enterprise-content-management.html?promoid=KOUER>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:07:09 [scrapy] ERROR: Error downloading <GET https://www.adobe.com/marketing-cloud/online-advertising-management.html?promoid=KOUES>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:07:20 [scrapy] ERROR: Error downloading <GET http://www.anchorboltcapital.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 12:07:20 [scrapy] INFO: Crawled 4634 pages (at 20 pages/min), scraped 4331 items (at 26 items/min)
2015-11-04 12:07:43 [scrapy] ERROR: Error downloading <GET https://accounts.google.com/ServiceLogin?service=oz&passive=1209600&continue=https://plus.google.com/settings/plus?gpsrc%3Dgplp0>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 12:07:56 [scrapy] ERROR: Error downloading <GET http://www.adelphi-europe.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:07:56 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:07:56 [scrapy] ERROR: Error downloading <GET https://business.twitter.com/help/how-twitter-ads-work>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 12:07:56 [scrapy] ERROR: Error downloading <GET http://www.oldmutualus.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:07:56 [scrapy] INFO: Closing spider (finished)
2015-11-04 12:07:56 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 689,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 28,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 66,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 10,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 30,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 546,
 'downloader/request_bytes': 3604014,
 'downloader/request_count': 6739,
 'downloader/request_method_count/GET': 6739,
 'downloader/response_bytes': 148099124,
 'downloader/response_count': 6050,
 'downloader/response_status_count/200': 4533,
 'downloader/response_status_count/301': 845,
 'downloader/response_status_count/302': 515,
 'downloader/response_status_count/303': 4,
 'downloader/response_status_count/307': 6,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 8,
 'downloader/response_status_count/403': 4,
 'downloader/response_status_count/404': 104,
 'downloader/response_status_count/408': 8,
 'downloader/response_status_count/410': 2,
 'downloader/response_status_count/416': 1,
 'downloader/response_status_count/500': 3,
 'downloader/response_status_count/503': 3,
 'downloader/response_status_count/999': 11,
 'dupefilter/filtered': 12580,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 12, 7, 56, 957442),
 'item_scraped_count': 4359,
 'log_count/ERROR': 214,
 'log_count/INFO': 82,
 'offsite/domains': 158,
 'offsite/filtered': 295,
 'request_depth_max': 2,
 'response_received_count': 4648,
 'scheduler/dequeued': 6739,
 'scheduler/dequeued/memory': 6739,
 'scheduler/enqueued': 6739,
 'scheduler/enqueued/memory': 6739,
 'spider_exceptions/AttributeError': 63,
 'spider_exceptions/IncompleteRead': 2,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/TypeError': 16,
 'spider_exceptions/XMLSyntaxError': 7,
 'start_time': datetime.datetime(2015, 11, 4, 10, 52, 20, 130672)}
2015-11-04 12:07:56 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 12:08:59 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 12:08:59 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 12:08:59 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 12:08:59 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 12:08:59 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 12:08:59 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 12:08:59 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 12:08:59 [scrapy] INFO: Spider opened
2015-11-04 12:08:59 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 12:08:59 [scrapy] ERROR: Error downloading <GET http://www.lar>: DNS lookup failed: address 'www.lar' not found: [Errno -2] Name or service not known.
2015-11-04 12:08:59 [scrapy] ERROR: Error downloading <GET http://www.ccm>: DNS lookup failed: address 'www.ccm' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:00 [scrapy] ERROR: Error downloading <GET http://www.pol>: DNS lookup failed: address 'www.pol' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:00 [scrapy] ERROR: Error downloading <GET http://www.cor>: DNS lookup failed: address 'www.cor' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:00 [scrapy] ERROR: Error downloading <GET http://www.riv>: DNS lookup failed: address 'www.riv' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:00 [scrapy] ERROR: Error downloading <GET http://www.eco>: DNS lookup failed: address 'www.eco' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:04 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:04 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:06 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:06 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:09:06 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:06 [scrapy] ERROR: Error downloading <GET http://www.vnc>: DNS lookup failed: address 'www.vnc' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:06 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:07 [scrapy] ERROR: Error downloading <GET http://www.ome>: DNS lookup failed: address 'www.ome' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:07 [scrapy] ERROR: Error downloading <GET http://www.arb>: DNS lookup failed: address 'www.arb' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:09 [scrapy] ERROR: Error downloading <GET http://www.formulainvesting.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 12:09:09 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 12:09:09 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:10:01 [scrapy] INFO: Crawled 170 pages (at 170 pages/min), scraped 78 items (at 78 items/min)
2015-11-04 12:10:01 [scrapy] ERROR: Error downloading <GET http://www.bellasset.com>: DNS lookup failed: address 'www.bellasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:10:59 [scrapy] INFO: Crawled 173 pages (at 3 pages/min), scraped 82 items (at 4 items/min)
2015-11-04 12:11:59 [scrapy] INFO: Crawled 173 pages (at 0 pages/min), scraped 82 items (at 0 items/min)
2015-11-04 12:12:59 [scrapy] INFO: Crawled 173 pages (at 0 pages/min), scraped 82 items (at 0 items/min)
2015-11-04 12:13:59 [scrapy] INFO: Crawled 173 pages (at 0 pages/min), scraped 82 items (at 0 items/min)
2015-11-04 12:14:22 [scrapy] ERROR: Error downloading <GET http://www.nokomiscapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 12:14:59 [scrapy] INFO: Crawled 173 pages (at 0 pages/min), scraped 82 items (at 0 items/min)
2015-11-04 12:15:21 [scrapy] ERROR: Error downloading <GET http://www.feplp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:15:25 [scrapy] ERROR: Error downloading <GET http://www.constellationcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:15:25 [scrapy] INFO: Closing spider (finished)
2015-11-04 12:15:25 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 66,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 48,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 8,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/request_bytes': 74784,
 'downloader/request_count': 302,
 'downloader/request_method_count/GET': 302,
 'downloader/response_bytes': 1243508,
 'downloader/response_count': 236,
 'downloader/response_status_count/200': 155,
 'downloader/response_status_count/301': 31,
 'downloader/response_status_count/302': 28,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 3,
 'downloader/response_status_count/404': 14,
 'downloader/response_status_count/503': 3,
 'dupefilter/filtered': 228,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 12, 15, 25, 694022),
 'item_scraped_count': 82,
 'log_count/ERROR': 22,
 'log_count/INFO': 13,
 'offsite/domains': 93,
 'offsite/filtered': 553,
 'request_depth_max': 2,
 'response_received_count': 173,
 'scheduler/dequeued': 302,
 'scheduler/dequeued/memory': 302,
 'scheduler/enqueued': 302,
 'scheduler/enqueued/memory': 302,
 'start_time': datetime.datetime(2015, 11, 4, 12, 8, 59, 617148)}
2015-11-04 12:15:25 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 12:16:27 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 12:16:27 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 12:16:27 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 12:16:27 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 12:16:27 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 12:16:27 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 12:16:27 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 12:16:27 [scrapy] INFO: Spider opened
2015-11-04 12:16:27 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.rid>: DNS lookup failed: address 'www.rid' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.investor.pccpllc.amiesdigital.com>: DNS lookup failed: address 'www.investor.pccpllc.amiesdigital.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.cre>: DNS lookup failed: address 'www.cre' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.ellislake.com>: DNS lookup failed: address 'www.ellislake.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.zca>: DNS lookup failed: address 'www.zca' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.arb>: DNS lookup failed: address 'www.arb' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.pro>: DNS lookup failed: address 'www.pro' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.cqs>: DNS lookup failed: address 'www.cqs' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.investor.gppfunds.com>: DNS lookup failed: address 'www.investor.gppfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:28 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:16:32 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:16:34 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:16:41 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:17:23 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q3+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:24 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q1+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:24 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q4+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-02+February+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2013-Q2+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:25 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-03+March+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:26 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q4+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:27 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-09+September+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:27 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-08+August+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:27 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-06+June+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:28 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-11+November+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:28 [scrapy] INFO: Crawled 209 pages (at 209 pages/min), scraped 119 items (at 119 items/min)
2015-11-04 12:17:28 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-10+October+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:29 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-04+April+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:30 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-07+July+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:30 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-04+April+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:30 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-05+May+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:31 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-03+March+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:31 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2014-12+December+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:31 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-06+June+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:32 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-07++Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:33 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-09+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:34 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-02+February+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:35 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-08++Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:36 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2015-01+January+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:38 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q2+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:38 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q3+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:39 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q1+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:40 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q4+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:41 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2015+Q1+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:42 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2015+Q2+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:42 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q1+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:42 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q3+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:43 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2013+Q4+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:43 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2014+Q2+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:43 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=TFP&fn=TFP+2015+Q3+Report.pdf> (referer: https://www.asallc.com/?code=TFP&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q2+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2011-Q3+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q1+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:51 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2012-Q3+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:55 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=EMM&fn=EMM+2011-Q4+Report.pdf> (referer: https://www.asallc.com/?code=EMM&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:17:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2011-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:07 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2011-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:24 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:28 [scrapy] INFO: Crawled 309 pages (at 100 pages/min), scraped 186 items (at 67 items/min)
2015-11-04 12:18:28 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:29 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2012-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:29 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:30 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:31 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2015-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:31 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q1+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:31 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:32 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:32 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2014-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:33 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q4+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:33 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2015-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:37 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2013-Q2+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:41 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASACNV&fn=CNV+2015-Q3+Report.pdf> (referer: https://www.asallc.com/?code=ASACNV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:44 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:47 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:48 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:48 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:49 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:49 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:50 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-10+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:50 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-11+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:50 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2012-12+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:51 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-01+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:51 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-02+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:52 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:52 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:53 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:53 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:54 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:54 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:54 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-10+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:55 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:55 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-11+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2013-12+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:56 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:57 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-01+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:57 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:58 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:58 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-02+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:58 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:59 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-10+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:18:59 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-11+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-04+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-02+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:00 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-01+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:01 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:01 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-12+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:02 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-03+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:02 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2014-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:02 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-05+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:06 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-07+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:07 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-08+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:08 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-06+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:08 [scrapy] ERROR: Spider error processing <GET https://www.asallc.com/media.aspx?f=ASATARV&fn=TARV+2015-09+Report.pdf> (referer: https://www.asallc.com/?code=ASATARV&nav=product&t=4)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 12:19:27 [scrapy] INFO: Crawled 385 pages (at 76 pages/min), scraped 210 items (at 24 items/min)
2015-11-04 12:20:27 [scrapy] INFO: Crawled 385 pages (at 0 pages/min), scraped 210 items (at 0 items/min)
2015-11-04 12:21:27 [scrapy] INFO: Crawled 385 pages (at 0 pages/min), scraped 210 items (at 0 items/min)
2015-11-04 12:22:27 [scrapy] INFO: Crawled 385 pages (at 0 pages/min), scraped 210 items (at 0 items/min)
2015-11-04 12:22:53 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:22:53 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:22:53 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:22:53 [scrapy] ERROR: Error downloading <GET http://www.quintanacapitalgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:22:53 [scrapy] ERROR: Error downloading <GET http://www.harvestmanagement.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:22:53 [scrapy] INFO: Closing spider (finished)
2015-11-04 12:22:53 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 84,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 60,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 15,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 9,
 'downloader/request_bytes': 176421,
 'downloader/request_count': 519,
 'downloader/request_method_count/GET': 519,
 'downloader/response_bytes': 33416546,
 'downloader/response_count': 435,
 'downloader/response_status_count/200': 380,
 'downloader/response_status_count/301': 22,
 'downloader/response_status_count/302': 25,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/401': 2,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 3,
 'dupefilter/filtered': 1281,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 12, 22, 53, 311966),
 'item_scraped_count': 210,
 'log_count/ERROR': 128,
 'log_count/INFO': 13,
 'offsite/domains': 115,
 'offsite/filtered': 668,
 'request_depth_max': 2,
 'response_received_count': 385,
 'scheduler/dequeued': 519,
 'scheduler/dequeued/memory': 519,
 'scheduler/enqueued': 519,
 'scheduler/enqueued/memory': 519,
 'spider_exceptions/AttributeError': 100,
 'start_time': datetime.datetime(2015, 11, 4, 12, 16, 27, 830616)}
2015-11-04 12:22:53 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 12:23:55 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 12:23:55 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 12:23:55 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 12:23:55 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 12:23:55 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 12:23:55 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 12:23:55 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 12:23:55 [scrapy] INFO: Spider opened
2015-11-04 12:23:55 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 12:23:55 [scrapy] ERROR: Error downloading <GET http://www.ecp.altareturn.com>: DNS lookup failed: address 'www.ecp.altareturn.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:55 [scrapy] ERROR: Error downloading <GET http://www.tif>: DNS lookup failed: address 'www.tif' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:55 [scrapy] ERROR: Error downloading <GET http://www.acc>: DNS lookup failed: address 'www.acc' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:55 [scrapy] ERROR: Error downloading <GET http://www.gim>: DNS lookup failed: address 'www.gim' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.due>: DNS lookup failed: address 'www.due' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.lan>: DNS lookup failed: address 'www.lan' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.beckerdrapkin.com>: DNS lookup failed: address 'www.beckerdrapkin.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.torshencapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.first.mitsubishiufjfundservices.com>: DNS lookup failed: address 'www.first.mitsubishiufjfundservices.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.enhancedcapct.com>: DNS lookup failed: address 'www.enhancedcapct.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:56 [scrapy] ERROR: Error downloading <GET http://www.clairvuecapital.com>: DNS lookup failed: address 'www.clairvuecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:58 [scrapy] ERROR: Error downloading <GET http://www.visicap.com>: DNS lookup failed: address 'www.visicap.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:23:58 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:24:11 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:24:56 [scrapy] INFO: Crawled 171 pages (at 171 pages/min), scraped 92 items (at 92 items/min)
2015-11-04 12:25:24 [scrapy] ERROR: Error downloading <GET http://www.polunin.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 12:25:26 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 12:25:28 [scrapy] ERROR: Error downloading <GET http://www.lightstreetcap.com>: DNS lookup failed: address 'www.lightstreetcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:25:56 [scrapy] INFO: Crawled 274 pages (at 103 pages/min), scraped 188 items (at 96 items/min)
2015-11-04 12:26:55 [scrapy] INFO: Crawled 350 pages (at 76 pages/min), scraped 273 items (at 85 items/min)
2015-11-04 12:27:27 [scrapy] ERROR: Error downloading <GET http://www.nokomiscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:27:55 [scrapy] INFO: Crawled 350 pages (at 0 pages/min), scraped 273 items (at 0 items/min)
2015-11-04 12:28:55 [scrapy] INFO: Crawled 350 pages (at 0 pages/min), scraped 273 items (at 0 items/min)
2015-11-04 12:29:55 [scrapy] INFO: Crawled 350 pages (at 0 pages/min), scraped 273 items (at 0 items/min)
2015-11-04 12:30:25 [scrapy] ERROR: Error downloading <GET http://www.oldmutualus.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:30:25 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:30:25 [scrapy] ERROR: Error downloading <GET http://www.seamarkcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:30:25 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:30:25 [scrapy] ERROR: Error downloading <GET http://www.mapleleaffunds.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:30:25 [scrapy] INFO: Closing spider (finished)
2015-11-04 12:30:25 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 78,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 8,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 51,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 16,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 176641,
 'downloader/request_count': 471,
 'downloader/request_method_count/GET': 471,
 'downloader/response_bytes': 5295432,
 'downloader/response_count': 393,
 'downloader/response_status_count/200': 344,
 'downloader/response_status_count/301': 22,
 'downloader/response_status_count/302': 13,
 'downloader/response_status_count/400': 6,
 'downloader/response_status_count/403': 5,
 'downloader/response_status_count/404': 3,
 'dupefilter/filtered': 468,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 12, 30, 25, 150745),
 'item_scraped_count': 273,
 'log_count/ERROR': 26,
 'log_count/INFO': 13,
 'offsite/domains': 86,
 'offsite/filtered': 519,
 'request_depth_max': 2,
 'response_received_count': 350,
 'scheduler/dequeued': 471,
 'scheduler/dequeued/memory': 471,
 'scheduler/enqueued': 471,
 'scheduler/enqueued/memory': 471,
 'start_time': datetime.datetime(2015, 11, 4, 12, 23, 55, 666983)}
2015-11-04 12:30:25 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 12:31:27 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 12:31:27 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 12:31:27 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 12:31:27 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 12:31:27 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 12:31:27 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 12:31:27 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 12:31:27 [scrapy] INFO: Spider opened
2015-11-04 12:31:27 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.dis>: DNS lookup failed: address 'www.dis' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.gra>: DNS lookup failed: address 'www.gra' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.sco>: DNS lookup failed: address 'www.sco' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.czc>: DNS lookup failed: address 'www.czc' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:27 [scrapy] ERROR: Error downloading <GET http://www.torshencapital.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 12:31:28 [scrapy] ERROR: Error downloading <GET http://www.zad>: DNS lookup failed: address 'www.zad' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:28 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:28 [scrapy] ERROR: Error downloading <GET http://www.sec>: DNS lookup failed: address 'www.sec' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.santanderasset.com>: DNS lookup failed: address 'www.santanderasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.cshg.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.dcp-us.com>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:29 [scrapy] ERROR: Error downloading <GET http://www.harvpartners.com>: DNS lookup failed: address 'www.harvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:30 [scrapy] ERROR: Error downloading <GET http://www.wellfieldpartners.com>: DNS lookup failed: address 'www.wellfieldpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:30 [scrapy] ERROR: Error downloading <GET http://emergingcapitalmarket.com>: DNS lookup failed: address 'emergingcapitalmarket.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:30 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 12:31:31 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:31:34 [scrapy] ERROR: Error downloading <GET http://www.meridianfunds.com>: DNS lookup failed: address 'www.meridianfunds.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:32:46 [scrapy] INFO: Crawled 132 pages (at 132 pages/min), scraped 43 items (at 43 items/min)
2015-11-04 12:32:47 [scrapy] ERROR: Error downloading <GET http://www.emergingmanagersgroup.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 12:33:30 [scrapy] INFO: Crawled 149 pages (at 17 pages/min), scraped 73 items (at 30 items/min)
2015-11-04 12:34:59 [scrapy] INFO: Crawled 163 pages (at 14 pages/min), scraped 86 items (at 13 items/min)
2015-11-04 12:36:18 [scrapy] ERROR: Spider error processing <GET http://www.fosuncapital.com/index.php/news/default/category/11> (referer: http://www.fosuncapital.com/index.php/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 12:36:34 [scrapy] INFO: Crawled 169 pages (at 6 pages/min), scraped 93 items (at 7 items/min)
2015-11-04 12:37:32 [scrapy] INFO: Crawled 180 pages (at 11 pages/min), scraped 102 items (at 9 items/min)
2015-11-04 12:38:29 [scrapy] INFO: Crawled 187 pages (at 7 pages/min), scraped 111 items (at 9 items/min)
2015-11-04 12:38:42 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:42 [scrapy] ERROR: Error downloading <GET http://www.vscapitalpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:42 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:42 [scrapy] ERROR: Error downloading <GET http://www.valuepartnersgroup.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 12:38:42 [scrapy] INFO: Closing spider (finished)
2015-11-04 12:38:42 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 88,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 7,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 60,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 12,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 87847,
 'downloader/request_count': 333,
 'downloader/request_method_count/GET': 333,
 'downloader/response_bytes': 3250359,
 'downloader/response_count': 245,
 'downloader/response_status_count/200': 185,
 'downloader/response_status_count/301': 26,
 'downloader/response_status_count/302': 26,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/404': 2,
 'dupefilter/filtered': 266,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 12, 38, 42, 175303),
 'item_scraped_count': 111,
 'log_count/ERROR': 30,
 'log_count/INFO': 13,
 'offsite/domains': 96,
 'offsite/filtered': 462,
 'request_depth_max': 2,
 'response_received_count': 187,
 'scheduler/dequeued': 333,
 'scheduler/dequeued/memory': 333,
 'scheduler/enqueued': 333,
 'scheduler/enqueued/memory': 333,
 'spider_exceptions/timeout': 1,
 'start_time': datetime.datetime(2015, 11, 4, 12, 31, 27, 589350)}
2015-11-04 12:38:42 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 12:39:44 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 12:39:44 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 12:39:44 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 12:39:44 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 12:39:44 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 12:39:44 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 12:39:44 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 12:39:44 [scrapy] INFO: Spider opened
2015-11-04 12:39:44 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.omn>: DNS lookup failed: address 'www.omn' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.portal.krfs.com>: DNS lookup failed: address 'www.portal.krfs.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.fun>: DNS lookup failed: address 'www.fun' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.citicapitaladvisors.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.clairvuecapital.com>: DNS lookup failed: address 'www.clairvuecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.gim>: DNS lookup failed: address 'www.gim' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.uni>: DNS lookup failed: address 'www.uni' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:44 [scrapy] ERROR: Error downloading <GET http://www.arb>: DNS lookup failed: address 'www.arb' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:45 [scrapy] ERROR: Error downloading <GET http://www.zad>: DNS lookup failed: address 'www.zad' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:45 [scrapy] ERROR: Error downloading <GET http://www.san>: DNS lookup failed: address 'www.san' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:45 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 12:39:51 [scrapy] ERROR: Error downloading <GET http://www.fsc>: DNS lookup failed: address 'www.fsc' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:04 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 12:40:04 [scrapy] ERROR: Error downloading <GET http://www.mad>: DNS lookup failed: address 'www.mad' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:06 [scrapy] ERROR: Error downloading <GET http://www.iam>: DNS lookup failed: address 'www.iam' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:06 [scrapy] ERROR: Spider error processing <GET http://www.talsonpartners.com/index.html> (referer: http://www.talsonpartners.com)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 12:40:06 [scrapy] ERROR: Error downloading <GET http://www.formulainvesting.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 12:40:06 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:06 [scrapy] ERROR: Error downloading <GET http://www.lp.lcpartners.com>: DNS lookup failed: address 'www.lp.lcpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:07 [scrapy] ERROR: Error downloading <GET http://www.dis>: DNS lookup failed: address 'www.dis' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:07 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:07 [scrapy] ERROR: Error downloading <GET http://www.acc>: DNS lookup failed: address 'www.acc' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:07 [scrapy] ERROR: Error downloading <GET http://www.harvpartners.com>: DNS lookup failed: address 'www.harvpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 12:40:44 [scrapy] INFO: Crawled 223 pages (at 223 pages/min), scraped 122 items (at 122 items/min)
2015-11-04 12:41:44 [scrapy] INFO: Crawled 291 pages (at 68 pages/min), scraped 192 items (at 70 items/min)
2015-11-04 12:42:47 [scrapy] INFO: Crawled 360 pages (at 69 pages/min), scraped 264 items (at 72 items/min)
2015-11-04 12:43:51 [scrapy] INFO: Crawled 438 pages (at 78 pages/min), scraped 346 items (at 82 items/min)
2015-11-04 12:44:48 [scrapy] INFO: Crawled 494 pages (at 56 pages/min), scraped 400 items (at 54 items/min)
2015-11-04 12:45:44 [scrapy] INFO: Crawled 556 pages (at 62 pages/min), scraped 460 items (at 60 items/min)
2015-11-04 12:46:48 [scrapy] INFO: Crawled 628 pages (at 72 pages/min), scraped 534 items (at 74 items/min)
2015-11-04 12:47:47 [scrapy] INFO: Crawled 699 pages (at 71 pages/min), scraped 605 items (at 71 items/min)
2015-11-04 12:48:49 [scrapy] INFO: Crawled 769 pages (at 70 pages/min), scraped 677 items (at 72 items/min)
2015-11-04 12:49:49 [scrapy] INFO: Crawled 833 pages (at 64 pages/min), scraped 741 items (at 64 items/min)
2015-11-04 12:50:45 [scrapy] INFO: Crawled 898 pages (at 65 pages/min), scraped 804 items (at 63 items/min)
2015-11-04 12:51:49 [scrapy] INFO: Crawled 974 pages (at 76 pages/min), scraped 882 items (at 78 items/min)
2015-11-04 12:52:48 [scrapy] INFO: Crawled 1049 pages (at 75 pages/min), scraped 954 items (at 72 items/min)
2015-11-04 12:53:48 [scrapy] INFO: Crawled 1118 pages (at 69 pages/min), scraped 1026 items (at 72 items/min)
2015-11-04 12:54:49 [scrapy] INFO: Crawled 1189 pages (at 71 pages/min), scraped 1097 items (at 71 items/min)
2015-11-04 12:55:49 [scrapy] INFO: Crawled 1259 pages (at 70 pages/min), scraped 1167 items (at 70 items/min)
2015-11-04 12:56:48 [scrapy] INFO: Crawled 1334 pages (at 75 pages/min), scraped 1239 items (at 72 items/min)
2015-11-04 12:57:49 [scrapy] INFO: Crawled 1403 pages (at 69 pages/min), scraped 1311 items (at 72 items/min)
2015-11-04 12:58:46 [scrapy] INFO: Crawled 1470 pages (at 67 pages/min), scraped 1375 items (at 64 items/min)
2015-11-04 12:59:49 [scrapy] INFO: Crawled 1539 pages (at 69 pages/min), scraped 1447 items (at 72 items/min)
2015-11-04 13:00:47 [scrapy] INFO: Crawled 1606 pages (at 67 pages/min), scraped 1511 items (at 64 items/min)
2015-11-04 13:01:48 [scrapy] INFO: Crawled 1676 pages (at 70 pages/min), scraped 1582 items (at 71 items/min)
2015-11-04 13:02:49 [scrapy] INFO: Crawled 1736 pages (at 60 pages/min), scraped 1644 items (at 62 items/min)
2015-11-04 13:03:44 [scrapy] INFO: Crawled 1799 pages (at 63 pages/min), scraped 1703 items (at 59 items/min)
2015-11-04 13:04:50 [scrapy] INFO: Crawled 1858 pages (at 59 pages/min), scraped 1767 items (at 64 items/min)
2015-11-04 13:05:55 [scrapy] INFO: Crawled 1935 pages (at 77 pages/min), scraped 1835 items (at 68 items/min)
2015-11-04 13:06:50 [scrapy] INFO: Crawled 1989 pages (at 54 pages/min), scraped 1894 items (at 59 items/min)
2015-11-04 13:07:44 [scrapy] INFO: Crawled 2009 pages (at 20 pages/min), scraped 1925 items (at 31 items/min)
2015-11-04 13:08:44 [scrapy] INFO: Crawled 2009 pages (at 0 pages/min), scraped 1925 items (at 0 items/min)
2015-11-04 13:09:44 [scrapy] INFO: Crawled 2009 pages (at 0 pages/min), scraped 1925 items (at 0 items/min)
2015-11-04 13:09:54 [scrapy] ERROR: Error downloading <GET http://www.ironsidespartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 13:09:54 [scrapy] INFO: Closing spider (finished)
2015-11-04 13:09:54 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 73,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 9,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 57,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 1063469,
 'downloader/request_count': 2162,
 'downloader/request_method_count/GET': 2162,
 'downloader/response_bytes': 16318334,
 'downloader/response_count': 2089,
 'downloader/response_status_count/200': 2018,
 'downloader/response_status_count/301': 22,
 'downloader/response_status_count/302': 39,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 3,
 'downloader/response_status_count/500': 2,
 'dupefilter/filtered': 4823,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 13, 9, 54, 429800),
 'item_scraped_count': 1925,
 'log_count/ERROR': 25,
 'log_count/INFO': 37,
 'offsite/domains': 97,
 'offsite/filtered': 488,
 'request_depth_max': 2,
 'response_received_count': 2009,
 'scheduler/dequeued': 2162,
 'scheduler/dequeued/memory': 2162,
 'scheduler/enqueued': 2162,
 'scheduler/enqueued/memory': 2162,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 12, 39, 44, 480581)}
2015-11-04 13:09:54 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 13:10:56 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 13:10:56 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 13:10:56 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 13:10:56 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 13:10:56 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 13:10:56 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 13:10:57 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 13:10:57 [scrapy] INFO: Spider opened
2015-11-04 13:10:57 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.tet>: DNS lookup failed: address 'www.tet' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.gra>: DNS lookup failed: address 'www.gra' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.imc>: DNS lookup failed: address 'www.imc' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.alphatitans.com>: DNS lookup failed: address 'www.alphatitans.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.dai>: DNS lookup failed: address 'www.dai' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.pia>: DNS lookup failed: address 'www.pia' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.zca>: DNS lookup failed: address 'www.zca' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.omn>: DNS lookup failed: address 'www.omn' not found: [Errno -2] Name or service not known.
2015-11-04 13:10:57 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 13:11:01 [scrapy] ERROR: Error downloading <GET http://www.acc>: DNS lookup failed: address 'www.acc' not found: [Errno -2] Name or service not known.
2015-11-04 13:12:20 [scrapy] INFO: Crawled 173 pages (at 173 pages/min), scraped 88 items (at 88 items/min)
2015-11-04 13:13:42 [scrapy] INFO: Crawled 182 pages (at 9 pages/min), scraped 94 items (at 6 items/min)
2015-11-04 13:15:30 [scrapy] INFO: Crawled 220 pages (at 38 pages/min), scraped 109 items (at 15 items/min)
2015-11-04 13:16:23 [scrapy] INFO: Crawled 227 pages (at 7 pages/min), scraped 113 items (at 4 items/min)
2015-11-04 13:20:22 [scrapy] INFO: Crawled 228 pages (at 1 pages/min), scraped 136 items (at 23 items/min)
2015-11-04 13:26:03 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 13:26:03 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fportfolio-view%2Fkevin-wodicka-2%2F>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://eightfoldcapital.com/?_escaped_fragment_=%2Fportfolio-view%2Fkevin-wodicka-2%2F took longer than 180.0 seconds..
2015-11-04 13:26:03 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fzachary-freedman%2F>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://eightfoldcapital.com/?_escaped_fragment_=%2Fzachary-freedman%2F took longer than 180.0 seconds..
2015-11-04 13:26:03 [scrapy] INFO: Crawled 228 pages (at 0 pages/min), scraped 143 items (at 7 items/min)
2015-11-04 13:26:03 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fadnan-charania%2F>: An error occurred while connecting: 32: Broken pipe.
2015-11-04 13:26:04 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fhaejin-baek%2F>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://eightfoldcapital.com/?_escaped_fragment_=%2Fhaejin-baek%2F took longer than 180.0 seconds..
2015-11-04 13:26:04 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fabout-us%2F>: User timeout caused connection failure.
2015-11-04 13:26:04 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fchristina-rahman%2F>: User timeout caused connection failure.
2015-11-04 13:26:04 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fportfolio-view%2Fbrian-tageson-2%2F>: User timeout caused connection failure.
2015-11-04 13:26:04 [scrapy] ERROR: Error downloading <GET http://eightfoldcapital.com/?_escaped_fragment_=%2Fportfolio-view%2Frandolph-wolpert-2%2F>: User timeout caused connection failure.
2015-11-04 13:26:58 [scrapy] INFO: Crawled 243 pages (at 15 pages/min), scraped 144 items (at 1 items/min)
2015-11-04 13:34:53 [scrapy] INFO: Crawled 262 pages (at 19 pages/min), scraped 162 items (at 18 items/min)
2015-11-04 13:38:10 [scrapy] INFO: Crawled 262 pages (at 0 pages/min), scraped 168 items (at 6 items/min)
2015-11-04 13:40:34 [scrapy] INFO: Crawled 262 pages (at 0 pages/min), scraped 177 items (at 9 items/min)
2015-11-04 13:41:25 [scrapy] INFO: Crawled 278 pages (at 16 pages/min), scraped 185 items (at 8 items/min)
2015-11-04 13:42:13 [scrapy] INFO: Crawled 278 pages (at 0 pages/min), scraped 193 items (at 8 items/min)
2015-11-04 13:43:36 [scrapy] INFO: Crawled 294 pages (at 16 pages/min), scraped 201 items (at 8 items/min)
2015-11-04 13:43:58 [scrapy] INFO: Crawled 302 pages (at 8 pages/min), scraped 211 items (at 10 items/min)
2015-11-04 13:44:58 [scrapy] INFO: Crawled 317 pages (at 15 pages/min), scraped 226 items (at 15 items/min)
2015-11-04 13:45:57 [scrapy] INFO: Crawled 331 pages (at 14 pages/min), scraped 239 items (at 13 items/min)
2015-11-04 13:46:59 [scrapy] INFO: Crawled 348 pages (at 17 pages/min), scraped 256 items (at 17 items/min)
2015-11-04 13:48:19 [scrapy] INFO: Crawled 349 pages (at 1 pages/min), scraped 263 items (at 7 items/min)
2015-11-04 13:49:20 [scrapy] INFO: Crawled 366 pages (at 17 pages/min), scraped 265 items (at 2 items/min)
2015-11-04 13:50:14 [scrapy] INFO: Crawled 366 pages (at 0 pages/min), scraped 266 items (at 1 items/min)
2015-11-04 13:51:42 [scrapy] INFO: Crawled 382 pages (at 16 pages/min), scraped 281 items (at 15 items/min)
2015-11-04 13:52:35 [scrapy] INFO: Crawled 401 pages (at 19 pages/min), scraped 296 items (at 15 items/min)
2015-11-04 13:53:03 [scrapy] INFO: Crawled 403 pages (at 2 pages/min), scraped 314 items (at 18 items/min)
2015-11-04 13:55:50 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/english/column/index-000200020003_FUND_OPEN_1103_000264.html> (referer: http://www.bosera.com/english/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 588, in read
    return self._read_chunked(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 657, in _read_chunked
    value.append(self._safe_read(chunk_left))
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 703, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 13:57:50 [scrapy] INFO: Crawled 416 pages (at 13 pages/min), scraped 324 items (at 10 items/min)
2015-11-04 13:58:37 [scrapy] INFO: Crawled 416 pages (at 0 pages/min), scraped 327 items (at 3 items/min)
2015-11-04 13:59:10 [scrapy] INFO: Crawled 424 pages (at 8 pages/min), scraped 329 items (at 2 items/min)
2015-11-04 14:00:33 [scrapy] INFO: Crawled 438 pages (at 14 pages/min), scraped 337 items (at 8 items/min)
2015-11-04 14:01:14 [scrapy] INFO: Crawled 440 pages (at 2 pages/min), scraped 342 items (at 5 items/min)
2015-11-04 14:03:06 [scrapy] INFO: Crawled 440 pages (at 0 pages/min), scraped 352 items (at 10 items/min)
2015-11-04 14:04:01 [scrapy] INFO: Crawled 452 pages (at 12 pages/min), scraped 356 items (at 4 items/min)
2015-11-04 14:05:29 [scrapy] INFO: Crawled 467 pages (at 15 pages/min), scraped 378 items (at 22 items/min)
2015-11-04 14:06:09 [scrapy] INFO: Crawled 486 pages (at 19 pages/min), scraped 384 items (at 6 items/min)
2015-11-04 14:06:10 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/specialFund/mySpecialFundDetail>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]
2015-11-04 14:08:10 [scrapy] ERROR: Spider error processing <GET http://www.fosuncapital.com/index.php/news/default/page/6> (referer: http://www.fosuncapital.com/index.php/news)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:08:29 [scrapy] INFO: Crawled 490 pages (at 4 pages/min), scraped 400 items (at 16 items/min)
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/openAcct/selectBankCard>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.fosuncapital.com/index.php/team/default/page/3>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.fosuncapital.com/index.php/team/view/id/34>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.fosuncapital.com/index.php/news/read/article/70>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.fosuncapital.com/index.php/team/default/category/1>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.fosuncapital.com/index.php/team/view/id/5>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.emergingmanagersgroup.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.seamarkcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctMgr/userFeedback/feedbackForm>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:09:09 [scrapy] INFO: Crawled 490 pages (at 0 pages/min), scraped 401 items (at 1 items/min)
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET http://www.nokomiscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:09:09 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/specialFund/specialFundIndex>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/specialFund/specialFundIndex took longer than 180.0 seconds..
2015-11-04 14:09:57 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_rights.html>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:10:48 [scrapy] INFO: Crawled 502 pages (at 12 pages/min), scraped 413 items (at 12 items/min)
2015-11-04 14:10:50 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/cashbox/chargeForm>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:10:50 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/myFundList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:11:14 [scrapy] INFO: Crawled 516 pages (at 14 pages/min), scraped 418 items (at 5 items/min)
2015-11-04 14:11:14 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=000734>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:11:17 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=000936>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:11:57 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/cashbox/myCashboxDetail>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:11:57 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/tradeMgr/buyFund?fundCode=001661>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/tradeMgr/buyFund?fundCode=001661 took longer than 180.0 seconds..
2015-11-04 14:11:57 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/myFund/scheduleBuy/scheduleBuyFundList>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctAsset/myFund/scheduleBuy/scheduleBuyFundList took longer than 180.0 seconds..
2015-11-04 14:11:57 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/acctAsset/specialFund/specialFundList>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/acctAsset/specialFund/specialFundList took longer than 180.0 seconds..
2015-11-04 14:11:57 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_duty.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/notes/index_duty.html took longer than 180.0 seconds..
2015-11-04 14:11:57 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/notes/index_risk.html>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://trade.bosera.com/notes/index_risk.html took longer than 180.0 seconds..
2015-11-04 14:11:57 [scrapy] INFO: Crawled 523 pages (at 7 pages/min), scraped 425 items (at 7 items/min)
2015-11-04 14:13:02 [scrapy] INFO: Crawled 543 pages (at 20 pages/min), scraped 441 items (at 16 items/min)
2015-11-04 14:14:25 [scrapy] INFO: Crawled 545 pages (at 2 pages/min), scraped 444 items (at 3 items/min)
2015-11-04 14:14:59 [scrapy] ERROR: Spider error processing <GET https://trade.bosera.com/acctAsset/myFund/scheduleBuy/scheduleBuyList> (referer: http://www.bosera.com/minisite/licaizaixian/index.jsp)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 734, in recv
    return self.read(buflen)
  File "/home/ubuntu/anaconda/lib/python2.7/ssl.py", line 621, in read
    v = self._sslobj.read(len or 1024)
SSLError: ('The read operation timed out',)
2015-11-04 14:15:09 [scrapy] ERROR: Error downloading <GET http://www.bosera.com/common/infoDetail.jsp?classid=00020002000500030003&infoid=1290508>: [<twisted.python.failure.Failure twisted.internet.error.ConnectionDone: Connection was closed cleanly.>]
2015-11-04 14:15:09 [scrapy] INFO: Crawled 545 pages (at 0 pages/min), scraped 449 items (at 5 items/min)
2015-11-04 14:16:06 [scrapy] INFO: Crawled 576 pages (at 31 pages/min), scraped 466 items (at 17 items/min)
2015-11-04 14:17:03 [scrapy] INFO: Crawled 586 pages (at 10 pages/min), scraped 479 items (at 13 items/min)
2015-11-04 14:18:26 [scrapy] INFO: Crawled 595 pages (at 9 pages/min), scraped 490 items (at 11 items/min)
2015-11-04 14:20:31 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/common/infoDetail.jsp?classid=0002000200080002&infoid=1299452> (referer: http://www.bosera.com/minisite/licaizaixian/index.jsp)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:20:31 [scrapy] INFO: Crawled 611 pages (at 16 pages/min), scraped 504 items (at 14 items/min)
2015-11-04 14:21:19 [scrapy] INFO: Crawled 630 pages (at 19 pages/min), scraped 515 items (at 11 items/min)
2015-11-04 14:22:01 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020001&infoid=1535340> (referer: http://www.bosera.com/aboutus/touziredianwenti.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
error: [Errno 104] Connection reset by peer
2015-11-04 14:22:11 [scrapy] INFO: Crawled 630 pages (at 0 pages/min), scraped 525 items (at 10 items/min)
2015-11-04 14:22:11 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index_s.jsp?tgtUrl=%2FacctQry%2FtradeRecordList>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:22:11 [scrapy] ERROR: Error downloading <GET https://trade.bosera.com/index.jsp>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:23:12 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020001&infoid=1592425> (referer: http://www.bosera.com/aboutus/touziredianwenti.html)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 14:23:12 [scrapy] INFO: Crawled 638 pages (at 8 pages/min), scraped 526 items (at 1 items/min)
2015-11-04 14:23:59 [scrapy] INFO: Crawled 649 pages (at 11 pages/min), scraped 536 items (at 10 items/min)
2015-11-04 14:24:53 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228800> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 14:25:04 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228801> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623313)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 14:25:06 [scrapy] INFO: Crawled 664 pages (at 15 pages/min), scraped 551 items (at 15 items/min)
2015-11-04 14:27:05 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228804> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623374)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 14:28:01 [scrapy] INFO: Crawled 665 pages (at 1 pages/min), scraped 555 items (at 4 items/min)
2015-11-04 14:29:26 [scrapy] INFO: Crawled 675 pages (at 10 pages/min), scraped 564 items (at 9 items/min)
2015-11-04 14:30:22 [scrapy] ERROR: Spider error processing <GET http://www.bosera.com/servlet/com.bosera.www.servlet.DownloadFileServlet?attachmentID=1228803> (referer: http://www.bosera.com/common/infoDetail.jsp?classid=00020002000200020002&infoid=1623374)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 14:30:22 [scrapy] INFO: Crawled 676 pages (at 1 pages/min), scraped 566 items (at 2 items/min)
2015-11-04 14:30:22 [scrapy] INFO: Closing spider (finished)
2015-11-04 14:30:22 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 288,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 21,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 42,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 11,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 94,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 4,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 116,
 'downloader/request_bytes': 349057,
 'downloader/request_count': 1070,
 'downloader/request_method_count/GET': 1070,
 'downloader/response_bytes': 16401481,
 'downloader/response_count': 782,
 'downloader/response_status_count/200': 654,
 'downloader/response_status_count/301': 43,
 'downloader/response_status_count/302': 40,
 'downloader/response_status_count/303': 1,
 'downloader/response_status_count/400': 3,
 'downloader/response_status_count/401': 1,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 10,
 'downloader/response_status_count/500': 28,
 'dupefilter/filtered': 2194,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 14, 30, 22, 839995),
 'item_scraped_count': 566,
 'log_count/ERROR': 62,
 'log_count/INFO': 59,
 'offsite/domains': 157,
 'offsite/filtered': 605,
 'request_depth_max': 2,
 'response_received_count': 676,
 'scheduler/dequeued': 1070,
 'scheduler/dequeued/memory': 1070,
 'scheduler/enqueued': 1070,
 'scheduler/enqueued/memory': 1070,
 'spider_exceptions/AttributeError': 5,
 'spider_exceptions/SSLError': 1,
 'spider_exceptions/error': 1,
 'spider_exceptions/timeout': 4,
 'start_time': datetime.datetime(2015, 11, 4, 13, 10, 57, 92341)}
2015-11-04 14:30:22 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 14:31:24 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 14:31:24 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 14:31:24 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 14:31:24 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 14:31:24 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 14:31:24 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 14:31:25 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 14:31:25 [scrapy] INFO: Spider opened
2015-11-04 14:31:25 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.gra>: DNS lookup failed: address 'www.gra' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.say>: DNS lookup failed: address 'www.say' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.par>: DNS lookup failed: address 'www.par' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.atl>: DNS lookup failed: address 'www.atl' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.fsc>: DNS lookup failed: address 'www.fsc' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.woodbinecapital.com>: DNS lookup failed: address 'www.woodbinecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.mountkellett.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.sec>: DNS lookup failed: address 'www.sec' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.aboutyou.bwater.com>: DNS lookup failed: address 'www.aboutyou.bwater.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.riverside-pm.com>: DNS lookup failed: address 'www.riverside-pm.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.mdc>: DNS lookup failed: address 'www.mdc' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:25 [scrapy] ERROR: Error downloading <GET http://www.57s>: DNS lookup failed: address 'www.57s' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:26 [scrapy] ERROR: Error downloading <GET http://www.zad>: DNS lookup failed: address 'www.zad' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:26 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:26 [scrapy] ERROR: Error downloading <GET http://www.securitycreditservcesllc.com>: DNS lookup failed: address 'www.securitycreditservcesllc.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:26 [scrapy] ERROR: Error downloading <GET http://www.dis>: DNS lookup failed: address 'www.dis' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:26 [scrapy] ERROR: Error downloading <GET http://www.kin>: DNS lookup failed: address 'www.kin' not found: [Errno -2] Name or service not known.
2015-11-04 14:31:26 [scrapy] ERROR: Error downloading <GET http://www.citicapitaladvisors.com>: Connection was refused by other side: 111: Connection refused.
2015-11-04 14:31:30 [scrapy] ERROR: Error downloading <GET http://www.glaxisllc.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 14:31:40 [scrapy] ERROR: Error downloading <GET http://www.greenwoodcap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 14:32:25 [scrapy] INFO: Crawled 174 pages (at 174 pages/min), scraped 91 items (at 91 items/min)
2015-11-04 14:33:25 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 91 items (at 0 items/min)
2015-11-04 14:34:25 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 91 items (at 0 items/min)
2015-11-04 14:35:25 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 91 items (at 0 items/min)
2015-11-04 14:36:25 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 91 items (at 0 items/min)
2015-11-04 14:37:25 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 91 items (at 0 items/min)
2015-11-04 14:37:47 [scrapy] ERROR: Error downloading <GET http://www.charteroakpartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:37:47 [scrapy] ERROR: Error downloading <GET http://www.harvestmanagement.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:37:47 [scrapy] ERROR: Error downloading <GET http://www.mainlineinvestmentadvisers.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:37:47 [scrapy] ERROR: Error downloading <GET http://www.sevenlockscapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:37:48 [scrapy] ERROR: Error downloading <GET http://www.cornwallcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:37:48 [scrapy] INFO: Closing spider (finished)
2015-11-04 14:37:48 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 91,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 63,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 15,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 6,
 'downloader/request_bytes': 75545,
 'downloader/request_count': 301,
 'downloader/request_method_count/GET': 301,
 'downloader/response_bytes': 1198661,
 'downloader/response_count': 210,
 'downloader/response_status_count/200': 159,
 'downloader/response_status_count/301': 20,
 'downloader/response_status_count/302': 14,
 'downloader/response_status_count/403': 4,
 'downloader/response_status_count/404': 13,
 'dupefilter/filtered': 192,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 14, 37, 48, 157639),
 'item_scraped_count': 91,
 'log_count/ERROR': 30,
 'log_count/INFO': 13,
 'offsite/domains': 61,
 'offsite/filtered': 448,
 'request_depth_max': 2,
 'response_received_count': 174,
 'scheduler/dequeued': 301,
 'scheduler/dequeued/memory': 301,
 'scheduler/enqueued': 301,
 'scheduler/enqueued/memory': 301,
 'start_time': datetime.datetime(2015, 11, 4, 14, 31, 25, 155378)}
2015-11-04 14:37:48 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 14:38:49 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 14:38:49 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 14:38:49 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 14:38:50 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 14:38:50 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 14:38:50 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 14:38:50 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 14:38:50 [scrapy] INFO: Spider opened
2015-11-04 14:38:50 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.fed>: DNS lookup failed: address 'www.fed' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.jrc>: DNS lookup failed: address 'www.jrc' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.car>: Connection was refused by other side: 111: Connection refused.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.bnp>: DNS lookup failed: address 'www.bnp' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.wellfieldpartners.com>: DNS lookup failed: address 'www.wellfieldpartners.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.beckerdrapkin.com>: DNS lookup failed: address 'www.beckerdrapkin.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.ballance-group.com>: DNS lookup failed: address 'www.ballance-group.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.sta>: DNS lookup failed: address 'www.sta' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.hig>: DNS lookup failed: address 'www.hig' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.aca>: DNS lookup failed: address 'www.aca' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.roc-bridge.com>: DNS lookup failed: address 'www.roc-bridge.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.lineagecapital.com>: DNS lookup failed: address 'www.lineagecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:50 [scrapy] ERROR: Error downloading <GET http://www.har>: DNS lookup failed: address 'www.har' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:51 [scrapy] ERROR: Error downloading <GET http://www.con>: DNS lookup failed: address 'www.con' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:52 [scrapy] ERROR: Error downloading <GET http://www.isp>: DNS lookup failed: address 'www.isp' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:53 [scrapy] ERROR: Error downloading <GET http://www.sandsbros.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 14:38:53 [scrapy] ERROR: Error downloading <GET http://www.horizoncash.com>: DNS lookup failed: address 'www.horizoncash.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:55 [scrapy] ERROR: Error downloading <GET http://www.ostracap.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 14:38:55 [scrapy] ERROR: Error downloading <GET http://www.gra>: DNS lookup failed: address 'www.gra' not found: [Errno -2] Name or service not known.
2015-11-04 14:38:57 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 14:39:01 [scrapy] ERROR: Error downloading <GET http://www.preludecapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 14:39:30 [scrapy] ERROR: Error downloading <GET http://www.emergingmanagersgroup.com>: An error occurred while connecting: 113: No route to host.
2015-11-04 14:39:50 [scrapy] INFO: Crawled 173 pages (at 173 pages/min), scraped 88 items (at 88 items/min)
2015-11-04 14:40:50 [scrapy] INFO: Crawled 173 pages (at 0 pages/min), scraped 88 items (at 0 items/min)
2015-11-04 14:41:50 [scrapy] INFO: Crawled 174 pages (at 1 pages/min), scraped 88 items (at 0 items/min)
2015-11-04 14:42:50 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 88 items (at 0 items/min)
2015-11-04 14:43:50 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 88 items (at 0 items/min)
2015-11-04 14:44:50 [scrapy] INFO: Crawled 174 pages (at 0 pages/min), scraped 88 items (at 0 items/min)
2015-11-04 14:45:12 [scrapy] ERROR: Error downloading <GET http://www.feplp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:45:12 [scrapy] ERROR: Error downloading <GET http://www.kcmc.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:45:12 [scrapy] ERROR: Error downloading <GET http://www.adelphi-europe.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:45:16 [scrapy] ERROR: Error downloading <GET http://www.pacgrp.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 14:45:16 [scrapy] INFO: Closing spider (finished)
2015-11-04 14:45:16 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 90,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 3,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 57,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 13,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 12,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 71622,
 'downloader/request_count': 298,
 'downloader/request_method_count/GET': 298,
 'downloader/response_bytes': 1422751,
 'downloader/response_count': 208,
 'downloader/response_status_count/200': 156,
 'downloader/response_status_count/301': 10,
 'downloader/response_status_count/302': 20,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 2,
 'downloader/response_status_count/404': 13,
 'downloader/response_status_count/503': 4,
 'dupefilter/filtered': 170,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 14, 45, 16, 542143),
 'item_scraped_count': 88,
 'log_count/ERROR': 29,
 'log_count/INFO': 13,
 'offsite/domains': 122,
 'offsite/filtered': 437,
 'request_depth_max': 2,
 'response_received_count': 174,
 'scheduler/dequeued': 298,
 'scheduler/dequeued/memory': 298,
 'scheduler/enqueued': 298,
 'scheduler/enqueued/memory': 298,
 'start_time': datetime.datetime(2015, 11, 4, 14, 38, 50, 262224)}
2015-11-04 14:45:16 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 14:46:18 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 14:46:18 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 14:46:18 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 14:46:18 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 14:46:18 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 14:46:18 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 14:46:18 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 14:46:18 [scrapy] INFO: Spider opened
2015-11-04 14:46:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 14:46:18 [scrapy] ERROR: Error downloading <GET http://www.cor>: DNS lookup failed: address 'www.cor' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:19 [scrapy] ERROR: Error downloading <GET http://www.tigersharklp.com>: DNS lookup failed: address 'www.tigersharklp.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:19 [scrapy] ERROR: Error downloading <GET http://www.fid>: DNS lookup failed: address 'www.fid' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:19 [scrapy] ERROR: Error downloading <GET http://www.inglesideadvisors.com>: DNS lookup failed: address 'www.inglesideadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:19 [scrapy] ERROR: Error downloading <GET http://www.portal.krfs.com>: DNS lookup failed: address 'www.portal.krfs.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:19 [scrapy] ERROR: Error downloading <GET http://www.mezzanine.alcentra.com>: DNS lookup failed: address 'www.mezzanine.alcentra.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:20 [scrapy] ERROR: Error downloading <GET http://www.careers.weissasset.com>: DNS lookup failed: address 'www.careers.weissasset.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:20 [scrapy] ERROR: Error downloading <GET http://www.ecosystemparters.com>: DNS lookup failed: address 'www.ecosystemparters.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:20 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:20 [scrapy] ERROR: Error downloading <GET http://www.arb>: DNS lookup failed: address 'www.arb' not found: [Errno -2] Name or service not known.
2015-11-04 14:46:25 [scrapy] ERROR: Error downloading <GET http://www.elementcapital.com>: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'wrong number of parts', 'HTTP/1.1 302')>]
2015-11-04 14:47:14 [scrapy] ERROR: Error downloading <GET http://www.esemplia.com>: DNS lookup failed: address 'www.esemplia.com' not found: [Errno -2] Name or service not known.
2015-11-04 14:48:09 [scrapy] INFO: Crawled 129 pages (at 129 pages/min), scraped 34 items (at 34 items/min)
2015-11-04 14:48:10 [scrapy] ERROR: Error downloading <GET https://cag.elliottadvisors.hk/my.policy>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 14:51:45 [scrapy] INFO: Crawled 138 pages (at 9 pages/min), scraped 39 items (at 5 items/min)
2015-11-04 14:53:32 [scrapy] INFO: Crawled 138 pages (at 0 pages/min), scraped 41 items (at 2 items/min)
2015-11-04 14:59:46 [scrapy] INFO: Crawled 138 pages (at 0 pages/min), scraped 48 items (at 7 items/min)
2015-11-04 14:59:46 [scrapy] ERROR: Error downloading <GET https://www.invesco.com/portal/site/us/etfs/>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting https://www.invesco.com/portal/site/us/etfs/ took longer than 180.0 seconds..
2015-11-04 15:00:41 [scrapy] INFO: Crawled 147 pages (at 9 pages/min), scraped 49 items (at 1 items/min)
2015-11-04 15:01:35 [scrapy] INFO: Crawled 154 pages (at 7 pages/min), scraped 50 items (at 1 items/min)
2015-11-04 15:07:31 [scrapy] INFO: Crawled 162 pages (at 8 pages/min), scraped 57 items (at 7 items/min)
2015-11-04 15:12:26 [scrapy] ERROR: Spider error processing <GET http://www.coronation.com/print> (referer: http://www.coronation.com/legal-terms-and-conditions)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 44, in parse_items
    il = ItemLoader(item=VcspiderItem(), response=response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/loader/__init__.py", line 29, in __init__
    selector = self.default_selector_class(response)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/unified.py", line 80, in __init__
    _root = LxmlDocument(response, self._parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 27, in __new__
    cache[parser] = _factory(response, parser)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/selector/lxmldocument.py", line 13, in _factory
    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'
AttributeError: 'Response' object has no attribute 'body_as_unicode'
2015-11-04 15:12:26 [scrapy] ERROR: Error downloading <GET http://www.oldmutualus.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 15:12:26 [scrapy] ERROR: Error downloading <GET http://www.icvcapital.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 15:12:26 [scrapy] ERROR: Error downloading <GET http://www.coastasset.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 15:12:26 [scrapy] ERROR: Error downloading <GET http://www.ironsidespartners.com>: TCP connection timed out: 110: Connection timed out.
2015-11-04 15:12:26 [scrapy] INFO: Crawled 162 pages (at 0 pages/min), scraped 64 items (at 7 items/min)
2015-11-04 15:18:34 [scrapy] ERROR: Error downloading <GET http://www.coronation.com/complaints-guidelines>
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/twisted/internet/defer.py", line 588, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/core/downloader/handlers/http11.py", line 226, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
TimeoutError: User timeout caused connection failure: Getting http://www.coronation.com/complaints-guidelines took longer than 180.0 seconds..
2015-11-04 15:18:34 [scrapy] INFO: Crawled 162 pages (at 0 pages/min), scraped 71 items (at 7 items/min)
2015-11-04 15:18:34 [scrapy] INFO: Closing spider (finished)
2015-11-04 15:18:34 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 65,
 'downloader/exception_type_count/twisted.internet.error.ConnectError': 9,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 33,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 12,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'downloader/request_bytes': 82660,
 'downloader/request_count': 285,
 'downloader/request_method_count/GET': 285,
 'downloader/response_bytes': 1362190,
 'downloader/response_count': 220,
 'downloader/response_status_count/200': 156,
 'downloader/response_status_count/301': 25,
 'downloader/response_status_count/302': 29,
 'downloader/response_status_count/401': 3,
 'downloader/response_status_count/403': 1,
 'downloader/response_status_count/404': 6,
 'dupefilter/filtered': 147,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 11, 4, 15, 18, 34, 575845),
 'item_scraped_count': 71,
 'log_count/ERROR': 20,
 'log_count/INFO': 16,
 'offsite/domains': 86,
 'offsite/filtered': 654,
 'request_depth_max': 2,
 'response_received_count': 162,
 'scheduler/dequeued': 285,
 'scheduler/dequeued/memory': 285,
 'scheduler/enqueued': 285,
 'scheduler/enqueued/memory': 285,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 11, 4, 14, 46, 18, 617057)}
2015-11-04 15:18:34 [scrapy] INFO: Spider closed (finished)
./vcscrape.sh: line 10: URLS: command not found
2015-11-04 15:19:36 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 15:19:36 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 15:19:36 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 15:19:36 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 15:19:36 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 15:19:36 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 15:19:36 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 15:19:36 [scrapy] INFO: Spider opened
2015-11-04 15:19:36 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 15:19:36 [scrapy] ERROR: Error downloading <GET http://www.car>: Connection was refused by other side: 111: Connection refused.
2015-11-04 15:19:36 [scrapy] ERROR: Error downloading <GET http://www.gol>: DNS lookup failed: address 'www.gol' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:36 [scrapy] ERROR: Error downloading <GET http://www.pro>: DNS lookup failed: address 'www.pro' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.visicap.com>: DNS lookup failed: address 'www.visicap.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.imc>: DNS lookup failed: address 'www.imc' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.secure.bcentralhost.com>: DNS lookup failed: address 'www.secure.bcentralhost.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.riverside-pm.com>: DNS lookup failed: address 'www.riverside-pm.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.clairvuecapital.com>: DNS lookup failed: address 'www.clairvuecapital.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.cqs>: DNS lookup failed: address 'www.cqs' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.tia>: DNS lookup failed: address 'www.tia' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.tet>: DNS lookup failed: address 'www.tet' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.inglesideadvisors.com>: DNS lookup failed: address 'www.inglesideadvisors.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.fun>: DNS lookup failed: address 'www.fun' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.hig>: DNS lookup failed: address 'www.hig' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.int>: DNS lookup failed: address 'www.int' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.lar>: DNS lookup failed: address 'www.lar' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.lmgaa.com>: DNS lookup failed: address 'www.lmgaa.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.arb>: DNS lookup failed: address 'www.arb' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.jefcap.com>: DNS lookup failed: address 'www.jefcap.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.nia>: DNS lookup failed: address 'www.nia' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.dwi>: DNS lookup failed: address 'www.dwi' not found: [Errno -2] Name or service not known.
2015-11-04 15:19:37 [scrapy] ERROR: Error downloading <GET http://www.pragmapatrimonio.com>: DNS lookup failed: address 'www.pragmapatrimonio.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:20:54 [scrapy] INFO: Crawled 210 pages (at 210 pages/min), scraped 122 items (at 122 items/min)
2015-11-04 15:21:52 [scrapy] INFO: Crawled 288 pages (at 78 pages/min), scraped 193 items (at 71 items/min)
2015-11-04 15:22:43 [scrapy] ERROR: Error downloading <GET https://get.adobe.com/flashplayer/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:43 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/IDIGazeleyBLP>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:43 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/ll-h2YAOgwU>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:43 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/y3qiA6xph58>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:22:44 [scrapy] INFO: Crawled 345 pages (at 57 pages/min), scraped 243 items (at 50 items/min)
2015-11-04 15:23:20 [scrapy] ERROR: Error downloading <GET https://twitter.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:23:35 [scrapy] ERROR: Error downloading <GET https://plus.google.com/108306343581548568740>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:23:35 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/VWkoD4q_zAc>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:23:57 [scrapy] ERROR: Error downloading <GET https://play.google.com/store/apps/details?id=com.godaddy.mobile.android>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:23:57 [scrapy] INFO: Crawled 417 pages (at 72 pages/min), scraped 314 items (at 71 items/min)
2015-11-04 15:23:57 [scrapy] ERROR: Error downloading <GET https://itunes.apple.com/us/app/godaddy.com-mobile-domain/id333201813?mt=8>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:18 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/subscription_center?add_user=godaddy>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:18 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/i69I_8Ad9Dg>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:29 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase+LIVE+Pulse&esheet=51206683&id=smartlink&index=4&lan=en-US&md5=b082c27a4dc5393f87d50dae85bc9e4a&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com%2Fproducts-overview%2Flive-pulse-product-suite%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1622: Tag footer invalid
2015-11-04 15:24:29 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=%40NetBase&esheet=51206683&id=smartlink&index=3&lan=en-US&md5=67c9a53f93d98177a86b9e3d18b9dbd1&newsitemid=20151021006518&url=https%3A%2F%2Ftwitter.com%2FNetBase> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1622: Tag footer invalid
2015-11-04 15:24:30 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=www.netbase.com&esheet=51206683&id=smartlink&index=2&lan=en-US&md5=cadcbc9bd53a2cca5118452730a3db3f&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com%2F%3Fls%3DPress> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 1622: Tag footer invalid
2015-11-04 15:24:37 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/6oN6BctxDlE>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:24:38 [scrapy] INFO: Crawled 467 pages (at 50 pages/min), scraped 361 items (at 47 items/min)
2015-11-04 15:25:05 [scrapy] ERROR: Error downloading <GET https://twitter.com/odinplatform>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:05 [scrapy] ERROR: Error downloading <GET https://www.twitter.com/godaddy>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:05 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/embed/-e9XYDc9vcE>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:17 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=J_Jgtz5u3CU>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:17 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/watch?v=yAHr7EKy0X8>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:47 [scrapy] ERROR: Spider error processing <GET http://cts.businesswire.com/ct/CT?anchor=NetBase&esheet=51206683&id=smartlink&index=1&lan=en-US&md5=5b222d35082163093383787f2190a7f7&newsitemid=20151021006518&url=http%3A%2F%2Fwww.netbase.com> (referer: http://springlakeequitypartners.com/netbase-enhances-audience-marketing-offerings-with-new-data-from-twitter/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 54, in fromstring
    self.doc = lxml.html.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 723, in fromstring
    doc = document_fromstring(html, parser=parser, base_url=base_url, **kw)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/__init__.py", line 613, in document_fromstring
    value = etree.fromstring(html, parser, **kw)
  File "lxml.etree.pyx", line 3092, in lxml.etree.fromstring (src/lxml/lxml.etree.c:70691)
  File "parser.pxi", line 1828, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:106689)
  File "parser.pxi", line 1709, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:105390)
  File "parser.pxi", line 1047, in lxml.etree._BaseParser._parseUnicodeDoc (src/lxml/lxml.etree.c:99763)
  File "parser.pxi", line 580, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:94543)
  File "parser.pxi", line 690, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:96003)
  File "parser.pxi", line 629, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:95212)
XMLSyntaxError: line 760: Tag footer invalid
2015-11-04 15:25:47 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/123TriadWebDesign>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:25:48 [scrapy] INFO: Crawled 538 pages (at 71 pages/min), scraped 426 items (at 65 items/min)
2015-11-04 15:26:18 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/c/IDIGazeley>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:26:38 [scrapy] INFO: Crawled 581 pages (at 43 pages/min), scraped 463 items (at 37 items/min)
2015-11-04 15:26:51 [scrapy] ERROR: Error downloading <GET https://www.americaneagle.com/demo>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:27:44 [scrapy] INFO: Crawled 659 pages (at 78 pages/min), scraped 552 items (at 89 items/min)
2015-11-04 15:28:36 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/user/ssctechnologies>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:28:58 [scrapy] INFO: Crawled 731 pages (at 72 pages/min), scraped 626 items (at 74 items/min)
2015-11-04 15:29:45 [scrapy] INFO: Crawled 769 pages (at 38 pages/min), scraped 665 items (at 39 items/min)
2015-11-04 15:30:08 [scrapy] ERROR: Error downloading <GET https://www.youtube.com/channel/UC6glMEaanKWD86NEjwbtgfg>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:30:43 [scrapy] INFO: Crawled 823 pages (at 54 pages/min), scraped 729 items (at 64 items/min)
2015-11-04 15:31:43 [scrapy] INFO: Crawled 869 pages (at 46 pages/min), scraped 768 items (at 39 items/min)
2015-11-04 15:32:37 [scrapy] INFO: Crawled 930 pages (at 61 pages/min), scraped 824 items (at 56 items/min)
2015-11-04 15:32:42 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY11-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:33:41 [scrapy] INFO: Crawled 996 pages (at 66 pages/min), scraped 889 items (at 65 items/min)
2015-11-04 15:34:43 [scrapy] INFO: Crawled 1050 pages (at 54 pages/min), scraped 941 items (at 52 items/min)
2015-11-04 15:35:43 [scrapy] INFO: Crawled 1102 pages (at 52 pages/min), scraped 997 items (at 56 items/min)
2015-11-04 15:36:20 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Prelim-release-Q1FY12.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:36:22 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/open-letter-shareholders.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:36:24 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q1FY12-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:36:26 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q3FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:36:28 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q2FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:36:41 [scrapy] INFO: Crawled 1157 pages (at 55 pages/min), scraped 1050 items (at 53 items/min)
2015-11-04 15:37:47 [scrapy] INFO: Crawled 1229 pages (at 72 pages/min), scraped 1113 items (at 63 items/min)
2015-11-04 15:38:39 [scrapy] INFO: Crawled 1275 pages (at 46 pages/min), scraped 1159 items (at 46 items/min)
2015-11-04 15:39:40 [scrapy] INFO: Crawled 1306 pages (at 31 pages/min), scraped 1197 items (at 38 items/min)
2015-11-04 15:40:05 [scrapy] ERROR: Spider error processing <GET http://s3.mentor.com/public_documents/news_article/company/news/Q4FY2012-earnings.pdf> (referer: https://www.mentor.com/company/news/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/vcs.py", line 40, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:40:50 [scrapy] INFO: Crawled 1381 pages (at 75 pages/min), scraped 1254 items (at 57 items/min)
2015-11-04 15:41:48 [scrapy] INFO: Crawled 1450 pages (at 69 pages/min), scraped 1312 items (at 58 items/min)
2015-11-04 15:42:13 [scrapy] INFO: Received SIGTERM, shutting down gracefully. Send again to force 
2015-11-04 15:42:17 [scrapy] INFO: Closing spider (shutdown)
2015-11-04 15:42:39 [scrapy] INFO: Crawled 1501 pages (at 51 pages/min), scraped 1381 items (at 69 items/min)
2015-11-04 15:42:41 [scrapy] INFO: Received SIGTERM twice, forcing unclean shutdown
