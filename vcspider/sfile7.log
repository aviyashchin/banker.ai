usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
nohup: ignoring input
2015-11-04 15:54:33 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 15:54:33 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 15:54:33 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 15:54:33 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 15:54:33 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 15:54:33 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 15:54:33 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 15:54:33 [scrapy] INFO: Spider opened
2015-11-04 15:54:33 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 15:54:33 [scrapy] ERROR: Error downloading <GET http://www.mimoon.de>: DNS lookup failed: address 'www.mimoon.de' not found: [Errno -2] Name or service not known.
2015-11-04 15:54:33 [scrapy] ERROR: Error downloading <GET http://www.coreoncology.com>: DNS lookup failed: address 'www.coreoncology.com' not found: [Errno -2] Name or service not known.
2015-11-04 15:55:44 [scrapy] INFO: Crawled 174 pages (at 174 pages/min), scraped 101 items (at 101 items/min)
2015-11-04 15:56:58 [scrapy] ERROR: Spider error processing <GET http://j2inn.com/finweekly/uploads/article_files/63/Floorplan.finp> (referer: http://blog.j2inn.com/tag/fin-weekly/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 123, in crawl
    doc = self.get_document(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 259, in get_document
    doc = self.parser.fromstring(raw_html)
  File "/home/ubuntu/programming/python-goose/goose/goose/parsers.py", line 243, in fromstring
    self.doc = soupparser.fromstring(html)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 23, in fromstring
    return _parse(data, beautifulsoup, makeelement, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/lxml/html/soupparser.py", line 66, in _parse
    tree = beautifulsoup(source, **bsargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1522, in __init__
    BeautifulStoneSoup.__init__(self, *args, **kwargs)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1147, in __init__
    self._feed(isHTML=isHTML)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/BeautifulSoup.py", line 1189, in _feed
    SGMLParser.feed(self, markup)
  File "/home/ubuntu/anaconda/lib/python2.7/sgmllib.py", line 103, in feed
    self.rawdata = self.rawdata + data
TypeError: cannot concatenate 'str' and 'NoneType' objects
2015-11-04 15:57:13 [scrapy] INFO: Crawled 237 pages (at 63 pages/min), scraped 177 items (at 76 items/min)
2015-11-04 15:57:35 [scrapy] INFO: Crawled 262 pages (at 25 pages/min), scraped 203 items (at 26 items/min)
2015-11-04 15:58:34 [scrapy] INFO: Crawled 338 pages (at 76 pages/min), scraped 251 items (at 48 items/min)
2015-11-04 15:59:56 [scrapy] INFO: Crawled 388 pages (at 50 pages/min), scraped 332 items (at 81 items/min)
2015-11-04 16:00:50 [scrapy] INFO: Crawled 433 pages (at 45 pages/min), scraped 370 items (at 38 items/min)
2015-11-04 16:01:41 [scrapy] INFO: Crawled 489 pages (at 56 pages/min), scraped 412 items (at 42 items/min)
2015-11-04 16:02:44 [scrapy] INFO: Crawled 562 pages (at 73 pages/min), scraped 480 items (at 68 items/min)
