usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
nohup: ignoring input
2015-11-04 15:54:32 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 15:54:32 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 15:54:32 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 15:54:32 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 15:54:32 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 15:54:32 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 15:54:33 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 15:54:33 [scrapy] INFO: Spider opened
2015-11-04 15:54:33 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 15:55:43 [scrapy] INFO: Crawled 158 pages (at 158 pages/min), scraped 69 items (at 69 items/min)
2015-11-04 15:57:16 [scrapy] INFO: Crawled 205 pages (at 47 pages/min), scraped 106 items (at 37 items/min)
2015-11-04 15:57:50 [scrapy] INFO: Crawled 206 pages (at 1 pages/min), scraped 127 items (at 21 items/min)
2015-11-04 15:58:42 [scrapy] INFO: Crawled 239 pages (at 33 pages/min), scraped 139 items (at 12 items/min)
2015-11-04 15:59:44 [scrapy] INFO: Crawled 262 pages (at 23 pages/min), scraped 165 items (at 26 items/min)
2015-11-04 16:02:22 [scrapy] INFO: Crawled 272 pages (at 10 pages/min), scraped 188 items (at 23 items/min)
2015-11-04 16:02:29 [scrapy] WARNING: Expected response size (122737399) larger than download warn size (33554432).
2015-11-04 16:02:29 [scrapy] WARNING: Expected response size (60917018) larger than download warn size (33554432).
2015-11-04 16:03:25 [scrapy] ERROR: Spider error processing <GET http://www.itiangua.com/html/about/> (referer: http://fsyx.itiangua.com/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 117, in crawl
    raw_html = self.get_html(crawl_candidate, parse_candidate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 215, in get_html
    html = self.htmlfetcher.get_html(parsing_candidate.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/network.py", line 59, in get_html
    return self.result.read()
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 355, in read
    data = self._sock.recv(rbufsize)
  File "/home/ubuntu/anaconda/lib/python2.7/httplib.py", line 612, in read
    s = self.fp.read(amt)
  File "/home/ubuntu/anaconda/lib/python2.7/socket.py", line 384, in read
    data = self._sock.recv(left)
timeout: timed out
2015-11-04 16:03:32 [scrapy] INFO: Crawled 293 pages (at 21 pages/min), scraped 198 items (at 10 items/min)
