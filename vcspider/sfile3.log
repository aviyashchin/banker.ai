usage = ./vcscrape.sh vcs &> file1.log & (VC scraper) -or- ./vcscrape.sh sus &> file1.log & (startup capital scraper)
nohup: ignoring input
2015-11-04 15:54:32 [scrapy] INFO: Scrapy 1.0.3 started (bot: vcspider)
2015-11-04 15:54:32 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-11-04 15:54:32 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'vcspider.spiders', 'LOG_LEVEL': 'INFO', 'CONCURRENT_REQUESTS': 100, 'SPIDER_MODULES': ['vcspider.spiders'], 'BOT_NAME': 'vcspider', 'DEPTH_LIMIT': 2}
2015-11-04 15:54:32 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState
2015-11-04 15:54:32 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-11-04 15:54:32 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-11-04 15:54:32 [scrapy] INFO: Enabled item pipelines: MySqlPipeline
2015-11-04 15:54:32 [scrapy] INFO: Spider opened
2015-11-04 15:54:32 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-11-04 15:54:50 [scrapy] ERROR: Error downloading <GET http://www.app.tutorconnect.me>: DNS lookup failed: address 'www.app.tutorconnect.me' not found: [Errno -2] Name or service not known.
2015-11-04 15:55:51 [scrapy] INFO: Crawled 156 pages (at 156 pages/min), scraped 94 items (at 94 items/min)
2015-11-04 15:56:45 [scrapy] INFO: Crawled 212 pages (at 56 pages/min), scraped 139 items (at 45 items/min)
2015-11-04 15:57:19 [scrapy] ERROR: Spider error processing <GET http://blog.tape.tv/> (referer: https://www.tape.tv/)
Traceback (most recent call last):
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 28, in process_spider_output
    for x in result:
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 54, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/ubuntu/anaconda/lib/python2.7/site-packages/scrapy/spiders/crawl.py", line 69, in _parse_response
    for requests_or_item in iterate_spider_output(cb_res):
  File "/home/ubuntu/programming/banker.ai/vcspider/vcspider/spiders/sus.py", line 41, in parse_items
    gooseobj = self.g.extract(response.url)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 56, in extract
    return self.crawl(cc)
  File "/home/ubuntu/programming/python-goose/goose/goose/__init__.py", line 66, in crawl
    article = crawler.crawl(crawl_candiate)
  File "/home/ubuntu/programming/python-goose/goose/goose/crawler.py", line 154, in crawl
    self.article.title = self.title_extractor.extract()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 104, in extract
    return self.get_title()
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 83, in get_title
    return self.clean_title(title)
  File "/home/ubuntu/programming/python-goose/goose/goose/extractors/title.py", line 66, in clean_title
    if title_words[-1] in TITLE_SPLITTERS:
IndexError: list index out of range
2015-11-04 15:57:40 [scrapy] INFO: Crawled 246 pages (at 34 pages/min), scraped 174 items (at 35 items/min)
2015-11-04 15:58:02 [scrapy] ERROR: Error downloading <GET https://thegrid.io/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:58:02 [scrapy] ERROR: Error downloading <GET https://www.gynzy.com/en/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl3_read_bytes', 'ssl handshake failure')]>]
2015-11-04 15:58:50 [scrapy] INFO: Crawled 295 pages (at 49 pages/min), scraped 237 items (at 63 items/min)
2015-11-04 15:59:38 [scrapy] INFO: Crawled 324 pages (at 29 pages/min), scraped 275 items (at 38 items/min)
2015-11-04 16:00:40 [scrapy] INFO: Crawled 363 pages (at 39 pages/min), scraped 313 items (at 38 items/min)
2015-11-04 16:01:39 [scrapy] INFO: Crawled 411 pages (at 48 pages/min), scraped 358 items (at 45 items/min)
2015-11-04 16:02:35 [scrapy] INFO: Crawled 467 pages (at 56 pages/min), scraped 393 items (at 35 items/min)
2015-11-04 16:03:38 [scrapy] INFO: Crawled 493 pages (at 26 pages/min), scraped 432 items (at 39 items/min)
